{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WSO2 Enterprise Integrator Documentation \u00b6 This is the home page of WSO2 enterprise integrator documentation.","title":"Home"},{"location":"#welcome-to-wso2-enterprise-integrator-documentation","text":"This is the home page of WSO2 enterprise integrator documentation.","title":"Welcome to WSO2 Enterprise Integrator Documentation"},{"location":"administer-and-observe/capturing_system_data_error_situations/","text":"Capturing System Data in Error Situations \u00b6 Carbon Dump is a tool for collecting all the necessary data(i.e., heap and thread dumps) from a running Carbon instance at the time of an error for a head dump and thread stack analysis. The Carbon Dump generates a ZIP archive with the collected data, which helps the WSO2 support team to analyze your system and determine the problem which caused the error. Therefore, it is recommended that you run this tool as soon as an error occurs in the Carbon instance. As with any other java product, if your WSO2 product cluster fails due to a resource exhaustion, the heap and thread dumps will always point you towards the cause of the leak. Therefore, it is important to be able to retrieve heap and thread dumps from an environment at the point when an error occurs. This will avoid the necessity of reproducing the exact issue again (specially, in the case of production issues). A resource exhaustion can happen for two reasons: Due to a bug in the system. An actual limitation of resources based on low configuration values. You can easily create a heap dump and thread dump using the CarbonDump tool that is shipped with your product. These will also provide information about the product version and any patch inconsistencies. Note If you are using an Ubuntu version 10.10 or above and if you get an error on being unable to attach the process, execute the following command to rectify it: $ echo 0 | sudo tee /proc/sys/kernel/ yama /ptrace_scope This changes the yama /ptrace_scope variable of the kernel temporarily (i.e., until the next reboot). For more information, go to Oracle documentation . When using the tool, you have to provide the process ID (pid) of the Carbon instance and the \\<PRODUCT_HOME> which is where your unzipped Carbon distribution files reside. The command takes the following format: sh carbondump.sh [-carbonHome path] [-pid of the carbon instance] For example, In Linux: sh carbondump.sh -carbonHome /home/user/wso2carbon-3.0.0/ -pid 5151 In Windows: carbondump.bat -carbonHome c:\\wso2carbon-3.0.0\\ -pid 5151 The tool captures the following information about the system: Operating system information ** OS (kernel) version Installed modules lists and their information List of running tasks in the system Memory information of the Java process ** Java heap memory dump Histogram of the heap Objects waiting for finalization Java heap summary. GC algo used, etc. Statistics on permgen space of Java heap Information about the running Carbon instance ** Product name and version Carbon framework version (This includes the patched version) \\<PRODUCT_HOME>, \\<JAVA_HOME> configuration files log files H2 database files Thread dump Checksum values of all the files found in the $CARBON_HOME","title":"Capturing System Data in Error Situations"},{"location":"administer-and-observe/capturing_system_data_error_situations/#capturing-system-data-in-error-situations","text":"Carbon Dump is a tool for collecting all the necessary data(i.e., heap and thread dumps) from a running Carbon instance at the time of an error for a head dump and thread stack analysis. The Carbon Dump generates a ZIP archive with the collected data, which helps the WSO2 support team to analyze your system and determine the problem which caused the error. Therefore, it is recommended that you run this tool as soon as an error occurs in the Carbon instance. As with any other java product, if your WSO2 product cluster fails due to a resource exhaustion, the heap and thread dumps will always point you towards the cause of the leak. Therefore, it is important to be able to retrieve heap and thread dumps from an environment at the point when an error occurs. This will avoid the necessity of reproducing the exact issue again (specially, in the case of production issues). A resource exhaustion can happen for two reasons: Due to a bug in the system. An actual limitation of resources based on low configuration values. You can easily create a heap dump and thread dump using the CarbonDump tool that is shipped with your product. These will also provide information about the product version and any patch inconsistencies. Note If you are using an Ubuntu version 10.10 or above and if you get an error on being unable to attach the process, execute the following command to rectify it: $ echo 0 | sudo tee /proc/sys/kernel/ yama /ptrace_scope This changes the yama /ptrace_scope variable of the kernel temporarily (i.e., until the next reboot). For more information, go to Oracle documentation . When using the tool, you have to provide the process ID (pid) of the Carbon instance and the \\<PRODUCT_HOME> which is where your unzipped Carbon distribution files reside. The command takes the following format: sh carbondump.sh [-carbonHome path] [-pid of the carbon instance] For example, In Linux: sh carbondump.sh -carbonHome /home/user/wso2carbon-3.0.0/ -pid 5151 In Windows: carbondump.bat -carbonHome c:\\wso2carbon-3.0.0\\ -pid 5151 The tool captures the following information about the system: Operating system information ** OS (kernel) version Installed modules lists and their information List of running tasks in the system Memory information of the Java process ** Java heap memory dump Histogram of the heap Objects waiting for finalization Java heap summary. GC algo used, etc. Statistics on permgen space of Java heap Information about the running Carbon instance ** Product name and version Carbon framework version (This includes the patched version) \\<PRODUCT_HOME>, \\<JAVA_HOME> configuration files log files H2 database files Thread dump Checksum values of all the files found in the $CARBON_HOME","title":"Capturing System Data in Error Situations"},{"location":"administer-and-observe/configuring_log4j_properties/","text":"Configuring Log4j Properties \u00b6 All WSO2 products are shipped with the log4j logging capabilities , which generates administrative activities and server side logs. The log4j.properties file, which governs how logging is performed by the server can be found in the <PRODUCT_HOME>/repository/conf directory. If the Logging Management feature is installed , log4j properties can be configured using the management console. There are three main components when configuring log4j. They are Loggers, Appenders, and Layouts. Using the management console allows you to change these parameters globally as well as individually at run time. First, the server stores new values in the database and then changes the appropriate components in the logging framework, enabling the logging properties to be updated immediately. Note In most systems, logging properties should be specified before the server starts and cannot be changed while it is running. However, as shown here, the logging properties of a running Carbon instance can be changed through its management console, while the server is up and running. To configure logging properties of the system and application logs of a Carbon server at run time: Log in to the management console of your product and go to Configure -> Logging in the navigator. The Logging Configuration window appears as shown below. If you select the Persist All Configuration Changes check box, all the modifications will persist and they will be available even after the server restarts. The Logging Configuration window consists of three sections, which you can use to configure the layout and the amount of information you want to receive about system activity. The following topics describes each of these settings: Global Log4J Configuration Configure Log4J Appenders Configure Log4J Loggers Global Log4J Configuration \u00b6 This section allows you to assign a single log level and log pattern to all loggers. Log Level - Severity of the message. Reflects a minimum level that the logger requires. See descriptions of the available log levels . Log Pattern - Defines the output format of the log file. This is the layout pattern which describes the log message format If you click Restore Defaults , the Registry will be overwritten by logging configurations specified in the log4j.properties file . Configure Log4J Appenders \u00b6 This section allows you to configure appenders individually. Log4j allows logging requests to print to multiple destinations. These output destinations are called 'Appenders'. You can attach several appenders to one logger. Name -The name of an appender. By default, a WSO2 product server is entered in this field with the following log appenders configured; CARBON_CONSOLE - Logs to the console when the server is running. CARBON_LOGFILE - Writes the logs to <PRODUCT_HOME>/repository/logs/wso2carbon.log . !!! info Some WSO2 products do not ship t he following appenders by default. - **SERVICE** \\_ **APPENDER** - Writes service invocations to \\< ` PRODUCT_HOME>/repository/logs/wso2-<PRODUCT_NAME>-service.log. ` - **ERROR\\_LOGFILE -** Writes warning/error messages to \\< ` ESB_HOME>/repository/logs/wso2-<PRODUCT_NAME>-service.log ` TRACE _ APPENDER - Writes tracing/debug messages to the \\< PRODUCT_HOME>/repository/logs/wso2-<PRODUCT_NAME>-trace.log for tracing enabled services. CARBON _ MEMORY CARBON_SYS_LOG - Allows separating the software that generates messages, from the system that stores them and the software that reports and analyzes them. CARBON_TRACE_LOGFILE Log pattern - Defines the output format of the log file. From Carbon 4.4.3 onwards, t he conversion character 'K' can be used in the pattern layout to log a UUID . For example, the log pattern can be [%K] [%T] [%S] [%d] %P%5p {%c} - %x %m {%c}%n, where [%K] is the UUID. !!! info **Note** that the following capability was introduced by the Carbon 4.4.3 release. Therefore, it is only applicable to products that a based on Carbon 4.4.3 or later versions. From Carbon 4.4.3 onwards, the UUID can be used for identifying forged messages in the log. By default, the UUID will be generated every time the server starts . If required, you can configure the UUID regeneration period by manually adding the following property to the ` log4j.properties ` file (stored in the ` <PRODUCT_HOME>/repository/conf ` directory) : log4j.appender.CARBON_LOGFILE.layout.LogUUIDUpdateInterval=<number_of_hours> Sys Log Host - The IP address of the system log server. The syslog server is a dedicated log server for many applications. It runs in a particular TCP port in a separate machine, which can be identified by an IP address. Facility - The log message type sent to the system log server. Threshold - Filters log entries based on their level. For example, threshold set to 'WARN' will allow the log entry to pass into appender. If its level is 'WARN', 'ERROR' or 'FATAL', other entries will be discarded. This is the minimum log level at which you can log a message. See descriptions of the available log levels . Configure Log4J Loggers \u00b6 A Logger is an object used to log messages for a specific system or application component. Loggers are normally named using a hierarchical dot-separated namespace and have a 'child-parent' relationship. For example, the logger named 'root.sv' is a parent of the logger named 'root.sv.sf' and a child of 'root'. When the server starts for the first time, all the loggers initially listed in the log4j.properties file appear on the logger name list. This section allows you to browse through all these loggers, define a log level and switch on/off additivity to any of them. After editing, the logging properties are read only from the database. Logger - The name of a logger. Parent Logger - The name of a parent logger. Level - Allows to select level (threshold) from the drop-down menu. After you specify the level for a certain logger, a log request for that logger will only be enabled if its level is equal or higher to the logger\u2019s level. If a given logger is not assigned a level, then it inherits one from its closest ancestor with an assigned level. Refer to the hierarchy of levels given above. See descriptions of the available log levels . Additivity - Allows to inherit all the appenders of the parent Logger if set as 'True'. Info In this section, loggers can be filtered by the first characters (use the Starts With button) of by a combination of characters (use the Contains button).","title":"Configuring Log4j Properties"},{"location":"administer-and-observe/configuring_log4j_properties/#configuring-log4j-properties","text":"All WSO2 products are shipped with the log4j logging capabilities , which generates administrative activities and server side logs. The log4j.properties file, which governs how logging is performed by the server can be found in the <PRODUCT_HOME>/repository/conf directory. If the Logging Management feature is installed , log4j properties can be configured using the management console. There are three main components when configuring log4j. They are Loggers, Appenders, and Layouts. Using the management console allows you to change these parameters globally as well as individually at run time. First, the server stores new values in the database and then changes the appropriate components in the logging framework, enabling the logging properties to be updated immediately. Note In most systems, logging properties should be specified before the server starts and cannot be changed while it is running. However, as shown here, the logging properties of a running Carbon instance can be changed through its management console, while the server is up and running. To configure logging properties of the system and application logs of a Carbon server at run time: Log in to the management console of your product and go to Configure -> Logging in the navigator. The Logging Configuration window appears as shown below. If you select the Persist All Configuration Changes check box, all the modifications will persist and they will be available even after the server restarts. The Logging Configuration window consists of three sections, which you can use to configure the layout and the amount of information you want to receive about system activity. The following topics describes each of these settings: Global Log4J Configuration Configure Log4J Appenders Configure Log4J Loggers","title":"Configuring Log4j Properties"},{"location":"administer-and-observe/configuring_log4j_properties/#global-log4j-configuration","text":"This section allows you to assign a single log level and log pattern to all loggers. Log Level - Severity of the message. Reflects a minimum level that the logger requires. See descriptions of the available log levels . Log Pattern - Defines the output format of the log file. This is the layout pattern which describes the log message format If you click Restore Defaults , the Registry will be overwritten by logging configurations specified in the log4j.properties file .","title":"Global Log4J Configuration"},{"location":"administer-and-observe/configuring_log4j_properties/#configure-log4j-appenders","text":"This section allows you to configure appenders individually. Log4j allows logging requests to print to multiple destinations. These output destinations are called 'Appenders'. You can attach several appenders to one logger. Name -The name of an appender. By default, a WSO2 product server is entered in this field with the following log appenders configured; CARBON_CONSOLE - Logs to the console when the server is running. CARBON_LOGFILE - Writes the logs to <PRODUCT_HOME>/repository/logs/wso2carbon.log . !!! info Some WSO2 products do not ship t he following appenders by default. - **SERVICE** \\_ **APPENDER** - Writes service invocations to \\< ` PRODUCT_HOME>/repository/logs/wso2-<PRODUCT_NAME>-service.log. ` - **ERROR\\_LOGFILE -** Writes warning/error messages to \\< ` ESB_HOME>/repository/logs/wso2-<PRODUCT_NAME>-service.log ` TRACE _ APPENDER - Writes tracing/debug messages to the \\< PRODUCT_HOME>/repository/logs/wso2-<PRODUCT_NAME>-trace.log for tracing enabled services. CARBON _ MEMORY CARBON_SYS_LOG - Allows separating the software that generates messages, from the system that stores them and the software that reports and analyzes them. CARBON_TRACE_LOGFILE Log pattern - Defines the output format of the log file. From Carbon 4.4.3 onwards, t he conversion character 'K' can be used in the pattern layout to log a UUID . For example, the log pattern can be [%K] [%T] [%S] [%d] %P%5p {%c} - %x %m {%c}%n, where [%K] is the UUID. !!! info **Note** that the following capability was introduced by the Carbon 4.4.3 release. Therefore, it is only applicable to products that a based on Carbon 4.4.3 or later versions. From Carbon 4.4.3 onwards, the UUID can be used for identifying forged messages in the log. By default, the UUID will be generated every time the server starts . If required, you can configure the UUID regeneration period by manually adding the following property to the ` log4j.properties ` file (stored in the ` <PRODUCT_HOME>/repository/conf ` directory) : log4j.appender.CARBON_LOGFILE.layout.LogUUIDUpdateInterval=<number_of_hours> Sys Log Host - The IP address of the system log server. The syslog server is a dedicated log server for many applications. It runs in a particular TCP port in a separate machine, which can be identified by an IP address. Facility - The log message type sent to the system log server. Threshold - Filters log entries based on their level. For example, threshold set to 'WARN' will allow the log entry to pass into appender. If its level is 'WARN', 'ERROR' or 'FATAL', other entries will be discarded. This is the minimum log level at which you can log a message. See descriptions of the available log levels .","title":"Configure Log4J Appenders"},{"location":"administer-and-observe/configuring_log4j_properties/#configure-log4j-loggers","text":"A Logger is an object used to log messages for a specific system or application component. Loggers are normally named using a hierarchical dot-separated namespace and have a 'child-parent' relationship. For example, the logger named 'root.sv' is a parent of the logger named 'root.sv.sf' and a child of 'root'. When the server starts for the first time, all the loggers initially listed in the log4j.properties file appear on the logger name list. This section allows you to browse through all these loggers, define a log level and switch on/off additivity to any of them. After editing, the logging properties are read only from the database. Logger - The name of a logger. Parent Logger - The name of a parent logger. Level - Allows to select level (threshold) from the drop-down menu. After you specify the level for a certain logger, a log request for that logger will only be enabled if its level is equal or higher to the logger\u2019s level. If a given logger is not assigned a level, then it inherits one from its closest ancestor with an assigned level. Refer to the hierarchy of levels given above. See descriptions of the available log levels . Additivity - Allows to inherit all the appenders of the parent Logger if set as 'True'. Info In this section, loggers can be filtered by the first characters (use the Starts With button) of by a combination of characters (use the Contains button).","title":"Configure Log4J Loggers"},{"location":"administer-and-observe/configuring_metrics_properties/","text":"Configuring Metrics Properties \u00b6 When the monitoring capability with metrics is enabled in your product, the metrics dashboard will be available when you log into the management console. The metrics dashboard will provide a page for viewing the statistics that use JVM metrics. Further, depending on the WSO2 product that you are using, there will be a separate page for viewing product-specific statistics using metrics. The <PRODUCT_HOME>/repository/conf/metrics.properties file specifies the properties that correspond to the gauges for JVM metrics in the metrics dashboard. The level defined for a property in this file determines the extent to which the relevant gauge in the dashboard should be updated with information. The different levels that can be defined for properties are as follows: Level Description Off Designates no informational events. Info Designates informational metric events that highlight the progress of the application at coarse-grained level. Debug Designates fine-grained informational events that are most useful to debug an application. Trace Designates finer-grained informational events than the DEBUG. All Designates all the informational events. If no specific level is configured for a property in the metrics.properties file, the metrics root level will apply. The root level is defined as shown in the following example in the metrics.properties file. metrics.rootLevel=OFF If you want to change the current root level, you can also use the following command. -Dmetrics.rootLevel=INFO The levels in the metrics.properties file can be configured to any hierarchy. However, if the level defined for an individual property is different to the level defined for its parent in the hierarchy, the level defined for the individual property will overrule that of the parent. For example, if we have metric.level.jvm.memory=INFO in the <PRODUCT_HOME>/repository/conf/metrics.properties file, all metrics under jvm.memory will have INFO as the configured level. However, if you have metric.level.jvm.memory.heap=TRACE , the TRACE level would apply for the metric.level.jvm.memory.heap property even though it is a child property of jvm.memory. The properties that are included in this file by default are as follows: JVM's direct and mapped buffer pools Class loading GC Memory Operating system load Threads JVM's direct and mapped buffer pools \u00b6 Property Default Level Description metric.level.jvm.buffers OFF The gauge showing the current number of distinct buffers. Class loading \u00b6 Property Default Level Description metric.level.jvm.class-loading INFO The gauge showing the number of classes currently loaded for the JVM. GC \u00b6 Property Default Level Description metric.level.jvm.gc DEBUG The gauge for showing garbage collection and memory usage. Monitoring this allows you to identify memory leaks that will have a negative impact on performance. Memory \u00b6 Property Default Level Description metric.level.jvm.memory INFO The gauge for showing the used and committed memory in WSO2 MB. metric.level.jvm.memory.heap INFO The gauge for showing the used and committed heap in WSO2 MB. metric.level.jvm.memory.non-heap INFO The gauge for showing the used code cache and used CMS Perm Gen in WSO2 MB. metric.level.jvm.memory.total INFO The gauge for showing the total memory currently available for the JVM. metric.level.jvm.memory.pools OFF The gauge for showing the used and available memory for JVM in the memory pool. Operating system load \u00b6 Property Default Level Description metric.level.jvm.os INFO The gauge for showing the current load imposed by the JVM on the operating system. Threads \u00b6 Property Default Level Description metric.level.jvm.threads OFF The parent property of all the gauges relating to the JVM thread pool. The metric level defined for this property will apply to all the remaining properties in this table. The metric level set via this property to a child property can be overruled if a different level is set for it. metric.level.jvm.threads.count DEBUG The gauge for showing the number of active and idle threads currently available in the JVM thread pool. metric.level.jvm.threads.daemon.count DEBUG The gauge for showing the number of active daemon threads currently available in the JVM thread pool. metric.level.jvm.threads.blocked.count OFF The gauge for showing the number of threads that are currently blocked in the JVM thread pool. metric.level.jvm.threads.deadlock.count OFF The gauge for showing the number of threads that are currently in deadlock in the JVM thread pool. metric.level.jvm.threads.new.count OFF The gauge for showing the number of new threads generated in the JVM thread pool. metric.level.jvm.threads.runnable.count OFF The gauge for showing the number of runnable threads currently available in the JVM thread pool. metric.level.jvm.threads.terminated.count OFF The gauge for showing the number of threads terminated from the JVM thread pool since you started running the WSO2 MB instance. metric.level.jvm.threads.timed_waiting.count OFF The gauge for showing the number of threads with Timed_Waiting status. metric.level.jvm.threads.waiting.count OFF The gauge for showing the number of threads with Waiting status in the JVM thread pool. One or more other threads are required to perform certain actions before these threads can proceed with their actions.","title":"Configuring Metrics Properties"},{"location":"administer-and-observe/configuring_metrics_properties/#configuring-metrics-properties","text":"When the monitoring capability with metrics is enabled in your product, the metrics dashboard will be available when you log into the management console. The metrics dashboard will provide a page for viewing the statistics that use JVM metrics. Further, depending on the WSO2 product that you are using, there will be a separate page for viewing product-specific statistics using metrics. The <PRODUCT_HOME>/repository/conf/metrics.properties file specifies the properties that correspond to the gauges for JVM metrics in the metrics dashboard. The level defined for a property in this file determines the extent to which the relevant gauge in the dashboard should be updated with information. The different levels that can be defined for properties are as follows: Level Description Off Designates no informational events. Info Designates informational metric events that highlight the progress of the application at coarse-grained level. Debug Designates fine-grained informational events that are most useful to debug an application. Trace Designates finer-grained informational events than the DEBUG. All Designates all the informational events. If no specific level is configured for a property in the metrics.properties file, the metrics root level will apply. The root level is defined as shown in the following example in the metrics.properties file. metrics.rootLevel=OFF If you want to change the current root level, you can also use the following command. -Dmetrics.rootLevel=INFO The levels in the metrics.properties file can be configured to any hierarchy. However, if the level defined for an individual property is different to the level defined for its parent in the hierarchy, the level defined for the individual property will overrule that of the parent. For example, if we have metric.level.jvm.memory=INFO in the <PRODUCT_HOME>/repository/conf/metrics.properties file, all metrics under jvm.memory will have INFO as the configured level. However, if you have metric.level.jvm.memory.heap=TRACE , the TRACE level would apply for the metric.level.jvm.memory.heap property even though it is a child property of jvm.memory. The properties that are included in this file by default are as follows: JVM's direct and mapped buffer pools Class loading GC Memory Operating system load Threads","title":"Configuring Metrics Properties"},{"location":"administer-and-observe/configuring_metrics_properties/#jvms-direct-and-mapped-buffer-pools","text":"Property Default Level Description metric.level.jvm.buffers OFF The gauge showing the current number of distinct buffers.","title":"JVM's direct and mapped buffer pools"},{"location":"administer-and-observe/configuring_metrics_properties/#class-loading","text":"Property Default Level Description metric.level.jvm.class-loading INFO The gauge showing the number of classes currently loaded for the JVM.","title":"Class loading"},{"location":"administer-and-observe/configuring_metrics_properties/#gc","text":"Property Default Level Description metric.level.jvm.gc DEBUG The gauge for showing garbage collection and memory usage. Monitoring this allows you to identify memory leaks that will have a negative impact on performance.","title":"GC"},{"location":"administer-and-observe/configuring_metrics_properties/#memory","text":"Property Default Level Description metric.level.jvm.memory INFO The gauge for showing the used and committed memory in WSO2 MB. metric.level.jvm.memory.heap INFO The gauge for showing the used and committed heap in WSO2 MB. metric.level.jvm.memory.non-heap INFO The gauge for showing the used code cache and used CMS Perm Gen in WSO2 MB. metric.level.jvm.memory.total INFO The gauge for showing the total memory currently available for the JVM. metric.level.jvm.memory.pools OFF The gauge for showing the used and available memory for JVM in the memory pool.","title":"Memory"},{"location":"administer-and-observe/configuring_metrics_properties/#operating-system-load","text":"Property Default Level Description metric.level.jvm.os INFO The gauge for showing the current load imposed by the JVM on the operating system.","title":"Operating system load"},{"location":"administer-and-observe/configuring_metrics_properties/#threads","text":"Property Default Level Description metric.level.jvm.threads OFF The parent property of all the gauges relating to the JVM thread pool. The metric level defined for this property will apply to all the remaining properties in this table. The metric level set via this property to a child property can be overruled if a different level is set for it. metric.level.jvm.threads.count DEBUG The gauge for showing the number of active and idle threads currently available in the JVM thread pool. metric.level.jvm.threads.daemon.count DEBUG The gauge for showing the number of active daemon threads currently available in the JVM thread pool. metric.level.jvm.threads.blocked.count OFF The gauge for showing the number of threads that are currently blocked in the JVM thread pool. metric.level.jvm.threads.deadlock.count OFF The gauge for showing the number of threads that are currently in deadlock in the JVM thread pool. metric.level.jvm.threads.new.count OFF The gauge for showing the number of new threads generated in the JVM thread pool. metric.level.jvm.threads.runnable.count OFF The gauge for showing the number of runnable threads currently available in the JVM thread pool. metric.level.jvm.threads.terminated.count OFF The gauge for showing the number of threads terminated from the JVM thread pool since you started running the WSO2 MB instance. metric.level.jvm.threads.timed_waiting.count OFF The gauge for showing the number of threads with Timed_Waiting status. metric.level.jvm.threads.waiting.count OFF The gauge for showing the number of threads with Waiting status in the JVM thread pool. One or more other threads are required to perform certain actions before these threads can proceed with their actions.","title":"Threads"},{"location":"administer-and-observe/configuring_the_log_provider/","text":"Configuring the Log Provider \u00b6 Logs of a system can be stored in many ways. For example, they can be stored in a file system, an sql server such as MySQL, a no-sql server like Cassandra, etc. According to the default configurations in a Carbon product, the logs are stored in the <PRODUCT_HOME>/repository/logs/ directory as .log files. To view and download the logs using the management console, the following configurations are required: the Logging Management feature should be installed, the log4j properties should be configured and the LogProvider and LogFileProvider interfaces should be implemented and configured for the server as described below. Implementing the LogProvider interface Implementing the LogFileProvider interface Configuring Carbon to plug the log provider Implementing the LogProvider interface \u00b6 This org.wso2.carbon.logging.service.provider.api.LogProvider interface is used for viewing logs in the management console. It is introduced as an extension point to provide logs to the \"Log Viewer\" (in the management console). Any log provider can implement this interface to fetch logs from any mechanism, and the Log Viewer will use this interface to retrieve and show logs in the management console. The LogProvider interface has the following methods: init(LoggingConfig loggingConfig) - Initialize the log provider by reading the properties defined in the logging configuration file. This will be called immediately after creating a new instance of LogProvider. getApplicationNames(String tenantDomain, String serverKey) - Return list of all application names deployed under provided tenant domain and server key. getSystemLogs() - Return a list of system LogEvents. getAllLogs(String tenantDomain, String serverKey) - Return list of all the logs available under given domain and server key getLogsByAppName(String appName, String tenantDomain, String serverKey) - Return list of all the LogEvents belonging to the application, which is deployed under given tenant domain and server key. getLogs(String type, String keyword, String appName, String tenantDomain, String serverKey) - Returns list of all LogEvents related to the given application, which match to given type and LogEvent message has given key word with it. User can use this api for search operations. logsCount(String tenantDomain, String serverKey) - Return LogEvent count clearLogs() - Clear operation. For example, if it is an \"in memory\" log provider, this method can be used to clear the memory. Implementing the LogFileProvider interface \u00b6 The org.wso2.carbon.logging.service.provider.api.LogFileProvider interface is used to list and download the archived log files using the management console. It is introduced as an extension point providing the list of log file names and the ability to download these logs to the \"Log Viewer\". The LogFileProvider interface has the following methods: init(LoggingConfig loggingConfig)- Initialize the file log provider by reading the properties defined in the logging configuration file. This will be called immediately after creating a new instance of LogFileProvider. getLogFileInfoList(String tenantDomain, String serviceName) - Return information about the log files, which is available under given tenant domain and serviceName. For example, info about logs: log name, log date, log size. downloadLogFile(String logFile, String tenantDomain, String serviceName) - Download the file. Info Default log provider in Carbon products A default \"in memory\" log provider, which implements the LogProvider interface has been created both as a sample and as the default log provider option in carbon. Main task of this class is to read the carbon logs available in the <PRODUCT_HOME>/repository/logs/ directory to a buffer stored in memory and enable the LogViewer to fetch and view these logs in the management console. A default log file provider that implements the LogFileProvider interface has also been implemented as a sample and as the default log file provider option in carbon. The main task of this class is to read the log file names (including the size and date of these files) from the <PRODUCT_HOME>/repository/logs/ directory and to enable the download of these logs. Configuring Carbon to plug the log provider \u00b6 After implementing the above interfaces, update the logging-config.xml file stored in the <PRODUCT_HOME>/repository/conf/etc/ directory. Shown below is the configuration for the the default log provider and the default log file provider of a Carbon product: <loggingConfig xmlns=\"http://wso2.org/projects/carbon/carbon.xml\"> <!-- Default log provider --> <logProviderConfig class=\"org.wso2.carbon.logging.service.provider.InMemoryLogProvider\"> <properties/> </logProviderConfig> <!-- Default log file provider --> <logFileProviderConfig class=\"org.wso2.carbon.logging.service.provider.FileLogProvider\"> <properties/> </logFileProviderConfig> </loggingConfig> !!! note The default \"InMemoryLogProvider\" uses the CarbonMemoryAppender. Therefore the log4j.properties file stored in \\<PRODUCT\\_HOME\\>/repository/conf/ directory should be updated with the following log4j.appender.CARBON\\_MEMORY property: log4j.appender.CARBON_MEMORY=org.wso2.carbon.logging.service.appender.CarbonMemoryAppender] If the implemented class requires additional properties to initialise the class, the <properties> element in the logging-config.xml file can be used. For example, a cassandra based log provider may need information on keyspace, column family, etc. You can configure these details in the logging-config.xml file and access them at runtime using the LoggingConfig class, which contains all configuration parameters. For a Cassandra based log provider, the following properties can be defined in the logging-config.xml file and later used in the implementation using the LoggingConfig class, which is assigned when initializing the class. The following properties can be configured in the logging-config.xml file for a Cassandra based log provider: <logProviderConfig xmlns=\"http://wso2.org/projects/carbon/carbon.xml\" class=\"org.wso2.carbon.logging.service.provider.CassandraLogProvider\"> <properties> <property name=\"userName\" value=\"admin\"/> <property name=\"password\" value=\"admin\"/> <property name=\"archivedHost\" value=\"http://127.0.0.1/logs/stratos/0/WSO2%20Stratos%20Manager/\"/> <property name=\"archivedHDFSPath\" value=\"/stratos/logs\"/> <property name=\"archivedUser\" value=\"admin\"/> <property name=\"archivedPassword\" value=\"admin\"/> <property name=\"archivedPort\" value=\"80\"/> <property name=\"archivedRealm\" value=\"Stratos\"/> <property name=\"cassandraHost\" value=\"localhost:9160\"/> <property name=\"isDataFromCassandra\" value=\"false\"/> <property name=\"cassandraConsistencyLevel\" value=\"ONE\"/> <property name=\"cassandraAutoDiscovery.enable\" value=\"false\"/> <property name=\"cassandraAutoDiscovery.delay\" value=\"1000\"/> <property name=\"retryDownedHosts.enable\" value=\"true\"/> <property name=\"retryDownedHosts.queueSize\" value=\"10\"/> <property name=\"columnFamily\" value=\"log\"/> <property name=\"cluster\" value=\"admin\"/> <property name=\"keyspace\" value=\"EVENT_KS\"/> </properties> </logProviderConfig>","title":"Configuring the Log Provider"},{"location":"administer-and-observe/configuring_the_log_provider/#configuring-the-log-provider","text":"Logs of a system can be stored in many ways. For example, they can be stored in a file system, an sql server such as MySQL, a no-sql server like Cassandra, etc. According to the default configurations in a Carbon product, the logs are stored in the <PRODUCT_HOME>/repository/logs/ directory as .log files. To view and download the logs using the management console, the following configurations are required: the Logging Management feature should be installed, the log4j properties should be configured and the LogProvider and LogFileProvider interfaces should be implemented and configured for the server as described below. Implementing the LogProvider interface Implementing the LogFileProvider interface Configuring Carbon to plug the log provider","title":"Configuring the Log Provider"},{"location":"administer-and-observe/configuring_the_log_provider/#implementing-the-logprovider-interface","text":"This org.wso2.carbon.logging.service.provider.api.LogProvider interface is used for viewing logs in the management console. It is introduced as an extension point to provide logs to the \"Log Viewer\" (in the management console). Any log provider can implement this interface to fetch logs from any mechanism, and the Log Viewer will use this interface to retrieve and show logs in the management console. The LogProvider interface has the following methods: init(LoggingConfig loggingConfig) - Initialize the log provider by reading the properties defined in the logging configuration file. This will be called immediately after creating a new instance of LogProvider. getApplicationNames(String tenantDomain, String serverKey) - Return list of all application names deployed under provided tenant domain and server key. getSystemLogs() - Return a list of system LogEvents. getAllLogs(String tenantDomain, String serverKey) - Return list of all the logs available under given domain and server key getLogsByAppName(String appName, String tenantDomain, String serverKey) - Return list of all the LogEvents belonging to the application, which is deployed under given tenant domain and server key. getLogs(String type, String keyword, String appName, String tenantDomain, String serverKey) - Returns list of all LogEvents related to the given application, which match to given type and LogEvent message has given key word with it. User can use this api for search operations. logsCount(String tenantDomain, String serverKey) - Return LogEvent count clearLogs() - Clear operation. For example, if it is an \"in memory\" log provider, this method can be used to clear the memory.","title":"Implementing the LogProvider interface"},{"location":"administer-and-observe/configuring_the_log_provider/#implementing-the-logfileprovider-interface","text":"The org.wso2.carbon.logging.service.provider.api.LogFileProvider interface is used to list and download the archived log files using the management console. It is introduced as an extension point providing the list of log file names and the ability to download these logs to the \"Log Viewer\". The LogFileProvider interface has the following methods: init(LoggingConfig loggingConfig)- Initialize the file log provider by reading the properties defined in the logging configuration file. This will be called immediately after creating a new instance of LogFileProvider. getLogFileInfoList(String tenantDomain, String serviceName) - Return information about the log files, which is available under given tenant domain and serviceName. For example, info about logs: log name, log date, log size. downloadLogFile(String logFile, String tenantDomain, String serviceName) - Download the file. Info Default log provider in Carbon products A default \"in memory\" log provider, which implements the LogProvider interface has been created both as a sample and as the default log provider option in carbon. Main task of this class is to read the carbon logs available in the <PRODUCT_HOME>/repository/logs/ directory to a buffer stored in memory and enable the LogViewer to fetch and view these logs in the management console. A default log file provider that implements the LogFileProvider interface has also been implemented as a sample and as the default log file provider option in carbon. The main task of this class is to read the log file names (including the size and date of these files) from the <PRODUCT_HOME>/repository/logs/ directory and to enable the download of these logs.","title":"Implementing the LogFileProvider interface"},{"location":"administer-and-observe/configuring_the_log_provider/#configuring-carbon-to-plug-the-log-provider","text":"After implementing the above interfaces, update the logging-config.xml file stored in the <PRODUCT_HOME>/repository/conf/etc/ directory. Shown below is the configuration for the the default log provider and the default log file provider of a Carbon product: <loggingConfig xmlns=\"http://wso2.org/projects/carbon/carbon.xml\"> <!-- Default log provider --> <logProviderConfig class=\"org.wso2.carbon.logging.service.provider.InMemoryLogProvider\"> <properties/> </logProviderConfig> <!-- Default log file provider --> <logFileProviderConfig class=\"org.wso2.carbon.logging.service.provider.FileLogProvider\"> <properties/> </logFileProviderConfig> </loggingConfig> !!! note The default \"InMemoryLogProvider\" uses the CarbonMemoryAppender. Therefore the log4j.properties file stored in \\<PRODUCT\\_HOME\\>/repository/conf/ directory should be updated with the following log4j.appender.CARBON\\_MEMORY property: log4j.appender.CARBON_MEMORY=org.wso2.carbon.logging.service.appender.CarbonMemoryAppender] If the implemented class requires additional properties to initialise the class, the <properties> element in the logging-config.xml file can be used. For example, a cassandra based log provider may need information on keyspace, column family, etc. You can configure these details in the logging-config.xml file and access them at runtime using the LoggingConfig class, which contains all configuration parameters. For a Cassandra based log provider, the following properties can be defined in the logging-config.xml file and later used in the implementation using the LoggingConfig class, which is assigned when initializing the class. The following properties can be configured in the logging-config.xml file for a Cassandra based log provider: <logProviderConfig xmlns=\"http://wso2.org/projects/carbon/carbon.xml\" class=\"org.wso2.carbon.logging.service.provider.CassandraLogProvider\"> <properties> <property name=\"userName\" value=\"admin\"/> <property name=\"password\" value=\"admin\"/> <property name=\"archivedHost\" value=\"http://127.0.0.1/logs/stratos/0/WSO2%20Stratos%20Manager/\"/> <property name=\"archivedHDFSPath\" value=\"/stratos/logs\"/> <property name=\"archivedUser\" value=\"admin\"/> <property name=\"archivedPassword\" value=\"admin\"/> <property name=\"archivedPort\" value=\"80\"/> <property name=\"archivedRealm\" value=\"Stratos\"/> <property name=\"cassandraHost\" value=\"localhost:9160\"/> <property name=\"isDataFromCassandra\" value=\"false\"/> <property name=\"cassandraConsistencyLevel\" value=\"ONE\"/> <property name=\"cassandraAutoDiscovery.enable\" value=\"false\"/> <property name=\"cassandraAutoDiscovery.delay\" value=\"1000\"/> <property name=\"retryDownedHosts.enable\" value=\"true\"/> <property name=\"retryDownedHosts.queueSize\" value=\"10\"/> <property name=\"columnFamily\" value=\"log\"/> <property name=\"cluster\" value=\"admin\"/> <property name=\"keyspace\" value=\"EVENT_KS\"/> </properties> </logProviderConfig>","title":"Configuring Carbon to plug the log provider"},{"location":"administer-and-observe/enabling_metrics_storage_types/","text":"Enabling Metrics and Storage Types \u00b6 Given below are the configurations that should be in place for your WSO2 product to use the metrics feature. You need to first enable metrics for your server and then enable the required storage types (reporters) that will be used for storing the metrics data. See the following topics for instructions: Enabling metrics Configuring the storage of metrics Sample configuration Enabling metrics \u00b6 To enable metrics for your product, set the Enabled parameter under the Metrics element to true in the \\< PRODUCT_HOME>/repository/conf/metrics.xml file. Alternatively, you can enable metrics at the time of starting the server by using the following command: -Dmetrics.enabled=true Once metrics are enabled, the metrics dashboard will be updated for your product. Configuring the storage of metrics \u00b6 WSO2 products (based on Carbon 4.4.x Kernel versions) are configured by default to store the information from metrics in the following reporters: JMX, CSV and JDBC. These reporters are configured in the metrics.xml file (stored in the <PRODUCT_HOME>/repository/conf directory). You can disable metrics for individual reporters by setting the Enabled parameter to false . Note If you set the the Enabled parameter under the Metrics element to false in the metrics.xml file, metrics will be disabled for all the reporters and it is not possible to enable metrics for individual reporters. See the following topics for information on configuring each of the available storage types. JMX CSV JDBC JMX \u00b6 The following parameters in the metrics.xml file can be used to configure a JMX storage for metrics data. Element Name Description Type Default Value Mandatory/Optional Enabled This parameter specifies whether ot noy metics monitoring is enabled for JMX. Boolean true Mandatory CSV \u00b6 The following parameters in the metrics.xml file can be used to configure a CSV storage for metrics data. Element Name Description Type Default Value Mandatory/Optional Enabled This parameter specifies whether or not metrics monitoring is enabled for CSV. Boolean false Mandatory Location The location where the CSV files are stored. String ${carbon.home}/repository/logs/metrics/ PollingPeriod The time interval between polling activities that are carried out to update the metrics dashboard based on latest information. For example, if the polling period is 60, polling would be carried out every 60 milliseconds. Integer 60 JDBC \u00b6 Warning H2 is not recommended in production The H2 database is NOT recommended in enterprise testing and production environments. It has lower performance, clustering limitations, and can cause file corruption failures. Please use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MS SQL instead. See the instructions on setting up an RDBMS . The following parameters in the metrics.xml file can be used to configure a JDBC storage for metrics data. Element Name Description Type Default Value Mandatory/Optional Notes Enabled This parameter specifies whether or not metrics monitoring is enabled for JDBC. Boolean true Mandatory DataSourceName The name of the datasource used. String jdbc/WSO2MetricsDB PollingPeriod The time interval between polling activities that are carried out to update the metrics dashboard based on latest information. For example, if the polling period is 60, polling would be carried out every 60 milliseconds. Integer 60 This value is specified in milliseconds. ScheduledCleanup This element contains parameters relating to scheduled cleanup. The possible values are Enabled , ScheduledCleanupPeriod and DaysToKeep . Scheduled cleanup involves scheduling a task to clear metric data in the database after a specified time interval. This is done to avoid excessive memory usage. ScheduledCleanup/Enabled This parameter specifies whether or not scheduled cleanup is enabled. Boolean true ScheduledCleanup/ScheduledCleanupPeriod The number of milliseconds that should elapse after a clean-up task before the next clean-up task is carried out. Integer 86400 ScheduledCleanup/DaysToKeep The number of days during which the scheduled clean-up task should be carried out. Integer 7 If you have enabled JDBC, then you also need to specify a datasource configuration, which will be used to create the connection between the WSO2 product and the JDBC data storage system. The metrics-datasources.xml file is used for configuring this datasource for metrics. Parameters that can be configured for a datasource are as follows: XML element Attribute Description Data type Default value Mandatory/Optional <datasources-configuration> xmlns The root element. The namespace is specified as: xmlns:svns=\"http://org.wso2.securevault/configuration \" Mandatory <providers> The container element for the datasource providers. Mandatory <provider> The datasource provider, which should implement org.wso2.carbon.ndatasource.common .spi.DataSourceReader . The datasources follow a pluggable model in providing datasource type implementations using this approach. Fully qualified Java class Optional <datasources> The container element for the datasources. Mandatory <datasource> The root element of a datasource. Mandatory <name> Name of the datasource. String Mandatory <description> Description of the datasource. String Optional <jndiConfig> The container element that allows you to expose this datasource as a JNDI datasource. Optional <name> The JNDI resource name to which this datasource will be bound. String Mandatory if specifying JNDI configuration <environment> The container element in which you specify the following JNDI properties: java.naming.factory.initial : Selects the registry service provider as the initial context. java.naming.provider.url : Specifies the location of the registry when the registry is being used as the initial context. Fully qualified Java class Optional <definition> type The container element for the data source definition. Set the type attribute to \"RDBMS\", or to \"custom\" if you're creating a custom type. The \"RDBMS\" datasource reader expects a configuration element with the sub elements listed below. String Mandatory <configuration> The container element for the RDBMS properties. Mandatory if definition type is RDBMS <url> The connection URL that passes the JDBC driver to establish the connection. URL Mandatory <username> The connection user name that passes the JDBC driver to establish the connection. String Optional <password> The connection password that passes the JDBC driver to establish the connection. String Optional <driverClassName> The class name of the JDBC driver. Fully qualified Java class Mandatory <maxActive> The maximum number of active connections that can be allocated from this pool at the same time. Integer 100 Optional <maxWait> Maximum number of milliseconds that the pool waits (when there are no available connections) for a connection to be returned before throwing an exception. Integer 30000 (30 seconds) Optional <testOnBorrow> Specifies whether objects will be validated before being borrowed from the pool. If the object fails to validate, it will be dropped from the pool, and we will attempt to borrow another. When set to true , the validationQuery parameter must be set to a non-null string. Boolean false Optional <validationQuery> The SQL query that is used for validating connections from this pool before returning them to the caller. If specified, this query does not have to return any data, as it cannot throw an SQLException. The default value is \"null\". Example values are SELECT 1 (mysql), select 1 from dual (oracle), SELECT 1 (MS Sql Server). String null Mandatory when testOnBorrow is set to true <validationInterval> To avoid excess validation, a connection will be validated at this frequency, at most (interval time in milliseconds). If a connection is due for validation, but has been validated previously within this interval, it will not be validated again. The default value is 30000 (30 seconds). Long 30000 (30 seconds) Optional Sample configuration \u00b6 Shown below is a sample metrics.xml file with the default configurations specifying the types of storages enabled for metrics data. See the above topics for instructions. {.expand-control-image} The default configurations in the metrics.xml file --> <!-- This is the main configuration file for metrics --> <Metrics xmlns=\"http://wso2.org/projects/carbon/metrics.xml\"> <!-- Enable Metrics --> <Enabled>false</Enabled> <!-- Metrics reporting configurations --> <Reporting> <JMX> <Enabled>true</Enabled> </JMX> <CSV> <Enabled>false</Enabled> <Location>${carbon.home}/repository/logs/metrics/</Location> <!-- Polling Period in seconds --> <PollingPeriod>60</PollingPeriod> </CSV> <JDBC> <Enabled>true</Enabled> <!-- Source of Metrics, which will be used to identify each metric in database --> <!-- Commented to use the hostname <Source>Carbon</Source> --> <!-- JNDI name of the data source to be used by the JDBC Reporter. This data source should be defined in a *-datasources.xml file in conf/datasources directory. --> <DataSourceName>jdbc/WSO2MetricsDB</DataSourceName> <!-- Polling Period in seconds --> <PollingPeriod>60</PollingPeriod> <ScheduledCleanup> <!-- Schedule regular deletion of metrics data older than a set number of days. It is strongly recommended that you enable this job to ensure your metrics tables do not get extremely large. Deleting data older than seven days should be sufficient. --> <Enabled>true</Enabled> <!-- This is the period for each cleanup operation in seconds --> <ScheduledCleanupPeriod>86400</ScheduledCleanupPeriod> <!-- The scheduled job will cleanup all data older than the specified days --> <DaysToKeep>7</DaysToKeep> </ScheduledCleanup> </JDBC> </Reporting> </Metrics> Info Once you have enabled Metrics as explained above, proceed to the section on configuring metric properties for information on how to configure the gauges on the metrics dashboard.","title":"Enabling Metrics and Storage Types"},{"location":"administer-and-observe/enabling_metrics_storage_types/#enabling-metrics-and-storage-types","text":"Given below are the configurations that should be in place for your WSO2 product to use the metrics feature. You need to first enable metrics for your server and then enable the required storage types (reporters) that will be used for storing the metrics data. See the following topics for instructions: Enabling metrics Configuring the storage of metrics Sample configuration","title":"Enabling Metrics and Storage Types"},{"location":"administer-and-observe/enabling_metrics_storage_types/#enabling-metrics","text":"To enable metrics for your product, set the Enabled parameter under the Metrics element to true in the \\< PRODUCT_HOME>/repository/conf/metrics.xml file. Alternatively, you can enable metrics at the time of starting the server by using the following command: -Dmetrics.enabled=true Once metrics are enabled, the metrics dashboard will be updated for your product.","title":"Enabling metrics"},{"location":"administer-and-observe/enabling_metrics_storage_types/#configuring-the-storage-of-metrics","text":"WSO2 products (based on Carbon 4.4.x Kernel versions) are configured by default to store the information from metrics in the following reporters: JMX, CSV and JDBC. These reporters are configured in the metrics.xml file (stored in the <PRODUCT_HOME>/repository/conf directory). You can disable metrics for individual reporters by setting the Enabled parameter to false . Note If you set the the Enabled parameter under the Metrics element to false in the metrics.xml file, metrics will be disabled for all the reporters and it is not possible to enable metrics for individual reporters. See the following topics for information on configuring each of the available storage types. JMX CSV JDBC","title":"Configuring the storage of metrics"},{"location":"administer-and-observe/enabling_metrics_storage_types/#jmx","text":"The following parameters in the metrics.xml file can be used to configure a JMX storage for metrics data. Element Name Description Type Default Value Mandatory/Optional Enabled This parameter specifies whether ot noy metics monitoring is enabled for JMX. Boolean true Mandatory","title":"JMX"},{"location":"administer-and-observe/enabling_metrics_storage_types/#csv","text":"The following parameters in the metrics.xml file can be used to configure a CSV storage for metrics data. Element Name Description Type Default Value Mandatory/Optional Enabled This parameter specifies whether or not metrics monitoring is enabled for CSV. Boolean false Mandatory Location The location where the CSV files are stored. String ${carbon.home}/repository/logs/metrics/ PollingPeriod The time interval between polling activities that are carried out to update the metrics dashboard based on latest information. For example, if the polling period is 60, polling would be carried out every 60 milliseconds. Integer 60","title":"CSV"},{"location":"administer-and-observe/enabling_metrics_storage_types/#jdbc","text":"Warning H2 is not recommended in production The H2 database is NOT recommended in enterprise testing and production environments. It has lower performance, clustering limitations, and can cause file corruption failures. Please use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MS SQL instead. See the instructions on setting up an RDBMS . The following parameters in the metrics.xml file can be used to configure a JDBC storage for metrics data. Element Name Description Type Default Value Mandatory/Optional Notes Enabled This parameter specifies whether or not metrics monitoring is enabled for JDBC. Boolean true Mandatory DataSourceName The name of the datasource used. String jdbc/WSO2MetricsDB PollingPeriod The time interval between polling activities that are carried out to update the metrics dashboard based on latest information. For example, if the polling period is 60, polling would be carried out every 60 milliseconds. Integer 60 This value is specified in milliseconds. ScheduledCleanup This element contains parameters relating to scheduled cleanup. The possible values are Enabled , ScheduledCleanupPeriod and DaysToKeep . Scheduled cleanup involves scheduling a task to clear metric data in the database after a specified time interval. This is done to avoid excessive memory usage. ScheduledCleanup/Enabled This parameter specifies whether or not scheduled cleanup is enabled. Boolean true ScheduledCleanup/ScheduledCleanupPeriod The number of milliseconds that should elapse after a clean-up task before the next clean-up task is carried out. Integer 86400 ScheduledCleanup/DaysToKeep The number of days during which the scheduled clean-up task should be carried out. Integer 7 If you have enabled JDBC, then you also need to specify a datasource configuration, which will be used to create the connection between the WSO2 product and the JDBC data storage system. The metrics-datasources.xml file is used for configuring this datasource for metrics. Parameters that can be configured for a datasource are as follows: XML element Attribute Description Data type Default value Mandatory/Optional <datasources-configuration> xmlns The root element. The namespace is specified as: xmlns:svns=\"http://org.wso2.securevault/configuration \" Mandatory <providers> The container element for the datasource providers. Mandatory <provider> The datasource provider, which should implement org.wso2.carbon.ndatasource.common .spi.DataSourceReader . The datasources follow a pluggable model in providing datasource type implementations using this approach. Fully qualified Java class Optional <datasources> The container element for the datasources. Mandatory <datasource> The root element of a datasource. Mandatory <name> Name of the datasource. String Mandatory <description> Description of the datasource. String Optional <jndiConfig> The container element that allows you to expose this datasource as a JNDI datasource. Optional <name> The JNDI resource name to which this datasource will be bound. String Mandatory if specifying JNDI configuration <environment> The container element in which you specify the following JNDI properties: java.naming.factory.initial : Selects the registry service provider as the initial context. java.naming.provider.url : Specifies the location of the registry when the registry is being used as the initial context. Fully qualified Java class Optional <definition> type The container element for the data source definition. Set the type attribute to \"RDBMS\", or to \"custom\" if you're creating a custom type. The \"RDBMS\" datasource reader expects a configuration element with the sub elements listed below. String Mandatory <configuration> The container element for the RDBMS properties. Mandatory if definition type is RDBMS <url> The connection URL that passes the JDBC driver to establish the connection. URL Mandatory <username> The connection user name that passes the JDBC driver to establish the connection. String Optional <password> The connection password that passes the JDBC driver to establish the connection. String Optional <driverClassName> The class name of the JDBC driver. Fully qualified Java class Mandatory <maxActive> The maximum number of active connections that can be allocated from this pool at the same time. Integer 100 Optional <maxWait> Maximum number of milliseconds that the pool waits (when there are no available connections) for a connection to be returned before throwing an exception. Integer 30000 (30 seconds) Optional <testOnBorrow> Specifies whether objects will be validated before being borrowed from the pool. If the object fails to validate, it will be dropped from the pool, and we will attempt to borrow another. When set to true , the validationQuery parameter must be set to a non-null string. Boolean false Optional <validationQuery> The SQL query that is used for validating connections from this pool before returning them to the caller. If specified, this query does not have to return any data, as it cannot throw an SQLException. The default value is \"null\". Example values are SELECT 1 (mysql), select 1 from dual (oracle), SELECT 1 (MS Sql Server). String null Mandatory when testOnBorrow is set to true <validationInterval> To avoid excess validation, a connection will be validated at this frequency, at most (interval time in milliseconds). If a connection is due for validation, but has been validated previously within this interval, it will not be validated again. The default value is 30000 (30 seconds). Long 30000 (30 seconds) Optional","title":"JDBC"},{"location":"administer-and-observe/enabling_metrics_storage_types/#sample-configuration","text":"Shown below is a sample metrics.xml file with the default configurations specifying the types of storages enabled for metrics data. See the above topics for instructions. {.expand-control-image} The default configurations in the metrics.xml file --> <!-- This is the main configuration file for metrics --> <Metrics xmlns=\"http://wso2.org/projects/carbon/metrics.xml\"> <!-- Enable Metrics --> <Enabled>false</Enabled> <!-- Metrics reporting configurations --> <Reporting> <JMX> <Enabled>true</Enabled> </JMX> <CSV> <Enabled>false</Enabled> <Location>${carbon.home}/repository/logs/metrics/</Location> <!-- Polling Period in seconds --> <PollingPeriod>60</PollingPeriod> </CSV> <JDBC> <Enabled>true</Enabled> <!-- Source of Metrics, which will be used to identify each metric in database --> <!-- Commented to use the hostname <Source>Carbon</Source> --> <!-- JNDI name of the data source to be used by the JDBC Reporter. This data source should be defined in a *-datasources.xml file in conf/datasources directory. --> <DataSourceName>jdbc/WSO2MetricsDB</DataSourceName> <!-- Polling Period in seconds --> <PollingPeriod>60</PollingPeriod> <ScheduledCleanup> <!-- Schedule regular deletion of metrics data older than a set number of days. It is strongly recommended that you enable this job to ensure your metrics tables do not get extremely large. Deleting data older than seven days should be sufficient. --> <Enabled>true</Enabled> <!-- This is the period for each cleanup operation in seconds --> <ScheduledCleanupPeriod>86400</ScheduledCleanupPeriod> <!-- The scheduled job will cleanup all data older than the specified days --> <DaysToKeep>7</DaysToKeep> </ScheduledCleanup> </JDBC> </Reporting> </Metrics> Info Once you have enabled Metrics as explained above, proceed to the section on configuring metric properties for information on how to configure the gauges on the metrics dashboard.","title":"Sample configuration"},{"location":"administer-and-observe/jmx_monitoring/","text":"JMX Monitoring \u00b6 Java Management Extensions (JMX) is a technology that lets you implement management interfaces for Java applications. JConsole is a JMX-compliant monitoring tool, which comes with the Java Development Kit (JDK) 1.5 or later versions. Therefore, when you use a WSO2 product, JMX is enabled by default, which allows you to monitor the product using JConsole. Java Management Extensions (JMX) is a technology that lets you implement management interfaces for Java applications. A management interface, as defined by JMX, is composed of named objects called MBeans (Management Beans). MBeans are registered with a name (an ObjectName) in an MBeanServer. To manage a resource or many resources in your application, you can write an MBean defining its management interface and register that MBean in your MBeanServer. The content of the MBeanServer can then be exposed through various protocols, implemented by protocol connectors, or protocol adaptors. Configuring JMX in a WSO2 product \u00b6 JMX is enabled in WSO2 products by default, which ensures that the JMX server starts automatically when you start a particular product. Additionally, you can enable JMX separately for the various datasources that are used by the product. Once JMX is enabled, you can log in to the JConsole tool and monitor your product as explained in the next section . Configuring JMX ports for the server \u00b6 The default JMX ports (RMIRegistryPort and the RMIServerPort) are configured in the carbon.xml file (stored in the <PRODUCT_HOME>/repository/conf directory) as shown below. If required, you can update these default values. <JMX> <!--The port RMI registry is exposed--> <RMIRegistryPort>9999</RMIRegistryPort> <!--The port RMI server should be exposed--> <RMIServerPort>11111</RMIServerPort> </JMX> Disabling JMX for the server \u00b6 The JMX configuration is available in the jmx .xml file (stored in the <PRODUCT_HOME>/repository/conf/etc directory) as shown below. You can disable the JMX server for your product by setting the <StartRMIServer> property to false . Note that this configuration refers to the JMX ports configured in the carbon. xml file . <JMX xmlns=\"http://wso2.org/projects/carbon/jmx.xml\"> <StartRMIServer>true</StartRMIServer> <!-- HostName, or Network interface to which this RMI server should be bound --> <HostName>localhost</HostName> <!-- ${Ports.JMX.RMIRegistryPort} is defined in the Ports section of the carbon.xml--> <RMIRegistryPort>${Ports.JMX.RMIRegistryPort}</RMIRegistryPort> <!-- ${Ports.JMX.RMIRegistryPort} is defined in the Ports section of the carbon.xml--> <RMIServerPort>${Ports.JMX.RMIServerPort}</RMIServerPort> </JMX> Enabling JMX for a datasource \u00b6 You can enable JMX for a datasource by adding the <jmxEnabled>true</jmxEnabled> element to the datasource configuration file. For example, to enable JMX for the default Carbon datasource in your product, add the following property to the master-datasources . xml file (stored in the <PRODUCT_HOME>/repository/conf/datasources directory). <datasource> <name>WSO2_CARBON_DB</name> <description>The datasource used for registry and user manager</description> <jndiConfig> <name>jdbc/WSO2CarbonDB</name> </jndiConfig> <definition type=\"RDBMS\"> <configuration> <url>jdbc:h2:./repository/database/WSO2CARBON_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000</url> <username>wso2carbon</username> <password>wso2carbon</password> <driverClassName>org.h2.Driver</driverClassName> <maxActive>50</maxActive> <maxWait>60000</maxWait> <testOnBorrow>true</testOnBorrow> <validationQuery>SELECT 1</validationQuery> <validationInterval>30000</validationInterval> <defaultAutoCommit>false</defaultAutoCommit> <jmxEnabled>true</jmxEnabled> </configuration> </definition> </datasource> Monitoring a WSO2 product with JConsole \u00b6 Jconsole is a JMX-compliant monitoring tool, which comes with the Java Development Kit (JDK) 1.5 and newer versions. You can find this tool inside your <JDK_HOME>/bin directory. See the instructions on Installing the JDK for more information. Starting the WSO2 product with JMX \u00b6 First, start the WSO2 product: Open a command prompt and navigate to the <PRODUCT_HOME>/bin directory. Execute the product startup script ( wso2server.sh for Linux and wso2server.bat for Windows) to start the server. !!! info If [JMX is enabled](_JMX-Based_Monitoring_) , the **JMX server URL** will be published on the console when the server starts as shown below. INFO {org.wso2.carbon.core.init.CarbonServerManager} - JMX Service URL : service:jmx:rmi://<your-ip>:11111/jndi/rmi://<your-ip>:9999/jmxrmi Once the product server is started, you can start the JC onsole tool as follows: Open a command prompt and navigate to the <JDK_HOME>/bin directory. Execute the j console command to open the log-in screen of the Java Monitoring & Management Console as shown below. {width=\"400\"} Enter the connection details in the above screen as follows: Enter the JMX server URL in the Remote Process field. This URL is published on the command prompt when you start the WSO2 server as explained above . !!! info Tip If you are connecting with a remote IP address instead of localhost, you need to bind the JMX service to the externally accessible IP address by adding the following system property to the product startup script stored in the ` <PRODUCT_HOME>/bin ` directory ( ` wso2server.sh ` for Linux and ` wso2server.bat ` for Windows). For more information, read [Troubleshooting Connection Problems in JConsole](https://blogs.oracle.com/jmxetc/entry/troubleshooting_connection_problems_in_jconsole) . -Djava.rmi.server.hostname=<IP_ADDRESS_WHICH_YOU_USE_TO_CONNECT_TO_SERVER> Be sure to restart the server after adding the above property. Enter values for the Username and Password fields to log in. If you are logging in as the administrator, you can use the same administrator account that is used to log in to the product's management console: admin/admin. !!! info Make sure that the user ID you are using for JMX monitoring is assigned a role that has the **Server Admin** permission. See [Configuring Roles](https://docs.wso2.com/display/ADMIN44x/Configuring+Roles) for further information about configuring roles assigned to users. Any user assigned to the **admin** role can log in to JMX. Click Connect to open the Java Monitoring & Management Console . The following tabs will be available: Overview Memory Threads Classes VM MBeans See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} Using the ServerAdmin MBean \u00b6 When you go to the MBeans tab in the JConsole, the ServerAdmin MBean will be listed under the \"org.wso2.carbon\" domain as shown below. {width=\"600\"} The ServerAdmin MBean is used for administering the product server instance. There are several server attributes such as \"ServerStatus\", \"ServerData\" and \"ServerVersion\". The \"ServerStatus\" attribute can take any of the following values: RUNNING SHUTTING_DOWN RESTARTING IN_MAINTENANCE {width=\"650\"} The ServerAdmin MBean has the following operations: Operation Description shutdown Forcefully shut down the server. restart Forcefully restart the server. restartGracefully Wait till all current requests are served and then restart. shutdownGracefully Wait till all current requests are served and then shutdown. startMaintenance Switch the server to maintenance mode. No new requests will be accepted while the server is in maintenance. endMaintenance Switch the server to normal mode if it was switched to maintenance mode earlier. {width=\"650\"} Using the ServiceAdmin MBean \u00b6 This MBean is used for administering services deployed in your product. Its attributes are as follows: Attribute Description NumberOfActiveServices The number of services which can currently serve requests. NumberOfInactiveServices The number of services which have been disabled by an administrator. NumberOfFaultyServices The number of services which are faulty. {width=\"600\"} The operations available in the ServiceAdmin MBean: Operation Description startService ( p1:string ) The p1 parameter is the service name. You can activate a service using this operation. stopService ( p1:string ) The p1 parameter is the service name. You can deactivate/disable a service using this operation. {width=\"600\"} Using the StatisticsAdmin MBean \u00b6 This MBean is used for monitoring system and server statistics. Its attributes are as follows: Attributes Description AvgSystemResponseTime The average response time for all the services deployed in the system. The beginning of the measurement is the time at which the server started. MaxSystemResponseTime The maximum response time for all the services deployed in the system. The beginning of the measurement is the time at which the server started. MinSystemResponseTime The minimum time for all the services deployed in the system. The beginning of the measurement is the time at which the server started. SystemFaultCount The total number of faults that occurred in the system since the server was started. SystemRequestCount The total number of requests that has been served by the system since the server was started. SystemResponseCount The total number of response that has been sent by the system since the server was started. {width=\"600\"} Operations available in the Statistics MBean: Operation Description getServiceRequestCount ( p1:string ) The p1 parameter is the service name. You can get the total number of requests received by this service since the time it was deployed, using this operation. getServiceResponseCount ( p1:string ) The p1 parameter is the service name. You can get the total number of responses sent by this service since the time it was deployed, using this operation. getServiceFaultCount ( p1:string ) The p1 parameter is the service name. You can get the total number of fault responses sent by this service since the time it was deployed, using this operation. getMaxServiceResponseTime ( p1:string ) The p1 parameter is the service name. You can get the maximum response time of this service since deployment. getMinServiceResponseTime ( p1:string ) The p1 parameter is the service name. You can get the minimum response time of this service since deployment. getAvgServiceResponseTime ( p1:string ) The p1 parameter is the service name. You can get the average response time of this service since deployment. getOperationRequestCount ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the total number of requests received by this operation since the time its service was deployed, using this operation. getOperationResponseCount ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the total number of responses sent by this operation since the time its service was deployed, using this operation. getOperationFaultCount ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the total number of fault responses sent by this operation since the time its service was deployed, using this operation. getMaxOperationResponseTime ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the maximum response time of this operation since deployment. getMinOperationResponseTime ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the minimum response time of this operation since deployment. getAvgOperationResponseTime ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the average response time of this operation since deployment. {width=\"600\"} Using the DataSource MBean \u00b6 If you have JMX enabled for a datasource connected to the product , you can monitor the performance of the datasource using this MBean. The DataSource MBean will be listed as shown below. {width=\"600\"} Example: If you have JMX enabled for the default Carbon datasource in the master-datasources.xml. file, the JDBC connection pool parameters that are configured for the Carbon datasource will be listed as attributes as shown below. See the performance tuning guide for instructions on how these parameters are configured for a datasource. {width=\"650\"} Using product-specific MBeans \u00b6 The WSO2 product that you are using may have product-specific MBeans enabled for monitoring and managing specific functions. See the documentation for your product for detailed instructions on such product-specific MBeans. Monitoring a WSO2 product with Jolokia \u00b6 Jolokia is a JMX-HTTP bridge, which is an alternative to JSR-160 connectors. It is an agent-based approach that supports many platforms. In addition to basic JMX operations, it enhances JMX monitoring with unique features like bulk requests and fine-grained security policies. Follow the steps below to use Jolokia to monitor a WSO2 product. Download Jolokia OSGi Agent . (These instructions are tested with the Jolokia OSGI Agent version 1.3.6 by downloading the jolokia-osgi-1.3.6.jar file.) Add it to the <PRODUCT-HOME>/repository/components/dropins/ directory. !!! tip In WSO2 EI, add it to the ` <EI-HOME> ` ` /dropins/ ` directory. Start the WSO2 product server. Once the server starts, you can read MBeans using Jolokia APIs. Following are a few examples. List all available MBeans: http://localhost:9763/jolokia/list (Change the appropriate hostname and port accordingly.) WSO2 ESB MBean: http://localhost:9763/jolokia/read/org.apache.synapse:Name=https-sender,Type=PassThroughConnections/ActiveConnections Reading Heap Memory: http://localhost:9763/jolokia/read/java.lang:type=Memory/HeapMemoryUsage For more information on the JMX MBeans that are available in WSO2 products, see Monitoring a WSO2 product with JConsole . MBeans for WSO2 EI \u00b6 When JMX is enabled, WSO2 EI exposes a number of management resources as JMX Management Beans (MBeans) that can be used for managing and monitoring the running server. When you start JConsole, you can monitor these MBeans from the MBeans tab. While some of these MBeans ( ServerAdmin and DataSource ) are common to all WSO2 products, some MBeans are specific to WSO2 EI. Tip The common MBeans are explained in detail in the WSO2 Administration Guide . Listed below are the MBeans that are specific to WSO2 EI. For details on using JMX based mediation flow statistics in your EI server and detailed explanation of the relevant MBeans, see Monitoring JMX Based Statistics . {height=\"400\"} This section summarizes the attributes and operations available for the following WSO2 EI specific MBeans: Connection MBeans Latency MBeans Threading MBeans Transport MBeans Broker-specific MBeans Connection MBeans \u00b6 These MBeans provide connection statistics for the HTTP and HTTPS transports. You can view the following Connection MBeans: org.apache.synapse/PassThroughConnections/http-listener org.apache.synapse/PassThroughConnections/http-sender org.apache.synapse/PassThroughConnections/https-listener org.apache.synapse/PassThroughConnections/https-sender Attributes Attribute Name Description ActiveConnections Number of currently active connections. ActiveConnectionsPerHosts A map of the number of connections against hosts. LastXxxConnections The number of connections created during last Xxx time period. RequestSizesMap A map of the number of requests against their sizes. ResponseSizesMap A map of the number of responses against their sizes. LastResetTime The time when the connection-statistic recordings were last reset. Operations Operation Name Description reset() Clear recorded connection statistics and restart recording. Latency MBeans \u00b6 This view provides statistics of the latencies from all backend services connected through the HTTP and HTTPS transports. These statistics are provided as an aggregate value. You can view the following Latency MBeans: org.apache.synapse/PassthroughLatencyView/nio-http-http org.apache.synapse/PassthroughLatencyView/nio-https-https Attributes Attribute Name Description AllTimeAvgLatency Average latency since the latency recording was last reset. LastxxxAvgLatency Average latency for last xxx time period. For example, LastHourAvgLatency returns the average latency for the last hour. LastResetTime The time when the latency-statistic recordings were last reset. Operations Operation Name Description reset() Clear recorded latency statistics and restart recording. Threading MBeans \u00b6 These MBeans are only available in the NHTTP transport and not in the default Pass-Through transport. You can view the following Threading MBeans: org.apache.synapse/Threading/HttpClientWorker org.apache.synapse/Threading/HttpServerWorker Attributes Attribute Name Description TotalWorkerCount Total worker threads related to this server/client. AvgUnblockedWorkerPercentage Time-averaged unblocked worker thread percentage. AvgBlockedWorkerPercentage Time-averaged blocked worker thread percentage. LastXxxBlockedWorkerPercentage Blocked worker thread percentage averaged for last Xxx time period. DeadLockedWorkers Number of deadlocked worker threads since last statistics reset. LastResetTime The time the thread statistic recordings were last reset. Operations Operation Name Description reset() Clear recorded thread statistic and restart recording. Transport MBeans \u00b6 For each transport listener and sender enabled in WSO2 EI, there will be an MBean under the org.apache.axis2/Transport domain. For example, when the JMS transport is enabled, the following MBean will be exposed: org.apache.axis2/Transport/jms-sender- n You can also view the following Transport MBeans: org.apache.synapse/Transport/passthru-http-receiver org.apache.synapse/Transport/passthru-http-sender org.apache.synapse/Transport/passthru-https-receiver org.apache.synapse/Transport/passthru-https-sender Attributes Attribute Name Description ActiveThreadCount Threads active in this transport listener/sender. AvgSizeReceived The average size of received messages. AvgSizeSent The average size of sent messages. BytesReceived The number of bytes received through this transport. BytesSent The number of bytes sent through this transport. FaultsReceiving The number of faults encountered while receiving. FaultsSending The number of faults encountered while sending. LastResetTime The time when the last transport listener/sender statistic recording was reset. MaxSizeReceived Maximum message size of received messages. MaxSizeSent Maximum message size of sent messages. MetricsWindow Time difference between current time and last reset time in milliseconds. MinSizeReceived Minimum message size of received messages. MinSizeSent Minimum message size of sent messages. MessagesReceived The total number of messages received through this transport. MessagesSent The total number of messages sent through this transport. QueueSize The number of messages currently queued. Messages get queued if all the worker threads in this transport thread pool are busy. ResponseCodeTable The number of messages sent against their response codes. TimeoutsReceiving Message receiving timeout. TimeoutsSending Message sending timeout. Operations Operation Name Description start() Start this transport listener/sender. stop() Stop this transport listener/sender. resume() Resume this transport listener/sender which is currently paused. resetStatistics() Clear recorded transport listener/sender statistics and restart recording. pause() Pause this transport listener/sender which has been started. maintenenceShutdown(long gracePeriod) Stop processing new messages, and wait the specified maximum time for in-flight requests to complete before a controlled shutdown for maintenence. Disabling the JMX thread view \u00b6 Dumping JMX threads is an expensive operation that affects the CPU consumption when many threads are being processed at the same time. WSO2 EI has enabled thread dumping by default. Therefore, to avoid dumping all the threads here , you can configure the property given below. Info It is recommended not to dump the thread especially when you have enabled WSO2 EI analytics in a production environment. Open the <EI_HOME>/ synapse.properties file. Add the following property to the file and save the file. synapse.jmx.thread.view.enabled=false","title":"JMX Monitoring"},{"location":"administer-and-observe/jmx_monitoring/#jmx-monitoring","text":"Java Management Extensions (JMX) is a technology that lets you implement management interfaces for Java applications. JConsole is a JMX-compliant monitoring tool, which comes with the Java Development Kit (JDK) 1.5 or later versions. Therefore, when you use a WSO2 product, JMX is enabled by default, which allows you to monitor the product using JConsole. Java Management Extensions (JMX) is a technology that lets you implement management interfaces for Java applications. A management interface, as defined by JMX, is composed of named objects called MBeans (Management Beans). MBeans are registered with a name (an ObjectName) in an MBeanServer. To manage a resource or many resources in your application, you can write an MBean defining its management interface and register that MBean in your MBeanServer. The content of the MBeanServer can then be exposed through various protocols, implemented by protocol connectors, or protocol adaptors.","title":"JMX Monitoring"},{"location":"administer-and-observe/jmx_monitoring/#configuring-jmx-in-a-wso2-product","text":"JMX is enabled in WSO2 products by default, which ensures that the JMX server starts automatically when you start a particular product. Additionally, you can enable JMX separately for the various datasources that are used by the product. Once JMX is enabled, you can log in to the JConsole tool and monitor your product as explained in the next section .","title":"Configuring JMX in a WSO2 product"},{"location":"administer-and-observe/jmx_monitoring/#configuring-jmx-ports-for-the-server","text":"The default JMX ports (RMIRegistryPort and the RMIServerPort) are configured in the carbon.xml file (stored in the <PRODUCT_HOME>/repository/conf directory) as shown below. If required, you can update these default values. <JMX> <!--The port RMI registry is exposed--> <RMIRegistryPort>9999</RMIRegistryPort> <!--The port RMI server should be exposed--> <RMIServerPort>11111</RMIServerPort> </JMX>","title":"Configuring JMX ports for the server"},{"location":"administer-and-observe/jmx_monitoring/#disabling-jmx-for-the-server","text":"The JMX configuration is available in the jmx .xml file (stored in the <PRODUCT_HOME>/repository/conf/etc directory) as shown below. You can disable the JMX server for your product by setting the <StartRMIServer> property to false . Note that this configuration refers to the JMX ports configured in the carbon. xml file . <JMX xmlns=\"http://wso2.org/projects/carbon/jmx.xml\"> <StartRMIServer>true</StartRMIServer> <!-- HostName, or Network interface to which this RMI server should be bound --> <HostName>localhost</HostName> <!-- ${Ports.JMX.RMIRegistryPort} is defined in the Ports section of the carbon.xml--> <RMIRegistryPort>${Ports.JMX.RMIRegistryPort}</RMIRegistryPort> <!-- ${Ports.JMX.RMIRegistryPort} is defined in the Ports section of the carbon.xml--> <RMIServerPort>${Ports.JMX.RMIServerPort}</RMIServerPort> </JMX>","title":"Disabling JMX for the server"},{"location":"administer-and-observe/jmx_monitoring/#enabling-jmx-for-a-datasource","text":"You can enable JMX for a datasource by adding the <jmxEnabled>true</jmxEnabled> element to the datasource configuration file. For example, to enable JMX for the default Carbon datasource in your product, add the following property to the master-datasources . xml file (stored in the <PRODUCT_HOME>/repository/conf/datasources directory). <datasource> <name>WSO2_CARBON_DB</name> <description>The datasource used for registry and user manager</description> <jndiConfig> <name>jdbc/WSO2CarbonDB</name> </jndiConfig> <definition type=\"RDBMS\"> <configuration> <url>jdbc:h2:./repository/database/WSO2CARBON_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000</url> <username>wso2carbon</username> <password>wso2carbon</password> <driverClassName>org.h2.Driver</driverClassName> <maxActive>50</maxActive> <maxWait>60000</maxWait> <testOnBorrow>true</testOnBorrow> <validationQuery>SELECT 1</validationQuery> <validationInterval>30000</validationInterval> <defaultAutoCommit>false</defaultAutoCommit> <jmxEnabled>true</jmxEnabled> </configuration> </definition> </datasource>","title":"Enabling JMX for a datasource"},{"location":"administer-and-observe/jmx_monitoring/#monitoring-a-wso2-product-with-jconsole","text":"Jconsole is a JMX-compliant monitoring tool, which comes with the Java Development Kit (JDK) 1.5 and newer versions. You can find this tool inside your <JDK_HOME>/bin directory. See the instructions on Installing the JDK for more information.","title":"Monitoring a WSO2 product with JConsole"},{"location":"administer-and-observe/jmx_monitoring/#starting-the-wso2-product-with-jmx","text":"First, start the WSO2 product: Open a command prompt and navigate to the <PRODUCT_HOME>/bin directory. Execute the product startup script ( wso2server.sh for Linux and wso2server.bat for Windows) to start the server. !!! info If [JMX is enabled](_JMX-Based_Monitoring_) , the **JMX server URL** will be published on the console when the server starts as shown below. INFO {org.wso2.carbon.core.init.CarbonServerManager} - JMX Service URL : service:jmx:rmi://<your-ip>:11111/jndi/rmi://<your-ip>:9999/jmxrmi Once the product server is started, you can start the JC onsole tool as follows: Open a command prompt and navigate to the <JDK_HOME>/bin directory. Execute the j console command to open the log-in screen of the Java Monitoring & Management Console as shown below. {width=\"400\"} Enter the connection details in the above screen as follows: Enter the JMX server URL in the Remote Process field. This URL is published on the command prompt when you start the WSO2 server as explained above . !!! info Tip If you are connecting with a remote IP address instead of localhost, you need to bind the JMX service to the externally accessible IP address by adding the following system property to the product startup script stored in the ` <PRODUCT_HOME>/bin ` directory ( ` wso2server.sh ` for Linux and ` wso2server.bat ` for Windows). For more information, read [Troubleshooting Connection Problems in JConsole](https://blogs.oracle.com/jmxetc/entry/troubleshooting_connection_problems_in_jconsole) . -Djava.rmi.server.hostname=<IP_ADDRESS_WHICH_YOU_USE_TO_CONNECT_TO_SERVER> Be sure to restart the server after adding the above property. Enter values for the Username and Password fields to log in. If you are logging in as the administrator, you can use the same administrator account that is used to log in to the product's management console: admin/admin. !!! info Make sure that the user ID you are using for JMX monitoring is assigned a role that has the **Server Admin** permission. See [Configuring Roles](https://docs.wso2.com/display/ADMIN44x/Configuring+Roles) for further information about configuring roles assigned to users. Any user assigned to the **admin** role can log in to JMX. Click Connect to open the Java Monitoring & Management Console . The following tabs will be available: Overview Memory Threads Classes VM MBeans See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"} See the Oracle documentation on using JConsole for more information on these tabs. {width=\"600\"}","title":"Starting the WSO2 product with JMX"},{"location":"administer-and-observe/jmx_monitoring/#using-the-serveradmin-mbean","text":"When you go to the MBeans tab in the JConsole, the ServerAdmin MBean will be listed under the \"org.wso2.carbon\" domain as shown below. {width=\"600\"} The ServerAdmin MBean is used for administering the product server instance. There are several server attributes such as \"ServerStatus\", \"ServerData\" and \"ServerVersion\". The \"ServerStatus\" attribute can take any of the following values: RUNNING SHUTTING_DOWN RESTARTING IN_MAINTENANCE {width=\"650\"} The ServerAdmin MBean has the following operations: Operation Description shutdown Forcefully shut down the server. restart Forcefully restart the server. restartGracefully Wait till all current requests are served and then restart. shutdownGracefully Wait till all current requests are served and then shutdown. startMaintenance Switch the server to maintenance mode. No new requests will be accepted while the server is in maintenance. endMaintenance Switch the server to normal mode if it was switched to maintenance mode earlier. {width=\"650\"}","title":"Using the ServerAdmin MBean"},{"location":"administer-and-observe/jmx_monitoring/#using-the-serviceadmin-mbean","text":"This MBean is used for administering services deployed in your product. Its attributes are as follows: Attribute Description NumberOfActiveServices The number of services which can currently serve requests. NumberOfInactiveServices The number of services which have been disabled by an administrator. NumberOfFaultyServices The number of services which are faulty. {width=\"600\"} The operations available in the ServiceAdmin MBean: Operation Description startService ( p1:string ) The p1 parameter is the service name. You can activate a service using this operation. stopService ( p1:string ) The p1 parameter is the service name. You can deactivate/disable a service using this operation. {width=\"600\"}","title":"Using the ServiceAdmin MBean"},{"location":"administer-and-observe/jmx_monitoring/#using-the-statisticsadmin-mbean","text":"This MBean is used for monitoring system and server statistics. Its attributes are as follows: Attributes Description AvgSystemResponseTime The average response time for all the services deployed in the system. The beginning of the measurement is the time at which the server started. MaxSystemResponseTime The maximum response time for all the services deployed in the system. The beginning of the measurement is the time at which the server started. MinSystemResponseTime The minimum time for all the services deployed in the system. The beginning of the measurement is the time at which the server started. SystemFaultCount The total number of faults that occurred in the system since the server was started. SystemRequestCount The total number of requests that has been served by the system since the server was started. SystemResponseCount The total number of response that has been sent by the system since the server was started. {width=\"600\"} Operations available in the Statistics MBean: Operation Description getServiceRequestCount ( p1:string ) The p1 parameter is the service name. You can get the total number of requests received by this service since the time it was deployed, using this operation. getServiceResponseCount ( p1:string ) The p1 parameter is the service name. You can get the total number of responses sent by this service since the time it was deployed, using this operation. getServiceFaultCount ( p1:string ) The p1 parameter is the service name. You can get the total number of fault responses sent by this service since the time it was deployed, using this operation. getMaxServiceResponseTime ( p1:string ) The p1 parameter is the service name. You can get the maximum response time of this service since deployment. getMinServiceResponseTime ( p1:string ) The p1 parameter is the service name. You can get the minimum response time of this service since deployment. getAvgServiceResponseTime ( p1:string ) The p1 parameter is the service name. You can get the average response time of this service since deployment. getOperationRequestCount ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the total number of requests received by this operation since the time its service was deployed, using this operation. getOperationResponseCount ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the total number of responses sent by this operation since the time its service was deployed, using this operation. getOperationFaultCount ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the total number of fault responses sent by this operation since the time its service was deployed, using this operation. getMaxOperationResponseTime ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the maximum response time of this operation since deployment. getMinOperationResponseTime ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the minimum response time of this operation since deployment. getAvgOperationResponseTime ( p1:string , p2:string ) The p1 parameter is the service name. The p2 parameter is the operation name. You can get the average response time of this operation since deployment. {width=\"600\"}","title":"Using the StatisticsAdmin MBean"},{"location":"administer-and-observe/jmx_monitoring/#using-the-datasource-mbean","text":"If you have JMX enabled for a datasource connected to the product , you can monitor the performance of the datasource using this MBean. The DataSource MBean will be listed as shown below. {width=\"600\"} Example: If you have JMX enabled for the default Carbon datasource in the master-datasources.xml. file, the JDBC connection pool parameters that are configured for the Carbon datasource will be listed as attributes as shown below. See the performance tuning guide for instructions on how these parameters are configured for a datasource. {width=\"650\"}","title":"Using the DataSource MBean"},{"location":"administer-and-observe/jmx_monitoring/#using-product-specific-mbeans","text":"The WSO2 product that you are using may have product-specific MBeans enabled for monitoring and managing specific functions. See the documentation for your product for detailed instructions on such product-specific MBeans.","title":"Using product-specific MBeans"},{"location":"administer-and-observe/jmx_monitoring/#monitoring-a-wso2-product-with-jolokia","text":"Jolokia is a JMX-HTTP bridge, which is an alternative to JSR-160 connectors. It is an agent-based approach that supports many platforms. In addition to basic JMX operations, it enhances JMX monitoring with unique features like bulk requests and fine-grained security policies. Follow the steps below to use Jolokia to monitor a WSO2 product. Download Jolokia OSGi Agent . (These instructions are tested with the Jolokia OSGI Agent version 1.3.6 by downloading the jolokia-osgi-1.3.6.jar file.) Add it to the <PRODUCT-HOME>/repository/components/dropins/ directory. !!! tip In WSO2 EI, add it to the ` <EI-HOME> ` ` /dropins/ ` directory. Start the WSO2 product server. Once the server starts, you can read MBeans using Jolokia APIs. Following are a few examples. List all available MBeans: http://localhost:9763/jolokia/list (Change the appropriate hostname and port accordingly.) WSO2 ESB MBean: http://localhost:9763/jolokia/read/org.apache.synapse:Name=https-sender,Type=PassThroughConnections/ActiveConnections Reading Heap Memory: http://localhost:9763/jolokia/read/java.lang:type=Memory/HeapMemoryUsage For more information on the JMX MBeans that are available in WSO2 products, see Monitoring a WSO2 product with JConsole .","title":"Monitoring a WSO2 product with Jolokia"},{"location":"administer-and-observe/jmx_monitoring/#mbeans-for-wso2-ei","text":"When JMX is enabled, WSO2 EI exposes a number of management resources as JMX Management Beans (MBeans) that can be used for managing and monitoring the running server. When you start JConsole, you can monitor these MBeans from the MBeans tab. While some of these MBeans ( ServerAdmin and DataSource ) are common to all WSO2 products, some MBeans are specific to WSO2 EI. Tip The common MBeans are explained in detail in the WSO2 Administration Guide . Listed below are the MBeans that are specific to WSO2 EI. For details on using JMX based mediation flow statistics in your EI server and detailed explanation of the relevant MBeans, see Monitoring JMX Based Statistics . {height=\"400\"} This section summarizes the attributes and operations available for the following WSO2 EI specific MBeans: Connection MBeans Latency MBeans Threading MBeans Transport MBeans Broker-specific MBeans","title":"MBeans for WSO2 EI"},{"location":"administer-and-observe/jmx_monitoring/#connection-mbeans","text":"These MBeans provide connection statistics for the HTTP and HTTPS transports. You can view the following Connection MBeans: org.apache.synapse/PassThroughConnections/http-listener org.apache.synapse/PassThroughConnections/http-sender org.apache.synapse/PassThroughConnections/https-listener org.apache.synapse/PassThroughConnections/https-sender Attributes Attribute Name Description ActiveConnections Number of currently active connections. ActiveConnectionsPerHosts A map of the number of connections against hosts. LastXxxConnections The number of connections created during last Xxx time period. RequestSizesMap A map of the number of requests against their sizes. ResponseSizesMap A map of the number of responses against their sizes. LastResetTime The time when the connection-statistic recordings were last reset. Operations Operation Name Description reset() Clear recorded connection statistics and restart recording.","title":"Connection MBeans"},{"location":"administer-and-observe/jmx_monitoring/#latency-mbeans","text":"This view provides statistics of the latencies from all backend services connected through the HTTP and HTTPS transports. These statistics are provided as an aggregate value. You can view the following Latency MBeans: org.apache.synapse/PassthroughLatencyView/nio-http-http org.apache.synapse/PassthroughLatencyView/nio-https-https Attributes Attribute Name Description AllTimeAvgLatency Average latency since the latency recording was last reset. LastxxxAvgLatency Average latency for last xxx time period. For example, LastHourAvgLatency returns the average latency for the last hour. LastResetTime The time when the latency-statistic recordings were last reset. Operations Operation Name Description reset() Clear recorded latency statistics and restart recording.","title":"Latency MBeans"},{"location":"administer-and-observe/jmx_monitoring/#threading-mbeans","text":"These MBeans are only available in the NHTTP transport and not in the default Pass-Through transport. You can view the following Threading MBeans: org.apache.synapse/Threading/HttpClientWorker org.apache.synapse/Threading/HttpServerWorker Attributes Attribute Name Description TotalWorkerCount Total worker threads related to this server/client. AvgUnblockedWorkerPercentage Time-averaged unblocked worker thread percentage. AvgBlockedWorkerPercentage Time-averaged blocked worker thread percentage. LastXxxBlockedWorkerPercentage Blocked worker thread percentage averaged for last Xxx time period. DeadLockedWorkers Number of deadlocked worker threads since last statistics reset. LastResetTime The time the thread statistic recordings were last reset. Operations Operation Name Description reset() Clear recorded thread statistic and restart recording.","title":"Threading MBeans"},{"location":"administer-and-observe/jmx_monitoring/#transport-mbeans","text":"For each transport listener and sender enabled in WSO2 EI, there will be an MBean under the org.apache.axis2/Transport domain. For example, when the JMS transport is enabled, the following MBean will be exposed: org.apache.axis2/Transport/jms-sender- n You can also view the following Transport MBeans: org.apache.synapse/Transport/passthru-http-receiver org.apache.synapse/Transport/passthru-http-sender org.apache.synapse/Transport/passthru-https-receiver org.apache.synapse/Transport/passthru-https-sender Attributes Attribute Name Description ActiveThreadCount Threads active in this transport listener/sender. AvgSizeReceived The average size of received messages. AvgSizeSent The average size of sent messages. BytesReceived The number of bytes received through this transport. BytesSent The number of bytes sent through this transport. FaultsReceiving The number of faults encountered while receiving. FaultsSending The number of faults encountered while sending. LastResetTime The time when the last transport listener/sender statistic recording was reset. MaxSizeReceived Maximum message size of received messages. MaxSizeSent Maximum message size of sent messages. MetricsWindow Time difference between current time and last reset time in milliseconds. MinSizeReceived Minimum message size of received messages. MinSizeSent Minimum message size of sent messages. MessagesReceived The total number of messages received through this transport. MessagesSent The total number of messages sent through this transport. QueueSize The number of messages currently queued. Messages get queued if all the worker threads in this transport thread pool are busy. ResponseCodeTable The number of messages sent against their response codes. TimeoutsReceiving Message receiving timeout. TimeoutsSending Message sending timeout. Operations Operation Name Description start() Start this transport listener/sender. stop() Stop this transport listener/sender. resume() Resume this transport listener/sender which is currently paused. resetStatistics() Clear recorded transport listener/sender statistics and restart recording. pause() Pause this transport listener/sender which has been started. maintenenceShutdown(long gracePeriod) Stop processing new messages, and wait the specified maximum time for in-flight requests to complete before a controlled shutdown for maintenence.","title":"Transport MBeans"},{"location":"administer-and-observe/jmx_monitoring/#disabling-the-jmx-thread-view","text":"Dumping JMX threads is an expensive operation that affects the CPU consumption when many threads are being processed at the same time. WSO2 EI has enabled thread dumping by default. Therefore, to avoid dumping all the threads here , you can configure the property given below. Info It is recommended not to dump the thread especially when you have enabled WSO2 EI analytics in a production environment. Open the <EI_HOME>/ synapse.properties file. Add the following property to the file and save the file. synapse.jmx.thread.view.enabled=false","title":"Disabling the\u00a0JMX thread view"},{"location":"administer-and-observe/masking_sensitive_info_in_logs/","text":"Masking Sensitive Information in Logs \u00b6 There can be business sensitive information that are added to logs in the WSO2 product console and/or WSO2 Carbon log files. When these logs are analyzed, the information is exposed to those who check this. To avoid this potential security pitfall, users can mask sensitive information from the log file at the time of logging. In this feature, you can define patterns that need to be masked from the logs. This is particularly useful in the case of credit card numbers, access tokens, etc. To configure this feature, follow the instructions below. Enabling log masking \u00b6 Open the <PRODUCT_HOME>/repository/conf/log4j.properties file in a text editor. Uncomment or add the following property under CarbonConsoleAppender or CarbonDailyRollingFileAppender . log4j.appender.CARBON_CONSOLE.maskingPatternFile=path-to-masking-patterns The path-to-masking-patterns value must be a absolute path to the masking patterns file. In this file, each pattern is defined as key, value pairs (patten-name=pattern). Please refer to the next section for information on this file. The following is a sample configuration for the above property. log4j.appender.CARBON_CONSOLE.maskingPatternFile=/home/conf/masking-patterns.properties For the DailyRollingFileAppender value, the above property would be similar to the following. log4j.appender.CARBON_LOGFILE.maskingPatternFile=path-to-masking-patterns In this case, you can define separate masking pattern files for console appender and file appender. (or configure only one property.) The masking pattern file \u00b6 The masking pattern file is a property file that can contain one or more masking patterns. The following is a sample configuration that showcases how to mask the credit card numbers from the logs. masking.pattern.sample.CREDIT_CARD_VISA=4[0-9]{6,}$ masking.pattern.sample.CREDIT_CARD_MASTER=(?:5[1-5][0-9]{2}|222[1-9]|22[3-9][0-9]|2[3-6][0-9]{2}|27[01][0-9]|2720)[0-9]{12} masking.pattern.sample.CREDIT_CARD_AMEX=[34|37][0-9]{14}$ With this configuration, each log line is checked for all the configured patterns. If any match is found, it is masked with \u2018*****\u2019. Note Note : If the pattern file that is configured in the log4j.properties file is not found, a default property file will be used ( wso2-log-masking.properties ). If there are no any patterns defined in the file, no masking happens. Warning Important : There can be a performance impact when using this feature with many masking patterns since each log line is matched with each of the patterns. So it is highly advisable to only use the most necessary patterns.","title":"Masking Sensitive Information in Logs"},{"location":"administer-and-observe/masking_sensitive_info_in_logs/#masking-sensitive-information-in-logs","text":"There can be business sensitive information that are added to logs in the WSO2 product console and/or WSO2 Carbon log files. When these logs are analyzed, the information is exposed to those who check this. To avoid this potential security pitfall, users can mask sensitive information from the log file at the time of logging. In this feature, you can define patterns that need to be masked from the logs. This is particularly useful in the case of credit card numbers, access tokens, etc. To configure this feature, follow the instructions below.","title":"Masking Sensitive Information in Logs"},{"location":"administer-and-observe/masking_sensitive_info_in_logs/#enabling-log-masking","text":"Open the <PRODUCT_HOME>/repository/conf/log4j.properties file in a text editor. Uncomment or add the following property under CarbonConsoleAppender or CarbonDailyRollingFileAppender . log4j.appender.CARBON_CONSOLE.maskingPatternFile=path-to-masking-patterns The path-to-masking-patterns value must be a absolute path to the masking patterns file. In this file, each pattern is defined as key, value pairs (patten-name=pattern). Please refer to the next section for information on this file. The following is a sample configuration for the above property. log4j.appender.CARBON_CONSOLE.maskingPatternFile=/home/conf/masking-patterns.properties For the DailyRollingFileAppender value, the above property would be similar to the following. log4j.appender.CARBON_LOGFILE.maskingPatternFile=path-to-masking-patterns In this case, you can define separate masking pattern files for console appender and file appender. (or configure only one property.)","title":"Enabling log masking"},{"location":"administer-and-observe/masking_sensitive_info_in_logs/#the-masking-pattern-file","text":"The masking pattern file is a property file that can contain one or more masking patterns. The following is a sample configuration that showcases how to mask the credit card numbers from the logs. masking.pattern.sample.CREDIT_CARD_VISA=4[0-9]{6,}$ masking.pattern.sample.CREDIT_CARD_MASTER=(?:5[1-5][0-9]{2}|222[1-9]|22[3-9][0-9]|2[3-6][0-9]{2}|27[01][0-9]|2720)[0-9]{12} masking.pattern.sample.CREDIT_CARD_AMEX=[34|37][0-9]{14}$ With this configuration, each log line is checked for all the configured patterns. If any match is found, it is masked with \u2018*****\u2019. Note Note : If the pattern file that is configured in the log4j.properties file is not found, a default property file will be used ( wso2-log-masking.properties ). If there are no any patterns defined in the file, no masking happens. Warning Important : There can be a performance impact when using this feature with many masking patterns since each log line is matched with each of the patterns. So it is highly advisable to only use the most necessary patterns.","title":"The masking pattern file"},{"location":"administer-and-observe/message_monitoring_with_tcpmon/","text":"Message Monitoring with TCPMon \u00b6 The most common usage of TCPMon is as an intermediary, which monitors the communication between the client (front end) and the back end server. That is, the messages sent from the client are received by the intermediary instead of the back end server. These messages are then forwarded to the back end server from the intermediary. Monitoring Messages between Client and Server \u00b6 The following diagram depicts a typical communication between the front end client and the back end server. 80 is the listening port of the back end server which receives the messages from the client: The following diagram depicts how TCPMon is placed between the client and the server in order to monitor the messages. 8081 is the listening port in TCPMon which receives the messages from the client instead of the back end server: Note As an intermediary, TCPMon only receives messages and forwards them to the back end server. Therefore, it is a safe tool to be used for debugging purposes. Info Note that TCPMon cannot be used to view messages transferred over https protocol. To monitor messages from client to server using TCPMon: Start TCPMon. Follow the instructions on Starting TCPMon . Give 8081 (the listening port of TCPMon) in the Listen Port field (This could be any unused port in your local machine). Give the address of the back end server as the target hostname. For example, if you are monitoring messages sent to www.apache.org , enter this web address as the hostname. Give 80 as the target port, which is the listening port of www.apache.org. Click Add to save the setting. Now, p oint the browser to 'localhost:8081' instead of www.apache.org . A new tab in TCPMon will indicate the 8081 port. You can view the requests and responses passing through TCPMon as shown below. The options at the bottom of the screen can be used to have the messages in XML format (useful in debugging Web services), to save and resend the messages and also to switch the layout of the message windows.","title":"Message Monitoring with TCPMon"},{"location":"administer-and-observe/message_monitoring_with_tcpmon/#message-monitoring-with-tcpmon","text":"The most common usage of TCPMon is as an intermediary, which monitors the communication between the client (front end) and the back end server. That is, the messages sent from the client are received by the intermediary instead of the back end server. These messages are then forwarded to the back end server from the intermediary.","title":"Message Monitoring with TCPMon"},{"location":"administer-and-observe/message_monitoring_with_tcpmon/#monitoring-messages-between-client-and-server","text":"The following diagram depicts a typical communication between the front end client and the back end server. 80 is the listening port of the back end server which receives the messages from the client: The following diagram depicts how TCPMon is placed between the client and the server in order to monitor the messages. 8081 is the listening port in TCPMon which receives the messages from the client instead of the back end server: Note As an intermediary, TCPMon only receives messages and forwards them to the back end server. Therefore, it is a safe tool to be used for debugging purposes. Info Note that TCPMon cannot be used to view messages transferred over https protocol. To monitor messages from client to server using TCPMon: Start TCPMon. Follow the instructions on Starting TCPMon . Give 8081 (the listening port of TCPMon) in the Listen Port field (This could be any unused port in your local machine). Give the address of the back end server as the target hostname. For example, if you are monitoring messages sent to www.apache.org , enter this web address as the hostname. Give 80 as the target port, which is the listening port of www.apache.org. Click Add to save the setting. Now, p oint the browser to 'localhost:8081' instead of www.apache.org . A new tab in TCPMon will indicate the 8081 port. You can view the requests and responses passing through TCPMon as shown below. The options at the bottom of the screen can be used to have the messages in XML format (useful in debugging Web services), to save and resend the messages and also to switch the layout of the message windows.","title":"Monitoring Messages between Client and Server"},{"location":"administer-and-observe/monitoring_access_logs/","text":"Monitoring Access Logs \u00b6 HTTP access logs help you monitor information such as the persons who access the product, how many hits are received, what the errors are, etc. This information is useful for troubleshooting errors. All WSO2 products can enable access logs for the HTTP servlet transport. This servlet transport works on 9443/9763 ports, and it receives admin/operation requests. Therefore, access logs for the servert transport is useful for analysing operational/admin-level access details. Additionally, in products such as WSO2 Enterprise Service Bus, WSO2 Enterprise Integrator (and WSO2 API Manager), access logs can be generated for the PassThrough and NIO transports. The PassThrough/NIO transports works on 8280/8243 ports and is used for API/Service invocations. By default, the access logs from both the servlet transport and the PassThrough/NIO transports are written to a common access log file located in the <EI_HOME>/repository/logs/ directory. See the topics given below to configure the default behaviour of HTTP access logs in the ESB. Configuring access logs for the HTTP servlet transport Configuring access logs for the PassThrough and NIO transports (Service/API invocation) Supported log pattern formats for the PassThrough and NIO transports Configuring access logs for the HTTP servlet transport \u00b6 Note Note that access logs for the HTTP servlet transport logs details of the request as well as the response on a single log line. As the runtime of WSO2 products is based on Apache Tomcat, you can use the Access_Log_Valve variable in Tomcat as explained below to configure access logs for the HTTP servlet transport: Open the \\< EI_HOME>/conf/tomcat/catalina-server.xml file (which is the server descriptor file for the embedded Tomcat integration) Customize the attributes for the Access_Log_Valve variable shown below. <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"${carbon.home}/repository/logs\" prefix=\"http_access_management_console_\" suffix=\".log\" pattern=\"combined\" /> The attributes that are used by default are explained below. See the descriptions of the Tomcat-supported Access Log Valve attributes and customize the required values. directory The path to the directory that will store the access log file. By default, this location is set to \" ${carbon.home}/repository/logs\" in all WSO2 products. prefix The prefix added to the log file's name. By default, this is \"http_access_management_console_\". suffix The suffix added to the log file's name. By default, this is \".log\". pattern The attribute defines the format for the log pattern, which consists of the information fields from the requests and responses that should be logged. The pattern format is created using the following attributes: A standard value to represent a particular string. For example, \"%h\" represents the remote host name in the request. See the list of string replacement values supported by the Tomcat valve . %{xxx}i is used to represent the header in the incoming request (xxx=header value). %{xxx}o is used to represents the header in the outgoing request (xxx=header value). While you can use the above attributes to define a custom pattern, the standard patterns shown below can be used. common ( Apache common log pattern ): pattern=%h %l %u %t \"%r\" %s %b combined ( Apache combined log pattern ): pattern=%h %l %u %t \"%r\" %s %b \"%{Referer}i\" \"%{User-Agent}i\" Note that, by default, the \"combined\" pattern is enabled in the ESB. Restart the server. According to the default configurations, a log file named http_access_management_console _ .{DATE}.log is created inside the \\< EI_HOME>/repository/logs directory. The log is rotated on a daily basis. Configuring access logs for the PassThrough and NIO transports (Service/API invocation) \u00b6 Note Note that access logs for the PassThrough/NIO transports log the request and the response on two separate log lines. By default, access logs related to service/API invocation are disabled for performance reasons in the above products. You should enable these access log only for troubleshooting errors. Follow the steps given below to enable access logs for the PassThrough transport: Change the log level from WARN to INFO for the following entry in the <EI_HOME>/conf/log4j.properties configuration file. log4j.logger.org.apache.synapse.transport.http.access=INFO You can customize the format of this access log by changing the following property values in the <EI_HOME>/conf/access-log.properties configuration file. If this file does not exist in the product by default, you can create a new file with the following parameters. access_log_directory Add this property ONLY if you want to change the default location of the log file. By default, the product is configured to store access logs in the <EI_HOME>/repository/logs directory. access_log_prefix The prefix added to the log file's name. The default value is as follows: access_log_prefix=http_access_ access_log_suffix The suffix added to the log file's name. The default value is as follows: access_log_suffix=. log access_log_file_date_format The date format used in access logs. The default value is as follows: access_log_file_date_format=yyyy-MM-dd access_log_pattern The attribute defines the format for the log pattern, which consists of the information fields from the requests and responses that should be logged. The pattern format is created using the following attributes: A standard value to represent a particular string. For example, \"%h\" represents the remote host name in the request. Note that all the string replacement values supported by Tomcat are NOT supported for the PassThrough transport's access logs. The list of supported values are given below . %{xxx}i is used to represent the header in the incoming request (xxx=header value). %{xxx}o is used to represents the header in the outgoing request (xxx=header value). While you can use the above attributes to define a custom pattern, the standard patterns shown below can be used. common ( Apache common log pattern ): access_log_pattern=%h %l %u %t \"%r\" %s %b combined ( Apache combined log pattern ): access_log_pattern=%h %l %u %t \"%r\" %s %b \"%{Referer}i\" \"%{User-Agent}i\" By default, a modified version of the Apache combined log format is enabled in the ESB as shown below. Note that the \"X-Forwarded-For\" header is appended to the beginning of the usually combined log format. This correctly identifies the original node that sent the request (in situations where requests go through a proxy such as a load balancer). The \"X-Forwarded-For\" header must be present in the incoming request for this to be logged. access_log_pattern=%{X-Forwarded-For}i %h %l %u %t \\ \"%r \\\" %s %b \\\" %{Referer}i \\\" \\\" %{User-Agent}i \\\" Restart the server. Invoke a proxy service or REST API that is deployed in the ESB. For testing purposes, use the artifacts in the quick start guide . The access log file for the service/API will be created in the <EI_HOME>/repository/logs directory. The default name of the log file is http_access_.log . !!! tip Note that there will be delay in printing the logs to the log file. Supported log pattern formats for the PassThrough and NIO transports \u00b6 Attribute Description %a Remote IP address %A Local IP address %b Bytes sent, excluding HTTP headers, or '-' if zero %B Bytes sent, excluding HTTP headers %c Cookie value %C Accept header %e Accept Encoding %E Transfer Encoding %h Remote host name (or IP address if enableLookups for the connector is false) %l Remote logical username from identd (always returns '-') %L Accept Language %k Keep Alive %m Request method (GET, POST, etc.) %n Content Encoding %r Request Element %s HTTP status code of the response %S Accept Chatset %t Date and time, in Common Log Format %T Time taken to process the request in seconds. %u Remote user that was authenticated (if any), else '-' %U Requested URL path %v Local server name %V Vary Header %x Connection Header %Z Server Header","title":"Monitoring Access Logs"},{"location":"administer-and-observe/monitoring_access_logs/#monitoring-access-logs","text":"HTTP access logs help you monitor information such as the persons who access the product, how many hits are received, what the errors are, etc. This information is useful for troubleshooting errors. All WSO2 products can enable access logs for the HTTP servlet transport. This servlet transport works on 9443/9763 ports, and it receives admin/operation requests. Therefore, access logs for the servert transport is useful for analysing operational/admin-level access details. Additionally, in products such as WSO2 Enterprise Service Bus, WSO2 Enterprise Integrator (and WSO2 API Manager), access logs can be generated for the PassThrough and NIO transports. The PassThrough/NIO transports works on 8280/8243 ports and is used for API/Service invocations. By default, the access logs from both the servlet transport and the PassThrough/NIO transports are written to a common access log file located in the <EI_HOME>/repository/logs/ directory. See the topics given below to configure the default behaviour of HTTP access logs in the ESB. Configuring access logs for the HTTP servlet transport Configuring access logs for the PassThrough and NIO transports (Service/API invocation) Supported log pattern formats for the PassThrough and NIO transports","title":"Monitoring Access Logs"},{"location":"administer-and-observe/monitoring_access_logs/#configuring-access-logs-for-the-http-servlet-transport","text":"Note Note that access logs for the HTTP servlet transport logs details of the request as well as the response on a single log line. As the runtime of WSO2 products is based on Apache Tomcat, you can use the Access_Log_Valve variable in Tomcat as explained below to configure access logs for the HTTP servlet transport: Open the \\< EI_HOME>/conf/tomcat/catalina-server.xml file (which is the server descriptor file for the embedded Tomcat integration) Customize the attributes for the Access_Log_Valve variable shown below. <Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"${carbon.home}/repository/logs\" prefix=\"http_access_management_console_\" suffix=\".log\" pattern=\"combined\" /> The attributes that are used by default are explained below. See the descriptions of the Tomcat-supported Access Log Valve attributes and customize the required values. directory The path to the directory that will store the access log file. By default, this location is set to \" ${carbon.home}/repository/logs\" in all WSO2 products. prefix The prefix added to the log file's name. By default, this is \"http_access_management_console_\". suffix The suffix added to the log file's name. By default, this is \".log\". pattern The attribute defines the format for the log pattern, which consists of the information fields from the requests and responses that should be logged. The pattern format is created using the following attributes: A standard value to represent a particular string. For example, \"%h\" represents the remote host name in the request. See the list of string replacement values supported by the Tomcat valve . %{xxx}i is used to represent the header in the incoming request (xxx=header value). %{xxx}o is used to represents the header in the outgoing request (xxx=header value). While you can use the above attributes to define a custom pattern, the standard patterns shown below can be used. common ( Apache common log pattern ): pattern=%h %l %u %t \"%r\" %s %b combined ( Apache combined log pattern ): pattern=%h %l %u %t \"%r\" %s %b \"%{Referer}i\" \"%{User-Agent}i\" Note that, by default, the \"combined\" pattern is enabled in the ESB. Restart the server. According to the default configurations, a log file named http_access_management_console _ .{DATE}.log is created inside the \\< EI_HOME>/repository/logs directory. The log is rotated on a daily basis.","title":"Configuring access logs for the HTTP servlet transport"},{"location":"administer-and-observe/monitoring_access_logs/#configuring-access-logs-for-the-passthrough-and-nio-transports-serviceapi-invocation","text":"Note Note that access logs for the PassThrough/NIO transports log the request and the response on two separate log lines. By default, access logs related to service/API invocation are disabled for performance reasons in the above products. You should enable these access log only for troubleshooting errors. Follow the steps given below to enable access logs for the PassThrough transport: Change the log level from WARN to INFO for the following entry in the <EI_HOME>/conf/log4j.properties configuration file. log4j.logger.org.apache.synapse.transport.http.access=INFO You can customize the format of this access log by changing the following property values in the <EI_HOME>/conf/access-log.properties configuration file. If this file does not exist in the product by default, you can create a new file with the following parameters. access_log_directory Add this property ONLY if you want to change the default location of the log file. By default, the product is configured to store access logs in the <EI_HOME>/repository/logs directory. access_log_prefix The prefix added to the log file's name. The default value is as follows: access_log_prefix=http_access_ access_log_suffix The suffix added to the log file's name. The default value is as follows: access_log_suffix=. log access_log_file_date_format The date format used in access logs. The default value is as follows: access_log_file_date_format=yyyy-MM-dd access_log_pattern The attribute defines the format for the log pattern, which consists of the information fields from the requests and responses that should be logged. The pattern format is created using the following attributes: A standard value to represent a particular string. For example, \"%h\" represents the remote host name in the request. Note that all the string replacement values supported by Tomcat are NOT supported for the PassThrough transport's access logs. The list of supported values are given below . %{xxx}i is used to represent the header in the incoming request (xxx=header value). %{xxx}o is used to represents the header in the outgoing request (xxx=header value). While you can use the above attributes to define a custom pattern, the standard patterns shown below can be used. common ( Apache common log pattern ): access_log_pattern=%h %l %u %t \"%r\" %s %b combined ( Apache combined log pattern ): access_log_pattern=%h %l %u %t \"%r\" %s %b \"%{Referer}i\" \"%{User-Agent}i\" By default, a modified version of the Apache combined log format is enabled in the ESB as shown below. Note that the \"X-Forwarded-For\" header is appended to the beginning of the usually combined log format. This correctly identifies the original node that sent the request (in situations where requests go through a proxy such as a load balancer). The \"X-Forwarded-For\" header must be present in the incoming request for this to be logged. access_log_pattern=%{X-Forwarded-For}i %h %l %u %t \\ \"%r \\\" %s %b \\\" %{Referer}i \\\" \\\" %{User-Agent}i \\\" Restart the server. Invoke a proxy service or REST API that is deployed in the ESB. For testing purposes, use the artifacts in the quick start guide . The access log file for the service/API will be created in the <EI_HOME>/repository/logs directory. The default name of the log file is http_access_.log . !!! tip Note that there will be delay in printing the logs to the log file.","title":"Configuring access logs for the PassThrough and NIO transports (Service/API invocation)"},{"location":"administer-and-observe/monitoring_access_logs/#supported-log-pattern-formats-for-the-passthrough-and-nio-transports","text":"Attribute Description %a Remote IP address %A Local IP address %b Bytes sent, excluding HTTP headers, or '-' if zero %B Bytes sent, excluding HTTP headers %c Cookie value %C Accept header %e Accept Encoding %E Transfer Encoding %h Remote host name (or IP address if enableLookups for the connector is false) %l Remote logical username from identd (always returns '-') %L Accept Language %k Keep Alive %m Request method (GET, POST, etc.) %n Content Encoding %r Request Element %s HTTP status code of the response %S Accept Chatset %t Date and time, in Common Log Format %T Time taken to process the request in seconds. %u Remote user that was authenticated (if any), else '-' %U Requested URL path %v Local server name %V Vary Header %x Connection Header %Z Server Header","title":"Supported log pattern formats for the PassThrough and NIO transports"},{"location":"administer-and-observe/monitoring_logs/","text":"Monitoring Logs \u00b6 Note This page is currently restricted to confluence administrators. There are two ways of configuring log4j logging in WSO2 Carbon products: you can manually edit the log4j.properties file or configure logging through the management console. Configuration made through the management console can be persisted in the WSO2 registry so that it is available even after the server restarts. There is also an option to restore the original Log4j configuration from the log4j.properties file using the management console. However, if you modify the log4j.properties file and restart the server, the earlier log4j configuration persisted in the registry is overwritten. Info For information on viewing the contents of Application Logs and System Logs in your WSO2 product, see View and Download Logs in the WSO2 Administration Guide. In the Integration profile of WSO2 EI, you can enable wire logs to monitor HTTP messages that flow through the ESB. Please note that wire logs should be enabled for the troubleshooting purposes only. Running productions systems with wire logs enabled is not recommended. Enabling wire logs \u00b6 Follow the steps given below to enable wire logs for the Integration profile: Go to the <EI_HOME>/conf/ directory and open the log4j.properties file. Enable the following entry in the file. This is commented out by default. log4j.logger.org.apache.synapse.transport.http.wire=DEBUG Run a mediation sequence \u00b6 ...... Reading wire logs \u00b6 In order to read the wire logs, you must first identify message direction. DEBUG - wire >> represents the message coming into the ESB from the wire. DEBUG - wire \\<\\< represents the message that goes to the wire from the ESB. There are two incoming messages and two outgoing messages in the above log. The f irst part of the message log contains the HTTP headers and is followed by the message payload. As shown in this example, wire logs are very useful when it comes to troubleshooting unexpected issues that occur while integrating systems. You can verify whether a message payload is correctly going out from the ESB, http headers like Content-Type is properly set in the outgoing message, etc. by looking at the wire logs. Logging is one of the most important aspects of a production-grade server. A properly configured logging system is vital for identifying errors, security threats, and usage patterns. See the following topics for details: Log types in WSO2 products Configuring products for log monitoring Setting the Log4j log level Managing log growth Managing the growth of Carbon logs Limiting the size of Carbon log files Limiting the size of audit log files Monitoring logs Log types in WSO2 products \u00b6 Listed below are the various log types that are used in WSO2 products. Info Separate log files are created for each of the log types given below in the <PRODUCT_HOME>/repository/logs directory . ** Carbon logs** : All WSO2 products are shipped with log4j logging capabilities that generate administrative activities and server side logs. The Carbon log ( wso2carbon.log ) is a log file that covers all the management features of a product. Carbon logs are configured in t he log4j.properties file (stored in the <PRODUCT_HOME>/repository/conf directory) . !!! info **Java logging and Log4j integration:** I n addition to the logs from libraries that use Log4j, all logs from libraries (such as, Tomcat, Hazelcast and more) that use Java logging framework are also visible in the same log files. That is, when Java logging is enabled in Carbon, only the Log4j appenders will write to the log files. If the Java Logging Handlers have logs, these logs will be delegated to the log events of the corresponding Log4j appenders. A Pub/Sub registry pattern implementation has been used in the latter mentioned scenario to plug the handlers and appenders. The following default log4j appenders in the ` log4j.properties ` file are used for this implementation: - ` org.wso2.carbon.logging.appenders.CarbonConsoleAppender ` - ` org.wso2.carbon.logging.appenders.CarbonDailyRollingFileAppender ` Audit logs: Audit logs are used for tracking the sequence of actions that affect a particular task carried out on the server. These are also configured in t he log4j.properties file. HTTP access logs: HTTP requests/responses are logged in access logs to monitor the activities related to an application's usage. These logs are configured in the catalina-server.xml file (stored in the \\< PRODUCT_HOME>/repository/conf/tomcat/ directory). Patch logs: These logs contain details related to patches applied to the product. Patch logs cannot be customized. See WSO2 Patch Application Process for more information. Service/Event logs: These are logs that are enabled in some WSO2 products for tracing services and events using a separate log file ( wso2-<product>-trace.log ). If server/event tracing logs are used in your WSO2 product, you can configure them in the log4j.properties file. Product-specific logs: Each WSO2 product may generate other log files in addition to the Carbon logs, Audit logs, HTTP access logs, Patch logs and Service/Event logs. See the product's documentation for descriptions of these log files and instructions on how to configure and use them. Configuring products for log monitoring \u00b6 See the following information on configuring Carbon logs , Audit logs, HTTP access logs , and Service/Event logs for your WSO2 product. ****Configuring Carbon logs**** You can easily configure Carbon logs using the management console of your product, or you can manually edit the log4j.properties file. It is recommended to use the management console to configure logging because all changes made to log4j through the management console persists in the WSO2 Registry. Therefore, those changes will be available after the server restarts and will get priority over what is defined in the log4j.properties file. Also, note that the logging configuration you define using the management console will apply at run time. However, if you modify the log4j.properties file and restart the server, the earlier log4j configuration that persisted in the registry will be overwritten. T here is also an option in the management console to restore the original log4j configuration from the log4j.properties file. The log levels that can be configured are listed below . Identifying forged messages: T he log pattern d efines the output format of the log file. From Carbon 4.4.3 onwards, the conversion character 'K' can be used in the pattern layout to log a UUID. For example, the log pattern can be [%K] [%T] [%S] [%d] %P%5p {%c} - %x %m {%c}%n, where [%K] is the UUID. The UUID can be used for identifying forged messages in the log. By default, the UUID will be generated every time the server starts . If required, you can configure the UUID regeneration period by manually adding the following property to the log4j.properties file (stored in the <PRODUCT_HOME>/repository/conf directory): log4j.appender.CARBON_LOGFILE.layout.LogUUIDUpdateInterval=<number_of_hours> !!! info **Carbon logs in [WSO2 Data Analytics Server](http://wso2.com/smart-analytics) (WSO2 DAS)** Carbon logs are configured in the ` log4j.properties ` file (stored in the ` <PRODUCT_HOME>/repository/conf ` directory) for all WSO2 products. However, WSO2 DAS generates some additional Carbon logs (which will be stored in the same [Carbon log file](#MonitoringLogs-Carbon_logs) ) that should be separately configured by creating a new ` log4j.properties ` file in the ` <DAS_HOME>/repository/conf/analytics/spark ` directory. **Note:** To create this file, you need to rename the ` log4j.properties.template ` file that is available in the ` <DAS_HOME>/repository/conf/analytics/spark ` directory to l ` og4j.properties ` . See the following topics for instructions: Configuring Log4j Properties Configuring the Log Provider Configuring Audit logs Audit logs are enabled in WSO2 products by default. You can change the following default configuration by manually updating the the log4j.properties file. The log levels that can be configured are listed below . log4j.logger.AUDIT_LOG=INFO, AUDIT_LOGFILE # Appender config to AUDIT_LOGFILE log4j.appender.AUDIT_LOGFILE=org.wso2.carbon.utils.logging.appenders.CarbonDailyRollingFileAppender log4j.appender.AUDIT_LOGFILE.File=${carbon.home}/repository/logs/audit.log log4j.appender.AUDIT_LOGFILE.Append=true log4j.appender.AUDIT_LOGFILE.layout=org.wso2.carbon.utils.logging.TenantAwarePatternLayout log4j.appender.AUDIT_LOGFILE.layout.ConversionPattern=[%d] %P%5p {%c}- %x %m %n log4j.appender.AUDIT_LOGFILE.layout.TenantPattern=%U%@%D [%T] [%S] log4j.appender.AUDIT_LOGFILE.threshold=INFO log4j.additivity.AUDIT_LOG=false Configuring HTTP access logs See HTTP Access Logging for instructions on how to configure and use HTTP access logs. Configuring Service/Event tracing logs A separate log file for tracing services/events are enabled for certain WSO2 products in the log4j.properties file using a specific appender. These logs are published to a file named wso2-<product>-trace.log . See the table given below for instructions relevant to your product: Product Description WSO2 DAS Event tracing logs are enabled in WSO2 DAS using the EVENT_TRACE_LOGGER appender as shown below (Click Message tracing log configuration ). This log file stores logs related to events in WSO2 DAS. By default, this appender uses the root log level, which is INFO. You can override the root log level by giving a specific log level for the appender as explained here . Message tracing log configuration log4j. category . EVENT_TRACE_LOGGER =INFO, EVENT_TRACE_APPENDER, EVENT_TRACE_MEMORYAPPENDER log4j. additivity . EVENT_TRACE_LOGGER = false log4j. appender . EVENT_TRACE_APPENDER =org. apache . log4j . DailyRollingFileAppender log4j. appender . EVENT_TRACE_APPENDER . File = {carbon.<span class=\"fu\">home</span>}/repository/logs/ {carbon.<span class=\"fu\">home</span>}/repository/logs/ {instance. log }/wso2-das-trace${instance. log }. log log4j. appender . EVENT_TRACE_APPENDER . Append = true log4j. appender . EVENT_TRACE_APPENDER . layout =org. apache . log4j . PatternLayout log4j. appender . EVENT_TRACE_APPENDER . layout . ConversionPattern =%d{HH ss,SSS} [%X{ip}-%X{host}] [%t] %5p %c{ 1 } %m%n # The memory appender for trace logger log4j. appender . EVENT_TRACE_MEMORYAPPENDER =org. wso2 . carbon . utils . logging . appenders . MemoryAppender log4j. appender . EVENT_TRACE_MEMORYAPPENDER . bufferSize = 2000 log4j. appender . EVENT_TRACE_MEMORYAPPENDER . layout =org. apache . log4j . PatternLayout log4j. appender . EVENT_TRACE_MEMORYAPPENDER . layout . ConversionPattern =%d{HH ss,SSS} [%X{ip}-%X{host}] [%t] %5p %m%n Setting the Log4j log level \u00b6 The log level can be set specifically for each appender in the log4j.properties file by setting the threshold value. If a log level is not specifically given for an appender as explained below, the root log level (INFO) will apply to all appenders by default. For example, shown below is how the log level is set to DEBUG for the CARBON_LOGFILE appender ( Carbon log ): log4j.appender.CARBON_LOGFILE.threshold=DEBUG Listed below are the log levels that can be configured: Level Description OFF The highest possible log level. This is intended for disabling logging. FATAL Indicates server errors that cause premature termination. These logs are expected to be immediately visible on the command line that you used for starting the server. ERROR Indicates other runtime errors or unexpected conditions. These logs are expected to be immediately visible on the command line that you used for starting the server. WARN Indicates the use of deprecated APIs, poor use of API, possible errors, and other runtime situations that are undesirable or unexpected but not necessarily wrong. These logs are expected to be immediately visible on the command line that you used for starting the server. INFO Indicates important runtime events, such as server startup/shutdown. These logs are expected to be immediately visible on the command line that you used for starting the server . It is recommended to keep these logs to a minimum. DEBUG Provides detailed information on the flow through the system. This information is expected to be written to logs only. Generally, most lines logged by your application should be written as DEBUG logs. TRACE Provides additional details on the behavior of events and services. This information is expected to be written to logs only. Managing log growth \u00b6 See the following content on managing the growth of Carbon logs and Audit logs : Managing the growth of Carbon logs \u00b6 Log growth (in Carbon logs ) can be managed by the following configurations in the <PRODUCT_HOME>/repository/conf/ log4j.properties file. Configurable log rotation: By default, log rotation is on a daily basis. Log rotation based on time as opposed to size: This helps to inspect the events that occurred during a specific time. Log files are archived to maximise the use of space. The log4j- based logging mechanism uses appenders to append all the log messages into a file. That is, at the end of the log rotation period, a new file will be created with the appended logs and archived. The name of the archived log file will always contain the date on which the file is archived. Limiting the size of Carbon log files \u00b6 You can limit the size of the <PRODUCT_HOME>/repository/logs/ wso2carbon.log file by following the steps given below. This is useful if you want to archive the logs and get backups periodically. Change the log4j.appender.CARBON_LOGFILE=org.wso2.carbon.utils.logging.appenders.CarbonDailyRollingFileAppender appender in the <PRODUCT_HOME>/repository/conf/ log4j.properties file as follows: log4j.appender.CARBON_LOGFILE=org.apache.log4j.RollingFileAppender Add the following two properties under RollingFileAppender log4j.appender.CARBON_LOGFILE.MaxFileSize=10MB log4j.appender.CARBON_LOGFILE.MaxBackupIndex=20 Info If the size of the log file is exceeding the value defined in the MaxFileSize property, the content is copied to a backup file and the logs are continued to be added to a new empty log file. The MaxBackupIndex property makes the Log4j maintain a maximum number of backup files for the logs. Limiting the size of audit log files \u00b6 In WSO2 servers, audit logs are enabled by default. We can limit the audit log files with the following configuration: Change the log4j.appender.AUDIT_LOGFILE=org.wso2.carbon.logging.appenders.CarbonDailyRollingFileAppender appender in the <PRODUCT_HOME>/repository/conf/log4j.properties file as follows: log4j.appender.AUDIT_LOGFILE=org.apache.log4j.RollingFileAppender Add the following two properties under RollingFileAppender : log4j.appender.AUDIT_LOGFILE.MaxFileSize=10MB log4j.appender.AUDIT_LOGFILE.MaxBackupIndex=20 Monitoring logs \u00b6 In each WSO2 product, users can configure and adjust the logging levels for each type of activity/transaction. There are several ways to view and monitor the logs: Carbon logs (system logs and application logs) of a running Carbon instance can be monitoring using the management console . Carbon logs, as well as HTTP access logs will be printed on the command terminal that open when you execute the product startup script. Alternatively, all log files can be viewed from the <PRODUCT_HOME>/repository/logs folder. This folder contains Audit logs , HTTP access logs as well as the Carbon logs in separate log files with time stamps . Note that older Carbon logs are archived in the wso2carbon.log file.","title":"Monitoring Logs"},{"location":"administer-and-observe/monitoring_logs/#monitoring-logs","text":"Note This page is currently restricted to confluence administrators. There are two ways of configuring log4j logging in WSO2 Carbon products: you can manually edit the log4j.properties file or configure logging through the management console. Configuration made through the management console can be persisted in the WSO2 registry so that it is available even after the server restarts. There is also an option to restore the original Log4j configuration from the log4j.properties file using the management console. However, if you modify the log4j.properties file and restart the server, the earlier log4j configuration persisted in the registry is overwritten. Info For information on viewing the contents of Application Logs and System Logs in your WSO2 product, see View and Download Logs in the WSO2 Administration Guide. In the Integration profile of WSO2 EI, you can enable wire logs to monitor HTTP messages that flow through the ESB. Please note that wire logs should be enabled for the troubleshooting purposes only. Running productions systems with wire logs enabled is not recommended.","title":"Monitoring Logs"},{"location":"administer-and-observe/monitoring_logs/#enabling-wire-logs","text":"Follow the steps given below to enable wire logs for the Integration profile: Go to the <EI_HOME>/conf/ directory and open the log4j.properties file. Enable the following entry in the file. This is commented out by default. log4j.logger.org.apache.synapse.transport.http.wire=DEBUG","title":"Enabling wire logs"},{"location":"administer-and-observe/monitoring_logs/#run-a-mediation-sequence","text":"......","title":"Run a mediation sequence"},{"location":"administer-and-observe/monitoring_logs/#reading-wire-logs","text":"In order to read the wire logs, you must first identify message direction. DEBUG - wire >> represents the message coming into the ESB from the wire. DEBUG - wire \\<\\< represents the message that goes to the wire from the ESB. There are two incoming messages and two outgoing messages in the above log. The f irst part of the message log contains the HTTP headers and is followed by the message payload. As shown in this example, wire logs are very useful when it comes to troubleshooting unexpected issues that occur while integrating systems. You can verify whether a message payload is correctly going out from the ESB, http headers like Content-Type is properly set in the outgoing message, etc. by looking at the wire logs. Logging is one of the most important aspects of a production-grade server. A properly configured logging system is vital for identifying errors, security threats, and usage patterns. See the following topics for details: Log types in WSO2 products Configuring products for log monitoring Setting the Log4j log level Managing log growth Managing the growth of Carbon logs Limiting the size of Carbon log files Limiting the size of audit log files Monitoring logs","title":"Reading wire logs"},{"location":"administer-and-observe/monitoring_logs/#log-types-in-wso2-products","text":"Listed below are the various log types that are used in WSO2 products. Info Separate log files are created for each of the log types given below in the <PRODUCT_HOME>/repository/logs directory . ** Carbon logs** : All WSO2 products are shipped with log4j logging capabilities that generate administrative activities and server side logs. The Carbon log ( wso2carbon.log ) is a log file that covers all the management features of a product. Carbon logs are configured in t he log4j.properties file (stored in the <PRODUCT_HOME>/repository/conf directory) . !!! info **Java logging and Log4j integration:** I n addition to the logs from libraries that use Log4j, all logs from libraries (such as, Tomcat, Hazelcast and more) that use Java logging framework are also visible in the same log files. That is, when Java logging is enabled in Carbon, only the Log4j appenders will write to the log files. If the Java Logging Handlers have logs, these logs will be delegated to the log events of the corresponding Log4j appenders. A Pub/Sub registry pattern implementation has been used in the latter mentioned scenario to plug the handlers and appenders. The following default log4j appenders in the ` log4j.properties ` file are used for this implementation: - ` org.wso2.carbon.logging.appenders.CarbonConsoleAppender ` - ` org.wso2.carbon.logging.appenders.CarbonDailyRollingFileAppender ` Audit logs: Audit logs are used for tracking the sequence of actions that affect a particular task carried out on the server. These are also configured in t he log4j.properties file. HTTP access logs: HTTP requests/responses are logged in access logs to monitor the activities related to an application's usage. These logs are configured in the catalina-server.xml file (stored in the \\< PRODUCT_HOME>/repository/conf/tomcat/ directory). Patch logs: These logs contain details related to patches applied to the product. Patch logs cannot be customized. See WSO2 Patch Application Process for more information. Service/Event logs: These are logs that are enabled in some WSO2 products for tracing services and events using a separate log file ( wso2-<product>-trace.log ). If server/event tracing logs are used in your WSO2 product, you can configure them in the log4j.properties file. Product-specific logs: Each WSO2 product may generate other log files in addition to the Carbon logs, Audit logs, HTTP access logs, Patch logs and Service/Event logs. See the product's documentation for descriptions of these log files and instructions on how to configure and use them.","title":"Log types in WSO2 products"},{"location":"administer-and-observe/monitoring_logs/#configuring-products-for-log-monitoring","text":"See the following information on configuring Carbon logs , Audit logs, HTTP access logs , and Service/Event logs for your WSO2 product. ****Configuring Carbon logs**** You can easily configure Carbon logs using the management console of your product, or you can manually edit the log4j.properties file. It is recommended to use the management console to configure logging because all changes made to log4j through the management console persists in the WSO2 Registry. Therefore, those changes will be available after the server restarts and will get priority over what is defined in the log4j.properties file. Also, note that the logging configuration you define using the management console will apply at run time. However, if you modify the log4j.properties file and restart the server, the earlier log4j configuration that persisted in the registry will be overwritten. T here is also an option in the management console to restore the original log4j configuration from the log4j.properties file. The log levels that can be configured are listed below . Identifying forged messages: T he log pattern d efines the output format of the log file. From Carbon 4.4.3 onwards, the conversion character 'K' can be used in the pattern layout to log a UUID. For example, the log pattern can be [%K] [%T] [%S] [%d] %P%5p {%c} - %x %m {%c}%n, where [%K] is the UUID. The UUID can be used for identifying forged messages in the log. By default, the UUID will be generated every time the server starts . If required, you can configure the UUID regeneration period by manually adding the following property to the log4j.properties file (stored in the <PRODUCT_HOME>/repository/conf directory): log4j.appender.CARBON_LOGFILE.layout.LogUUIDUpdateInterval=<number_of_hours> !!! info **Carbon logs in [WSO2 Data Analytics Server](http://wso2.com/smart-analytics) (WSO2 DAS)** Carbon logs are configured in the ` log4j.properties ` file (stored in the ` <PRODUCT_HOME>/repository/conf ` directory) for all WSO2 products. However, WSO2 DAS generates some additional Carbon logs (which will be stored in the same [Carbon log file](#MonitoringLogs-Carbon_logs) ) that should be separately configured by creating a new ` log4j.properties ` file in the ` <DAS_HOME>/repository/conf/analytics/spark ` directory. **Note:** To create this file, you need to rename the ` log4j.properties.template ` file that is available in the ` <DAS_HOME>/repository/conf/analytics/spark ` directory to l ` og4j.properties ` . See the following topics for instructions: Configuring Log4j Properties Configuring the Log Provider Configuring Audit logs Audit logs are enabled in WSO2 products by default. You can change the following default configuration by manually updating the the log4j.properties file. The log levels that can be configured are listed below . log4j.logger.AUDIT_LOG=INFO, AUDIT_LOGFILE # Appender config to AUDIT_LOGFILE log4j.appender.AUDIT_LOGFILE=org.wso2.carbon.utils.logging.appenders.CarbonDailyRollingFileAppender log4j.appender.AUDIT_LOGFILE.File=${carbon.home}/repository/logs/audit.log log4j.appender.AUDIT_LOGFILE.Append=true log4j.appender.AUDIT_LOGFILE.layout=org.wso2.carbon.utils.logging.TenantAwarePatternLayout log4j.appender.AUDIT_LOGFILE.layout.ConversionPattern=[%d] %P%5p {%c}- %x %m %n log4j.appender.AUDIT_LOGFILE.layout.TenantPattern=%U%@%D [%T] [%S] log4j.appender.AUDIT_LOGFILE.threshold=INFO log4j.additivity.AUDIT_LOG=false Configuring HTTP access logs See HTTP Access Logging for instructions on how to configure and use HTTP access logs. Configuring Service/Event tracing logs A separate log file for tracing services/events are enabled for certain WSO2 products in the log4j.properties file using a specific appender. These logs are published to a file named wso2-<product>-trace.log . See the table given below for instructions relevant to your product: Product Description WSO2 DAS Event tracing logs are enabled in WSO2 DAS using the EVENT_TRACE_LOGGER appender as shown below (Click Message tracing log configuration ). This log file stores logs related to events in WSO2 DAS. By default, this appender uses the root log level, which is INFO. You can override the root log level by giving a specific log level for the appender as explained here . Message tracing log configuration log4j. category . EVENT_TRACE_LOGGER =INFO, EVENT_TRACE_APPENDER, EVENT_TRACE_MEMORYAPPENDER log4j. additivity . EVENT_TRACE_LOGGER = false log4j. appender . EVENT_TRACE_APPENDER =org. apache . log4j . DailyRollingFileAppender log4j. appender . EVENT_TRACE_APPENDER . File = {carbon.<span class=\"fu\">home</span>}/repository/logs/ {carbon.<span class=\"fu\">home</span>}/repository/logs/ {instance. log }/wso2-das-trace${instance. log }. log log4j. appender . EVENT_TRACE_APPENDER . Append = true log4j. appender . EVENT_TRACE_APPENDER . layout =org. apache . log4j . PatternLayout log4j. appender . EVENT_TRACE_APPENDER . layout . ConversionPattern =%d{HH ss,SSS} [%X{ip}-%X{host}] [%t] %5p %c{ 1 } %m%n # The memory appender for trace logger log4j. appender . EVENT_TRACE_MEMORYAPPENDER =org. wso2 . carbon . utils . logging . appenders . MemoryAppender log4j. appender . EVENT_TRACE_MEMORYAPPENDER . bufferSize = 2000 log4j. appender . EVENT_TRACE_MEMORYAPPENDER . layout =org. apache . log4j . PatternLayout log4j. appender . EVENT_TRACE_MEMORYAPPENDER . layout . ConversionPattern =%d{HH ss,SSS} [%X{ip}-%X{host}] [%t] %5p %m%n","title":"Configuring products for log monitoring"},{"location":"administer-and-observe/monitoring_logs/#setting-the-log4j-log-level","text":"The log level can be set specifically for each appender in the log4j.properties file by setting the threshold value. If a log level is not specifically given for an appender as explained below, the root log level (INFO) will apply to all appenders by default. For example, shown below is how the log level is set to DEBUG for the CARBON_LOGFILE appender ( Carbon log ): log4j.appender.CARBON_LOGFILE.threshold=DEBUG Listed below are the log levels that can be configured: Level Description OFF The highest possible log level. This is intended for disabling logging. FATAL Indicates server errors that cause premature termination. These logs are expected to be immediately visible on the command line that you used for starting the server. ERROR Indicates other runtime errors or unexpected conditions. These logs are expected to be immediately visible on the command line that you used for starting the server. WARN Indicates the use of deprecated APIs, poor use of API, possible errors, and other runtime situations that are undesirable or unexpected but not necessarily wrong. These logs are expected to be immediately visible on the command line that you used for starting the server. INFO Indicates important runtime events, such as server startup/shutdown. These logs are expected to be immediately visible on the command line that you used for starting the server . It is recommended to keep these logs to a minimum. DEBUG Provides detailed information on the flow through the system. This information is expected to be written to logs only. Generally, most lines logged by your application should be written as DEBUG logs. TRACE Provides additional details on the behavior of events and services. This information is expected to be written to logs only.","title":"Setting the Log4j log level"},{"location":"administer-and-observe/monitoring_logs/#managing-log-growth","text":"See the following content on managing the growth of Carbon logs and Audit logs :","title":"Managing log growth"},{"location":"administer-and-observe/monitoring_logs/#managing-the-growth-of-carbon-logs","text":"Log growth (in Carbon logs ) can be managed by the following configurations in the <PRODUCT_HOME>/repository/conf/ log4j.properties file. Configurable log rotation: By default, log rotation is on a daily basis. Log rotation based on time as opposed to size: This helps to inspect the events that occurred during a specific time. Log files are archived to maximise the use of space. The log4j- based logging mechanism uses appenders to append all the log messages into a file. That is, at the end of the log rotation period, a new file will be created with the appended logs and archived. The name of the archived log file will always contain the date on which the file is archived.","title":"Managing the growth of Carbon logs"},{"location":"administer-and-observe/monitoring_logs/#limiting-the-size-of-carbon-log-files","text":"You can limit the size of the <PRODUCT_HOME>/repository/logs/ wso2carbon.log file by following the steps given below. This is useful if you want to archive the logs and get backups periodically. Change the log4j.appender.CARBON_LOGFILE=org.wso2.carbon.utils.logging.appenders.CarbonDailyRollingFileAppender appender in the <PRODUCT_HOME>/repository/conf/ log4j.properties file as follows: log4j.appender.CARBON_LOGFILE=org.apache.log4j.RollingFileAppender Add the following two properties under RollingFileAppender log4j.appender.CARBON_LOGFILE.MaxFileSize=10MB log4j.appender.CARBON_LOGFILE.MaxBackupIndex=20 Info If the size of the log file is exceeding the value defined in the MaxFileSize property, the content is copied to a backup file and the logs are continued to be added to a new empty log file. The MaxBackupIndex property makes the Log4j maintain a maximum number of backup files for the logs.","title":"Limiting the size of Carbon log files"},{"location":"administer-and-observe/monitoring_logs/#limiting-the-size-of-audit-log-files","text":"In WSO2 servers, audit logs are enabled by default. We can limit the audit log files with the following configuration: Change the log4j.appender.AUDIT_LOGFILE=org.wso2.carbon.logging.appenders.CarbonDailyRollingFileAppender appender in the <PRODUCT_HOME>/repository/conf/log4j.properties file as follows: log4j.appender.AUDIT_LOGFILE=org.apache.log4j.RollingFileAppender Add the following two properties under RollingFileAppender : log4j.appender.AUDIT_LOGFILE.MaxFileSize=10MB log4j.appender.AUDIT_LOGFILE.MaxBackupIndex=20","title":"Limiting the size of audit log files"},{"location":"administer-and-observe/monitoring_logs/#monitoring-logs_1","text":"In each WSO2 product, users can configure and adjust the logging levels for each type of activity/transaction. There are several ways to view and monitor the logs: Carbon logs (system logs and application logs) of a running Carbon instance can be monitoring using the management console . Carbon logs, as well as HTTP access logs will be printed on the command terminal that open when you execute the product startup script. Alternatively, all log files can be viewed from the <PRODUCT_HOME>/repository/logs folder. This folder contains Audit logs , HTTP access logs as well as the Carbon logs in separate log files with time stamps . Note that older Carbon logs are archived in the wso2carbon.log file.","title":"Monitoring logs"},{"location":"administer-and-observe/monitoring_with_prometheus/","text":"Monitoring the ESB Profile with Prometheus \u00b6 Prometheus is an open source toolkit that can monitor systems and produce useful information such as graphs and alerts. It collects statistical data exposed over an HTTP endpoint in the form of multi dimensional time series data, which can be then be visualized and queried. For more information about Prometheus, see Prometheus Documentation . Overview Configuring the Prometheus server Starting the ESB server Viewing statistics Overview \u00b6 The HTTP endpoint exposing the metric data is a service exposed by an internal API, bundled as an OSGi component and added as a feature to the WSO2 EI product. WSO2 ESB exposes its statistical data through JMX as MBeans. The Prometheus publisher in WSO2 EI scrapes these bean data, and converts them to the Prometheus format. The converted metrics are then exposed through an HTTP endpoint, which is used by Prometheus to scrape the statistical data. {width=\"878\"} Configuring the Prometheus server \u00b6 Download and install Prometheus. Open the <PROMETHEUS_HOME>/prometheus.yml file. Add a scrape config as to this file as shown below. The port number and the endpoint name should be as specified below. scrape_configs: - job_name: \"esb_stats\" static_configs: - targets: ['localhost:9191'] metrics_path: \"metric-service/metrics\" Save the configuration file. To start the Prometheus server, navigate to the <PROMETHEUS_HOME> and execute the start-up script located there. Starting the ESB server \u00b6 To start the ESB server with Prometheus enabled, navigate to the <EI_HOME>/bin directory and issue one of the following commands. For Windows: integrator.bat -DenablePrometheusApi For Linux : ./integrator.sh -DenablePrometheusApi Viewing statistics \u00b6 The stats can be viewed in following urls. http://localhost:9191/metric-service/metrics You may also visit following url in Prometheus server to plot the graphs. http://localhost:9191/graph","title":"Monitoring the ESB Profile with Prometheus"},{"location":"administer-and-observe/monitoring_with_prometheus/#monitoring-the-esb-profile-with-prometheus","text":"Prometheus is an open source toolkit that can monitor systems and produce useful information such as graphs and alerts. It collects statistical data exposed over an HTTP endpoint in the form of multi dimensional time series data, which can be then be visualized and queried. For more information about Prometheus, see Prometheus Documentation . Overview Configuring the Prometheus server Starting the ESB server Viewing statistics","title":"Monitoring the ESB Profile with Prometheus"},{"location":"administer-and-observe/monitoring_with_prometheus/#overview","text":"The HTTP endpoint exposing the metric data is a service exposed by an internal API, bundled as an OSGi component and added as a feature to the WSO2 EI product. WSO2 ESB exposes its statistical data through JMX as MBeans. The Prometheus publisher in WSO2 EI scrapes these bean data, and converts them to the Prometheus format. The converted metrics are then exposed through an HTTP endpoint, which is used by Prometheus to scrape the statistical data. {width=\"878\"}","title":"Overview"},{"location":"administer-and-observe/monitoring_with_prometheus/#configuring-the-prometheus-server","text":"Download and install Prometheus. Open the <PROMETHEUS_HOME>/prometheus.yml file. Add a scrape config as to this file as shown below. The port number and the endpoint name should be as specified below. scrape_configs: - job_name: \"esb_stats\" static_configs: - targets: ['localhost:9191'] metrics_path: \"metric-service/metrics\" Save the configuration file. To start the Prometheus server, navigate to the <PROMETHEUS_HOME> and execute the start-up script located there.","title":"Configuring the Prometheus server"},{"location":"administer-and-observe/monitoring_with_prometheus/#starting-the-esb-server","text":"To start the ESB server with Prometheus enabled, navigate to the <EI_HOME>/bin directory and issue one of the following commands. For Windows: integrator.bat -DenablePrometheusApi For Linux : ./integrator.sh -DenablePrometheusApi","title":"Starting the ESB server"},{"location":"administer-and-observe/monitoring_with_prometheus/#viewing-statistics","text":"The stats can be viewed in following urls. http://localhost:9191/metric-service/metrics You may also visit following url in Prometheus server to plot the graphs. http://localhost:9191/graph","title":"Viewing statistics"},{"location":"administer-and-observe/observability/","text":"Working with Observability \u00b6 Product observability enables rapid debugging of product issues. The ESB profile of WSO2 Enterprise Integrator (WSO2 EI) enables observability using correlation logs. Correlation logs allow you to monitor individual HTTP requests from the point that a message is received by the ESB until the corresponding response message is sent back to the original message sender. That is, the complete round trip of an HTTP message (client \u2192 ESB \u2192 back-end \u2192 ESB \u2192 client) can be tracked and anlyzed using a log file. When correlation logs are enabled for the ESB server, a separate log file named correlation.log is created in the <EI_HOME>/repository/logs/ directory. Every HTTP message that flows through the ESB and between the ESB and external clients undergoes several state changes. A new log entry is created in the correlation.log file corresponding to the state changes in the round trip of a single HTTP request. A correlation ID assigned to the incoming HTTP request is assigned to all the log entries corresponding to the request. Therefore, you can use this correlation ID to easily locate the logs relevant to the round trip of a specific HTTP request and, thereby, analyze the behaviour of the message flow. Note By default, product observability is not enabled as it impacts on the product's performance. In order to use this feature, apply the WUM update that is released on 2018-11-24. Warning If you want to deploy a WUM update into production, you need to have a paid subscription. If you do not have a paid subscription, you can use this feature with the next version of WSO2 Enterprise Integrator when it is released. For more information on updating WSO2 products using WUM, see Getting Started with WUM . See the following topics for details: Configuring correlation logs Enabling correlation logs in the ESB Sending an HTTP request with a correlation ID Accessing the correlation logs Reading correlation logs Configuring correlation logs \u00b6 Follow the steps given below to configure correlation logs in the ESB server. Add the following parameters to the log4j.properties file (stored in the <EI_HOME>/conf/ directory): # correlation logs log4j.logger.correlation=INFO, CORRELATION log4j.additivity.correlation=false # Appender config for correlation logs log4j.appender.CORRELATION=org.apache.log4j.RollingFileAppender log4j.appender.CORRELATION.File=${carbon.home}/repository/logs/${instance.log}/correlation.log log4j.appender.CORRELATION.MaxFileSize=10MB log4j.appender.CORRELATION.layout=org.apache.log4j.PatternLayout log4j.appender.CORRELATION.Threshold=INFO log4j.appender.CORRELATION.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS}|%X{Correlation-ID}|%t|%m%n Note that the maximum file size of the correlation log is set to 10MB in the above configuration. That is, when the size of the file exceeds 10MB, a new log file is created. If required, you can change this file size. If required , you can change the default HTTP header (which is 'activity_id') that is used to carry the correlation ID by adding the following property to the passthru-http.properties file (stored in the <EI_HOME>/conf/ directory). Replace <correlation_id> with a value of your choice. correlation_header_name=<correlation_id> Once the logs are configured, correlation logging should be enabled in the ESB as explained in the next section. Enabling correlation logs in the ESB \u00b6 You can enable correlation logging by passing a system property. If you want correlation logs to be enabled every time the server starts, add the following system property to the product start-up script (stored in the <EI_HOME>/bin/ directory) and set it to true . -DenableCorrelationLogs=true \\ Alternatively, you can pass the system property at the time of starting the server by executing the following command: On Linux/MacOS/CentOS sh integrator.sh -DenableCorrelationLogs= true On Windows integrator.bat -DenableCorrelationLogs= true Now when you start the ESB server, the correlation.log file is created in the <EI_HOME>/repository/logs/ directory. Sending an HTTP request with a correlation ID \u00b6 When the client sends an HTTP request to the ESB, a correlation ID for the request can be passed using the correlation header that is configured in the ESB. By default, the correlation header is 'activity_id'. If you want to change the default correlation header, wee the topic on configuring correlation logs . If the client does not pass a correlation ID in the request, the ESB will generate an internal value and assign it to the request. The correlation ID assigned to the incoming request is assigned to all the log entries that are related to the same request. Shown below is the POST request that is sent using the CURL client in the quick start guide . Note that the correlation ID is set in this request. curl -X POST --data @request.json http://localhost:8280/healthcare/categories/surgery/reserve -H \"Content-Type:application/json\" -H \"activityid:correlationID\" Accessing the correlation logs \u00b6 If you know the correlation ID of the HTTP request that you want to analyze, you can isolate the relevant logs as explained below. Open a terminal and navigate to the <EI_HOME>/repository/logs/ directory where the correlation.log file is saved. Execute the following command with the required correlation ID. Replace \\<correlation_ID> with the required value. cat correlation.log | grep \"<correlation_ID>\" Shown below is an example of correlation log entries corresponding to the round trip of a single HTTP request. 2018-11-30 15:27:27,262|correlationID|HTTP-Listener I/O dispatcher-5|0|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|REQUEST_HEAD 2018-11-30 15:27:27,262|correlationID|HTTP-Listener I/O dispatcher-5|0|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|REQUEST_BODY 2018-11-30 15:27:27,263|correlationID|HTTP-Listener I/O dispatcher-5|1|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|REQUEST_DONE 2018-11-30 15:27:27,265|correlationID|HTTP-Sender I/O dispatcher-4|42173|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|REQUEST_HEAD 2018-11-30 15:27:27,265|correlationID|HTTP-Sender I/O dispatcher-4|0|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|REQUEST_DONE 2018-11-30 15:27:27,267|correlationID|HTTP-Sender I/O dispatcher-4|2 |HTTP|sourhttp://localhost:9090/grandoaks/categories/surgery/reserve|BACKEND LATENCY 2018-11-30 15:27:27,267|correlationID|HTTP-Sender I/O dispatcher-4|2|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|RESPONSE_HEAD 2018-11-30 15:27:27,267|correlationID|HTTP-Sender I/O dispatcher-4|0|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|RESPONSE_BODY 2018-11-30 15:27:27,267|correlationID|HTTP-Sender I/O dispatcher-4|0|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|RESPONSE_DONE 2018-11-30 15:27:27,269|correlationID|HTTP-Listener I/O dispatcher-5|6|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|RESPONSE_HEAD 2018-11-30 15:27:27,269|correlationID|HTTP-Listener I/O dispatcher-5|0|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|RESPONSE_BODY 2018-11-30 15:27:27,269|correlationID|HTTP-Listener I/O dispatcher-5|0|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|RESPONSE_DONE 2018-11-30 15:27:27,269|correlationID|HTTP-Listener I/O dispatcher-5|7|HTTP|http-incoming-17|POST|/healthcare/categories/surgery/reserve|ROUND-TRIP LATENCY Reading correlation logs \u00b6 The pattern/format of a correlation log is shown below along with an example log entry. Log Pattern Example Log Time Stamp|Correlation ID|Thread name|Duration|Call type|Connection name|Method type|Connection URL|HTTP state 2018-10-26 17:34:40,464|de461a83-fc74-4660-93ed-1b609ecfac23|HTTP-Listener I/O dispatcher-3|535|HTTP|http-incoming-3|GET|/api/querydoctor/surgery|ROUND-TRIP LATENCY The detail recorded in a log entry is described below. Time Stamp The time at which the log is created. Example 2018 - 10 - 26 17 : 34 : 40 , 464 Correlation ID Each log contains a correlation ID, which is unique to the HTTP request. A client can send the correlation ID in the header of the HTTP request. If this correlation ID is missing in the incoming request, the ESB will generate one for the request. The HTTP header that carries the correlation ID is configured in the ESB. Example de461a83-fc74- 4660 -93ed-1b609ecfac23 Thread name The identifier of the thread. Example HTTP-Listener I/O dispatcher- 3 Duration The duration (given in milliseconds) depends on the type of log entry: If the state in the log entry is ROUND-TRIP LATENCY, the duration corresponds to the time gap between the REQUEST_HEAD state and the ROUND-TRIP LATENCY state. That is, the total time of the round trip. If the state in the log entry is BACKEND LATENCY, the duration corresponds to the total time taken by the backend to process the message. For all other log entries, the duration corresponds to the time gap between the current log entry and the immediately previous log entry. That is, the time taken for the HTTP request to move from one state to another. Example 535 Call type There are two possible call types: HTTP call type identifies logs that correspond to either back-end latency or round-trip latency states. That is, in the case of an individual request, one log will be recorded to identify back-end latency, and another log for round-trip latency. Since these logs relate to HTTP calls between the ESB and external clients, these logs are categorized using the HTTP call type. HTTP State Transition call type identifies logs that correspond to the state transitions in the HTTP transport related to a particular message. Connection name This is a name that is generated to identify the connection between the ESB and the external client (back-end or message sender). Example http-incoming- 3 Method type The HTTP method used for the request. Example GET Connection URL The connection URL of the external client with which the message is being communicated. For example, if the message is being read from the client, the connection URL corresponds to the client sending the message. However, if the message is being written to the backend, the URL corresponds to the backend client. Example /api/querydoctor/surgery HTTP state Listed below are the state changes that a message goes through when it flows through the ESB, and when the message flows between the ESB and external clients. Typically, a new log entry is generated for each of the states. However, there can be two separate log entries created for one particular state (e xcept for BACKEND LATENCY and ROUND-TRIP LATENCY) depending on whether the message is being read or written. You can identify the two separate log entries from the connection URL explained above. REQUEST_HEAD: All HTTP headers in the incoming request are being read/or being written to the backend. REQUEST_BODY : The body of the incoming request is being read/or being written to the backend. REQUEST_DONE : The request is completely read (content decoded)/ or is completely written to the backend. BACKEND LATENCY : The response message is received by the ESB. This status corresponds to the time total time taken by the backend to process the message. RESPONSE_HEAD : All HTTP headers in the response message are being read/or being written to the client. RESPONSE_BODY : The body of the response message is being read/or being written to the client. RESPONSE_DONE : The response is completely read/ or completely written to the client. ROUND-TRIP LATENCY : The response message is completely written to the client. This status corresponds to the total time taken by the HTTP request to compete the round trip (from the point of receiving the HTTP request from a client until the response message is sent back to the client)","title":"Observability"},{"location":"administer-and-observe/observability/#working-with-observability","text":"Product observability enables rapid debugging of product issues. The ESB profile of WSO2 Enterprise Integrator (WSO2 EI) enables observability using correlation logs. Correlation logs allow you to monitor individual HTTP requests from the point that a message is received by the ESB until the corresponding response message is sent back to the original message sender. That is, the complete round trip of an HTTP message (client \u2192 ESB \u2192 back-end \u2192 ESB \u2192 client) can be tracked and anlyzed using a log file. When correlation logs are enabled for the ESB server, a separate log file named correlation.log is created in the <EI_HOME>/repository/logs/ directory. Every HTTP message that flows through the ESB and between the ESB and external clients undergoes several state changes. A new log entry is created in the correlation.log file corresponding to the state changes in the round trip of a single HTTP request. A correlation ID assigned to the incoming HTTP request is assigned to all the log entries corresponding to the request. Therefore, you can use this correlation ID to easily locate the logs relevant to the round trip of a specific HTTP request and, thereby, analyze the behaviour of the message flow. Note By default, product observability is not enabled as it impacts on the product's performance. In order to use this feature, apply the WUM update that is released on 2018-11-24. Warning If you want to deploy a WUM update into production, you need to have a paid subscription. If you do not have a paid subscription, you can use this feature with the next version of WSO2 Enterprise Integrator when it is released. For more information on updating WSO2 products using WUM, see Getting Started with WUM . See the following topics for details: Configuring correlation logs Enabling correlation logs in the ESB Sending an HTTP request with a correlation ID Accessing the correlation logs Reading correlation logs","title":"Working with Observability"},{"location":"administer-and-observe/observability/#configuring-correlation-logs","text":"Follow the steps given below to configure correlation logs in the ESB server. Add the following parameters to the log4j.properties file (stored in the <EI_HOME>/conf/ directory): # correlation logs log4j.logger.correlation=INFO, CORRELATION log4j.additivity.correlation=false # Appender config for correlation logs log4j.appender.CORRELATION=org.apache.log4j.RollingFileAppender log4j.appender.CORRELATION.File=${carbon.home}/repository/logs/${instance.log}/correlation.log log4j.appender.CORRELATION.MaxFileSize=10MB log4j.appender.CORRELATION.layout=org.apache.log4j.PatternLayout log4j.appender.CORRELATION.Threshold=INFO log4j.appender.CORRELATION.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS}|%X{Correlation-ID}|%t|%m%n Note that the maximum file size of the correlation log is set to 10MB in the above configuration. That is, when the size of the file exceeds 10MB, a new log file is created. If required, you can change this file size. If required , you can change the default HTTP header (which is 'activity_id') that is used to carry the correlation ID by adding the following property to the passthru-http.properties file (stored in the <EI_HOME>/conf/ directory). Replace <correlation_id> with a value of your choice. correlation_header_name=<correlation_id> Once the logs are configured, correlation logging should be enabled in the ESB as explained in the next section.","title":"Configuring correlation logs"},{"location":"administer-and-observe/observability/#enabling-correlation-logs-in-the-esb","text":"You can enable correlation logging by passing a system property. If you want correlation logs to be enabled every time the server starts, add the following system property to the product start-up script (stored in the <EI_HOME>/bin/ directory) and set it to true . -DenableCorrelationLogs=true \\ Alternatively, you can pass the system property at the time of starting the server by executing the following command: On Linux/MacOS/CentOS sh integrator.sh -DenableCorrelationLogs= true On Windows integrator.bat -DenableCorrelationLogs= true Now when you start the ESB server, the correlation.log file is created in the <EI_HOME>/repository/logs/ directory.","title":"Enabling correlation logs in the ESB"},{"location":"administer-and-observe/observability/#sending-an-http-request-with-a-correlation-id","text":"When the client sends an HTTP request to the ESB, a correlation ID for the request can be passed using the correlation header that is configured in the ESB. By default, the correlation header is 'activity_id'. If you want to change the default correlation header, wee the topic on configuring correlation logs . If the client does not pass a correlation ID in the request, the ESB will generate an internal value and assign it to the request. The correlation ID assigned to the incoming request is assigned to all the log entries that are related to the same request. Shown below is the POST request that is sent using the CURL client in the quick start guide . Note that the correlation ID is set in this request. curl -X POST --data @request.json http://localhost:8280/healthcare/categories/surgery/reserve -H \"Content-Type:application/json\" -H \"activityid:correlationID\"","title":"Sending an HTTP request with a correlation ID"},{"location":"administer-and-observe/observability/#accessing-the-correlation-logs","text":"If you know the correlation ID of the HTTP request that you want to analyze, you can isolate the relevant logs as explained below. Open a terminal and navigate to the <EI_HOME>/repository/logs/ directory where the correlation.log file is saved. Execute the following command with the required correlation ID. Replace \\<correlation_ID> with the required value. cat correlation.log | grep \"<correlation_ID>\" Shown below is an example of correlation log entries corresponding to the round trip of a single HTTP request. 2018-11-30 15:27:27,262|correlationID|HTTP-Listener I/O dispatcher-5|0|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|REQUEST_HEAD 2018-11-30 15:27:27,262|correlationID|HTTP-Listener I/O dispatcher-5|0|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|REQUEST_BODY 2018-11-30 15:27:27,263|correlationID|HTTP-Listener I/O dispatcher-5|1|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|REQUEST_DONE 2018-11-30 15:27:27,265|correlationID|HTTP-Sender I/O dispatcher-4|42173|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|REQUEST_HEAD 2018-11-30 15:27:27,265|correlationID|HTTP-Sender I/O dispatcher-4|0|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|REQUEST_DONE 2018-11-30 15:27:27,267|correlationID|HTTP-Sender I/O dispatcher-4|2 |HTTP|sourhttp://localhost:9090/grandoaks/categories/surgery/reserve|BACKEND LATENCY 2018-11-30 15:27:27,267|correlationID|HTTP-Sender I/O dispatcher-4|2|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|RESPONSE_HEAD 2018-11-30 15:27:27,267|correlationID|HTTP-Sender I/O dispatcher-4|0|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|RESPONSE_BODY 2018-11-30 15:27:27,267|correlationID|HTTP-Sender I/O dispatcher-4|0|HTTP State Transition|http-outgoing-4|POST|http://localhost:9090/grandoaks/categories/surgery/reserve|RESPONSE_DONE 2018-11-30 15:27:27,269|correlationID|HTTP-Listener I/O dispatcher-5|6|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|RESPONSE_HEAD 2018-11-30 15:27:27,269|correlationID|HTTP-Listener I/O dispatcher-5|0|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|RESPONSE_BODY 2018-11-30 15:27:27,269|correlationID|HTTP-Listener I/O dispatcher-5|0|HTTP State Transition|http-incoming-17|POST|/healthcare/categories/surgery/reserve|RESPONSE_DONE 2018-11-30 15:27:27,269|correlationID|HTTP-Listener I/O dispatcher-5|7|HTTP|http-incoming-17|POST|/healthcare/categories/surgery/reserve|ROUND-TRIP LATENCY","title":"Accessing the correlation logs"},{"location":"administer-and-observe/observability/#reading-correlation-logs","text":"The pattern/format of a correlation log is shown below along with an example log entry. Log Pattern Example Log Time Stamp|Correlation ID|Thread name|Duration|Call type|Connection name|Method type|Connection URL|HTTP state 2018-10-26 17:34:40,464|de461a83-fc74-4660-93ed-1b609ecfac23|HTTP-Listener I/O dispatcher-3|535|HTTP|http-incoming-3|GET|/api/querydoctor/surgery|ROUND-TRIP LATENCY The detail recorded in a log entry is described below. Time Stamp The time at which the log is created. Example 2018 - 10 - 26 17 : 34 : 40 , 464 Correlation ID Each log contains a correlation ID, which is unique to the HTTP request. A client can send the correlation ID in the header of the HTTP request. If this correlation ID is missing in the incoming request, the ESB will generate one for the request. The HTTP header that carries the correlation ID is configured in the ESB. Example de461a83-fc74- 4660 -93ed-1b609ecfac23 Thread name The identifier of the thread. Example HTTP-Listener I/O dispatcher- 3 Duration The duration (given in milliseconds) depends on the type of log entry: If the state in the log entry is ROUND-TRIP LATENCY, the duration corresponds to the time gap between the REQUEST_HEAD state and the ROUND-TRIP LATENCY state. That is, the total time of the round trip. If the state in the log entry is BACKEND LATENCY, the duration corresponds to the total time taken by the backend to process the message. For all other log entries, the duration corresponds to the time gap between the current log entry and the immediately previous log entry. That is, the time taken for the HTTP request to move from one state to another. Example 535 Call type There are two possible call types: HTTP call type identifies logs that correspond to either back-end latency or round-trip latency states. That is, in the case of an individual request, one log will be recorded to identify back-end latency, and another log for round-trip latency. Since these logs relate to HTTP calls between the ESB and external clients, these logs are categorized using the HTTP call type. HTTP State Transition call type identifies logs that correspond to the state transitions in the HTTP transport related to a particular message. Connection name This is a name that is generated to identify the connection between the ESB and the external client (back-end or message sender). Example http-incoming- 3 Method type The HTTP method used for the request. Example GET Connection URL The connection URL of the external client with which the message is being communicated. For example, if the message is being read from the client, the connection URL corresponds to the client sending the message. However, if the message is being written to the backend, the URL corresponds to the backend client. Example /api/querydoctor/surgery HTTP state Listed below are the state changes that a message goes through when it flows through the ESB, and when the message flows between the ESB and external clients. Typically, a new log entry is generated for each of the states. However, there can be two separate log entries created for one particular state (e xcept for BACKEND LATENCY and ROUND-TRIP LATENCY) depending on whether the message is being read or written. You can identify the two separate log entries from the connection URL explained above. REQUEST_HEAD: All HTTP headers in the incoming request are being read/or being written to the backend. REQUEST_BODY : The body of the incoming request is being read/or being written to the backend. REQUEST_DONE : The request is completely read (content decoded)/ or is completely written to the backend. BACKEND LATENCY : The response message is received by the ESB. This status corresponds to the time total time taken by the backend to process the message. RESPONSE_HEAD : All HTTP headers in the response message are being read/or being written to the client. RESPONSE_BODY : The body of the response message is being read/or being written to the client. RESPONSE_DONE : The response is completely read/ or completely written to the client. ROUND-TRIP LATENCY : The response message is completely written to the client. This status corresponds to the total time taken by the HTTP request to compete the round trip (from the point of receiving the HTTP request from a client until the response message is sent back to the client)","title":"Reading correlation logs"},{"location":"administer-and-observe/other_usages_of_tcpmon/","text":"Other Usages of TCPMon \u00b6 TCPMon is primarily used for message monitoring. Additionally, TCPMon can also be used for sending requests to web services and as a proxy service. Refer Starting TCPMon for details on how to start the tool. Sending Requests for Web Services As a Proxy Advanced Settings Sending Requests for Web Services \u00b6 TCPMon can also be used as a request sender for Web services. The request SOAP message can be pasted on the send screen and sent directly to the server. As a Proxy \u00b6 TCPMon can act as a proxy. To start it in proxy mode, select the Proxy option. When acting as a proxy, TCPMon only needs the listener port to be configured. Advanced Settings \u00b6 TCPMon can simulate a slow connection, in which case the delay and the bytes to be dropped can be configured. This is useful when testing Web services. Also, if HTTP proxy support is required, that can also be set on the admin screen.","title":"Other Usages of TCPMon"},{"location":"administer-and-observe/other_usages_of_tcpmon/#other-usages-of-tcpmon","text":"TCPMon is primarily used for message monitoring. Additionally, TCPMon can also be used for sending requests to web services and as a proxy service. Refer Starting TCPMon for details on how to start the tool. Sending Requests for Web Services As a Proxy Advanced Settings","title":"Other Usages of TCPMon"},{"location":"administer-and-observe/other_usages_of_tcpmon/#sending-requests-for-web-services","text":"TCPMon can also be used as a request sender for Web services. The request SOAP message can be pasted on the send screen and sent directly to the server.","title":"Sending Requests for Web Services"},{"location":"administer-and-observe/other_usages_of_tcpmon/#as-a-proxy","text":"TCPMon can act as a proxy. To start it in proxy mode, select the Proxy option. When acting as a proxy, TCPMon only needs the listener port to be configured.","title":"As a Proxy"},{"location":"administer-and-observe/other_usages_of_tcpmon/#advanced-settings","text":"TCPMon can simulate a slow connection, in which case the delay and the bytes to be dropped can be configured. This is useful when testing Web services. Also, if HTTP proxy support is required, that can also be set on the admin screen.","title":"Advanced Settings"},{"location":"administer-and-observe/snmp_monitoring/","text":"SNMP Monitoring \u00b6 Simple Network Management Protocol (SNMP) is an Internet-standard protocol for managing devices on IP networks. Given below is how to configure SNMP in WSO2 Enterprise Integrator(WSO2 EI), which exposes various MBeans via SNMP. Download the following jar files from http://www.snmp4j.org and add them to <EI_HOME>/ lib . snmp4j-2.1.0.jar snmp4j-agent-2.0.6.jar Enable SNMP in the <EI_HOME>/conf/synapse.properties file by adding the following entry: synapse.snmp.enabled=true WSO2 EI can now monitor MBeans with SNMP. For example: Monitoring Info : OID branch \"1.3.6.1.4.1.18060.14\" with the following sub-branches: 1 - ServerManager MBean 2 - Transport MBeans 3 - NHttpConnections MBeans 4 - NHTTPLatency MBeans 5 - NHTTPS2SLatency MBeans MBean OID mappings \u00b6 Following are the OID equivalents of the server manager and transport MBeans, which are described in JMX Monitoring : Name=ServerManager@ServerState as OID: 1.3.6.1.4.1.18060.14.1.21.1.0 Name=passthru-http-sender@ActiveThreadCount as OID: 1.3.6.1.4.1.18060.14.2.17.1.0 Name=passthru-http-sender@AvgSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.17.2.0 Name=passthru-http-sender@AvgSizeSent as OID: 1.3.6.1.4.1.18060.14.2.17.3.0 Name=passthru-http-sender@BytesReceived as OID: 1.3.6.1.4.1.18060.14.2.17.4.0 Name=passthru-http-sender@BytesSent as OID: 1.3.6.1.4.1.18060.14.2.17.5.0 Name=passthru-http-sender@FaultsReceiving as OID: 1.3.6.1.4.1.18060.14.2.17.6.0 Name=passthru-http-sender@FaultsSending as OID: 1.3.6.1.4.1.18060.14.2.17.7.0 Name=passthru-http-sender@LastResetTime as OID: 1.3.6.1.4.1.18060.14.2.17.8.0 Name=passthru-http-sender@MaxSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.17.9.0 Name=passthru-http-sender@MaxSizeSent as OID: 1.3.6.1.4.1.18060.14.2.17.10.0 Name=passthru-http-sender@MessagesReceived as OID: 1.3.6.1.4.1.18060.14.2.17.11.0 Name=passthru-http-sender@MessagesSent as OID: 1.3.6.1.4.1.18060.14.2.17.12.0 Name=passthru-http-sender@MetricsWindow as OID: 1.3.6.1.4.1.18060.14.2.17.13.0 Name=passthru-http-sender@MinSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.17.14.0 Name=passthru-http-sender@MinSizeSent as OID: 1.3.6.1.4.1.18060.14.2.17.15.0 Name=passthru-http-sender@QueueSize as OID: 1.3.6.1.4.1.18060.14.2.17.16.0 Name=passthru-http-sender@TimeoutsReceiving as OID: 1.3.6.1.4.1.18060.14.2.17.18.0 Name=passthru-http-sender@TimeoutsSending as OID: 1.3.6.1.4.1.18060.14.2.17.19.0 Name=passthru-https-sender@ActiveThreadCount as OID: 1.3.6.1.4.1.18060.14.2.18.1.0 Name=passthru-https-sender@AvgSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.18.2.0 Name=passthru-https-sender@AvgSizeSent as OID: 1.3.6.1.4.1.18060.14.2.18.3.0 Name=passthru-https-sender@BytesReceived as OID: 1.3.6.1.4.1.18060.14.2.18.4.0 Name=passthru-https-sender@BytesSent as OID: 1.3.6.1.4.1.18060.14.2.18.5.0 Name=passthru-https-sender@FaultsReceiving as OID: 1.3.6.1.4.1.18060.14.2.18.6.0 Name=passthru-https-sender@FaultsSending as OID: 1.3.6.1.4.1.18060.14.2.18.7.0 Name=passthru-https-sender@LastResetTime as OID: 1.3.6.1.4.1.18060.14.2.18.8.0 Name=passthru-https-sender@MaxSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.18.9.0 Name=passthru-https-sender@MaxSizeSent as OID: 1.3.6.1.4.1.18060.14.2.18.10.0 Name=passthru-https-sender@MessagesReceived as OID: 1.3.6.1.4.1.18060.14.2.18.11.0 Name=passthru-https-sender@MessagesSent as OID: 1.3.6.1.4.1.18060.14.2.18.12.0 Name=passthru-https-sender@MetricsWindow as OID: 1.3.6.1.4.1.18060.14.2.18.13.0 Name=passthru-https-sender@MinSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.18.14.0 Name=passthru-https-sender@MinSizeSent as OID: 1.3.6.1.4.1.18060.14.2.18.15.0 Name=passthru-https-sender@QueueSize as OID: 1.3.6.1.4.1.18060.14.2.18.16.0 Name=passthru-https-sender@TimeoutsReceiving as OID: 1.3.6.1.4.1.18060.14.2.18.18.0 Name=passthru-https-sender@TimeoutsSending as OID: 1.3.6.1.4.1.18060.14.2.18.19.0 Name=passthru-http-receiver@ActiveThreadCount as OID: 1.3.6.1.4.1.18060.14.2.19.1.0 Name=passthru-http-receiver@AvgSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.19.2.0 Name=passthru-http-receiver@AvgSizeSent as OID: 1.3.6.1.4.1.18060.14.2.19.3.0 Name=passthru-http-receiver@BytesReceived as OID: 1.3.6.1.4.1.18060.14.2.19.4.0 Name=passthru-http-receiver@BytesSent as OID: 1.3.6.1.4.1.18060.14.2.19.5.0 Name=passthru-http-receiver@FaultsReceiving as OID: 1.3.6.1.4.1.18060.14.2.19.6.0 Name=passthru-http-receiver@FaultsSending as OID: 1.3.6.1.4.1.18060.14.2.19.7.0 Name=passthru-http-receiver@LastResetTime as OID: 1.3.6.1.4.1.18060.14.2.19.8.0 Name=passthru-http-receiver@MaxSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.19.9.0 Name=passthru-http-receiver@MaxSizeSent as OID: 1.3.6.1.4.1.18060.14.2.19.10.0 Name=passthru-http-receiver@MessagesReceived as OID: 1.3.6.1.4.1.18060.14.2.19.11.0 Name=passthru-http-receiver@MessagesSent as OID: 1.3.6.1.4.1.18060.14.2.19.12.0 Name=passthru-http-receiver@MetricsWindow as OID: 1.3.6.1.4.1.18060.14.2.19.13.0 Name=passthru-http-receiver@MinSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.19.14.0 Name=passthru-http-receiver@MinSizeSent as OID: 1.3.6.1.4.1.18060.14.2.19.15.0 Name=passthru-http-receiver@QueueSize as OID: 1.3.6.1.4.1.18060.14.2.19.16.0 Name=passthru-http-receiver@TimeoutsReceiving as OID: 1.3.6.1.4.1.18060.14.2.19.18.0 Name=passthru-http-receiver@TimeoutsSending as OID: 1.3.6.1.4.1.18060.14.2.19.19.0 Name=passthru-https-receiver@ActiveThreadCount as OID: 1.3.6.1.4.1.18060.14.2.20.1.0 Name=passthru-https-receiver@AvgSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.20.2.0 Name=passthru-https-receiver@AvgSizeSent as OID: 1.3.6.1.4.1.18060.14.2.20.3.0 Name=passthru-https-receiver@BytesReceived as OID: 1.3.6.1.4.1.18060.14.2.20.4.0 Name=passthru-https-receiver@BytesSent as OID: 1.3.6.1.4.1.18060.14.2.20.5.0 Name=passthru-https-receiver@FaultsReceiving as OID: 1.3.6.1.4.1.18060.14.2.20.6.0 Name=passthru-https-receiver@FaultsSending as OID: 1.3.6.1.4.1.18060.14.2.20.7.0 Name=passthru-https-receiver@LastResetTime as OID: 1.3.6.1.4.1.18060.14.2.20.8.0 Name=passthru-https-receiver@MaxSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.20.9.0 Name=passthru-https-receiver@MaxSizeSent as OID: 1.3.6.1.4.1.18060.14.2.20.10.0 Name=passthru-https-receiver@MessagesReceived as OID: 1.3.6.1.4.1.18060.14.2.20.11.0 Name=passthru-https-receiver@MessagesSent as OID: 1.3.6.1.4.1.18060.14.2.20.12.0 Name=passthru-https-receiver@MetricsWindow as OID: 1.3.6.1.4.1.18060.14.2.20.13.0 Name=passthru-https-receiver@MinSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.20.14.0 Name=passthru-https-receiver@MinSizeSent as OID: 1.3.6.1.4.1.18060.14.2.20.15.0 Name=passthru-https-receiver@QueueSize as OID: 1.3.6.1.4.1.18060.14.2.20.16.0 Name=passthru-https-receiver@TimeoutsReceiving as OID: 1.3.6.1.4.1.18060.14.2.20.18.0 Name=passthru-https-receiver@TimeoutsSending as OID: 1.3.6.1.4.1.18060.14.2.20.19.0","title":"SNMP Monitoring"},{"location":"administer-and-observe/snmp_monitoring/#snmp-monitoring","text":"Simple Network Management Protocol (SNMP) is an Internet-standard protocol for managing devices on IP networks. Given below is how to configure SNMP in WSO2 Enterprise Integrator(WSO2 EI), which exposes various MBeans via SNMP. Download the following jar files from http://www.snmp4j.org and add them to <EI_HOME>/ lib . snmp4j-2.1.0.jar snmp4j-agent-2.0.6.jar Enable SNMP in the <EI_HOME>/conf/synapse.properties file by adding the following entry: synapse.snmp.enabled=true WSO2 EI can now monitor MBeans with SNMP. For example: Monitoring Info : OID branch \"1.3.6.1.4.1.18060.14\" with the following sub-branches: 1 - ServerManager MBean 2 - Transport MBeans 3 - NHttpConnections MBeans 4 - NHTTPLatency MBeans 5 - NHTTPS2SLatency MBeans","title":"SNMP Monitoring"},{"location":"administer-and-observe/snmp_monitoring/#mbean-oid-mappings","text":"Following are the OID equivalents of the server manager and transport MBeans, which are described in JMX Monitoring : Name=ServerManager@ServerState as OID: 1.3.6.1.4.1.18060.14.1.21.1.0 Name=passthru-http-sender@ActiveThreadCount as OID: 1.3.6.1.4.1.18060.14.2.17.1.0 Name=passthru-http-sender@AvgSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.17.2.0 Name=passthru-http-sender@AvgSizeSent as OID: 1.3.6.1.4.1.18060.14.2.17.3.0 Name=passthru-http-sender@BytesReceived as OID: 1.3.6.1.4.1.18060.14.2.17.4.0 Name=passthru-http-sender@BytesSent as OID: 1.3.6.1.4.1.18060.14.2.17.5.0 Name=passthru-http-sender@FaultsReceiving as OID: 1.3.6.1.4.1.18060.14.2.17.6.0 Name=passthru-http-sender@FaultsSending as OID: 1.3.6.1.4.1.18060.14.2.17.7.0 Name=passthru-http-sender@LastResetTime as OID: 1.3.6.1.4.1.18060.14.2.17.8.0 Name=passthru-http-sender@MaxSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.17.9.0 Name=passthru-http-sender@MaxSizeSent as OID: 1.3.6.1.4.1.18060.14.2.17.10.0 Name=passthru-http-sender@MessagesReceived as OID: 1.3.6.1.4.1.18060.14.2.17.11.0 Name=passthru-http-sender@MessagesSent as OID: 1.3.6.1.4.1.18060.14.2.17.12.0 Name=passthru-http-sender@MetricsWindow as OID: 1.3.6.1.4.1.18060.14.2.17.13.0 Name=passthru-http-sender@MinSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.17.14.0 Name=passthru-http-sender@MinSizeSent as OID: 1.3.6.1.4.1.18060.14.2.17.15.0 Name=passthru-http-sender@QueueSize as OID: 1.3.6.1.4.1.18060.14.2.17.16.0 Name=passthru-http-sender@TimeoutsReceiving as OID: 1.3.6.1.4.1.18060.14.2.17.18.0 Name=passthru-http-sender@TimeoutsSending as OID: 1.3.6.1.4.1.18060.14.2.17.19.0 Name=passthru-https-sender@ActiveThreadCount as OID: 1.3.6.1.4.1.18060.14.2.18.1.0 Name=passthru-https-sender@AvgSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.18.2.0 Name=passthru-https-sender@AvgSizeSent as OID: 1.3.6.1.4.1.18060.14.2.18.3.0 Name=passthru-https-sender@BytesReceived as OID: 1.3.6.1.4.1.18060.14.2.18.4.0 Name=passthru-https-sender@BytesSent as OID: 1.3.6.1.4.1.18060.14.2.18.5.0 Name=passthru-https-sender@FaultsReceiving as OID: 1.3.6.1.4.1.18060.14.2.18.6.0 Name=passthru-https-sender@FaultsSending as OID: 1.3.6.1.4.1.18060.14.2.18.7.0 Name=passthru-https-sender@LastResetTime as OID: 1.3.6.1.4.1.18060.14.2.18.8.0 Name=passthru-https-sender@MaxSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.18.9.0 Name=passthru-https-sender@MaxSizeSent as OID: 1.3.6.1.4.1.18060.14.2.18.10.0 Name=passthru-https-sender@MessagesReceived as OID: 1.3.6.1.4.1.18060.14.2.18.11.0 Name=passthru-https-sender@MessagesSent as OID: 1.3.6.1.4.1.18060.14.2.18.12.0 Name=passthru-https-sender@MetricsWindow as OID: 1.3.6.1.4.1.18060.14.2.18.13.0 Name=passthru-https-sender@MinSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.18.14.0 Name=passthru-https-sender@MinSizeSent as OID: 1.3.6.1.4.1.18060.14.2.18.15.0 Name=passthru-https-sender@QueueSize as OID: 1.3.6.1.4.1.18060.14.2.18.16.0 Name=passthru-https-sender@TimeoutsReceiving as OID: 1.3.6.1.4.1.18060.14.2.18.18.0 Name=passthru-https-sender@TimeoutsSending as OID: 1.3.6.1.4.1.18060.14.2.18.19.0 Name=passthru-http-receiver@ActiveThreadCount as OID: 1.3.6.1.4.1.18060.14.2.19.1.0 Name=passthru-http-receiver@AvgSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.19.2.0 Name=passthru-http-receiver@AvgSizeSent as OID: 1.3.6.1.4.1.18060.14.2.19.3.0 Name=passthru-http-receiver@BytesReceived as OID: 1.3.6.1.4.1.18060.14.2.19.4.0 Name=passthru-http-receiver@BytesSent as OID: 1.3.6.1.4.1.18060.14.2.19.5.0 Name=passthru-http-receiver@FaultsReceiving as OID: 1.3.6.1.4.1.18060.14.2.19.6.0 Name=passthru-http-receiver@FaultsSending as OID: 1.3.6.1.4.1.18060.14.2.19.7.0 Name=passthru-http-receiver@LastResetTime as OID: 1.3.6.1.4.1.18060.14.2.19.8.0 Name=passthru-http-receiver@MaxSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.19.9.0 Name=passthru-http-receiver@MaxSizeSent as OID: 1.3.6.1.4.1.18060.14.2.19.10.0 Name=passthru-http-receiver@MessagesReceived as OID: 1.3.6.1.4.1.18060.14.2.19.11.0 Name=passthru-http-receiver@MessagesSent as OID: 1.3.6.1.4.1.18060.14.2.19.12.0 Name=passthru-http-receiver@MetricsWindow as OID: 1.3.6.1.4.1.18060.14.2.19.13.0 Name=passthru-http-receiver@MinSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.19.14.0 Name=passthru-http-receiver@MinSizeSent as OID: 1.3.6.1.4.1.18060.14.2.19.15.0 Name=passthru-http-receiver@QueueSize as OID: 1.3.6.1.4.1.18060.14.2.19.16.0 Name=passthru-http-receiver@TimeoutsReceiving as OID: 1.3.6.1.4.1.18060.14.2.19.18.0 Name=passthru-http-receiver@TimeoutsSending as OID: 1.3.6.1.4.1.18060.14.2.19.19.0 Name=passthru-https-receiver@ActiveThreadCount as OID: 1.3.6.1.4.1.18060.14.2.20.1.0 Name=passthru-https-receiver@AvgSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.20.2.0 Name=passthru-https-receiver@AvgSizeSent as OID: 1.3.6.1.4.1.18060.14.2.20.3.0 Name=passthru-https-receiver@BytesReceived as OID: 1.3.6.1.4.1.18060.14.2.20.4.0 Name=passthru-https-receiver@BytesSent as OID: 1.3.6.1.4.1.18060.14.2.20.5.0 Name=passthru-https-receiver@FaultsReceiving as OID: 1.3.6.1.4.1.18060.14.2.20.6.0 Name=passthru-https-receiver@FaultsSending as OID: 1.3.6.1.4.1.18060.14.2.20.7.0 Name=passthru-https-receiver@LastResetTime as OID: 1.3.6.1.4.1.18060.14.2.20.8.0 Name=passthru-https-receiver@MaxSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.20.9.0 Name=passthru-https-receiver@MaxSizeSent as OID: 1.3.6.1.4.1.18060.14.2.20.10.0 Name=passthru-https-receiver@MessagesReceived as OID: 1.3.6.1.4.1.18060.14.2.20.11.0 Name=passthru-https-receiver@MessagesSent as OID: 1.3.6.1.4.1.18060.14.2.20.12.0 Name=passthru-https-receiver@MetricsWindow as OID: 1.3.6.1.4.1.18060.14.2.20.13.0 Name=passthru-https-receiver@MinSizeReceived as OID: 1.3.6.1.4.1.18060.14.2.20.14.0 Name=passthru-https-receiver@MinSizeSent as OID: 1.3.6.1.4.1.18060.14.2.20.15.0 Name=passthru-https-receiver@QueueSize as OID: 1.3.6.1.4.1.18060.14.2.20.16.0 Name=passthru-https-receiver@TimeoutsReceiving as OID: 1.3.6.1.4.1.18060.14.2.20.18.0 Name=passthru-https-receiver@TimeoutsSending as OID: 1.3.6.1.4.1.18060.14.2.20.19.0","title":"MBean OID mappings"},{"location":"administer-and-observe/starting_tcp_mon/","text":"Starting TCPMon \u00b6 TCPMon is available in the <PRODUCT_HOME>/bin directory of any WSO2 Carbon based product distribution. Alternatively, you can download TCPMon from Apache and run the tool. Running TCPMon (from Carbon product pack) Running TCPMon (downloaded from Apache) Running TCPMon (from Carbon product pack) \u00b6 Ensure that the following prerequisites are fulfilled in order to run TCPMon. Install JDK 1.4 or later version. Set the JAVA_HOME variable. This setting is required only if you are using the TCPMon available in the WSO2 Carbon based product pack. !!! info For information on how to set the ` JAVA_HOME ` variable, go to [Installing the Product](https://docs.wso2.com/display/Carbon440/Installing+the+Product) , select the instructions relevant to your operating system and refer the 'Setting JAVA\\_HOME' section. To run the TCPMon available with your WSO2 Carbon product pack: Go to <PRODUCT_HOME>/bin directory of your product pack. Execute the following command to run the tool. For Windows tcpmon.bat For Linux ./tcpmon.sh Running TCPMon (downloaded from Apache) \u00b6 To download TCPMon from Apache and run the tool: Download TCPMon from the following location: http://archive.apache.org/dist/ws/tcpmon/1.0/tcpmon-1.0-bin.zip . Extract tcpmon-1.0-bin.zip archive. Go to the build of the extracted directory to find the execution script. Execute the following command to run the tool. For Windows tcpmon.bat For Linux ./tcpmon.sh","title":"Starting TCPMon"},{"location":"administer-and-observe/starting_tcp_mon/#starting-tcpmon","text":"TCPMon is available in the <PRODUCT_HOME>/bin directory of any WSO2 Carbon based product distribution. Alternatively, you can download TCPMon from Apache and run the tool. Running TCPMon (from Carbon product pack) Running TCPMon (downloaded from Apache)","title":"Starting TCPMon"},{"location":"administer-and-observe/starting_tcp_mon/#running-tcpmon-from-carbon-product-pack","text":"Ensure that the following prerequisites are fulfilled in order to run TCPMon. Install JDK 1.4 or later version. Set the JAVA_HOME variable. This setting is required only if you are using the TCPMon available in the WSO2 Carbon based product pack. !!! info For information on how to set the ` JAVA_HOME ` variable, go to [Installing the Product](https://docs.wso2.com/display/Carbon440/Installing+the+Product) , select the instructions relevant to your operating system and refer the 'Setting JAVA\\_HOME' section. To run the TCPMon available with your WSO2 Carbon product pack: Go to <PRODUCT_HOME>/bin directory of your product pack. Execute the following command to run the tool. For Windows tcpmon.bat For Linux ./tcpmon.sh","title":"Running TCPMon (from Carbon product pack)"},{"location":"administer-and-observe/starting_tcp_mon/#running-tcpmon-downloaded-from-apache","text":"To download TCPMon from Apache and run the tool: Download TCPMon from the following location: http://archive.apache.org/dist/ws/tcpmon/1.0/tcpmon-1.0-bin.zip . Extract tcpmon-1.0-bin.zip archive. Go to the build of the extracted directory to find the execution script. Execute the following command to run the tool. For Windows tcpmon.bat For Linux ./tcpmon.sh","title":"Running TCPMon (downloaded from Apache)"},{"location":"administer-and-observe/using_jvm_metrics/","text":"Using JVM Metrics \u00b6 JVM metrics are Java metrics enabled by default in WSO2 products for the purpose of monitoring general statistics related to server performance. Follow the steps given below to use the JVM metrics dashboard in a WSO2 product. Info For detailed instructions on enabling/disabling JVM metrics and configuring the metric gauges, see Setting up Carbon Metrics . Log in to the management console of your WSO2 product. Click Monitor -> Metrics -> JVM Metrics to open the View Metrics page. Specify the source for the JVM metrics by selecting a value from the drop-down list for the Source parameter in the top panel. Specify the time interval for which the statistics should be displayed in the dashboard by selecting a value from the following drop-down list in the top panel. Click the required buttons opposite Views in the top panel to select the types of information you want to view in the dashboard and refresh the web page. Statistics corresponding to each button can be viewed as follows: **CPU ** Click this button to view statistics relating to the CPU as shown below. Memory Click Memory to view statistics relating to the memory as shown below. Threading ** Click **Threading to view statistics relating to threading as shown below. Class Loading ** Click **Class Loading to view statistics relating to class loading as shown below. File Descriptor ** Click **File Descriptor to view information relating to the file descriptor count as shown below.","title":"Using JVM Metrics"},{"location":"administer-and-observe/using_jvm_metrics/#using-jvm-metrics","text":"JVM metrics are Java metrics enabled by default in WSO2 products for the purpose of monitoring general statistics related to server performance. Follow the steps given below to use the JVM metrics dashboard in a WSO2 product. Info For detailed instructions on enabling/disabling JVM metrics and configuring the metric gauges, see Setting up Carbon Metrics . Log in to the management console of your WSO2 product. Click Monitor -> Metrics -> JVM Metrics to open the View Metrics page. Specify the source for the JVM metrics by selecting a value from the drop-down list for the Source parameter in the top panel. Specify the time interval for which the statistics should be displayed in the dashboard by selecting a value from the following drop-down list in the top panel. Click the required buttons opposite Views in the top panel to select the types of information you want to view in the dashboard and refresh the web page. Statistics corresponding to each button can be viewed as follows: **CPU ** Click this button to view statistics relating to the CPU as shown below. Memory Click Memory to view statistics relating to the memory as shown below. Threading ** Click **Threading to view statistics relating to threading as shown below. Class Loading ** Click **Class Loading to view statistics relating to class loading as shown below. File Descriptor ** Click **File Descriptor to view information relating to the file descriptor count as shown below.","title":"Using JVM Metrics"},{"location":"administer-and-observe/using_messaging_metrics/","text":"Using Messaging Metrics \u00b6 Messaging metrics are the Java metrics enabled in the WSO2 Message Broker (MB) or the Broker Profile of WSO2 EI for the purpose of monitoring broker-specific statistics. Note The metrics feature is enabled in the broker by default. See the topic on setting up WSO2 Carbon metrics for details on how metrics are enabled. Follow the steps given below for instructions on using the Messaging Metrics dashboard. Log in to the management console of the broker and click Monitor -> Metrics -> Messaging Metrics . The View Metrics page will open. At the top of this page, you will find the following panel: First, select the Source from the drop-down list. In a clustered setup, you must specify the broker node that you want to monitor. You can specify the time interval for which the statistics displayed are valid. By default, you will see statistics from the last 5 minutes. In the Views section, you will find buttons corresponding to the different types of metrics that you want to view. You can click the relevant button to view the statistics. Given below are the statistics corresponding to each button: Disruptor Metric Description Total Messages in Inbound Disruptor The Disruptor is a new open-source concurrency framework, designed as a high-performance mechanism for inter-thread messaging. The current number of messages in the inbound disruptor can be viewed here. Total Acks in Inbound Disruptor The current number of acknowledgments in the inbound disruptor. Total Messages in Outbound Disruptor The current number of messages in the outbound disruptor. Publish & Subscribe Metric Description Total Queue Subscribers This metric shows the total number of active queue subscribers for a particular broker node. This is an INFO level metric. Total Topic Subscribers The total number of active topic subscribers. Total Channels The total number of active channels. Messages & Acknowledgements Metric Description Messages Received/ sec This metric provides the number of messages received per second by a particular broker node. This metric is calculated when a message reaches the server. Messages Published/ sec This metric provides the number of messages published per second. This metric is calculated when the server publishes a message to a subscriber. Acknowledges Received /sec This metric provides the number of acknowledgments received from publishers per second. Acknowledges Sent /sec This metric provides the number of acknowledgments sent to publishers per second. Database Metric Description Database write latency The average time taken for database write calls. Database read latency The average time taken for database read calls. Database Method latency The average time taken by message store implementation methods.","title":"Using Messaging Metrics"},{"location":"administer-and-observe/using_messaging_metrics/#using-messaging-metrics","text":"Messaging metrics are the Java metrics enabled in the WSO2 Message Broker (MB) or the Broker Profile of WSO2 EI for the purpose of monitoring broker-specific statistics. Note The metrics feature is enabled in the broker by default. See the topic on setting up WSO2 Carbon metrics for details on how metrics are enabled. Follow the steps given below for instructions on using the Messaging Metrics dashboard. Log in to the management console of the broker and click Monitor -> Metrics -> Messaging Metrics . The View Metrics page will open. At the top of this page, you will find the following panel: First, select the Source from the drop-down list. In a clustered setup, you must specify the broker node that you want to monitor. You can specify the time interval for which the statistics displayed are valid. By default, you will see statistics from the last 5 minutes. In the Views section, you will find buttons corresponding to the different types of metrics that you want to view. You can click the relevant button to view the statistics. Given below are the statistics corresponding to each button: Disruptor Metric Description Total Messages in Inbound Disruptor The Disruptor is a new open-source concurrency framework, designed as a high-performance mechanism for inter-thread messaging. The current number of messages in the inbound disruptor can be viewed here. Total Acks in Inbound Disruptor The current number of acknowledgments in the inbound disruptor. Total Messages in Outbound Disruptor The current number of messages in the outbound disruptor. Publish & Subscribe Metric Description Total Queue Subscribers This metric shows the total number of active queue subscribers for a particular broker node. This is an INFO level metric. Total Topic Subscribers The total number of active topic subscribers. Total Channels The total number of active channels. Messages & Acknowledgements Metric Description Messages Received/ sec This metric provides the number of messages received per second by a particular broker node. This metric is calculated when a message reaches the server. Messages Published/ sec This metric provides the number of messages published per second. This metric is calculated when the server publishes a message to a subscriber. Acknowledges Received /sec This metric provides the number of acknowledgments received from publishers per second. Acknowledges Sent /sec This metric provides the number of acknowledgments sent to publishers per second. Database Metric Description Database write latency The average time taken for database write calls. Database read latency The average time taken for database read calls. Database Method latency The average time taken by message store implementation methods.","title":"Using Messaging Metrics"},{"location":"concepts/concepts/","text":"Concepts \u00b6","title":"Concepts"},{"location":"concepts/concepts/#concepts","text":"","title":"Concepts"},{"location":"develop/cicd/","text":"","title":"Continuous Development and Deployment"},{"location":"develop/designing-an-integration-usecase/","text":"","title":"Designing an Integration Use Case"},{"location":"develop/error_handling/","text":"Error Handling \u00b6 The main role of WSO2 Enterprise Integrator (WSO2 EI) is to act as the backbone of an organization\u2019s service-oriented architecture. It is the spine through which all the systems and applications within the enterprise (and external applications that integrate with the enterprise) communicate with each other. For example, an ESB (which is contained in WSO2 EI) often has to deal with many wire-level protocols, messaging standards, and remote APIs. But applications and networks can be full of errors. Applications crash. Network routers and links get into states where they cannot pass messages through with the expected efficiency. These error conditions are very likely to cause a fault or trigger a runtime exception in the ESB. Using fault sequences \u00b6 WSO2 EI provides fault sequences for dealing with errors. A fault sequence is a collection of mediators just like any other sequence, and it can be associated with another sequence or a proxy service. When the sequence or the proxy service encounters an error during mediation or while forwarding a message, the message that triggered the error is delegated to the specified fault sequence. Using the available mediators it is possible to log the erroneous message, forward it to a special error-tracking service, and send a SOAP fault back to the client indicating the error or even send an email to the system admin. It is not mandatory to associate each sequence and proxy service with a fault sequence. In situations where a fault sequence is not specified explicitly, a default fault sequence will be used to handle errors. Sample 4: Specifying a Fault Sequence with a Regular Mediation Sequence shows how to specify a fault sequence with a regular mediation sequence. Whenever an error occurs in WSO2 EI, the mediation engine attempts to provide as much information as possible on the error to the user by initializing the following properties on the erroneous message: ERROR_CODE ERROR_MESSAGE ERROR_DETAIL ERROR_EXCEPTION Within the fault sequence, you can access these property values using the get-property XPath function. Sample 4 uses the log mediator as follows to log the actual error message: <log level=\"custom\"> <property name=\"text\" value=\"An unexpected error occured\"/> <property name=\"message\" expression=\"get-property('ERROR_MESSAGE')\"/> </log> Note how the ERROR_MESSAGE property is being used to get the error message text. If you want to customize the error message that is sent back to the client, you can use the Fault mediator as demonstrated in Sample 5: Creating SOAP Fault Messages and Changing the Direction of a Message . Error codes \u00b6 This section describes error codes and their meanings. Transport error codes \u00b6 Error Code Detail 101000 Receiver input/output error sending 101001 Receiver input/output error receiving 101500 Sender input/output error sending 101501 Sender input/output error receiving 101503 Connection failed 101504 Connection timed out (no input was detected on this connection over the maximum period of inactivity) 101505 Connection closed 101506 NHTTP protocol violation 101507 Connection canceled 101508 Request to establish new connection timed out 101509 Send abort 101510 Response processing failed If the HTTP PassThrough transport is used, and a connection-level error occurs, the error code is calculated using the following equation: Error code = Base error code + Protocol State The sender side of the transport matches the protocol state, which changes according to the phase of the message. Following are the possible protocol states and the description for each: Protocol State Description REQUEST_READY (0) Connection is at the initial stage ready to send a request REQUEST_HEAD(1) Sending the request headers through the connection REQUEST_BODY(2) Sending the request body REQUEST_DONE(3) Request is completely sent RESPONSE_HEAD(4) The connection is reading the response headers RESPONSE_BODY(5) The connection is reading the response body RESPONSE_DONE(6) The response is completed CLOSING(7) The connection is closing CLOSED(8) The connection is closed Since there are several possible protocol states in which a request can time out, you can calculate the error code accordingly using the values in the table above. For example, in a scenario where you send a request and the request is completely sent to the backend, but a timeout happens before the response headers are received, the error code is calculated as follows: In this scenario, the base error code is CONNECTION_TIMEOUT(101504) and the protocol state is REQUEST_DONE(3). Therefore, Error code = 101504 + 3 = 101507 Endpoint failures \u00b6 This section describes the error codes for endpoint failures. For more information on handling endpoint errors, see Endpoint Error Handling . General errors Error Code Detail 303000 Load Balance endpoint is not ready to connect 303000 Recipient List Endpoint is not ready 303000 Failover endpoint is not ready to connect 303001 Address Endpoint is not ready to connect 303002 WSDL Address is not ready to connect Failure on endpoint in the session Error Code Detail 309001 Session aware load balance endpoint, No ready child endpoints 309002 Session aware load balance endpoint, Invalid reference 309003 Session aware load balance endpoint, Failed session Non-fatal warnings Error Code Detail 303100 A failover occurred in a Load balance endpoint 304100 A failover occurred in a Failover endpoint Referring real endpoint is null Error Code Detail 305100 Indirect endpoint not ready Callout operation failed Error Code Detail 401000 Callout operation failed (from the Callout mediator) 401001 Blocking call operation failed (from the Call mediator when you have enabled blocking in it). 401002 Blocking sender operation failed (from the Call mediator when you have enabled blocking in it). Custom error codes \u00b6 Error Code Detail 500000 Endpoint Custom Error - This error is triggered when the endpoint is prefixed by <property name=\"FORCE_ERROR_ON_SOAP_FAULT\" value=\"true\"/> , which enhances the failover logic by marking an endpoint as suspended when the response is a SOAP fault.","title":"Error Handling"},{"location":"develop/error_handling/#error-handling","text":"The main role of WSO2 Enterprise Integrator (WSO2 EI) is to act as the backbone of an organization\u2019s service-oriented architecture. It is the spine through which all the systems and applications within the enterprise (and external applications that integrate with the enterprise) communicate with each other. For example, an ESB (which is contained in WSO2 EI) often has to deal with many wire-level protocols, messaging standards, and remote APIs. But applications and networks can be full of errors. Applications crash. Network routers and links get into states where they cannot pass messages through with the expected efficiency. These error conditions are very likely to cause a fault or trigger a runtime exception in the ESB.","title":"Error Handling"},{"location":"develop/error_handling/#using-fault-sequences","text":"WSO2 EI provides fault sequences for dealing with errors. A fault sequence is a collection of mediators just like any other sequence, and it can be associated with another sequence or a proxy service. When the sequence or the proxy service encounters an error during mediation or while forwarding a message, the message that triggered the error is delegated to the specified fault sequence. Using the available mediators it is possible to log the erroneous message, forward it to a special error-tracking service, and send a SOAP fault back to the client indicating the error or even send an email to the system admin. It is not mandatory to associate each sequence and proxy service with a fault sequence. In situations where a fault sequence is not specified explicitly, a default fault sequence will be used to handle errors. Sample 4: Specifying a Fault Sequence with a Regular Mediation Sequence shows how to specify a fault sequence with a regular mediation sequence. Whenever an error occurs in WSO2 EI, the mediation engine attempts to provide as much information as possible on the error to the user by initializing the following properties on the erroneous message: ERROR_CODE ERROR_MESSAGE ERROR_DETAIL ERROR_EXCEPTION Within the fault sequence, you can access these property values using the get-property XPath function. Sample 4 uses the log mediator as follows to log the actual error message: <log level=\"custom\"> <property name=\"text\" value=\"An unexpected error occured\"/> <property name=\"message\" expression=\"get-property('ERROR_MESSAGE')\"/> </log> Note how the ERROR_MESSAGE property is being used to get the error message text. If you want to customize the error message that is sent back to the client, you can use the Fault mediator as demonstrated in Sample 5: Creating SOAP Fault Messages and Changing the Direction of a Message .","title":"Using fault sequences"},{"location":"develop/error_handling/#error-codes","text":"This section describes error codes and their meanings.","title":"Error codes"},{"location":"develop/error_handling/#transport-error-codes","text":"Error Code Detail 101000 Receiver input/output error sending 101001 Receiver input/output error receiving 101500 Sender input/output error sending 101501 Sender input/output error receiving 101503 Connection failed 101504 Connection timed out (no input was detected on this connection over the maximum period of inactivity) 101505 Connection closed 101506 NHTTP protocol violation 101507 Connection canceled 101508 Request to establish new connection timed out 101509 Send abort 101510 Response processing failed If the HTTP PassThrough transport is used, and a connection-level error occurs, the error code is calculated using the following equation: Error code = Base error code + Protocol State The sender side of the transport matches the protocol state, which changes according to the phase of the message. Following are the possible protocol states and the description for each: Protocol State Description REQUEST_READY (0) Connection is at the initial stage ready to send a request REQUEST_HEAD(1) Sending the request headers through the connection REQUEST_BODY(2) Sending the request body REQUEST_DONE(3) Request is completely sent RESPONSE_HEAD(4) The connection is reading the response headers RESPONSE_BODY(5) The connection is reading the response body RESPONSE_DONE(6) The response is completed CLOSING(7) The connection is closing CLOSED(8) The connection is closed Since there are several possible protocol states in which a request can time out, you can calculate the error code accordingly using the values in the table above. For example, in a scenario where you send a request and the request is completely sent to the backend, but a timeout happens before the response headers are received, the error code is calculated as follows: In this scenario, the base error code is CONNECTION_TIMEOUT(101504) and the protocol state is REQUEST_DONE(3). Therefore, Error code = 101504 + 3 = 101507","title":"Transport error codes"},{"location":"develop/error_handling/#endpoint-failures","text":"This section describes the error codes for endpoint failures. For more information on handling endpoint errors, see Endpoint Error Handling . General errors Error Code Detail 303000 Load Balance endpoint is not ready to connect 303000 Recipient List Endpoint is not ready 303000 Failover endpoint is not ready to connect 303001 Address Endpoint is not ready to connect 303002 WSDL Address is not ready to connect Failure on endpoint in the session Error Code Detail 309001 Session aware load balance endpoint, No ready child endpoints 309002 Session aware load balance endpoint, Invalid reference 309003 Session aware load balance endpoint, Failed session Non-fatal warnings Error Code Detail 303100 A failover occurred in a Load balance endpoint 304100 A failover occurred in a Failover endpoint Referring real endpoint is null Error Code Detail 305100 Indirect endpoint not ready Callout operation failed Error Code Detail 401000 Callout operation failed (from the Callout mediator) 401001 Blocking call operation failed (from the Call mediator when you have enabled blocking in it). 401002 Blocking sender operation failed (from the Call mediator when you have enabled blocking in it).","title":"Endpoint failures"},{"location":"develop/error_handling/#custom-error-codes","text":"Error Code Detail 500000 Endpoint Custom Error - This error is triggered when the endpoint is prefixed by <property name=\"FORCE_ERROR_ON_SOAP_FAULT\" value=\"true\"/> , which enhances the failover logic by marking an endpoint as suspended when the response is a SOAP fault.","title":"Custom error codes"},{"location":"develop/testing/","text":"","title":"Testing an Integration Use Case"},{"location":"develop/troubleshooting-WSO2-Integration-Studio/","text":"Troubleshooting WSO2 Integration Studio \u00b6 The following are some common errors that you may encounter when using WSO2 Integration Studio and the way to troubleshoot and fix them. Getting the 'Save could not be completed' error? Removing an artifact Retrieving a missing project view Getting the 'Save could not be completed' error? \u00b6 Are you getting the following error while trying to save the artifacts using the MacOS? {width=\"631\" height=\"250\"} To avoid this error, Mac users need to copy the Eclipse.app file into the Applications directory . If the Eclipse link is outside the Applications directory it does not have the permission to write files. As a result, this error is thrown. Removing an artifact \u00b6 Once you remove an artifact, you need to save and reopen the CApp .pom file. If not, the .pom file gets corrupted once the artifact is removed. Retrieving a missing project view \u00b6 If your project view suddenly goes missing, you can get it back by navigating to Window -> Perspective -> Reset Perspective from the toolbar.","title":"Troubleshooting WSO2 Integration Studio"},{"location":"develop/troubleshooting-WSO2-Integration-Studio/#troubleshooting-wso2-integration-studio","text":"The following are some common errors that you may encounter when using WSO2 Integration Studio and the way to troubleshoot and fix them. Getting the 'Save could not be completed' error? Removing an artifact Retrieving a missing project view","title":"Troubleshooting WSO2 Integration Studio"},{"location":"develop/troubleshooting-WSO2-Integration-Studio/#getting-the-save-could-not-be-completed-error","text":"Are you getting the following error while trying to save the artifacts using the MacOS? {width=\"631\" height=\"250\"} To avoid this error, Mac users need to copy the Eclipse.app file into the Applications directory . If the Eclipse link is outside the Applications directory it does not have the permission to write files. As a result, this error is thrown.","title":"Getting the 'Save could not be completed' error?"},{"location":"develop/troubleshooting-WSO2-Integration-Studio/#removing-an-artifact","text":"Once you remove an artifact, you need to save and reopen the CApp .pom file. If not, the .pom file gets corrupted once the artifact is removed.","title":"Removing an artifact"},{"location":"develop/troubleshooting-WSO2-Integration-Studio/#retrieving-a-missing-project-view","text":"If your project view suddenly goes missing, you can get it back by navigating to Window -> Perspective -> Reset Perspective from the toolbar.","title":"Retrieving a missing project view"},{"location":"develop/wSO2-Integration-Studio/","text":"WSO2 Integration Studio \u00b6 WSO2 Integration Studio is a development environment used to design your integration scenarios and develop them. This can be used to develop services, features and artifacts as well as manage their links and dependencies through a simplified graphical editor. Workbench | Editor | Project explorer | Outline | Properties | Console | Template guide Workbench \u00b6 When you start using WSO2 Integration Studio, one of the first decisions you have to make is where to store your projects and files. This location is called the workspace. The Workbench is essentially different windows and editors that can be used for various purposes related to your workspace. This includes, the Project Explorer, the Editor, Template Guide, Outline, etc. Multiple Workbench windows can be opened simultaneously for an enhanced and fine-tuned developer experience. {width=\"900\"} Editor \u00b6 You can associate different editors with different types of files. For example, when you open a file for editing by double-clicking it in one of the navigation views, the associated editor opens in the Workbench. If there is no associated editor for a resource, the Workbench attempts to launch an external editor outside the Workbench. (On Windows, the Workbench will first attempt to launch the editor in place as an OLE document. This type of editor is referred to as an embedded editor. For example, if you have a .doc file in the Workbench and Microsoft Word is registered as the editor for .doc files in your operating system, then opening the file will launch Word as an OLE document within the Workbench editor area. The Workbench menu bar and toolbar will be updated with options for Microsoft Word.) Any number of editors can be open at once, but only one can be active at a time. The main menu bar and toolbar for the Workbench window contain operations that are applicable to the active editor. Tabs in the editor area indicate the names of resources that are currently open for editing. An asterisk (*) indicates that an editor has unsaved changes. By default, editors are stacked in the editor area, but you can choose to tile them in order to view source files simultaneously. Here is an example of a text editor in the Workbench. This is the source view of a .pom file. {width=\"700\"} Project explorer \u00b6 The Project Explorer provides a view of all the files and folder structure within a project. Outline \u00b6 The Outline view displays an outline of a structured file that is currently open in the editor area, and lists structural elements. It enables you to hide certain fields, methods, and types, and also allows you to sort and filter to find what you want. The contents of the Outline view are editor specific. For example, in a Java source file, the structural elements are classes, fields, and methods. The contents of the toolbar are also editor specific. Properties \u00b6 The properties view displays property names and values for a selected item such as a resource. Toolbar buttons allow you to toggle to display properties by category or to filter advanced properties. Another toolbar button allows you to restore the selected property to its default value. {width=\"700\"} Console \u00b6 The Consol e View displays a variety of consol e types depending on the type of development and the current set of user settings. The three consol e s that are provided by default with WSO2 Integration Studio are: Process Consol e - Shows standard output, error, and input Stacktrace Consol e - Well-formatted Java stacktrace with hyperlinks to specific source code locations CVS Consol e - Displays output from CVS operations Template guide \u00b6 The template guide includes a list of sample projects that represent integration scenarios. You can use these to explore WSO2 EI and see a sample of how it addresses common integration problems. What's next See Installing WSO2 Integration Studio for installation instructions. See Working with WSO2 Integration Studio for more information on how to setup and use tooling. See Troubleshooting WSO2 Integration Studio for information on troubleshooting errors you may run into while using EI Tooling.","title":"WSO2 Integration Studio"},{"location":"develop/wSO2-Integration-Studio/#wso2-integration-studio","text":"WSO2 Integration Studio is a development environment used to design your integration scenarios and develop them. This can be used to develop services, features and artifacts as well as manage their links and dependencies through a simplified graphical editor. Workbench | Editor | Project explorer | Outline | Properties | Console | Template guide","title":"WSO2 Integration Studio"},{"location":"develop/wSO2-Integration-Studio/#workbench","text":"When you start using WSO2 Integration Studio, one of the first decisions you have to make is where to store your projects and files. This location is called the workspace. The Workbench is essentially different windows and editors that can be used for various purposes related to your workspace. This includes, the Project Explorer, the Editor, Template Guide, Outline, etc. Multiple Workbench windows can be opened simultaneously for an enhanced and fine-tuned developer experience. {width=\"900\"}","title":"Workbench"},{"location":"develop/wSO2-Integration-Studio/#editor","text":"You can associate different editors with different types of files. For example, when you open a file for editing by double-clicking it in one of the navigation views, the associated editor opens in the Workbench. If there is no associated editor for a resource, the Workbench attempts to launch an external editor outside the Workbench. (On Windows, the Workbench will first attempt to launch the editor in place as an OLE document. This type of editor is referred to as an embedded editor. For example, if you have a .doc file in the Workbench and Microsoft Word is registered as the editor for .doc files in your operating system, then opening the file will launch Word as an OLE document within the Workbench editor area. The Workbench menu bar and toolbar will be updated with options for Microsoft Word.) Any number of editors can be open at once, but only one can be active at a time. The main menu bar and toolbar for the Workbench window contain operations that are applicable to the active editor. Tabs in the editor area indicate the names of resources that are currently open for editing. An asterisk (*) indicates that an editor has unsaved changes. By default, editors are stacked in the editor area, but you can choose to tile them in order to view source files simultaneously. Here is an example of a text editor in the Workbench. This is the source view of a .pom file. {width=\"700\"}","title":"Editor"},{"location":"develop/wSO2-Integration-Studio/#project-explorer","text":"The Project Explorer provides a view of all the files and folder structure within a project.","title":"Project explorer"},{"location":"develop/wSO2-Integration-Studio/#outline","text":"The Outline view displays an outline of a structured file that is currently open in the editor area, and lists structural elements. It enables you to hide certain fields, methods, and types, and also allows you to sort and filter to find what you want. The contents of the Outline view are editor specific. For example, in a Java source file, the structural elements are classes, fields, and methods. The contents of the toolbar are also editor specific.","title":"Outline"},{"location":"develop/wSO2-Integration-Studio/#properties","text":"The properties view displays property names and values for a selected item such as a resource. Toolbar buttons allow you to toggle to display properties by category or to filter advanced properties. Another toolbar button allows you to restore the selected property to its default value. {width=\"700\"}","title":"Properties"},{"location":"develop/wSO2-Integration-Studio/#console","text":"The Consol e View displays a variety of consol e types depending on the type of development and the current set of user settings. The three consol e s that are provided by default with WSO2 Integration Studio are: Process Consol e - Shows standard output, error, and input Stacktrace Consol e - Well-formatted Java stacktrace with hyperlinks to specific source code locations CVS Consol e - Displays output from CVS operations","title":"Console"},{"location":"develop/wSO2-Integration-Studio/#template-guide","text":"The template guide includes a list of sample projects that represent integration scenarios. You can use these to explore WSO2 EI and see a sample of how it addresses common integration problems. What's next See Installing WSO2 Integration Studio for installation instructions. See Working with WSO2 Integration Studio for more information on how to setup and use tooling. See Troubleshooting WSO2 Integration Studio for information on troubleshooting errors you may run into while using EI Tooling.","title":"Template guide"},{"location":"develop/working-with-Mediators-via-Tooling/","text":"Working with Mediators via Tooling \u00b6 If you need to create a custom mediator that performs some logic on a message, you can either create a new mediator project , or import an existing mediator project using WSO2 Integration Studio. Tip You need to have WSO2 Integration Studio installed to create a new message store or to import an existing message store. For instructions, see Installing WSO2 Integration Studio . Once a mediator project is finalised, you can export it as a deployable artifact by right-clicking on the project and selecting Export Project as Deployable Archive . This creates a JAR file that you can deploy to the EI. Alternatively, you can group the mediator project as a Composite Application Project, create a Composite Application Archive (CAR), and deploy it to the EI. Info A URL classloader is used to load classes in the mediator (class mediators are not deployed as OSGi bundles). Therefore, it is only possible to refer to the class mediator from artifacts packed in the same CAR file in which the class mediator is packed. Accessing the class mediator from an artifact packed in another CAR file is not possible. However, it is possible to refer to the class mediator from a sequence packed in the same CAR file and call that sequence from any other artifact packed in other CAR files. Creating a mediator project \u00b6 Follow these steps to create a new mediator. Tip Alternatively, you can import a mediator project. To do this, o pen WSO2 Integration Studio, click File , and then click Import . Next, select Existing WSO2 Projects into workspace under the WSO2 category, click Next and upload the pre-packaged project. Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Mediator Project **** in the ****Getting Started**** tab as shown below. Leave the first option selected and click Next . The New Mediator Creation Wizard appears. {width=\"550\"} Do the following: Type a unique name for the project. Specify the package and class names you are creating. Optionally specify the location where you want to save the project (or leave the default location specified). Optionally specify the working set, if any, that you want to include in this project. A Maven POM file will be generated automatically for this project. If you want to include parent POM information in the file from another project in this workspace, click Next , click the Specify Parent from Workspace check box, and then select the parent project. Click Finish . The mediator project is created in the workspace location you specified with a new mediator class that extends org.apache.synapse.mediators.AbstractMediator . Importing a Java Mediator Project \u00b6 Follow the steps below to import a Java mediator project (that includes a Java class, which extends the org.apache.synapse.mediators.AbstractMediator class) to WSO2 Integration Studio. Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Mediator Project **** in the ****Getting Started**** tab as shown below. Select Import From Workspace and click Next . Specify the mediator project in this workspace that you want to import. Only projects with source files that extend org.apache.synapse.mediators.AbstractMediator are listed. Optionally, you can change the location where the mediator project will be created and add it to working sets. Click Finish . The mediator project you selected is created in the location you specified. Info The mediator projects you create using WOS2 Integration Studio are of the org.wso2.developerstudio.eclipse.artifact.mediator.project.nature nature by default. Follow the steps below to view this nature added to the <PROJECT_NAME>/target/.project file of the Java mediator project you imported. Click the View Menu icon, and click Filters and Customization . {width=\"613\" height=\"294\"} Deselect .*resources , and click OK .","title":"Working with Mediators via Tooling"},{"location":"develop/working-with-Mediators-via-Tooling/#working-with-mediators-via-tooling","text":"If you need to create a custom mediator that performs some logic on a message, you can either create a new mediator project , or import an existing mediator project using WSO2 Integration Studio. Tip You need to have WSO2 Integration Studio installed to create a new message store or to import an existing message store. For instructions, see Installing WSO2 Integration Studio . Once a mediator project is finalised, you can export it as a deployable artifact by right-clicking on the project and selecting Export Project as Deployable Archive . This creates a JAR file that you can deploy to the EI. Alternatively, you can group the mediator project as a Composite Application Project, create a Composite Application Archive (CAR), and deploy it to the EI. Info A URL classloader is used to load classes in the mediator (class mediators are not deployed as OSGi bundles). Therefore, it is only possible to refer to the class mediator from artifacts packed in the same CAR file in which the class mediator is packed. Accessing the class mediator from an artifact packed in another CAR file is not possible. However, it is possible to refer to the class mediator from a sequence packed in the same CAR file and call that sequence from any other artifact packed in other CAR files.","title":"Working with Mediators via Tooling"},{"location":"develop/working-with-Mediators-via-Tooling/#creating-a-mediator-project","text":"Follow these steps to create a new mediator. Tip Alternatively, you can import a mediator project. To do this, o pen WSO2 Integration Studio, click File , and then click Import . Next, select Existing WSO2 Projects into workspace under the WSO2 category, click Next and upload the pre-packaged project. Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Mediator Project **** in the ****Getting Started**** tab as shown below. Leave the first option selected and click Next . The New Mediator Creation Wizard appears. {width=\"550\"} Do the following: Type a unique name for the project. Specify the package and class names you are creating. Optionally specify the location where you want to save the project (or leave the default location specified). Optionally specify the working set, if any, that you want to include in this project. A Maven POM file will be generated automatically for this project. If you want to include parent POM information in the file from another project in this workspace, click Next , click the Specify Parent from Workspace check box, and then select the parent project. Click Finish . The mediator project is created in the workspace location you specified with a new mediator class that extends org.apache.synapse.mediators.AbstractMediator .","title":"Creating a mediator project"},{"location":"develop/working-with-Mediators-via-Tooling/#importing-a-java-mediator-project","text":"Follow the steps below to import a Java mediator project (that includes a Java class, which extends the org.apache.synapse.mediators.AbstractMediator class) to WSO2 Integration Studio. Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Mediator Project **** in the ****Getting Started**** tab as shown below. Select Import From Workspace and click Next . Specify the mediator project in this workspace that you want to import. Only projects with source files that extend org.apache.synapse.mediators.AbstractMediator are listed. Optionally, you can change the location where the mediator project will be created and add it to working sets. Click Finish . The mediator project you selected is created in the location you specified. Info The mediator projects you create using WOS2 Integration Studio are of the org.wso2.developerstudio.eclipse.artifact.mediator.project.nature nature by default. Follow the steps below to view this nature added to the <PROJECT_NAME>/target/.project file of the Java mediator project you imported. Click the View Menu icon, and click Filters and Customization . {width=\"613\" height=\"294\"} Deselect .*resources , and click OK .","title":"Importing a Java Mediator Project"},{"location":"develop/working-with-WSO2-Integration-Studio/","text":"Working with WSO2 Integration Studio \u00b6 The following sections describe how you can use the development experience provided by WSO2 Integration Studio to create and manage the artifacts for your integration use case. Installing WSO2 Integration Studio Using Templates Working with ESB artifacts Creating ESB projects Importing ESB projects Creating ESB artifacts Packaging ESB artifacts Using an existing composite application Creating a new composite application Generating Docker images Exporting the ESB artifacts Testing the ESB artifacts Deploying ESB artifacts Using the ESB profile Deploying EI solutions in Integration Cloud Working with business process artifacts Creating business workflows Packaging BPMN artifacts Deploying BPMN artifacts Packaging BPEL/Human Task artifacts Deploying BPEL/Human Task artifacts Installing WSO2 Integration Studio \u00b6 For instructions, see Installing WSO2 Integration Studio . Using Templates \u00b6 WSO2 Integration Studio provides a set of artifact templates that will help you get started with the most prominent integration use cases. C lick the icon on the top-right of the WSO2 Integration Studio interface to open the Getting Started view shown below and then select the required template. There are ESB templates, data services templates, as well as business process templates. {width=\"850\"} Working with ESB artifacts \u00b6 See the topics given below for instructions on how to use WSO2 Integration Studio to build your integration use case. Creating ESB projects \u00b6 An ESB solution consists of one or several project directories. These directories (listed below) store the various ESB artifacts that you create for your integration sequence. ESB Config Project This project directory stores the ESB artifacts that are used when defining a mediation flow. Use one of the following approaches to create an ESB config project: Creating an ESB Solution that includes an ESB Config project Open WSO2 Integration Studio and click ESB Project \u2192 Create New in the Getting Started view as shown below. In the New ESB Solution Project dialog that opens, enter a name for the ESB config project. Select the relevant check boxes if you want to create a Registry Resources project , Connector Exporter project , and/or a Composite Application project along with the ESB Config project. Click Finish to save the projects. The ESB projects are listed in the project explorer as shown below. Creating an individual ESB Config project Open WSO2 Integration Studio and click Miscellaneous \u2192 Create New Config Project in the Getting Started view as shown below. In the dialog that opens, select New ESB Config Project and click Next . Enter a name for the ESB config project. Click Finish and see that the project is now listed in the project explorer. You can now start creating the ESB config artifacts in your ESB Config project. Registry Resource Project Create this project directory if you want to create registry resources for your mediation flow. You can later use these registry artifacts when you define your mediation sequences in the ESB config project . Creating a Registry project Open WSO2 Integration Studio and click Miscellaneous \u2192 Create New Registry Project in the Getting Started view as shown below. In the dialog that opens, enter a name for the registry project. Click Finish and see that the project is now listed in the project explorer. See the instructions on creating and using registry artifacts . Mediator Project Create this project directory to start creating custom mediator artifacts. You can use these customer mediators when you define the mediation flow in your ESB config project . Creating a Mediator project Open WSO2 Integration Studio and click Miscellaneous \u2192 Create Mediator Project in the Getting Started view as shown below. In the dialog that opens, select Create New Mediator and click Next . Enter a project name, package name, and class name. Click Finish and see that the project is now listed in the project explorer. See the instructions on creating and using custom mediators . Data Service Project Create this project directory to start creating data services (.dbs files) for exposing various datasources as a service. Creating a Data Service project Open WSO2 Integration Studio and click DS Project \u2192 Create New Data Service in the Getting Started view as shown below. In the dialog that opens, enter a project name and click Next . Click Finish and see that the project is now listed in the project explorer. See instructions on managing data service artifacts using WSO2 Integration Studio . Connector Exporter Project Create this project directory if you have used ESB connectors in your medition sequence (defined in the ESB config project ). All connector artifacts need to be stored in a connector exporter project before packaging . See the instructions on creating and using connectors . Composite Application Project This poject directory allows you to package all the artifacts (stored in other ESB projects) into one composite application (C-APP). This C-APP can then be deployed in the ESB server. See the instructions on packaging ESB artifacts . You can use the above ESB projects and other various projects as follows: Right-click the Project Explorer and click New \u2192 Project as shown below. {width=\"354\" height=\"250\"} In the New Project dialog that opens, select the required project. {width=\"450\"} Importing ESB projects \u00b6 If you have an already created ESB project file, you can import it to your WSO2 Integration Studio workspace. Open WSO2 Integration Studio, navigate to File -> Import , select Existing WSO2 Projects into workspace, and click Next : {width=\"400\" height=\"417\"} If you have a ZIP file of your project, browse for the archive file , or if you have an extracted project folder, browse for the root directory : {width=\"400\" height=\"463\"} !!! tip Select **Copy projects into workspace** check box if you want to save the project in the workspace. Click Finish , and see that the project files are imported in the project explorer: {width=\"300\"} Creating ESB artifacts \u00b6 Once you have created the ESB projects described above, you can create the artifacts under those projects. ESB Config Artifacts Registry Artifacts Connectors Data Service Custom Mediators After creating the ESB Config project , you can define the mediation flow by using the required integration artifacts. R ight-click the ESB config project, click New , and select the required ESB artifact. See the links given below for more information on each of the artifacts: Proxy Service | REST API | Inbound Endpoint | Scheduled Task | Sequence | Template | Endpoint | Local Entry | Message Processor | Message Store {width=\"600\" height=\"471\"} Registry artifacts are resources (such as images, WSDLs, XSLTs), which are stored in a central repository. To create such artifacts, right-click the Registry Resource project , click New , and select Registry Resource . See the instructions on creating and using registry artifacts . {width=\"600\" height=\"381\"} After creating the Connector Exporter project , right-click the project, click New , and select Add/Remove Connectors to start adding connector artifacts to your project. See the instructions on working with connectors . {width=\"600\" height=\"389\"} Data service artifacts are used for exposing data as a service. After creating the Data Service project , right-click the project, click New , and select required artifact types. See the instructions on creating and using data services . {width=\"600\" height=\"420\"} After creating the Mediator project , right-click the project, click New , and select required artifact types. See the instructions on creating and using custom mediators . {width=\"621\" height=\"250\"} Packaging ESB artifacts \u00b6 To package the ESB artifacts, you need to create a Composite Application Project . Use one of the following methods: Using an existing composite application \u00b6 If you have an already created composite appliction project, do the following to package the ESB artifacts into the composite application: Select the pom.xml file that is under the composite application project in the project explorer. {width=\"350\" height=\"150\"} In the Dependencies section, select the artifacts from each of the projects. !!! info **Note:** If you have created a custom mediator artifact, it should be packaged in the same composite application along with the other artifacts that uses the mediator. {width=\"600\" height=\"197\"} Save the artifacts. Creating a new composite application \u00b6 If you have not previously created a composite application project, do the following to package the artifacts in your ESB Config project. Open the Getting Started view and click Miscellaneous \u2192 Create New Composite Application . {width=\"700\" height=\"343\"} In the New Composite Application Project dialog that opens, s elect the artifacts from the relevant ESB projects and click Finish . {width=\"500\" height=\"506\"} Alternatively, Right-click the project explorer and click New -> Project . {width=\"900\" height=\"379\"} In the New Project dialog that opens, select Composite Application Project from the list and click Next . {width=\"500\"} Give a name for the Composite Application project and select the artifacts that you want to package. {width=\"500\" height=\"548\"} In the Composite Application Project POM Editor that opens, under Dependencies , note the information for each of the projects you selected earlier. {width=\"800\" height=\"392\"} Generating Docker images \u00b6 To generate Docker images, follow the steps below: Tip Before you begin: Install Docker from the Docker Site . Create a Docker Account at Docker Hub and log in. Start the Docker server. Open the WSO2 Integration Studio interface. Open an existing project. Right-click on Composite Project and then click Generate Docker Image . {width=\"350\" height=\"698\"} The WSO2 Platform Distribution - Generate Docker Image wizard opens. Enter information in the wizard as follows: In the Generate Docker Image page, enter the following details: {width=\"732\" height=\"290\"} Name of the application : The name of the composite application with the artifacts created for your EI project. The name of the EI projecty is displayed by default, but it can be changed if required. Application version : The version of the composite application. Name of the Docker Image : A name for the Docker image. Docker Image Tag : A tag for the Docker image to be used for reference. Export Destination : Browse for the preferred location in your machine to export the Docker image. Once you have entered the required details, click Next . 2. In the next page, select the EI projects that you want to include in the Docker image and click Finish . {width=\"732\"} Once the Docker image is successfully created, a message similar to the following appears in your screen. {width=\"300\"} Exporting the ESB artifacts \u00b6 Once you have created a composite application of your artifacts, you can export it into a CAR file (.car file): Select the composite application project in the project explorer, right-click, and click Export Composite Application Project . {width=\"450\" height=\"535\"} In the dialog that opens, give a name for the CAR file, the destination where the file should be saved, and click Next . You can select the artifacts that should be packaged in the CAR file. Click Finish to generate the CAR file. Testing the ESB artifacts \u00b6 You can test artifacts by deploying the packaged artifacts in the built-in Micro Integrator: Be sure to create a composite application project and include your artifacts. Right-click the composite application project and click Export Project Artifacts and Run . {width=\"450\" height=\"498\"} In the dialog that opens, select the artifacts form the composite application project that you want to deploy. {width=\"500\" height=\"307\"} Click Finish . The artifacts will be deployed in the WSO2 Micro Integrator and the server will start. See the startup log in the Console tab: {width=\"656\" height=\"250\"} If you find errors in your mediation sequence, use the debugging features to troubleshoot. Deploying ESB artifacts \u00b6 WSO2 EI includes an ESB profile and WSO2 Integration Studio. The light-weight Micro Integrator is already included in your WSO2 Integration Studio package, which allows you to deploy and run the artifacts instantly. Alternatively, you can add the ESB profile server to your evironment and then deploy and run the artifacts in the ESB. See the instructions given below. Using the ESB profile \u00b6 To deploy the packaged artifacts in the ESB profile of WSO2 EI, you need to first add the ESB server to the tool. Follow the steps given below. Open the Getting Started view and click Miscellaneous \u2192Add New Server to open the New Server dialog. {width=\"700\" height=\"344\"} In the New Server dialog that opens, expand the WSO2 folder and select the version of your server. {width=\"500\" height=\"544\"} Click Next . In the CARBON_HOME field, provide the path to your product's home directory and then click Next again. Review the default port details for your server and click Next . Typically, you can leave these unchanged but if you are already running another server on these ports, give unused ports. !!! tip See [Default Ports of WSO2 Products](https://docs.wso2.com/display/ADMIN44x/Default+Ports+of+WSO2+Products) for more information. {width=\"500\"} To deploy the C-App project to your server, select the composite application from the list, click Add to move it into the configured list, and then click Finish . On the Servers tab, note that the server is currently stopped. Click the 'play' icon on the tool bar. If prompted to save changes to any of the artifact files you created earlier, click Yes . {width=\"600\" height=\"88\"} As the server starts, the Console tab appears. Note messages indicating that the composite app was successfully deployed. You can also deploy/redeploy or remove C-Apps from a running server: To deploy/remove C-Apps, right-click the server, click Add and Remove and follow the instructions on the wizard. {width=\"500\"} If you want to redeploy a C-App after modifying the included artifacts, select the already deployed C-App, right-click and click Redeploy . Deploying EI solutions in Integration Cloud \u00b6 Once you have developed an EI solution, you can host it on the Integration Cloud to make it available for multiple users. To understand how to host a solution on Integration Cloud, follow the steps below: Tip Before you begin: Register as a user of the Integration Cloud . Download WSO2 Integration Studio . Create an EI application as follows: Open WSO2 Integration Studio. In the Getting Started page, click the Hello World Service template to start creating a new EI application based on this template. {width=\"800\" height=\"388\"} In the Create Project Using Hello World Service Template dialog box, enter a name for the application. In this example, let's enter HelloWorldApps as the name. {width=\"600\"} Click Finish to add the project for the application. The project currently has the configurations derived from the template. Let's modify them as follows: !!! tip The purpose of this step is to change the default values. You can skip it if required. !!! info You can skip this step if required. In the left navigator, open the HelloWorldApplication/src/main/synapse-config/proxy-services/HelloWorld.xml file. Then click on the PayloadFactory icon to open the Payload Factory Mediator configuration in the Properties tab. {width=\"900\"} In the Payload field, replace the existing value with {\u201cdata\u201d: \u201cHelloWorld\u201d} . Before deploying the composite application, you need to know the key of the organization to which you are deploying it. To get the organization ID, sign in to the Integration Cloud and access your organization as follows: !!! tip If you already know the key of the organization to which the application needs to be deployed, you can skip this step. Sign in to the Integration Cloud with your credentials. Click on the following icon tray in the right end of the top bar. {width=\"52\"} Then click Organizations to open the Manage Organizations page. {width=\"500\"} The keys of the available organizations are displayed as shown below. {width=\"714\"} Deploy the Hello World Application that you created as follows: In the WSO2 Integration Studio, open your workbench. Then right-click on HelloWorldAppsCompositeApplication , and then click Deploy to Integration Cloud . The WSO2 Integration Cloud - Authentication wizard opens as follows. {width=\"700\"} 2. Enter the following information in the wizard: - Organization Key : The key of the organization to which you want to deploy the EI application. The required organization key needs to be already registered under your Integration Cloud account. - Email : The email address with which you are registered in the Integration Cloud. - Password : The password with which you sign in to the Integration Cloud. 3. Click Finish . The WSO2 Platform Distribution wizard opens. 4. In the WSO2 Platform Distribution wizard, select the applications that you want to include in the CAR file that you are deploying to the Integration Cloud. For this example, select HelloWorldApps as shown below. {width=\"700\"} 5. Click Next , and then click Finish . A message appears to inform you that your application is being deployed to the cloud. Once the deployment is complete, the following message appears. {width=\"400\"} 5. Access your organization on Integration Cloud as you did in step 3. The HelloWorldAppsComposite Application you deployed is displayed as follows. {width=\"600\"} 6. To create a new version, repeat step 4, sub steps a-c. Then follow the steps below to create a new version. 1. In the page where you select deployable artifacts, select HelloWorldApps and click Next . 2. In the next page, select the Create New Version option and update the value displayed in the Application Version field. {width=\"645\" height=\"467\"} 3. Click Finish . 4. Sign in to the Integration Cloud and click on the HelloWorldApps application. {width=\"900\"} The application opens, and the updated version is displayed as shown below. {width=\"900\"} Working with business process artifacts \u00b6 The following topics explain the steps involved in building BPMN/BPEL artifacts and Human Tasks for business workflows, and for deploying them in the Business Process Server profile of WSO2 EI. Creating business workflows \u00b6 BPMN workflows If you are using BPMN to define a business workflow, the BPMN artifacts should be stored in the BPMN project. After creating a BPMN project, add a BPMN diagram to that project. See the instructions given below. 1. Creating a BPMN project Open the Getting Started view and click BP Project \u2192 Create New BPMN Project . In the Create an Activiti Project dialog, e nter a name for the project and click Next . In the next step, select any referenced projects and click Finish . Click Open Perspective in the below message. !!! tip You will not get this message if you are already in the Activiti perspective. You can access the current perspective from the project explorer. When you click Finish , the project will be listed in the project explorer (Activity explorer). 2. Creating BPMN artifacts Right-click your BPMN project and go to New \u2192 Other . Select BPMN Diagram and click Next . Select the diagrams folder in your BPMN project, give a file name for your diagram and click Next . Select a template diagram or choose to create an empty diagram and click Finish . You will see that the BPMN diagram has been added under the project you specified and a new empty diagram will open up along with a palette. You can drag and drop notations from the Pallete to create the desired diagram. See the BPMN tutorials for step-by-step instructions on how to create a BPMN workflow. BPEL workflows If you are using BPEL to define a business workflow, you need to create a BPEL workflow . See the instructions given below. Creating a BPEL workflow Open the Getting Started view and click BP Project \u2192 Create New BPEL Workflow . In the New BPEL Project dialog that opens, select Create New BPEL Workflow and click Next . Enter the project name, process name, process namespace, and template accordingly, and click Finish . Click Open Perspective in the below message. !!! tip You will not get this message if you are already in the BPEL perspective. You can access the current perspective from the project explorer. A n ew BPEL diagram is added to the project explorer under the BPEL project. You can drag and drop artifact symbols from the pallette to create the desired diagram. See the BPEL tutorials for step-by-step instructions on how to create a BPEL workflow. Human Tasks If you want to integrate a 'human task' into a business workflow, you need to defined a Human Task artifact as explained below. Creating human task artifacts Open the Getting Started view and click BP Project \u2192 Create New Human Task as shown below. In the Human Task Project Wizard that opens, enter a project name, process name, process namespace, and click Finish . The Human Task project and artifact files are added to the project explorer as shown below. The 'newfile.ht' file in the project lists the various properties of the human task artifact that should be defined. Click on each of the topics ( Task Properties , Task Input , Task Output , Presentation Elements , People Assignment ) to expand the view as shown below. See the tutorial on simulating a simple order approval process for instructions defining these properties. Task Properties Task Input Task Output Presentation Elements People Assignments See the Human Task tutorials for step-by-step instructions on integrating human tasks into a business workflow. Packaging BPMN artifacts \u00b6 Follow the steps given below. Select Window -> Show View -> Other in the top menu of your screen. {width=\"408\" height=\"250\"} Search for Package Explorer and click Open . {width=\"300\"} In the Package Explorer , right-click the BPMN project and click Create deployment artifacts . {width=\"500\" height=\"530\"} The BPMN artifacts will be pacakged into a .bar file and stored in the / deployment folder as shown below. {width=\"337\" height=\"250\"} Deploying BPMN artifacts \u00b6 You can deploy the .bar file of the BPMN process using the management console of the Business Process profile in WSO2 EI. Install WSO2 Enterprise Integrator and start the Business Process profile by executing one of the given commands. See the installation guide for more information on setting up and running WSO2 EI. On MacOS/Linux/CentOS On Windows Open a terminal and execute the following command: wso2ei-6.5.0-business-process Go to Start Menu -> Programs -> WSO2 -> Enterprise Integrator 6.5.0 **Business Process .** This will open a terminal and start the business process profile. Open the management console from https://localhost:9445/carbon/ . Log in by using admin as the username and password. Go to Main -> Manage -> Add ->BPMN and upload the .bar file. {width=\"526\" height=\"193\"} Packaging BPEL/Human Task artifacts \u00b6 BPEL artifacts and Human Task artifacts can be packaged into separate .zip files. Follow the steps given below. Select one of the projects (BPEL or Human Task) from the Project Explorer . {width=\"300\"} Right-click the project and select Export Project as a Deployable Archive . {width=\"250\" height=\"447\"} When the Project Export dialog opens, provide the location where you want to save the artifact and click Finish . {width=\"450\" height=\"211\"} This will generate a .zip archive that can be deployed directly in the Business Process profile of WSO2 EI. Deploying BPEL/Human Task artifacts \u00b6 Once you have packaged your BPEL or Human Task artifacts, deploy them in the Business Process profile as follows: Install WSO2 Enterprise Integrator and start the Business Process profile by executing one of the given commands. See the installation guide for more information on setting up and running WSO2 EI. On MacOS/Linux/CentOS On Windows Open a terminal and execute the following command: wso2ei-6.5.0-business-process Go to Start Menu -> Programs -> WSO2 -> Enterprise Integrator 6.5.0 **Business Process .** This will open a terminal and start the business process profile. Open the management console from https://localhost:9445/carbon/ . Log in by using admin as the username and password. Go to the Main tab and upload the relevant .zip files. Click Add ->BPEL and upload the .zip file with your BPEL artifacts. {width=\"700\" height=\"191\"} Click Human Tasks \u2192Add and upload the .zip file with your Human Task artifacts. {width=\"676\" height=\"250\"}","title":"Working with Integration Studio"},{"location":"develop/working-with-WSO2-Integration-Studio/#working-with-wso2-integration-studio","text":"The following sections describe how you can use the development experience provided by WSO2 Integration Studio to create and manage the artifacts for your integration use case. Installing WSO2 Integration Studio Using Templates Working with ESB artifacts Creating ESB projects Importing ESB projects Creating ESB artifacts Packaging ESB artifacts Using an existing composite application Creating a new composite application Generating Docker images Exporting the ESB artifacts Testing the ESB artifacts Deploying ESB artifacts Using the ESB profile Deploying EI solutions in Integration Cloud Working with business process artifacts Creating business workflows Packaging BPMN artifacts Deploying BPMN artifacts Packaging BPEL/Human Task artifacts Deploying BPEL/Human Task artifacts","title":"Working with WSO2 Integration Studio"},{"location":"develop/working-with-WSO2-Integration-Studio/#installing-wso2-integration-studio","text":"For instructions, see Installing WSO2 Integration Studio .","title":"Installing WSO2 Integration Studio"},{"location":"develop/working-with-WSO2-Integration-Studio/#using-templates","text":"WSO2 Integration Studio provides a set of artifact templates that will help you get started with the most prominent integration use cases. C lick the icon on the top-right of the WSO2 Integration Studio interface to open the Getting Started view shown below and then select the required template. There are ESB templates, data services templates, as well as business process templates. {width=\"850\"}","title":"Using Templates"},{"location":"develop/working-with-WSO2-Integration-Studio/#working-with-esb-artifacts","text":"See the topics given below for instructions on how to use WSO2 Integration Studio to build your integration use case.","title":"Working with ESB artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#creating-esb-projects","text":"An ESB solution consists of one or several project directories. These directories (listed below) store the various ESB artifacts that you create for your integration sequence. ESB Config Project This project directory stores the ESB artifacts that are used when defining a mediation flow. Use one of the following approaches to create an ESB config project: Creating an ESB Solution that includes an ESB Config project Open WSO2 Integration Studio and click ESB Project \u2192 Create New in the Getting Started view as shown below. In the New ESB Solution Project dialog that opens, enter a name for the ESB config project. Select the relevant check boxes if you want to create a Registry Resources project , Connector Exporter project , and/or a Composite Application project along with the ESB Config project. Click Finish to save the projects. The ESB projects are listed in the project explorer as shown below. Creating an individual ESB Config project Open WSO2 Integration Studio and click Miscellaneous \u2192 Create New Config Project in the Getting Started view as shown below. In the dialog that opens, select New ESB Config Project and click Next . Enter a name for the ESB config project. Click Finish and see that the project is now listed in the project explorer. You can now start creating the ESB config artifacts in your ESB Config project. Registry Resource Project Create this project directory if you want to create registry resources for your mediation flow. You can later use these registry artifacts when you define your mediation sequences in the ESB config project . Creating a Registry project Open WSO2 Integration Studio and click Miscellaneous \u2192 Create New Registry Project in the Getting Started view as shown below. In the dialog that opens, enter a name for the registry project. Click Finish and see that the project is now listed in the project explorer. See the instructions on creating and using registry artifacts . Mediator Project Create this project directory to start creating custom mediator artifacts. You can use these customer mediators when you define the mediation flow in your ESB config project . Creating a Mediator project Open WSO2 Integration Studio and click Miscellaneous \u2192 Create Mediator Project in the Getting Started view as shown below. In the dialog that opens, select Create New Mediator and click Next . Enter a project name, package name, and class name. Click Finish and see that the project is now listed in the project explorer. See the instructions on creating and using custom mediators . Data Service Project Create this project directory to start creating data services (.dbs files) for exposing various datasources as a service. Creating a Data Service project Open WSO2 Integration Studio and click DS Project \u2192 Create New Data Service in the Getting Started view as shown below. In the dialog that opens, enter a project name and click Next . Click Finish and see that the project is now listed in the project explorer. See instructions on managing data service artifacts using WSO2 Integration Studio . Connector Exporter Project Create this project directory if you have used ESB connectors in your medition sequence (defined in the ESB config project ). All connector artifacts need to be stored in a connector exporter project before packaging . See the instructions on creating and using connectors . Composite Application Project This poject directory allows you to package all the artifacts (stored in other ESB projects) into one composite application (C-APP). This C-APP can then be deployed in the ESB server. See the instructions on packaging ESB artifacts . You can use the above ESB projects and other various projects as follows: Right-click the Project Explorer and click New \u2192 Project as shown below. {width=\"354\" height=\"250\"} In the New Project dialog that opens, select the required project. {width=\"450\"}","title":"Creating ESB projects"},{"location":"develop/working-with-WSO2-Integration-Studio/#importing-esb-projects","text":"If you have an already created ESB project file, you can import it to your WSO2 Integration Studio workspace. Open WSO2 Integration Studio, navigate to File -> Import , select Existing WSO2 Projects into workspace, and click Next : {width=\"400\" height=\"417\"} If you have a ZIP file of your project, browse for the archive file , or if you have an extracted project folder, browse for the root directory : {width=\"400\" height=\"463\"} !!! tip Select **Copy projects into workspace** check box if you want to save the project in the workspace. Click Finish , and see that the project files are imported in the project explorer: {width=\"300\"}","title":"Importing ESB projects"},{"location":"develop/working-with-WSO2-Integration-Studio/#creating-esb-artifacts","text":"Once you have created the ESB projects described above, you can create the artifacts under those projects. ESB Config Artifacts Registry Artifacts Connectors Data Service Custom Mediators After creating the ESB Config project , you can define the mediation flow by using the required integration artifacts. R ight-click the ESB config project, click New , and select the required ESB artifact. See the links given below for more information on each of the artifacts: Proxy Service | REST API | Inbound Endpoint | Scheduled Task | Sequence | Template | Endpoint | Local Entry | Message Processor | Message Store {width=\"600\" height=\"471\"} Registry artifacts are resources (such as images, WSDLs, XSLTs), which are stored in a central repository. To create such artifacts, right-click the Registry Resource project , click New , and select Registry Resource . See the instructions on creating and using registry artifacts . {width=\"600\" height=\"381\"} After creating the Connector Exporter project , right-click the project, click New , and select Add/Remove Connectors to start adding connector artifacts to your project. See the instructions on working with connectors . {width=\"600\" height=\"389\"} Data service artifacts are used for exposing data as a service. After creating the Data Service project , right-click the project, click New , and select required artifact types. See the instructions on creating and using data services . {width=\"600\" height=\"420\"} After creating the Mediator project , right-click the project, click New , and select required artifact types. See the instructions on creating and using custom mediators . {width=\"621\" height=\"250\"}","title":"Creating ESB artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#packaging-esb-artifacts","text":"To package the ESB artifacts, you need to create a Composite Application Project . Use one of the following methods:","title":"Packaging ESB artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#using-an-existing-composite-application","text":"If you have an already created composite appliction project, do the following to package the ESB artifacts into the composite application: Select the pom.xml file that is under the composite application project in the project explorer. {width=\"350\" height=\"150\"} In the Dependencies section, select the artifacts from each of the projects. !!! info **Note:** If you have created a custom mediator artifact, it should be packaged in the same composite application along with the other artifacts that uses the mediator. {width=\"600\" height=\"197\"} Save the artifacts.","title":"Using an existing composite application"},{"location":"develop/working-with-WSO2-Integration-Studio/#creating-a-new-composite-application","text":"If you have not previously created a composite application project, do the following to package the artifacts in your ESB Config project. Open the Getting Started view and click Miscellaneous \u2192 Create New Composite Application . {width=\"700\" height=\"343\"} In the New Composite Application Project dialog that opens, s elect the artifacts from the relevant ESB projects and click Finish . {width=\"500\" height=\"506\"} Alternatively, Right-click the project explorer and click New -> Project . {width=\"900\" height=\"379\"} In the New Project dialog that opens, select Composite Application Project from the list and click Next . {width=\"500\"} Give a name for the Composite Application project and select the artifacts that you want to package. {width=\"500\" height=\"548\"} In the Composite Application Project POM Editor that opens, under Dependencies , note the information for each of the projects you selected earlier. {width=\"800\" height=\"392\"}","title":"Creating a new composite application"},{"location":"develop/working-with-WSO2-Integration-Studio/#generating-docker-images","text":"To generate Docker images, follow the steps below: Tip Before you begin: Install Docker from the Docker Site . Create a Docker Account at Docker Hub and log in. Start the Docker server. Open the WSO2 Integration Studio interface. Open an existing project. Right-click on Composite Project and then click Generate Docker Image . {width=\"350\" height=\"698\"} The WSO2 Platform Distribution - Generate Docker Image wizard opens. Enter information in the wizard as follows: In the Generate Docker Image page, enter the following details: {width=\"732\" height=\"290\"} Name of the application : The name of the composite application with the artifacts created for your EI project. The name of the EI projecty is displayed by default, but it can be changed if required. Application version : The version of the composite application. Name of the Docker Image : A name for the Docker image. Docker Image Tag : A tag for the Docker image to be used for reference. Export Destination : Browse for the preferred location in your machine to export the Docker image. Once you have entered the required details, click Next . 2. In the next page, select the EI projects that you want to include in the Docker image and click Finish . {width=\"732\"} Once the Docker image is successfully created, a message similar to the following appears in your screen. {width=\"300\"}","title":"Generating Docker images"},{"location":"develop/working-with-WSO2-Integration-Studio/#exporting-the-esb-artifacts","text":"Once you have created a composite application of your artifacts, you can export it into a CAR file (.car file): Select the composite application project in the project explorer, right-click, and click Export Composite Application Project . {width=\"450\" height=\"535\"} In the dialog that opens, give a name for the CAR file, the destination where the file should be saved, and click Next . You can select the artifacts that should be packaged in the CAR file. Click Finish to generate the CAR file.","title":"Exporting the ESB artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#testing-the-esb-artifacts","text":"You can test artifacts by deploying the packaged artifacts in the built-in Micro Integrator: Be sure to create a composite application project and include your artifacts. Right-click the composite application project and click Export Project Artifacts and Run . {width=\"450\" height=\"498\"} In the dialog that opens, select the artifacts form the composite application project that you want to deploy. {width=\"500\" height=\"307\"} Click Finish . The artifacts will be deployed in the WSO2 Micro Integrator and the server will start. See the startup log in the Console tab: {width=\"656\" height=\"250\"} If you find errors in your mediation sequence, use the debugging features to troubleshoot.","title":"Testing the ESB artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#deploying-esb-artifacts","text":"WSO2 EI includes an ESB profile and WSO2 Integration Studio. The light-weight Micro Integrator is already included in your WSO2 Integration Studio package, which allows you to deploy and run the artifacts instantly. Alternatively, you can add the ESB profile server to your evironment and then deploy and run the artifacts in the ESB. See the instructions given below.","title":"Deploying ESB artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#using-the-esb-profile","text":"To deploy the packaged artifacts in the ESB profile of WSO2 EI, you need to first add the ESB server to the tool. Follow the steps given below. Open the Getting Started view and click Miscellaneous \u2192Add New Server to open the New Server dialog. {width=\"700\" height=\"344\"} In the New Server dialog that opens, expand the WSO2 folder and select the version of your server. {width=\"500\" height=\"544\"} Click Next . In the CARBON_HOME field, provide the path to your product's home directory and then click Next again. Review the default port details for your server and click Next . Typically, you can leave these unchanged but if you are already running another server on these ports, give unused ports. !!! tip See [Default Ports of WSO2 Products](https://docs.wso2.com/display/ADMIN44x/Default+Ports+of+WSO2+Products) for more information. {width=\"500\"} To deploy the C-App project to your server, select the composite application from the list, click Add to move it into the configured list, and then click Finish . On the Servers tab, note that the server is currently stopped. Click the 'play' icon on the tool bar. If prompted to save changes to any of the artifact files you created earlier, click Yes . {width=\"600\" height=\"88\"} As the server starts, the Console tab appears. Note messages indicating that the composite app was successfully deployed. You can also deploy/redeploy or remove C-Apps from a running server: To deploy/remove C-Apps, right-click the server, click Add and Remove and follow the instructions on the wizard. {width=\"500\"} If you want to redeploy a C-App after modifying the included artifacts, select the already deployed C-App, right-click and click Redeploy .","title":"Using the ESB profile"},{"location":"develop/working-with-WSO2-Integration-Studio/#deploying-ei-solutions-in-integration-cloud","text":"Once you have developed an EI solution, you can host it on the Integration Cloud to make it available for multiple users. To understand how to host a solution on Integration Cloud, follow the steps below: Tip Before you begin: Register as a user of the Integration Cloud . Download WSO2 Integration Studio . Create an EI application as follows: Open WSO2 Integration Studio. In the Getting Started page, click the Hello World Service template to start creating a new EI application based on this template. {width=\"800\" height=\"388\"} In the Create Project Using Hello World Service Template dialog box, enter a name for the application. In this example, let's enter HelloWorldApps as the name. {width=\"600\"} Click Finish to add the project for the application. The project currently has the configurations derived from the template. Let's modify them as follows: !!! tip The purpose of this step is to change the default values. You can skip it if required. !!! info You can skip this step if required. In the left navigator, open the HelloWorldApplication/src/main/synapse-config/proxy-services/HelloWorld.xml file. Then click on the PayloadFactory icon to open the Payload Factory Mediator configuration in the Properties tab. {width=\"900\"} In the Payload field, replace the existing value with {\u201cdata\u201d: \u201cHelloWorld\u201d} . Before deploying the composite application, you need to know the key of the organization to which you are deploying it. To get the organization ID, sign in to the Integration Cloud and access your organization as follows: !!! tip If you already know the key of the organization to which the application needs to be deployed, you can skip this step. Sign in to the Integration Cloud with your credentials. Click on the following icon tray in the right end of the top bar. {width=\"52\"} Then click Organizations to open the Manage Organizations page. {width=\"500\"} The keys of the available organizations are displayed as shown below. {width=\"714\"} Deploy the Hello World Application that you created as follows: In the WSO2 Integration Studio, open your workbench. Then right-click on HelloWorldAppsCompositeApplication , and then click Deploy to Integration Cloud . The WSO2 Integration Cloud - Authentication wizard opens as follows. {width=\"700\"} 2. Enter the following information in the wizard: - Organization Key : The key of the organization to which you want to deploy the EI application. The required organization key needs to be already registered under your Integration Cloud account. - Email : The email address with which you are registered in the Integration Cloud. - Password : The password with which you sign in to the Integration Cloud. 3. Click Finish . The WSO2 Platform Distribution wizard opens. 4. In the WSO2 Platform Distribution wizard, select the applications that you want to include in the CAR file that you are deploying to the Integration Cloud. For this example, select HelloWorldApps as shown below. {width=\"700\"} 5. Click Next , and then click Finish . A message appears to inform you that your application is being deployed to the cloud. Once the deployment is complete, the following message appears. {width=\"400\"} 5. Access your organization on Integration Cloud as you did in step 3. The HelloWorldAppsComposite Application you deployed is displayed as follows. {width=\"600\"} 6. To create a new version, repeat step 4, sub steps a-c. Then follow the steps below to create a new version. 1. In the page where you select deployable artifacts, select HelloWorldApps and click Next . 2. In the next page, select the Create New Version option and update the value displayed in the Application Version field. {width=\"645\" height=\"467\"} 3. Click Finish . 4. Sign in to the Integration Cloud and click on the HelloWorldApps application. {width=\"900\"} The application opens, and the updated version is displayed as shown below. {width=\"900\"}","title":"Deploying EI solutions in Integration Cloud"},{"location":"develop/working-with-WSO2-Integration-Studio/#working-with-business-process-artifacts","text":"The following topics explain the steps involved in building BPMN/BPEL artifacts and Human Tasks for business workflows, and for deploying them in the Business Process Server profile of WSO2 EI.","title":"Working with business process artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#creating-business-workflows","text":"BPMN workflows If you are using BPMN to define a business workflow, the BPMN artifacts should be stored in the BPMN project. After creating a BPMN project, add a BPMN diagram to that project. See the instructions given below. 1. Creating a BPMN project Open the Getting Started view and click BP Project \u2192 Create New BPMN Project . In the Create an Activiti Project dialog, e nter a name for the project and click Next . In the next step, select any referenced projects and click Finish . Click Open Perspective in the below message. !!! tip You will not get this message if you are already in the Activiti perspective. You can access the current perspective from the project explorer. When you click Finish , the project will be listed in the project explorer (Activity explorer). 2. Creating BPMN artifacts Right-click your BPMN project and go to New \u2192 Other . Select BPMN Diagram and click Next . Select the diagrams folder in your BPMN project, give a file name for your diagram and click Next . Select a template diagram or choose to create an empty diagram and click Finish . You will see that the BPMN diagram has been added under the project you specified and a new empty diagram will open up along with a palette. You can drag and drop notations from the Pallete to create the desired diagram. See the BPMN tutorials for step-by-step instructions on how to create a BPMN workflow. BPEL workflows If you are using BPEL to define a business workflow, you need to create a BPEL workflow . See the instructions given below. Creating a BPEL workflow Open the Getting Started view and click BP Project \u2192 Create New BPEL Workflow . In the New BPEL Project dialog that opens, select Create New BPEL Workflow and click Next . Enter the project name, process name, process namespace, and template accordingly, and click Finish . Click Open Perspective in the below message. !!! tip You will not get this message if you are already in the BPEL perspective. You can access the current perspective from the project explorer. A n ew BPEL diagram is added to the project explorer under the BPEL project. You can drag and drop artifact symbols from the pallette to create the desired diagram. See the BPEL tutorials for step-by-step instructions on how to create a BPEL workflow. Human Tasks If you want to integrate a 'human task' into a business workflow, you need to defined a Human Task artifact as explained below. Creating human task artifacts Open the Getting Started view and click BP Project \u2192 Create New Human Task as shown below. In the Human Task Project Wizard that opens, enter a project name, process name, process namespace, and click Finish . The Human Task project and artifact files are added to the project explorer as shown below. The 'newfile.ht' file in the project lists the various properties of the human task artifact that should be defined. Click on each of the topics ( Task Properties , Task Input , Task Output , Presentation Elements , People Assignment ) to expand the view as shown below. See the tutorial on simulating a simple order approval process for instructions defining these properties. Task Properties Task Input Task Output Presentation Elements People Assignments See the Human Task tutorials for step-by-step instructions on integrating human tasks into a business workflow.","title":"Creating business workflows"},{"location":"develop/working-with-WSO2-Integration-Studio/#packaging-bpmn-artifacts","text":"Follow the steps given below. Select Window -> Show View -> Other in the top menu of your screen. {width=\"408\" height=\"250\"} Search for Package Explorer and click Open . {width=\"300\"} In the Package Explorer , right-click the BPMN project and click Create deployment artifacts . {width=\"500\" height=\"530\"} The BPMN artifacts will be pacakged into a .bar file and stored in the / deployment folder as shown below. {width=\"337\" height=\"250\"}","title":"Packaging BPMN artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#deploying-bpmn-artifacts","text":"You can deploy the .bar file of the BPMN process using the management console of the Business Process profile in WSO2 EI. Install WSO2 Enterprise Integrator and start the Business Process profile by executing one of the given commands. See the installation guide for more information on setting up and running WSO2 EI. On MacOS/Linux/CentOS On Windows Open a terminal and execute the following command: wso2ei-6.5.0-business-process Go to Start Menu -> Programs -> WSO2 -> Enterprise Integrator 6.5.0 **Business Process .** This will open a terminal and start the business process profile. Open the management console from https://localhost:9445/carbon/ . Log in by using admin as the username and password. Go to Main -> Manage -> Add ->BPMN and upload the .bar file. {width=\"526\" height=\"193\"}","title":"Deploying BPMN artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#packaging-bpelhuman-task-artifacts","text":"BPEL artifacts and Human Task artifacts can be packaged into separate .zip files. Follow the steps given below. Select one of the projects (BPEL or Human Task) from the Project Explorer . {width=\"300\"} Right-click the project and select Export Project as a Deployable Archive . {width=\"250\" height=\"447\"} When the Project Export dialog opens, provide the location where you want to save the artifact and click Finish . {width=\"450\" height=\"211\"} This will generate a .zip archive that can be deployed directly in the Business Process profile of WSO2 EI.","title":"Packaging BPEL/Human Task artifacts"},{"location":"develop/working-with-WSO2-Integration-Studio/#deploying-bpelhuman-task-artifacts","text":"Once you have packaged your BPEL or Human Task artifacts, deploy them in the Business Process profile as follows: Install WSO2 Enterprise Integrator and start the Business Process profile by executing one of the given commands. See the installation guide for more information on setting up and running WSO2 EI. On MacOS/Linux/CentOS On Windows Open a terminal and execute the following command: wso2ei-6.5.0-business-process Go to Start Menu -> Programs -> WSO2 -> Enterprise Integrator 6.5.0 **Business Process .** This will open a terminal and start the business process profile. Open the management console from https://localhost:9445/carbon/ . Log in by using admin as the username and password. Go to the Main tab and upload the relevant .zip files. Click Add ->BPEL and upload the .zip file with your BPEL artifacts. {width=\"700\" height=\"191\"} Click Human Tasks \u2192Add and upload the .zip file with your Human Task artifacts. {width=\"676\" height=\"250\"}","title":"Deploying BPEL/Human Task artifacts"},{"location":"develop/working_with_synapse_passwords/","text":"Working with Passwords in the ESB profile \u00b6 All WSO2 products are shipped with a Secure Vault implementation that allows you to store encrypted passwords that are mapped to aliases. This approach allows you to use the aliases instead of the actual passwords in your configurations for better security. For example, some configurations require the admin username and password. If the admin user's password is \"admin\", you could use UserManager.AdminUser.Password as the password alias. You will then map that alias to the actual \"admin\" password using Secure Vault. The WSO2 product will then look up this alias in Secure Vault during runtime, decrypt and use its password. Note Go to the WSO2 administration guide for more information about the Secure Vault implementation in WSO2 products . In all WSO2 products, Secure Vault is commonly used for encrypting passwords and other sensitive information in configuration files. When you use the ESB profile of WSO2 EI, you can encrypt sensitive information contained in synapse configurations in addition to the information in configuration files. See the following topics: Encrypting passwords in configuration files Encrypting passwords for synapse configurations Using encrypted passwords in synapse configurations Updating the password validation Encrypting passwords in configuration files \u00b6 To encrypt passwords in configuration files, you simply have to update the cipher-text.properties and cipher-tool.properties files that are stored in the <EI_HOME>/conf/security/ directory and then run the Cipher tool that is shipped with the product. Go to the links given below to see instructions in the WSO2 administration guide: Encrypting passwords using the automated process . Encrypting passwords using the manual process . This is relevant when the location of the configuration files (that contain the elements to be encrypted) cannot be specified using an xpath in the cipher - tool.properties file. Changing already encrypted passwords . Resolving already encrypted passwords . Encrypting passwords for synapse configurations \u00b6 The ESB profile of WSO2 EI provides a UI that can be used for encrypting passwords and other sensitive information in synapse configurations. Tip Before you begin , be sure that your registry database has write-access enabled. Open the registry.xml file (stored in the <EI_HOME>/conf/ directory) and ensure that the <readOnly> element is set to false as shown below. <currentDBConfig>wso2registry</currentDBConfig> <readOnly>false</readOnly> <enableCache>true</enableCache> <registryRoot>/</registryRoot> This is necessary because the passwords you encrypt using the management console of the ESB profile are written to the registry DB. If the registry does not have write-access enabled, the required functions on the management console will be disabled. Follow the steps given below if you are using the ESB profile. If you are using the Cipher tool for the first time in your environment, you must first enable the Cipher tool by executing the -Dconfigure command with the cipher tool script: Open a terminal and navigate to the <EI_HOME>/bin directory. Execute one of the following commands: On Linux: ./ciphertool.sh -Dconfigure On Windows: ./ciphertool.bat -Dconfigure Start the ESB profile of WSO2 EI and sign in to the management console: Open a terminal and navigate to the <EI_HOME>/bin directory. Execute one of the following scripts: On Windows: integrator.bat --run On Linux/Mac OS: sh integrator.sh Sign in to the management console. Go to Manage -> Secure Vault Tool and then click Manage Passwords on the Main tab of the management console. The Secure Vault Password Management screen appears. Click Add New Password to encrypt and store, and then specify values for the given fields as shown below. This creates a new password entry in the registry , which is encrypted with the alias (Vault Key) that you specify. Vault Key: The alias for the password. Password: The actual password. Re-enter password: The password that you specified as the actual password. {width=\"669\" height=\"309\"} Using encrypted passwords in synapse configurations \u00b6 To use the alias of an encrypted password in a synapse configuration, you need to add the {wso2:vault-lookup('alias')} custom path expression when you define the synapse configuration. For example, instead of hard coding the admin user's password as <Password>admin</Password> , you can encrypt and store the password using the AdminUser.Password alias as follows: <Password>{wso2:vault-lookup('AdminUser.Password')}</Password>. This password in the synapse configuration can now be retrieved by using the {wso2:vault-lookup('alias')} custom path expression to logically reference the password mapping. Updating the password validation \u00b6 The default expression used for password validation is ^[\\\\S]{5,30}$ . This allows the password to have 5 to 30 characters. If you want to change the expression that is used to validate the password, you need to add the org.wso2.SecureVaultPasswordRegEx system property to the <EI_HOME>/conf/carbon.properties file. Example: org.wso2.SecureVaultPasswordRegEx=^[\\\\S]{5,60}$","title":"Working with Passwords in the ESB profile"},{"location":"develop/working_with_synapse_passwords/#working-with-passwords-in-the-esb-profile","text":"All WSO2 products are shipped with a Secure Vault implementation that allows you to store encrypted passwords that are mapped to aliases. This approach allows you to use the aliases instead of the actual passwords in your configurations for better security. For example, some configurations require the admin username and password. If the admin user's password is \"admin\", you could use UserManager.AdminUser.Password as the password alias. You will then map that alias to the actual \"admin\" password using Secure Vault. The WSO2 product will then look up this alias in Secure Vault during runtime, decrypt and use its password. Note Go to the WSO2 administration guide for more information about the Secure Vault implementation in WSO2 products . In all WSO2 products, Secure Vault is commonly used for encrypting passwords and other sensitive information in configuration files. When you use the ESB profile of WSO2 EI, you can encrypt sensitive information contained in synapse configurations in addition to the information in configuration files. See the following topics: Encrypting passwords in configuration files Encrypting passwords for synapse configurations Using encrypted passwords in synapse configurations Updating the password validation","title":"Working with Passwords in the ESB profile"},{"location":"develop/working_with_synapse_passwords/#encrypting-passwords-in-configuration-files","text":"To encrypt passwords in configuration files, you simply have to update the cipher-text.properties and cipher-tool.properties files that are stored in the <EI_HOME>/conf/security/ directory and then run the Cipher tool that is shipped with the product. Go to the links given below to see instructions in the WSO2 administration guide: Encrypting passwords using the automated process . Encrypting passwords using the manual process . This is relevant when the location of the configuration files (that contain the elements to be encrypted) cannot be specified using an xpath in the cipher - tool.properties file. Changing already encrypted passwords . Resolving already encrypted passwords .","title":"Encrypting passwords in configuration files"},{"location":"develop/working_with_synapse_passwords/#encrypting-passwords-for-synapse-configurations","text":"The ESB profile of WSO2 EI provides a UI that can be used for encrypting passwords and other sensitive information in synapse configurations. Tip Before you begin , be sure that your registry database has write-access enabled. Open the registry.xml file (stored in the <EI_HOME>/conf/ directory) and ensure that the <readOnly> element is set to false as shown below. <currentDBConfig>wso2registry</currentDBConfig> <readOnly>false</readOnly> <enableCache>true</enableCache> <registryRoot>/</registryRoot> This is necessary because the passwords you encrypt using the management console of the ESB profile are written to the registry DB. If the registry does not have write-access enabled, the required functions on the management console will be disabled. Follow the steps given below if you are using the ESB profile. If you are using the Cipher tool for the first time in your environment, you must first enable the Cipher tool by executing the -Dconfigure command with the cipher tool script: Open a terminal and navigate to the <EI_HOME>/bin directory. Execute one of the following commands: On Linux: ./ciphertool.sh -Dconfigure On Windows: ./ciphertool.bat -Dconfigure Start the ESB profile of WSO2 EI and sign in to the management console: Open a terminal and navigate to the <EI_HOME>/bin directory. Execute one of the following scripts: On Windows: integrator.bat --run On Linux/Mac OS: sh integrator.sh Sign in to the management console. Go to Manage -> Secure Vault Tool and then click Manage Passwords on the Main tab of the management console. The Secure Vault Password Management screen appears. Click Add New Password to encrypt and store, and then specify values for the given fields as shown below. This creates a new password entry in the registry , which is encrypted with the alias (Vault Key) that you specify. Vault Key: The alias for the password. Password: The actual password. Re-enter password: The password that you specified as the actual password. {width=\"669\" height=\"309\"}","title":"Encrypting passwords for synapse configurations"},{"location":"develop/working_with_synapse_passwords/#using-encrypted-passwords-in-synapse-configurations","text":"To use the alias of an encrypted password in a synapse configuration, you need to add the {wso2:vault-lookup('alias')} custom path expression when you define the synapse configuration. For example, instead of hard coding the admin user's password as <Password>admin</Password> , you can encrypt and store the password using the AdminUser.Password alias as follows: <Password>{wso2:vault-lookup('AdminUser.Password')}</Password>. This password in the synapse configuration can now be retrieved by using the {wso2:vault-lookup('alias')} custom path expression to logically reference the password mapping.","title":"Using encrypted passwords in synapse configurations"},{"location":"develop/working_with_synapse_passwords/#updating-the-password-validation","text":"The default expression used for password validation is ^[\\\\S]{5,30}$ . This allows the password to have 5 to 30 characters. If you want to change the expression that is used to validate the password, you need to add the org.wso2.SecureVaultPasswordRegEx system property to the <EI_HOME>/conf/carbon.properties file. Example: org.wso2.SecureVaultPasswordRegEx=^[\\\\S]{5,60}$","title":"Updating the password validation"},{"location":"overview/about-this-release/","text":"WSO2 Enterprise Integrator (WSO2 EI) version 6.5.0 is the next level of the ESB, which provides a powerful solution for integrating systems. This brings together the functionalities encapsulated in the following WSO2 products: WSO2 Enterprise Service Bus (ESB) WSO2 Data Services Server (DSS) WSO2 Business Process Server (BPS) WSO2 Message Broker (MB) The features of the above products are encapsulated in WSO2 EI as separate runtimes (profiles). These profiles (which includes developer tooling, and analytics) are listed below. WSO2 EI version 6.5.0 is the successor of version 6.4.0. ESB profile: Consists of WSO2 ESB and WSO2 DSS. Business Process profile: Consists of WSO2 BPS for handling long-running business processes. Message Broker profile: Consists of WSO2 MB for brokering and reliable messaging. MSF4J profile: Provides the capability of creating microservices in Java to facilitate your integration use cases. Analytics profile: Provides the capability of monitoring statistics of integration flows (processed in the ESB runtime) as well as business process (processed in the Business Process runtime). For more information, see Introduction .","title":"About this Release"},{"location":"overview/architecture/","text":"Architecture \u00b6","title":"Architecture"},{"location":"overview/architecture/#architecture","text":"","title":"Architecture"},{"location":"overview/introduction/","text":"Introduction \u00b6 Integration is at the heart of any digital transformation. By connecting different systems that make up your enterprise, you can build an organization that acts as one seamless digital system. WSO2 Enterprise Integrator (EI) is an open source product that enables comprehensive integration for cloud native and container-native projects. WSO2 EI enables you to do the following: Optimize systems and resources Leverage the cloud Reuse legacy systems Create a connected ecosystem for both your customers and partners. Connect enterprise systems to one another Make data accessible accross the enterprise Provide intuitive and visual development tools for continous integration and continous development Help integration developers to create new services and assets WSO2 EI comprises of profiles that offer different integration capabilities. The ESB profile in WSO2 EI provides its fundamental services through an event-driven and standards-based messaging engine (the bus), which allows integration architects to exploit the value of messaging without writing code. This ESB profile is a step ahead of the previous releases of WSO2 Enterprise Service Bus, as it provides data integration capabilities within the same runtime. This eliminates the need to use a separate data services server for your integration processes. The following diagram illustrates the message-flow architecture in the ESB profile of WSO2 EI, which is used for implementing integration flows. This shows how a request propagates to its actual endpoint through the ESB profile. Response handling is the reverse of this operation. Note that the components of the pipes are not in a specific order. An application (client) sends a message to the ESB profile of WSO2 EI. The message is picked up by a transport. The transport sends the message through a message pipe, which handles quality of service aspects such as security. Internally, this pipe is the in-flow and out-flow of the Axis2 engine. The ESB profile can operate in two modes: Mediating Messages - A single pipe is used. Proxy Services - Separate pipes connecting the transport to different proxy services are used. Both message transformation and routing can be considered as a single unit. As the diagram specifies, there is no clear separation between message transformation components and routing components. In the ESB profile of WSO2 EI, this is known as the mediation framework. Some transformations take place before the routing decision has been made while others take place after the routing decision. This is part of the Synapse implementation. The message is injected to the separate pipes depending on the destinations. Here again, quality of service aspects of the messages are determined. The transport layer takes care of the transport protocol transformations that are required before sending the message to the receiver application. The message is sent to the receiver application.","title":"Overview"},{"location":"overview/introduction/#introduction","text":"Integration is at the heart of any digital transformation. By connecting different systems that make up your enterprise, you can build an organization that acts as one seamless digital system. WSO2 Enterprise Integrator (EI) is an open source product that enables comprehensive integration for cloud native and container-native projects. WSO2 EI enables you to do the following: Optimize systems and resources Leverage the cloud Reuse legacy systems Create a connected ecosystem for both your customers and partners. Connect enterprise systems to one another Make data accessible accross the enterprise Provide intuitive and visual development tools for continous integration and continous development Help integration developers to create new services and assets WSO2 EI comprises of profiles that offer different integration capabilities. The ESB profile in WSO2 EI provides its fundamental services through an event-driven and standards-based messaging engine (the bus), which allows integration architects to exploit the value of messaging without writing code. This ESB profile is a step ahead of the previous releases of WSO2 Enterprise Service Bus, as it provides data integration capabilities within the same runtime. This eliminates the need to use a separate data services server for your integration processes. The following diagram illustrates the message-flow architecture in the ESB profile of WSO2 EI, which is used for implementing integration flows. This shows how a request propagates to its actual endpoint through the ESB profile. Response handling is the reverse of this operation. Note that the components of the pipes are not in a specific order. An application (client) sends a message to the ESB profile of WSO2 EI. The message is picked up by a transport. The transport sends the message through a message pipe, which handles quality of service aspects such as security. Internally, this pipe is the in-flow and out-flow of the Axis2 engine. The ESB profile can operate in two modes: Mediating Messages - A single pipe is used. Proxy Services - Separate pipes connecting the transport to different proxy services are used. Both message transformation and routing can be considered as a single unit. As the diagram specifies, there is no clear separation between message transformation components and routing components. In the ESB profile of WSO2 EI, this is known as the mediation framework. Some transformations take place before the routing decision has been made while others take place after the routing decision. This is part of the Synapse implementation. The message is injected to the separate pipes depending on the destinations. Here again, quality of service aspects of the messages are determined. The transport layer takes care of the transport protocol transformations that are required before sending the message to the receiver application. The message is sent to the receiver application.","title":"Introduction"},{"location":"quick-start-guide/quick-start-guide/","text":"Quick Start with WSO2 Micro Integrator \u00b6 Let's get started with WSO2 Micro Integrator by running a simple use case on your local environment. In this example, we use a REST API to simulate a simple HTTP service ( HelloWorld service) deployed in an instance of WSO2 Micro Integrator. You can deploy this HelloWorld service on a Docker container or on Kubernetes . When invoked, the HelloWorld service will return the following response: {\"Hello\":\"World\"} Set up the workspace \u00b6 Install Docker (version 17.09.0-ce or higher). Install Curl . To set up the integration workspace for this quick guide, we will use an integration project that was built using WSO2 Integration Studio : Download the project file for this guide, and extract it to a known location. Let's call this <MI_QSG_HOME> . Go to the <MI_GS_HOME> directory . The following project files, and Docker/Kubernetes configuration files are available. hello-world-config-project This is the ESB Config Project folder with the integration artifacts (synapse artifacts) for the HelloWorld service. This service consists of the following REST API: HelloWorld.xml <api context= \"/hello-world\" name= \"HelloWorld\" xmlns= \"http://ws.apache.org/ns/synapse\" > <resource methods= \"GET\" > <inSequence> <payloadFactory media-type= \"json\" > <format>{ \"Hello\" : \"World\" }</format> <args/> </payloadFactory> <respond/> </inSequence> <outSequence/> <faultSequence/> </resource> </api> hello-world-config-projectCompositeApplication This is the Composite Application Project folder, which contains the packaged CAR file of the HelloWorld service. Dockerfile This Docker configuration file is configured to build a Docker image for WSO2 Micro Integrator with the HelloWorld service. Dockerfile FROM wso2/micro-integrator: 1.0. 0 COPY hello-world-config-projectCompositeApplication/target/hello-world-config-projectCompositeApplication_ 1.0.0. car /home/wso2carbon/wso2mi/repository/deployment/server/carbonapps Note that this file is configured to use the community version of the WSO2 Micro Integrator base Docker image (from DockerHub ). If you want to use the Micro Integrator that includes the latest product updates, you can update the image name in this Docker file as explained here . k8s-deployment.yaml This is sample Kubernetes configuration file that is configured to deploy WSO2 Micro Integrator in a Kubernetes cluster. k8s-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mi-helloworld-deployment labels: event: mi-helloworld spec: strategy: type: Recreate replicas: 2 selector: matchLabels: event: mi-helloworld template: metadata: labels: event: mi-helloworld spec: containers: - image: wso2-mi-hello-world name: helloworld imagePullPolicy: IfNotPresent ports: - name: web containerPort: 8290 --- apiVersion: v1 kind: Service metadata: name: mi-helloworld-service labels: event: mi-helloworld spec: type: NodePort ports: - name: web port: 8290 targetPort: 8290 nodePort: 32100 selector: event: mi-helloworld Run on Docker \u00b6 Once you have set up your workspace , you can run the HelloWorld service on Docker: Build a Docker image \u00b6 Open a terminal, navigate to the <MI_QSG_HOME> directory (which stores the Dockerfile), and execute the following command to build a Docker image with WSO2 Micro Integrator and the integration artifacts. docker build -t hello_world_docker_image . This command executes the following tasks: The base Docker image of WSO2 Micro Integrator is downloaded from DockerHub and a custom, deployable Docker image of the Micro Integrator is created in a Docker container. The CAR file with the integration artifacts that define the HelloWorld service is deployed. Run Docker container \u00b6 From the <MI_QSG_HOME> directory, execute the following command to start a Docker container for the Micro Integrator. docker run -d -p 8290:8290 hello_world_docker_image The Docker container with the Micro Integrator is started. Invoke the Micro Integrator (on Docker) \u00b6 Open a terminal, and execute the following command to invoke the HelloWorld service: curl http://localhost:8290/hello-world Upon invocation, you should be able to observe the following response: {\"Hello\":\"World\"} Run on Kubernetes \u00b6 Once you have set up your workspace , you can run the HelloWorld service on Kubernetes: Install Minikube \u00b6 Let's use Minikube to run a Kubernetes cluster for this example. Install Minikube . Once you have completed this step, you should have kubectl configured to use Minikube from your terminal. Start Minikube from your terminal: minikube start Execute the command given below to start using Minikube's built-in Docker daemon. You need this Docker daemon to be able to create Docker images for the Minikube environment. eval $(minikube docker-env) Now you can build a Docker image for your HelloWorld service in Minikube as explained below. Build a Docker image \u00b6 Open a terminal, navigate to the <MI_QSG_HOME> directory (which stores the Dockerfile), and execute the following command to build a Docker image (with WSO2 Micro Integrator and the integration artifacts) in the Minikube environment . docker build -t wso2-mi-hello-world . Run container (on Minikube) \u00b6 Follow the steps given below to start a Docker container for Docker image on Minikube. Navigate to the <MI_QSG_HOME> directory (which stores the k8s-deployment.yaml file), and execute the following command: kubectl create -f k8s-deployment.yaml Check whether all the Kubernetes artifacts are deployed successfully by executing the following command: kubectl get all You will get a result similar to the following. Be sure that the deployment is in 'Running' state. NAME READY STATUS RESTARTS AGE pod/mi-helloworld-deployment-56f58c9676-djbwh 1/1 Running 0 14m pod/mi-helloworld-deployment-56f58c9676-xj4fq 1/1 Running 0 14m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 25m service/mi-helloworld-service NodePort 10.110.50.146 <none> 8290:32100/TCP 14m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mi-helloworld-deployment 2/2 2 2 14m NAME DESIRED CURRENT READY AGE replicaset.apps/mi-helloworld-deployment-56f58c9676 2 2 2 14m Invoke the Micro Integrator (on Minikube) \u00b6 Open a terminal, and execute the command given below to invoke the HelloWorld service. Be sure to replace MINIKUBE_IP with the IP of your Minikube installation. curl http://MINIKUBE_IP:32100/hello-world Upon invocation, you should be able to observe the following response: {\"Hello\":\"World\"}","title":"Quick Start Guide"},{"location":"quick-start-guide/quick-start-guide/#quick-start-with-wso2-micro-integrator","text":"Let's get started with WSO2 Micro Integrator by running a simple use case on your local environment. In this example, we use a REST API to simulate a simple HTTP service ( HelloWorld service) deployed in an instance of WSO2 Micro Integrator. You can deploy this HelloWorld service on a Docker container or on Kubernetes . When invoked, the HelloWorld service will return the following response: {\"Hello\":\"World\"}","title":"Quick Start with WSO2 Micro Integrator"},{"location":"quick-start-guide/quick-start-guide/#set-up-the-workspace","text":"Install Docker (version 17.09.0-ce or higher). Install Curl . To set up the integration workspace for this quick guide, we will use an integration project that was built using WSO2 Integration Studio : Download the project file for this guide, and extract it to a known location. Let's call this <MI_QSG_HOME> . Go to the <MI_GS_HOME> directory . The following project files, and Docker/Kubernetes configuration files are available. hello-world-config-project This is the ESB Config Project folder with the integration artifacts (synapse artifacts) for the HelloWorld service. This service consists of the following REST API: HelloWorld.xml <api context= \"/hello-world\" name= \"HelloWorld\" xmlns= \"http://ws.apache.org/ns/synapse\" > <resource methods= \"GET\" > <inSequence> <payloadFactory media-type= \"json\" > <format>{ \"Hello\" : \"World\" }</format> <args/> </payloadFactory> <respond/> </inSequence> <outSequence/> <faultSequence/> </resource> </api> hello-world-config-projectCompositeApplication This is the Composite Application Project folder, which contains the packaged CAR file of the HelloWorld service. Dockerfile This Docker configuration file is configured to build a Docker image for WSO2 Micro Integrator with the HelloWorld service. Dockerfile FROM wso2/micro-integrator: 1.0. 0 COPY hello-world-config-projectCompositeApplication/target/hello-world-config-projectCompositeApplication_ 1.0.0. car /home/wso2carbon/wso2mi/repository/deployment/server/carbonapps Note that this file is configured to use the community version of the WSO2 Micro Integrator base Docker image (from DockerHub ). If you want to use the Micro Integrator that includes the latest product updates, you can update the image name in this Docker file as explained here . k8s-deployment.yaml This is sample Kubernetes configuration file that is configured to deploy WSO2 Micro Integrator in a Kubernetes cluster. k8s-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mi-helloworld-deployment labels: event: mi-helloworld spec: strategy: type: Recreate replicas: 2 selector: matchLabels: event: mi-helloworld template: metadata: labels: event: mi-helloworld spec: containers: - image: wso2-mi-hello-world name: helloworld imagePullPolicy: IfNotPresent ports: - name: web containerPort: 8290 --- apiVersion: v1 kind: Service metadata: name: mi-helloworld-service labels: event: mi-helloworld spec: type: NodePort ports: - name: web port: 8290 targetPort: 8290 nodePort: 32100 selector: event: mi-helloworld","title":"Set up the workspace"},{"location":"quick-start-guide/quick-start-guide/#run-on-docker","text":"Once you have set up your workspace , you can run the HelloWorld service on Docker:","title":"Run on Docker"},{"location":"quick-start-guide/quick-start-guide/#build-a-docker-image","text":"Open a terminal, navigate to the <MI_QSG_HOME> directory (which stores the Dockerfile), and execute the following command to build a Docker image with WSO2 Micro Integrator and the integration artifacts. docker build -t hello_world_docker_image . This command executes the following tasks: The base Docker image of WSO2 Micro Integrator is downloaded from DockerHub and a custom, deployable Docker image of the Micro Integrator is created in a Docker container. The CAR file with the integration artifacts that define the HelloWorld service is deployed.","title":"Build a Docker image"},{"location":"quick-start-guide/quick-start-guide/#run-docker-container","text":"From the <MI_QSG_HOME> directory, execute the following command to start a Docker container for the Micro Integrator. docker run -d -p 8290:8290 hello_world_docker_image The Docker container with the Micro Integrator is started.","title":"Run Docker container"},{"location":"quick-start-guide/quick-start-guide/#invoke-the-micro-integrator-on-docker","text":"Open a terminal, and execute the following command to invoke the HelloWorld service: curl http://localhost:8290/hello-world Upon invocation, you should be able to observe the following response: {\"Hello\":\"World\"}","title":"Invoke the Micro Integrator (on Docker)"},{"location":"quick-start-guide/quick-start-guide/#run-on-kubernetes","text":"Once you have set up your workspace , you can run the HelloWorld service on Kubernetes:","title":"Run on Kubernetes"},{"location":"quick-start-guide/quick-start-guide/#install-minikube","text":"Let's use Minikube to run a Kubernetes cluster for this example. Install Minikube . Once you have completed this step, you should have kubectl configured to use Minikube from your terminal. Start Minikube from your terminal: minikube start Execute the command given below to start using Minikube's built-in Docker daemon. You need this Docker daemon to be able to create Docker images for the Minikube environment. eval $(minikube docker-env) Now you can build a Docker image for your HelloWorld service in Minikube as explained below.","title":"Install Minikube"},{"location":"quick-start-guide/quick-start-guide/#build-a-docker-image_1","text":"Open a terminal, navigate to the <MI_QSG_HOME> directory (which stores the Dockerfile), and execute the following command to build a Docker image (with WSO2 Micro Integrator and the integration artifacts) in the Minikube environment . docker build -t wso2-mi-hello-world .","title":"Build a Docker image"},{"location":"quick-start-guide/quick-start-guide/#run-container-on-minikube","text":"Follow the steps given below to start a Docker container for Docker image on Minikube. Navigate to the <MI_QSG_HOME> directory (which stores the k8s-deployment.yaml file), and execute the following command: kubectl create -f k8s-deployment.yaml Check whether all the Kubernetes artifacts are deployed successfully by executing the following command: kubectl get all You will get a result similar to the following. Be sure that the deployment is in 'Running' state. NAME READY STATUS RESTARTS AGE pod/mi-helloworld-deployment-56f58c9676-djbwh 1/1 Running 0 14m pod/mi-helloworld-deployment-56f58c9676-xj4fq 1/1 Running 0 14m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 25m service/mi-helloworld-service NodePort 10.110.50.146 <none> 8290:32100/TCP 14m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mi-helloworld-deployment 2/2 2 2 14m NAME DESIRED CURRENT READY AGE replicaset.apps/mi-helloworld-deployment-56f58c9676 2 2 2 14m","title":"Run container (on Minikube)"},{"location":"quick-start-guide/quick-start-guide/#invoke-the-micro-integrator-on-minikube","text":"Open a terminal, and execute the command given below to invoke the HelloWorld service. Be sure to replace MINIKUBE_IP with the IP of your Minikube installation. curl http://MINIKUBE_IP:32100/hello-world Upon invocation, you should be able to observe the following response: {\"Hello\":\"World\"}","title":"Invoke the Micro Integrator (on Minikube)"},{"location":"references/accessing-Properties-with-XPath/","text":"Accessing Properties with XPath \u00b6 The ESB profile of WSO2 Enterprise Integrator(WSO2 EI) supports standard XPath functions and variables through its underlying XPath engine. The ESB profile also provides custom XPath functions and variables for accessing message properties. XPath Extension Functions base64Encode() function base64Decode() function get-property() function Synapse scope axis2 scope axis2-client transport scope registry scope system scope operation scope url-encode() function Synapse XPath Variables [ body](#AccessingPropertieswithXPath- body](#AccessingPropertieswithXPath- body) [ header](#AccessingPropertieswithXPath- header](#AccessingPropertieswithXPath- header) [ axis2](#AccessingPropertieswithXPath- axis2](#AccessingPropertieswithXPath- axis2) [ ctx](#AccessingPropertieswithXPath- ctx](#AccessingPropertieswithXPath- ctx) [ trp](#AccessingPropertieswithXPath- trp](#AccessingPropertieswithXPath- trp) [ url](#AccessingPropertieswithXPath- url](#AccessingPropertieswithXPath- url) [ func](#AccessingPropertieswithXPath- func](#AccessingPropertieswithXPath- func) [ env](#AccessingPropertieswithXPath- env](#AccessingPropertieswithXPath- env) XPath Extension Functions \u00b6 In addition to standard XPath functions, the ESB profile of WSO2 Enterprise Integrator supports the following custom functions for working with XPath expressions: base64Encode() function base64Decode() function get-property() function url-encode() function [ body](#AccessingPropertieswithXPath- body](#AccessingPropertieswithXPath- body) [ header](#AccessingPropertieswithXPath- header](#AccessingPropertieswithXPath- header) [ axis2](#AccessingPropertieswithXPath- axis2](#AccessingPropertieswithXPath- axis2) [ ctx](#AccessingPropertieswithXPath- ctx](#AccessingPropertieswithXPath- ctx) [ trp](#AccessingPropertieswithXPath- trp](#AccessingPropertieswithXPath- trp) [ url](#AccessingPropertieswithXPath- url](#AccessingPropertieswithXPath- url) [ func](#AccessingPropertieswithXPath- func](#AccessingPropertieswithXPath- func) [ env](#AccessingPropertieswithXPath- env](#AccessingPropertieswithXPath- env) base64Encode() function \u00b6 The base64Encode function returns the base64-encoded value of the specified string. Syntax: base64Encode(string value) base64Encode(string value, string charset) base64Decode() function \u00b6 The base64Decode function returns the original value of the specified base64-encoded value. Syntax: base64Decode(string encodedValue) base64Decode(string encodedValue , string charset) get-property() function \u00b6 The get-property() function allows any XPath expression used in a configuration to look up information from the current message context. Using the Property mediator , you can retrieve properties from the message context and header. The syntax of the function takes the following format. get-property(String propertyName) get-property(String scope, String propertyName) The function accepts scope as an optional parameter. It retrieves a message property at the given scope, which can be one of the following. Synapse scope axis2 scope axis2-client transport scope registry scope system scope operation scope If you provide only the property name without the scope, the default s ynapse scope will be used. Info When the result of an XPath evaluation results in a single XML node, the evaluator will return the text content of this node by default (equivalent of doing /root/body/node/text()). If you want to retrieve the node itself, you can configure the Enrich mediator as shown in the following example. <inSequence> <log level=\"custom\"> <property name=\"WHERE\" value=\"before doing stuff\"/> </log> <enrich> <source type=\"body\" clone=\"true\"/> <target type=\"property\" property=\"ENRICH_PROPERTY\"/> </enrich> <property name=\"PROPERTY_PROPERTY\" expression=\"$body/child::node()\" scope=\"default\"/> <log level=\"custom\"> <property name=\"WHERE\" value=\"before doing stuff\"/> <property name=\"ENRICH_PROPERTY\" expression=\"get-property('ENRICH_PROPERTY')\"/> <property name=\"PROPERTY_PROPERTY\" expression=\"get-property('PROPERTY_PROPERTY')\"/> </log> <enrich> <source type=\"property\" clone=\"true\" property=\"ENRICH_PROPERTY\"/> <target type=\"body\" action=\"sibling\"/> </enrich> <log level=\"full\"/> </inSequence> Synapse scope \u00b6 When the scope of a property mediator is synapse , its value is available throughout both the in sequence and the out sequence. In addition to the user-defined properties, you can retrieve the following special properties from the synapse scope. Name Return Value To Incoming URL as a String, or empty string (\u00ab\u00bb) if a To address is not defined. From From address as a String, or empty string (\u00ab\u00bb) if a From address is not defined. Action SOAP Addressing Action header value as a String, or empty string (\u00ab\u00bb) if an Action is not defined. FaultTo SOAP FaultTo header value as a String, or empty string (\u00ab\u00bb) if a FaultTo address is not defined. ReplyTo ReplyTo header value as a String, or empty string (\u00ab\u00bb) if a ReplyTo address is not defined. MessageID A unique identifier (UUID) for the message as a String, or empty string (\u00ab\u00bb) if a MessageID is not defined. This ID is guaranteed to be unique. FAULT TRUE if the message has a fault, or empty string if the message does not have a fault. MESSAGE_FORMAT Returns pox, get, soap11, or soap12 depending on the message. If a message type is unknown this returns soap12 OperationName Operation name corresponding to the message. A proxy service with a WSDL can have different operations. If the WSDL is not defined, ESB defines fixed operations. To access a property with the synapses cope inside the mediate() method of a mediator, you can include the following configuration in a custom mediator created using the Class mediator : public boolean mediate(org.apache.synapse.MessageContext mc) { // Available in both in-sequence and out-sequenc String propValue = (String) mc.getProperty(\"PropName\"); System.out.println(\"SCOPE_SYNAPSE : \" + propValue); return true; } axis2 scope \u00b6 When the scope of a property mediator is axis2 , its value is available only throughout the sequence for which the property is defined (e.g., if you add the property to an in sequence, its value will be available only throughout the in sequence). You can retrieve message context properties within the axis2 scope using the following syntax. Syntax: get-property('axis2', String propertyName) To access a property with the axis2 scope inside the mediate() method of a mediator, you can include the following configuration in a custom mediator created using the Class mediator : public boolean mediate(org.apache.synapse.MessageContext mc) { org.apache.axis2.context.MessageContext axis2MsgContext; axis2MsgContext = ((Axis2MessageContext) mc).getAxis2MessageContext(); // Available only in the sequence the property is defined. String propValue = (String) axis2MsgContext.getProperty(\"PropName\"); System.out.println(\"SCOPE_AXIS2 : \" + propValue); return true; } axis2-client \u00b6 This is similar to the synapse scope. The difference is that it can be accessed inside the mediate() method of a mediator by including one of the following configurations in a custom mediator, created using the Class mediator : public boolean mediate(org.apache.synapse.MessageContext mc) { org.apache.axis2.context.MessageContext axis2MsgContext; axis2MsgContext = ((Axis2MessageContext) mc).getAxis2MessageContext(); String propValue = (String) axis2MsgContext.getProperty(\"PropName\"); System.out.println(\"SCOPE_AXIS2_CLIENT - 1 : \" + propValue); or propValue = (String) axis2MsgContext.getOptions().getProperty(\"PropName\"); System.out.println(\"SCOPE_AXIS2_CLIENT - 2: \" + propValue); return true; } transport scope \u00b6 When the scope of a property mediator is transport , it will be added to the transport header of the outgoing message from the ESB profile. You can retrieve message context properties within the transport scope using the following syntax. Syntax: get-property('transport', String propertyName) registry scope \u00b6 You can retrieve properties within the registry using the following syntax. Syntax: get-property('registry', String registryPath@propertyName) get-property('registry', String registryPath) system scope \u00b6 You can retrieve Java System properties using the following syntax . Syntax: get-property('system', String propertyName) operation scope \u00b6 You can retrieve a property in the operation context level from the operation scope. The properties within iterated/cloned message with the operation scope are preserved in the in sequence even if you have configured your API resources to be sent through the fault sequence when faults exist. A given property with the operation scope only exists in a single request and can be accessed by a single resource. The properties in this scope are passed to the error handler when the FORCE_ERROR_ON_SOAP_FAULT property is set to true . See FORCE_ERROR_ON_SOAP_FAULT section in Generic Properties for more information. Syntax: get-property('operation', String propertyName) url-encode() function \u00b6 The url-encode function returns the URL-encoded value of the specified string. Syntax: url-encode(string value) url-encode(string value, string charset) Synapse XPath Variables \u00b6 There is a set of predefined XPath variables that you can directly use to write XPaths in the Synapse configuration, instead of using the synapse:get-property() function . These XPath variables get properties of various scopes as follows: XPath Extension Functions Synapse XPath Variables $body \u00b6 The SOAP 1.1 or 1.2 body element. For example, the expression $body//getQuote refers to the first getQuote element in a SOAP body, regardless of whether the message is SOAP-11 or SOAP-12. We have discussed an example below. Example of $body usage : Deploy the following proxy service using instructions in Creating a Proxy Service . Note the property, <property xmlns:m0=\" http://services.samples \" name=\"stockprop\" expression=\"$body//m0:getQuote\"/> in the configuration. It is used to log the first <m0:getQuote> element of the request SOAP body. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <log> <property xmlns:m0=\"http://services.samples\" name=\"stockprop\" expression=\"$body//m0:getQuote\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </inSequence> <outSequence> <send/> </outSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. [2013-03-18 14:04:41,019] INFO - LogMediator To: /services/StockQuoteProxy, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:930f68f5-199a-4eff-90d2-ea679c2362ab, Direction: request, stockprop = <m0:getQuotexmlns:m0=\"http://services.samples\"><m0:request><m0:symbol>IBM</m0:symbol></m0:request></m0:getQuote> $header \u00b6 The SOAP 1.1 or 1.2 header element. For example, the expression $header/wsa:To refers to the addressing To header regardless of whether this message is SOAP-11 or SOAP-12. We have discussed an example below. Example of $header usage : Deploy the following proxy service using instructions in Creating a Proxy Service . Note the property, <property xmlns:wsa=\" http://www.w3.org/2005/08/addressing \" name=\"stockprop\" expression=\"$header/wsa:To\"/> in the configuration. It is used to log the value of wsa:To header of the SOAP request. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <log> <property xmlns:wsa=\"http://www.w3.org/2005/08/addressing\" name=\"stockprop\" expression=\"$header/wsa:To\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </inSequence> <outSequence> <send/> </outSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. [2013-03-18 14:14:16,356] INFO - LogMediator To: http://localhost:9000/services/SimpleStockQuoteService, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:8a64c9cb-b82f-4d6f-a45d-bef37f8b664a, Direction: request, stockprop = http://localhost:9000/services/SimpleStockQuoteService $axis2 \u00b6 Prefix for Axis2 MessageContext properties. This is used to get the property value at the axis2 scope. For example, to get the value of Axis2 message context property with name REST_URL_POSTFIX, use the XPath expression $axis2:REST_URL_POSTFIX . We have discussed an example below. Example of $axis2 usage : Deploy the following proxy service. For instructions, see Creating a Proxy Service . Note the property, <property name=\"stockprop\" expression=\"$axis2:REST_URL_POSTFIX\"/> in the configuration which is used to log the REST_URL_POSTFIX value of the request message. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <log> <property name=\"stockprop\" expression=\"$axis2:REST_URL_POSTFIX\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </inSequence> <outSequence> <send/> </outSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy/test/prefix Note the following message in the log. INFO - LogMediator To: http://localhost:8280/services/StockQuoteProxy/test/prefix, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:ecd228c5-106a-4448-9c83-3b1e957e2fe5, Direction: request, stockprop = /test/prefix In this example, the property definition, <property name=\"stockprop\" expression=\"$axis2:REST_URL_POSTFIX\"/> is equivalent to <property name=\"stockprop\" expression=\"get-property('axis2','REST_URL_POSTFIX')\"/> Similarly, you can use $axis2 prefix with HTTP Transport Properties . $ctx \u00b6 Prefix for Synapse MessageContext properties and gets a property at the default scope. For example, to get the value of Synapse message context property with name ERROR_MESSAGE, use the XPath expression $ctx:ERROR_MESSAGE . We have discussed an example below. Example of $ctx usage : This example sends a request to a sample proxy service, and sets the target endpoint to a non-existent endpoint reference key. It causes a mediation fault, which triggers the fault sequence. Deploy the following proxy service. For instructions, see Creating a Proxy Service . Note the property, \\<property name=\"stockerrorprop\" expression=\"$ctx:ERROR_MESSAGE\"/> in the fault sequence configuration. It is used to log the error message that occurs due to a mediation fault. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <send> <endpoint key=\"ep2\"/> </send> </inSequence> <outSequence> <send/> </outSequence> <faultSequence> <log> <property name=\"stockerrorprop\" expression=\"$ctx:ERROR_MESSAGE\"/> <property name=\"Cause\" expression=\"get-property('ERROR_MESSAGE')\"/> </log> </faultSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Dtrpurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. INFO - LogMediator To: /services/StockQuoteProxy, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:54205f7d-359b-4e82-9099-0f8e3bf9d014, Direction: request, stockerrorprop = Couldn't find the endpoint with the key : ep2 In this example, the property definition, \\<property name=\"stockerrorprop\" expression=\"$ctx:ERROR_MESSAGE\"/> is equivalent to \\<property name=\"stockerrorprop\" expression=\"get-property('ERROR_MESSAGE')\"/>. Similarly, you can use $ctx prefix with Generic Properties . $trp \u00b6 Prefix used to get the transport headers. For example, to get the transport header named Content-Type of the current message, use the XPath expression $trp:Content-Type . HTTP transport headers are not case sensitive. Therefore, $trp:Content-Type and $trp:CONTENT-TYPE are regarded as the same. We have discussed an example below. Example of $trp usage: Deploy the following proxy service. For instructions, see Creating a Proxy Service . Note the property, \\<property name=\"stockprop\" expression=\"$trp:Content-Type\"/> in the configuration, which is used to log the Content-Type HTTP header of the request message. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <log> <property name=\"stockprop\" expression=\"$trp:Content-Type\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </inSequence> <outSequence> <send/> </outSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. [2013-03-18 12:23:14,101] INFO - LogMediator To: http://localhost:8280/services/StockQuoteProxy, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:25a3143a-5b18-4cbb-b8e4-27d4dd1895d2, Direction: request, stockprop = text/xml; charset=UTF-8 In this example, the property definition, \\<property name=\"stockprop\" expression=\"$trp:Content-Type\"/> is equivalent to \\<property name=\"stockprop\" expression=\"get-property('transport','Content-Type')\"/>. Similarly, you can use $trp prefix with HTTP Transport Properties . $url \u00b6 The prefix used to get the URI element of a request URL. Example of $url usage: Create a REST API with the following configuration using instructions given in page Working with APIs . <api xmlns=\"http://ws.apache.org/ns/synapse\" name=\"Editing\" context=\"/editing\"> <resource methods=\"GET\" uri-template=\"/edit?a={symbol}&b={value}\"> <inSequence> <log level=\"full\"> <property name=\"SYMBOL\" expression=\"$url:a\"></property> <property name=\"VALUE\" expression=\"$url:b\"></property> </log> <respond></respond> </inSequence> </resource> </api> Send a request to the REST API you created using a browser as follows: http://10.100.5.73:8280/editing/edit?a=wso2&b=2.4 You will see the following in the log: LogMediator To: /editing/edit?a=wso2&b=2.4, MessageID: urn:uuid:36cb5ad7-f150-490d-897a-ee7b86a9307d, Direction: request, SYMBOL = wso2, VALUE = 2.4, Envelope: <?xml version=\"1.0\" encoding=\"utf-8\"?><soapenv:Envelope xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope\"><soapenv:Body></soapenv:Body></soapenv:Envelope> $func \u00b6 The prefix used to refer to a particular parameter value passed externally by an invoker such as the Call Template Mediator . Example of $func usage: Add a sequence template with the following configuration. See Adding a New Sequence Template for detailed instructions. <template xmlns=\"http://ws.apache.org/ns/synapse\" name=\"HelloWordLogger\"> <sequence> <log level=\"full\"> <property xmlns:ns2=\"http://org.apache.synapse/xsd\" xmlns:ns=\"http://org.apache.synapse/xsd\" name=\"message\" expression=\"$func:message\"></property> </log> </sequence> </template> Deploy the following proxy service. For instructions, see Creating a Proxy Service . <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <call-template target=\"HelloWorldLogger\"> <with-param name=\"message\" value=\"HelloWorld\"/> </call-template> <log/> </inSequence> <outSequence> <send/> </outSequence> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </target> <description/> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. LogMediator To: http://localhost:8280/services/StockQuoteProxy, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:8d90e21b-b5cc-4a02-98e2-24b324fa704c, Direction: request, message = HelloWorld $env \u00b6 Prefix used to get a SOAP 1.1 or 1.2 envelope level element. For example, to get the body element from the SOAP envelope, use the expression $env/*[local-name()='Body'] . Example of $env usage: Create an API with the following configuration. For information on how to create an API, see Working with APIs . <api context=\"/soapEnvelopeTest\" name=\"SoapEnvelopeTest\"> <resource url-mapping=\"/*\"> <inSequence> <loopback/> </inSequence> <outSequence> <property name=\"messageType\" scope=\"axis2\" value=\"application/xml\"/> <payloadFactory media-type=\"xml\"> <format> <theData xmlns=\"http://some.namespace\"> <item>$1</item> </theData> </format> <args> <arg evaluator=\"xml\" expression=\"$env/*[local-name()='Body']/*[local-name()='jsonObject']/*\"/> </payloadFactory> <property name=\"messageType\" scope=\"axis2\" value=\"application/json\"/> <send/> </outSequence> </resource> </api> Send a post request to the API you created (i.e., http://localhost:8280/soapEnvelopeTest) , with the following json payload using a rest client. {\"content\":{ \"paramA\": \"ValueA\", \"paramB\": \"valueB\" }} You will receive the following response: {\"theData\":{\"item\":{\"content\":{\"paramA\":\"ValueA\",\"paramB\":\"valueB\"}}}}","title":"Accessing Properties with XPath"},{"location":"references/accessing-Properties-with-XPath/#accessing-properties-with-xpath","text":"The ESB profile of WSO2 Enterprise Integrator(WSO2 EI) supports standard XPath functions and variables through its underlying XPath engine. The ESB profile also provides custom XPath functions and variables for accessing message properties. XPath Extension Functions base64Encode() function base64Decode() function get-property() function Synapse scope axis2 scope axis2-client transport scope registry scope system scope operation scope url-encode() function Synapse XPath Variables [ body](#AccessingPropertieswithXPath- body](#AccessingPropertieswithXPath- body) [ header](#AccessingPropertieswithXPath- header](#AccessingPropertieswithXPath- header) [ axis2](#AccessingPropertieswithXPath- axis2](#AccessingPropertieswithXPath- axis2) [ ctx](#AccessingPropertieswithXPath- ctx](#AccessingPropertieswithXPath- ctx) [ trp](#AccessingPropertieswithXPath- trp](#AccessingPropertieswithXPath- trp) [ url](#AccessingPropertieswithXPath- url](#AccessingPropertieswithXPath- url) [ func](#AccessingPropertieswithXPath- func](#AccessingPropertieswithXPath- func) [ env](#AccessingPropertieswithXPath- env](#AccessingPropertieswithXPath- env)","title":"Accessing Properties with XPath"},{"location":"references/accessing-Properties-with-XPath/#xpath-extension-functions","text":"In addition to standard XPath functions, the ESB profile of WSO2 Enterprise Integrator supports the following custom functions for working with XPath expressions: base64Encode() function base64Decode() function get-property() function url-encode() function [ body](#AccessingPropertieswithXPath- body](#AccessingPropertieswithXPath- body) [ header](#AccessingPropertieswithXPath- header](#AccessingPropertieswithXPath- header) [ axis2](#AccessingPropertieswithXPath- axis2](#AccessingPropertieswithXPath- axis2) [ ctx](#AccessingPropertieswithXPath- ctx](#AccessingPropertieswithXPath- ctx) [ trp](#AccessingPropertieswithXPath- trp](#AccessingPropertieswithXPath- trp) [ url](#AccessingPropertieswithXPath- url](#AccessingPropertieswithXPath- url) [ func](#AccessingPropertieswithXPath- func](#AccessingPropertieswithXPath- func) [ env](#AccessingPropertieswithXPath- env](#AccessingPropertieswithXPath- env)","title":"XPath Extension Functions"},{"location":"references/accessing-Properties-with-XPath/#base64encode-function","text":"The base64Encode function returns the base64-encoded value of the specified string. Syntax: base64Encode(string value) base64Encode(string value, string charset)","title":"base64Encode() function"},{"location":"references/accessing-Properties-with-XPath/#base64decode-function","text":"The base64Decode function returns the original value of the specified base64-encoded value. Syntax: base64Decode(string encodedValue) base64Decode(string encodedValue , string charset)","title":"base64Decode() function"},{"location":"references/accessing-Properties-with-XPath/#get-property-function","text":"The get-property() function allows any XPath expression used in a configuration to look up information from the current message context. Using the Property mediator , you can retrieve properties from the message context and header. The syntax of the function takes the following format. get-property(String propertyName) get-property(String scope, String propertyName) The function accepts scope as an optional parameter. It retrieves a message property at the given scope, which can be one of the following. Synapse scope axis2 scope axis2-client transport scope registry scope system scope operation scope If you provide only the property name without the scope, the default s ynapse scope will be used. Info When the result of an XPath evaluation results in a single XML node, the evaluator will return the text content of this node by default (equivalent of doing /root/body/node/text()). If you want to retrieve the node itself, you can configure the Enrich mediator as shown in the following example. <inSequence> <log level=\"custom\"> <property name=\"WHERE\" value=\"before doing stuff\"/> </log> <enrich> <source type=\"body\" clone=\"true\"/> <target type=\"property\" property=\"ENRICH_PROPERTY\"/> </enrich> <property name=\"PROPERTY_PROPERTY\" expression=\"$body/child::node()\" scope=\"default\"/> <log level=\"custom\"> <property name=\"WHERE\" value=\"before doing stuff\"/> <property name=\"ENRICH_PROPERTY\" expression=\"get-property('ENRICH_PROPERTY')\"/> <property name=\"PROPERTY_PROPERTY\" expression=\"get-property('PROPERTY_PROPERTY')\"/> </log> <enrich> <source type=\"property\" clone=\"true\" property=\"ENRICH_PROPERTY\"/> <target type=\"body\" action=\"sibling\"/> </enrich> <log level=\"full\"/> </inSequence>","title":"get-property() function"},{"location":"references/accessing-Properties-with-XPath/#synapse-scope","text":"When the scope of a property mediator is synapse , its value is available throughout both the in sequence and the out sequence. In addition to the user-defined properties, you can retrieve the following special properties from the synapse scope. Name Return Value To Incoming URL as a String, or empty string (\u00ab\u00bb) if a To address is not defined. From From address as a String, or empty string (\u00ab\u00bb) if a From address is not defined. Action SOAP Addressing Action header value as a String, or empty string (\u00ab\u00bb) if an Action is not defined. FaultTo SOAP FaultTo header value as a String, or empty string (\u00ab\u00bb) if a FaultTo address is not defined. ReplyTo ReplyTo header value as a String, or empty string (\u00ab\u00bb) if a ReplyTo address is not defined. MessageID A unique identifier (UUID) for the message as a String, or empty string (\u00ab\u00bb) if a MessageID is not defined. This ID is guaranteed to be unique. FAULT TRUE if the message has a fault, or empty string if the message does not have a fault. MESSAGE_FORMAT Returns pox, get, soap11, or soap12 depending on the message. If a message type is unknown this returns soap12 OperationName Operation name corresponding to the message. A proxy service with a WSDL can have different operations. If the WSDL is not defined, ESB defines fixed operations. To access a property with the synapses cope inside the mediate() method of a mediator, you can include the following configuration in a custom mediator created using the Class mediator : public boolean mediate(org.apache.synapse.MessageContext mc) { // Available in both in-sequence and out-sequenc String propValue = (String) mc.getProperty(\"PropName\"); System.out.println(\"SCOPE_SYNAPSE : \" + propValue); return true; }","title":"Synapse scope"},{"location":"references/accessing-Properties-with-XPath/#axis2-scope","text":"When the scope of a property mediator is axis2 , its value is available only throughout the sequence for which the property is defined (e.g., if you add the property to an in sequence, its value will be available only throughout the in sequence). You can retrieve message context properties within the axis2 scope using the following syntax. Syntax: get-property('axis2', String propertyName) To access a property with the axis2 scope inside the mediate() method of a mediator, you can include the following configuration in a custom mediator created using the Class mediator : public boolean mediate(org.apache.synapse.MessageContext mc) { org.apache.axis2.context.MessageContext axis2MsgContext; axis2MsgContext = ((Axis2MessageContext) mc).getAxis2MessageContext(); // Available only in the sequence the property is defined. String propValue = (String) axis2MsgContext.getProperty(\"PropName\"); System.out.println(\"SCOPE_AXIS2 : \" + propValue); return true; }","title":"axis2 scope"},{"location":"references/accessing-Properties-with-XPath/#axis2-client","text":"This is similar to the synapse scope. The difference is that it can be accessed inside the mediate() method of a mediator by including one of the following configurations in a custom mediator, created using the Class mediator : public boolean mediate(org.apache.synapse.MessageContext mc) { org.apache.axis2.context.MessageContext axis2MsgContext; axis2MsgContext = ((Axis2MessageContext) mc).getAxis2MessageContext(); String propValue = (String) axis2MsgContext.getProperty(\"PropName\"); System.out.println(\"SCOPE_AXIS2_CLIENT - 1 : \" + propValue); or propValue = (String) axis2MsgContext.getOptions().getProperty(\"PropName\"); System.out.println(\"SCOPE_AXIS2_CLIENT - 2: \" + propValue); return true; }","title":"axis2-client"},{"location":"references/accessing-Properties-with-XPath/#transport-scope","text":"When the scope of a property mediator is transport , it will be added to the transport header of the outgoing message from the ESB profile. You can retrieve message context properties within the transport scope using the following syntax. Syntax: get-property('transport', String propertyName)","title":"transport\u00a0scope"},{"location":"references/accessing-Properties-with-XPath/#registry-scope","text":"You can retrieve properties within the registry using the following syntax. Syntax: get-property('registry', String registryPath@propertyName) get-property('registry', String registryPath)","title":"registry scope"},{"location":"references/accessing-Properties-with-XPath/#system-scope","text":"You can retrieve Java System properties using the following syntax . Syntax: get-property('system', String propertyName)","title":"system scope"},{"location":"references/accessing-Properties-with-XPath/#operation-scope","text":"You can retrieve a property in the operation context level from the operation scope. The properties within iterated/cloned message with the operation scope are preserved in the in sequence even if you have configured your API resources to be sent through the fault sequence when faults exist. A given property with the operation scope only exists in a single request and can be accessed by a single resource. The properties in this scope are passed to the error handler when the FORCE_ERROR_ON_SOAP_FAULT property is set to true . See FORCE_ERROR_ON_SOAP_FAULT section in Generic Properties for more information. Syntax: get-property('operation', String propertyName)","title":"operation scope"},{"location":"references/accessing-Properties-with-XPath/#url-encode-function","text":"The url-encode function returns the URL-encoded value of the specified string. Syntax: url-encode(string value) url-encode(string value, string charset)","title":"url-encode() function"},{"location":"references/accessing-Properties-with-XPath/#synapse-xpath-variables","text":"There is a set of predefined XPath variables that you can directly use to write XPaths in the Synapse configuration, instead of using the synapse:get-property() function . These XPath variables get properties of various scopes as follows: XPath Extension Functions Synapse XPath Variables","title":"Synapse XPath Variables"},{"location":"references/accessing-Properties-with-XPath/#body","text":"The SOAP 1.1 or 1.2 body element. For example, the expression $body//getQuote refers to the first getQuote element in a SOAP body, regardless of whether the message is SOAP-11 or SOAP-12. We have discussed an example below. Example of $body usage : Deploy the following proxy service using instructions in Creating a Proxy Service . Note the property, <property xmlns:m0=\" http://services.samples \" name=\"stockprop\" expression=\"$body//m0:getQuote\"/> in the configuration. It is used to log the first <m0:getQuote> element of the request SOAP body. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <log> <property xmlns:m0=\"http://services.samples\" name=\"stockprop\" expression=\"$body//m0:getQuote\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </inSequence> <outSequence> <send/> </outSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. [2013-03-18 14:04:41,019] INFO - LogMediator To: /services/StockQuoteProxy, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:930f68f5-199a-4eff-90d2-ea679c2362ab, Direction: request, stockprop = <m0:getQuotexmlns:m0=\"http://services.samples\"><m0:request><m0:symbol>IBM</m0:symbol></m0:request></m0:getQuote>","title":"$body"},{"location":"references/accessing-Properties-with-XPath/#header","text":"The SOAP 1.1 or 1.2 header element. For example, the expression $header/wsa:To refers to the addressing To header regardless of whether this message is SOAP-11 or SOAP-12. We have discussed an example below. Example of $header usage : Deploy the following proxy service using instructions in Creating a Proxy Service . Note the property, <property xmlns:wsa=\" http://www.w3.org/2005/08/addressing \" name=\"stockprop\" expression=\"$header/wsa:To\"/> in the configuration. It is used to log the value of wsa:To header of the SOAP request. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <log> <property xmlns:wsa=\"http://www.w3.org/2005/08/addressing\" name=\"stockprop\" expression=\"$header/wsa:To\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </inSequence> <outSequence> <send/> </outSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. [2013-03-18 14:14:16,356] INFO - LogMediator To: http://localhost:9000/services/SimpleStockQuoteService, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:8a64c9cb-b82f-4d6f-a45d-bef37f8b664a, Direction: request, stockprop = http://localhost:9000/services/SimpleStockQuoteService","title":"$header"},{"location":"references/accessing-Properties-with-XPath/#axis2","text":"Prefix for Axis2 MessageContext properties. This is used to get the property value at the axis2 scope. For example, to get the value of Axis2 message context property with name REST_URL_POSTFIX, use the XPath expression $axis2:REST_URL_POSTFIX . We have discussed an example below. Example of $axis2 usage : Deploy the following proxy service. For instructions, see Creating a Proxy Service . Note the property, <property name=\"stockprop\" expression=\"$axis2:REST_URL_POSTFIX\"/> in the configuration which is used to log the REST_URL_POSTFIX value of the request message. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <log> <property name=\"stockprop\" expression=\"$axis2:REST_URL_POSTFIX\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </inSequence> <outSequence> <send/> </outSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy/test/prefix Note the following message in the log. INFO - LogMediator To: http://localhost:8280/services/StockQuoteProxy/test/prefix, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:ecd228c5-106a-4448-9c83-3b1e957e2fe5, Direction: request, stockprop = /test/prefix In this example, the property definition, <property name=\"stockprop\" expression=\"$axis2:REST_URL_POSTFIX\"/> is equivalent to <property name=\"stockprop\" expression=\"get-property('axis2','REST_URL_POSTFIX')\"/> Similarly, you can use $axis2 prefix with HTTP Transport Properties .","title":"$axis2"},{"location":"references/accessing-Properties-with-XPath/#ctx","text":"Prefix for Synapse MessageContext properties and gets a property at the default scope. For example, to get the value of Synapse message context property with name ERROR_MESSAGE, use the XPath expression $ctx:ERROR_MESSAGE . We have discussed an example below. Example of $ctx usage : This example sends a request to a sample proxy service, and sets the target endpoint to a non-existent endpoint reference key. It causes a mediation fault, which triggers the fault sequence. Deploy the following proxy service. For instructions, see Creating a Proxy Service . Note the property, \\<property name=\"stockerrorprop\" expression=\"$ctx:ERROR_MESSAGE\"/> in the fault sequence configuration. It is used to log the error message that occurs due to a mediation fault. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <send> <endpoint key=\"ep2\"/> </send> </inSequence> <outSequence> <send/> </outSequence> <faultSequence> <log> <property name=\"stockerrorprop\" expression=\"$ctx:ERROR_MESSAGE\"/> <property name=\"Cause\" expression=\"get-property('ERROR_MESSAGE')\"/> </log> </faultSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Dtrpurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. INFO - LogMediator To: /services/StockQuoteProxy, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:54205f7d-359b-4e82-9099-0f8e3bf9d014, Direction: request, stockerrorprop = Couldn't find the endpoint with the key : ep2 In this example, the property definition, \\<property name=\"stockerrorprop\" expression=\"$ctx:ERROR_MESSAGE\"/> is equivalent to \\<property name=\"stockerrorprop\" expression=\"get-property('ERROR_MESSAGE')\"/>. Similarly, you can use $ctx prefix with Generic Properties .","title":"$ctx"},{"location":"references/accessing-Properties-with-XPath/#trp","text":"Prefix used to get the transport headers. For example, to get the transport header named Content-Type of the current message, use the XPath expression $trp:Content-Type . HTTP transport headers are not case sensitive. Therefore, $trp:Content-Type and $trp:CONTENT-TYPE are regarded as the same. We have discussed an example below. Example of $trp usage: Deploy the following proxy service. For instructions, see Creating a Proxy Service . Note the property, \\<property name=\"stockprop\" expression=\"$trp:Content-Type\"/> in the configuration, which is used to log the Content-Type HTTP header of the request message. html/xml <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <log> <property name=\"stockprop\" expression=\"$trp:Content-Type\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </inSequence> <outSequence> <send/> </outSequence> </target> <description></description> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. [2013-03-18 12:23:14,101] INFO - LogMediator To: http://localhost:8280/services/StockQuoteProxy, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:25a3143a-5b18-4cbb-b8e4-27d4dd1895d2, Direction: request, stockprop = text/xml; charset=UTF-8 In this example, the property definition, \\<property name=\"stockprop\" expression=\"$trp:Content-Type\"/> is equivalent to \\<property name=\"stockprop\" expression=\"get-property('transport','Content-Type')\"/>. Similarly, you can use $trp prefix with HTTP Transport Properties .","title":"$trp"},{"location":"references/accessing-Properties-with-XPath/#url","text":"The prefix used to get the URI element of a request URL. Example of $url usage: Create a REST API with the following configuration using instructions given in page Working with APIs . <api xmlns=\"http://ws.apache.org/ns/synapse\" name=\"Editing\" context=\"/editing\"> <resource methods=\"GET\" uri-template=\"/edit?a={symbol}&b={value}\"> <inSequence> <log level=\"full\"> <property name=\"SYMBOL\" expression=\"$url:a\"></property> <property name=\"VALUE\" expression=\"$url:b\"></property> </log> <respond></respond> </inSequence> </resource> </api> Send a request to the REST API you created using a browser as follows: http://10.100.5.73:8280/editing/edit?a=wso2&b=2.4 You will see the following in the log: LogMediator To: /editing/edit?a=wso2&b=2.4, MessageID: urn:uuid:36cb5ad7-f150-490d-897a-ee7b86a9307d, Direction: request, SYMBOL = wso2, VALUE = 2.4, Envelope: <?xml version=\"1.0\" encoding=\"utf-8\"?><soapenv:Envelope xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope\"><soapenv:Body></soapenv:Body></soapenv:Envelope>","title":"$url"},{"location":"references/accessing-Properties-with-XPath/#func","text":"The prefix used to refer to a particular parameter value passed externally by an invoker such as the Call Template Mediator . Example of $func usage: Add a sequence template with the following configuration. See Adding a New Sequence Template for detailed instructions. <template xmlns=\"http://ws.apache.org/ns/synapse\" name=\"HelloWordLogger\"> <sequence> <log level=\"full\"> <property xmlns:ns2=\"http://org.apache.synapse/xsd\" xmlns:ns=\"http://org.apache.synapse/xsd\" name=\"message\" expression=\"$func:message\"></property> </log> </sequence> </template> Deploy the following proxy service. For instructions, see Creating a Proxy Service . <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"StockQuoteProxy\" transports=\"https,http\" statistics=\"disable\" trace=\"disable\" startOnLoad=\"true\"> <target> <inSequence> <call-template target=\"HelloWorldLogger\"> <with-param name=\"message\" value=\"HelloWorld\"/> </call-template> <log/> </inSequence> <outSequence> <send/> </outSequence> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </target> <description/> </proxy> Send the following StockQuote request using the sample StockQuote client. For information on working with the sample client, see Using the Sample Clients . ant stockquote -Daddurl=http://localhost:8280/services/StockQuoteProxy Note the following message in the log. LogMediator To: http://localhost:8280/services/StockQuoteProxy, WSAction: urn:getQuote, SOAPAction: urn:getQuote, ReplyTo: http://www.w3.org/2005/08/addressing/anonymous, MessageID: urn:uuid:8d90e21b-b5cc-4a02-98e2-24b324fa704c, Direction: request, message = HelloWorld","title":"$func"},{"location":"references/accessing-Properties-with-XPath/#env","text":"Prefix used to get a SOAP 1.1 or 1.2 envelope level element. For example, to get the body element from the SOAP envelope, use the expression $env/*[local-name()='Body'] . Example of $env usage: Create an API with the following configuration. For information on how to create an API, see Working with APIs . <api context=\"/soapEnvelopeTest\" name=\"SoapEnvelopeTest\"> <resource url-mapping=\"/*\"> <inSequence> <loopback/> </inSequence> <outSequence> <property name=\"messageType\" scope=\"axis2\" value=\"application/xml\"/> <payloadFactory media-type=\"xml\"> <format> <theData xmlns=\"http://some.namespace\"> <item>$1</item> </theData> </format> <args> <arg evaluator=\"xml\" expression=\"$env/*[local-name()='Body']/*[local-name()='jsonObject']/*\"/> </payloadFactory> <property name=\"messageType\" scope=\"axis2\" value=\"application/json\"/> <send/> </outSequence> </resource> </api> Send a post request to the API you created (i.e., http://localhost:8280/soapEnvelopeTest) , with the following json payload using a rest client. {\"content\":{ \"paramA\": \"ValueA\", \"paramB\": \"valueB\" }} You will receive the following response: {\"theData\":{\"item\":{\"content\":{\"paramA\":\"ValueA\",\"paramB\":\"valueB\"}}}}","title":"$env"},{"location":"references/adding-a-Priority-Executor/","text":"Adding a Priority Executor \u00b6 Info Note Please note that this feature is deprecated. Priority executors can be used to execute sequences with a given priority. This allows the user to control the resources allocated to executing sequences and prevent high priority messages from getting delayed and dropped. Follow the instructions below to add a new priority executor to the WSO2 EI. 1. Sign in. Enter your user name and password to log on to the EI Management Console. 2. Click on \"Main\" in the left menu to access the \"Manage\" menu. 3. In the \"Manage\" menu, click on \"Priority Executors\" under \"Service Bus.\" 4. In the \"Priority Executors\" window, click on the \"Add Executor\" link. 5. Specify the options of a new priority executor in the \"Priority Executor Design\" widow. Executor Name - Name of the Executor. Fixed Size Queues - Whether fixed size queues are used or not. Queues - Individual Queue Configurations. See the detailed information below. Next Queue Algorithm Max - Maximum Number of Threads in the Executor. Core - Core Number of Threads in the Executor. Keep-Alive - Keep Alive time for Threads. Info Tip An executor should have at least two or more queues. If only one queue is used, there is no point in using a priority executor. 6. Each and every priority has a queue associated with it. A message is put in to one of the queues corresponding to its priority. To add a queue to an executor, click on the \"Add Queue\" link. 7. Specify the \"Queues\" options: Priority - Priority of the Queue. Size - Size of the Queue. This option is visible only if fixed size queues are selected. Info Tip You can remove a queue from an executor clicking on the \"Delete\" link in the actions column. 8. Click on the \"Save\" button to add an executor to the list. 9. A new priority executor appears in the list.","title":"Adding a Priority Executor"},{"location":"references/adding-a-Priority-Executor/#adding-a-priority-executor","text":"Info Note Please note that this feature is deprecated. Priority executors can be used to execute sequences with a given priority. This allows the user to control the resources allocated to executing sequences and prevent high priority messages from getting delayed and dropped. Follow the instructions below to add a new priority executor to the WSO2 EI. 1. Sign in. Enter your user name and password to log on to the EI Management Console. 2. Click on \"Main\" in the left menu to access the \"Manage\" menu. 3. In the \"Manage\" menu, click on \"Priority Executors\" under \"Service Bus.\" 4. In the \"Priority Executors\" window, click on the \"Add Executor\" link. 5. Specify the options of a new priority executor in the \"Priority Executor Design\" widow. Executor Name - Name of the Executor. Fixed Size Queues - Whether fixed size queues are used or not. Queues - Individual Queue Configurations. See the detailed information below. Next Queue Algorithm Max - Maximum Number of Threads in the Executor. Core - Core Number of Threads in the Executor. Keep-Alive - Keep Alive time for Threads. Info Tip An executor should have at least two or more queues. If only one queue is used, there is no point in using a priority executor. 6. Each and every priority has a queue associated with it. A message is put in to one of the queues corresponding to its priority. To add a queue to an executor, click on the \"Add Queue\" link. 7. Specify the \"Queues\" options: Priority - Priority of the Queue. Size - Size of the Queue. This option is visible only if fixed size queues are selected. Info Tip You can remove a queue from an executor clicking on the \"Delete\" link in the actions column. 8. Click on the \"Save\" button to add an executor to the list. 9. A new priority executor appears in the list.","title":"Adding a Priority Executor"},{"location":"references/advanced-Callback-Properties/","text":"Advanced Callback Properties \u00b6 The abstract EntitlementCallbackHandler class supports the following properties for getting the XACML subject (user name), specifying the action, and setting the service name. The various implementations of this class (UTEntitlementCallbackHandler, X509EntitlementCallbackHandler, etc.) can use some or all of these properties. You implement these properties by adding Property mediators before the Entitlement mediator in the sequence. The default UTEntitlementCallbackHandler looks for a property called username in the Axis2 message context, which it uses as the XACML request subject-id value. Likewise, the other handlers look at various properties for values for the attributes and construct the XACML request. The following attribute IDs are used by the default handlers. urn:oasis:names:tc:xacml:1.0:subject:subject-id of category urn:oasis:names:tc:xacml:1.0:subject-category:access-subject urn:oasis:names:tc:xacml:1.0:action:action-id of category urn:oasis:names:tc:xacml:3.0:attribute-category:action urn:oasis:names:tc:xacml:1.0:resource:resource-id of category urn:oasis:names:tc:xacml:3.0:attribute-category:resource IssuerDN of category urn:oasis:names:tc:xacml:3.0:attribute-category:environment (used only by X509 handler) SignatureAlgorithm of category urn:oasis:names:tc:xacml:3.0:attribute-category:environment (used only by X509 handler) Info In most scenarios, you do not need to configure any of these properties. Property name Acceptable values Scope Description xacml_subject_identifier string axis2 By default, the Entitlement mediator expects to find the XACML subject (user name) in a property called username in the message's Axis2 context. If your authentication mechanism specifies the user name by adding a property of a different name, create a property called xacml_subject_identifier and set it to the name of the property in the message context that contains the subject. xacml_action string axis2 If you are using REST and want to specify a different HTTP verb to use with the service, specify it with the xacml_action property and specify the xacml_use_rest property to true. xacml_use_rest true/false axis2 If you are using REST, and you want to override the HTTP verb to send with the request, you can set this property to true to set to true. xacml_resource_prefix string axis2 If you want to change the service name, use this property to specify the new service name or the text you want to prepend to the service name. xacml_resource_prefix_only true/false axis2 If set to true, the xacml_resource_prefix value is used as the whole service name. If set to false (default), the xacml_resource_prefix is prepended to the service name.","title":"Advanced Callback Properties"},{"location":"references/advanced-Callback-Properties/#advanced-callback-properties","text":"The abstract EntitlementCallbackHandler class supports the following properties for getting the XACML subject (user name), specifying the action, and setting the service name. The various implementations of this class (UTEntitlementCallbackHandler, X509EntitlementCallbackHandler, etc.) can use some or all of these properties. You implement these properties by adding Property mediators before the Entitlement mediator in the sequence. The default UTEntitlementCallbackHandler looks for a property called username in the Axis2 message context, which it uses as the XACML request subject-id value. Likewise, the other handlers look at various properties for values for the attributes and construct the XACML request. The following attribute IDs are used by the default handlers. urn:oasis:names:tc:xacml:1.0:subject:subject-id of category urn:oasis:names:tc:xacml:1.0:subject-category:access-subject urn:oasis:names:tc:xacml:1.0:action:action-id of category urn:oasis:names:tc:xacml:3.0:attribute-category:action urn:oasis:names:tc:xacml:1.0:resource:resource-id of category urn:oasis:names:tc:xacml:3.0:attribute-category:resource IssuerDN of category urn:oasis:names:tc:xacml:3.0:attribute-category:environment (used only by X509 handler) SignatureAlgorithm of category urn:oasis:names:tc:xacml:3.0:attribute-category:environment (used only by X509 handler) Info In most scenarios, you do not need to configure any of these properties. Property name Acceptable values Scope Description xacml_subject_identifier string axis2 By default, the Entitlement mediator expects to find the XACML subject (user name) in a property called username in the message's Axis2 context. If your authentication mechanism specifies the user name by adding a property of a different name, create a property called xacml_subject_identifier and set it to the name of the property in the message context that contains the subject. xacml_action string axis2 If you are using REST and want to specify a different HTTP verb to use with the service, specify it with the xacml_action property and specify the xacml_use_rest property to true. xacml_use_rest true/false axis2 If you are using REST, and you want to override the HTTP verb to send with the request, you can set this property to true to set to true. xacml_resource_prefix string axis2 If you want to change the service name, use this property to specify the new service name or the text you want to prepend to the service name. xacml_resource_prefix_only true/false axis2 If set to true, the xacml_resource_prefix value is used as the whole service name. If set to false (default), the xacml_resource_prefix is prepended to the service name.","title":"Advanced Callback Properties"},{"location":"references/aggregate-Mediator/","text":"Aggregate Mediator \u00b6 The Aggregate mediator implements the Aggregator enterprise integration pattern . It combines (aggregates) the response messages of messages that were split by the split by the Clone or Iterate mediator. Note that the responses are not necessarily aggregated in the same order that the requests were sent, even if you set the sequential attribute to true on the Iterate mediator. Info The Aggregate mediator is a content-aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <aggregate> <correlateOn expression=\"xpath | json-eval(JSON-Path)\"/>? <completeCondition [timeout=\"time-in-seconds\"]> <messageCount min=\"int-min\" max=\"int-max\"/>? </completeCondition>? <onComplete expression=\"xpath | json-eval(JSON-Path)\" [sequence=\"sequence-ref\"]> (mediator +)? </onComplete> </aggregate> Configuration \u00b6 The parameters available for configuring the Aggregate mediator are as follows. Parameter Name Description Aggregate ID This optional attribute can be used to aggregate only responses for split messages that are created by a specific clone/iterate mediator. Aggregate ID should be the same as the ID of the corresponding clone/iterate mediator that creates split messages. This is particularly useful when aggregating responses for messages that are created using nested clone/iterate mediators. Aggregation Expression An XPath expression specifying which elements should be aggregated. A set of messages that are selected for aggregation is determined by the value specified in the Correlation Expression field. Completion Timeout The number of seconds taken by the Aggregate mediator to wait for messages. When this time duration elapses, the aggregation will be completed. If the number of response messages reaches the number specified in the Completion Max-messages field, the aggregation will be completed even if the time duration specified for the Completion Timeout field has not elapsed. Completion Max-messages Maximum number of messages that can exist in an aggregation. When the number of response messages received reaches this number, the aggregation will be completed. Completion Min-messages Minimum number of messages required for the aggregation to complete. When the time duration entered in the Completion Timeout field is elapsed, the aggregation will be completed even if the number of minimum response messages specified has not been received. If no value is entered in the Completion Timeout field, the aggregation will not be completed until the number of response messages entered in the Completion Min-messages field is received. Correlation Expression This is an XPath expression which provides the basis on which response messages should be selected for aggregation. This is done by specifying a set of elements for which the messages selected should have matching values. A specific aggregation condition is set via the Aggregation Expression field. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Enclosing Element Property This parameter is used to accumulate the aggregated messages inside a single property. The name of the relevant property is entered in this field. On Complete The sequence to run when the aggregation is complete. You can select one of the following options: Anonymous : Select this value if you want to specify the sequence to run by adding child mediators to the Aggregate mediator instead of selecting an existing sequence. For example, if you want to send the aggregated message via the Send mediator , you can add the Send mediator as a child mediator. Pick from Registry : Select this option if you want to specify a sequence which is already defined and saved in the registry. You can select the sequence from the Configuration Registry or Governance Registry. Examples \u00b6 Example 1 - Sending aggregated messages through the send mediator Example 2 - Sending aggregated messages with the enclosing element Samples Example 1 - Sending aggregated messages through the send mediator \u00b6 <outSequence> <aggregate> <onComplete expression=\"//m0:getQuoteResponse\" xmlns:m0=\"http://services.samples\"> <send/> </onComplete> </aggregate> </outSequence> In this example, the mediator aggregates the responses coming into the ESB profile , and on completion it sends the aggregated message through the Send mediator. Example 2 - Sending aggregated messages with the enclosing element \u00b6 The following example shows how to configure the Aggregate mediator to annotate the responses sent from multiple backends before forwarding them to the client. <outSequence> <property name=\"info\" scope=\"default\"> <ns:Information xmlns:ns=\"www.asankatechtalks.com\" /> </property> <aggregate id=\"sa\"> <completeCondition /> <onComplete expression=\"$body/*[1]\" enclosingElementProperty=\"info\"> <send /> </onComplete> </aggregate> </outSequence> The above configuration includes the following: Parameter Description <property name=\"info\" scope=\"default\"> <ns:Information xmlns:ns=\"www.asankatechtalks.com\" /> </property> This creates the property named info of the OM type in which all the aggregated responses are accumulated. < aggregate id = \"sa\" > The ID of the corresponding Clone mediator that splits the messages to be aggregated by the Aggregate mediator. < onComplete expression = \"$body/*[1]\" enclosingElementProperty = \"info\" > This expression is used to add the info property (created earlier in this configuration) to be added to the payload of the message and for accumulating all the aggregated messages from different endpoints inside the tag created inside this property. < send /> This is the Send mediator added as a child mediator to the Aggregate mediator in order to send the aggregated and annotated messages back to the client once the aggregation is complete. Samples \u00b6 For more examples, see: Sample 62: Routing a Message to a Dynamic List of Recipients and Aggregating Responses Sample 400: Message Splitting and Aggregating the Responses Sample 751: Message Split Aggregate Using Templates","title":"Aggregate Mediator"},{"location":"references/aggregate-Mediator/#aggregate-mediator","text":"The Aggregate mediator implements the Aggregator enterprise integration pattern . It combines (aggregates) the response messages of messages that were split by the split by the Clone or Iterate mediator. Note that the responses are not necessarily aggregated in the same order that the requests were sent, even if you set the sequential attribute to true on the Iterate mediator. Info The Aggregate mediator is a content-aware mediator. Syntax | Configuration | Examples","title":"Aggregate Mediator"},{"location":"references/aggregate-Mediator/#syntax","text":"<aggregate> <correlateOn expression=\"xpath | json-eval(JSON-Path)\"/>? <completeCondition [timeout=\"time-in-seconds\"]> <messageCount min=\"int-min\" max=\"int-max\"/>? </completeCondition>? <onComplete expression=\"xpath | json-eval(JSON-Path)\" [sequence=\"sequence-ref\"]> (mediator +)? </onComplete> </aggregate>","title":"Syntax"},{"location":"references/aggregate-Mediator/#configuration","text":"The parameters available for configuring the Aggregate mediator are as follows. Parameter Name Description Aggregate ID This optional attribute can be used to aggregate only responses for split messages that are created by a specific clone/iterate mediator. Aggregate ID should be the same as the ID of the corresponding clone/iterate mediator that creates split messages. This is particularly useful when aggregating responses for messages that are created using nested clone/iterate mediators. Aggregation Expression An XPath expression specifying which elements should be aggregated. A set of messages that are selected for aggregation is determined by the value specified in the Correlation Expression field. Completion Timeout The number of seconds taken by the Aggregate mediator to wait for messages. When this time duration elapses, the aggregation will be completed. If the number of response messages reaches the number specified in the Completion Max-messages field, the aggregation will be completed even if the time duration specified for the Completion Timeout field has not elapsed. Completion Max-messages Maximum number of messages that can exist in an aggregation. When the number of response messages received reaches this number, the aggregation will be completed. Completion Min-messages Minimum number of messages required for the aggregation to complete. When the time duration entered in the Completion Timeout field is elapsed, the aggregation will be completed even if the number of minimum response messages specified has not been received. If no value is entered in the Completion Timeout field, the aggregation will not be completed until the number of response messages entered in the Completion Min-messages field is received. Correlation Expression This is an XPath expression which provides the basis on which response messages should be selected for aggregation. This is done by specifying a set of elements for which the messages selected should have matching values. A specific aggregation condition is set via the Aggregation Expression field. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Enclosing Element Property This parameter is used to accumulate the aggregated messages inside a single property. The name of the relevant property is entered in this field. On Complete The sequence to run when the aggregation is complete. You can select one of the following options: Anonymous : Select this value if you want to specify the sequence to run by adding child mediators to the Aggregate mediator instead of selecting an existing sequence. For example, if you want to send the aggregated message via the Send mediator , you can add the Send mediator as a child mediator. Pick from Registry : Select this option if you want to specify a sequence which is already defined and saved in the registry. You can select the sequence from the Configuration Registry or Governance Registry.","title":"Configuration"},{"location":"references/aggregate-Mediator/#examples","text":"Example 1 - Sending aggregated messages through the send mediator Example 2 - Sending aggregated messages with the enclosing element Samples","title":"Examples"},{"location":"references/aggregate-Mediator/#example-1-sending-aggregated-messages-through-the-send-mediator","text":"<outSequence> <aggregate> <onComplete expression=\"//m0:getQuoteResponse\" xmlns:m0=\"http://services.samples\"> <send/> </onComplete> </aggregate> </outSequence> In this example, the mediator aggregates the responses coming into the ESB profile , and on completion it sends the aggregated message through the Send mediator.","title":"Example 1 - Sending aggregated messages through the send mediator"},{"location":"references/aggregate-Mediator/#example-2-sending-aggregated-messages-with-the-enclosing-element","text":"The following example shows how to configure the Aggregate mediator to annotate the responses sent from multiple backends before forwarding them to the client. <outSequence> <property name=\"info\" scope=\"default\"> <ns:Information xmlns:ns=\"www.asankatechtalks.com\" /> </property> <aggregate id=\"sa\"> <completeCondition /> <onComplete expression=\"$body/*[1]\" enclosingElementProperty=\"info\"> <send /> </onComplete> </aggregate> </outSequence> The above configuration includes the following: Parameter Description <property name=\"info\" scope=\"default\"> <ns:Information xmlns:ns=\"www.asankatechtalks.com\" /> </property> This creates the property named info of the OM type in which all the aggregated responses are accumulated. < aggregate id = \"sa\" > The ID of the corresponding Clone mediator that splits the messages to be aggregated by the Aggregate mediator. < onComplete expression = \"$body/*[1]\" enclosingElementProperty = \"info\" > This expression is used to add the info property (created earlier in this configuration) to be added to the payload of the message and for accumulating all the aggregated messages from different endpoints inside the tag created inside this property. < send /> This is the Send mediator added as a child mediator to the Aggregate mediator in order to send the aggregated and annotated messages back to the client once the aggregation is complete.","title":"Example 2 - Sending aggregated messages with the enclosing element"},{"location":"references/aggregate-Mediator/#samples","text":"For more examples, see: Sample 62: Routing a Message to a Dynamic List of Recipients and Aggregating Responses Sample 400: Message Splitting and Aggregating the Responses Sample 751: Message Split Aggregate Using Templates","title":"Samples"},{"location":"references/axis2-Properties/","text":"Axis2 Properties \u00b6 [ CacheLevel ] [ ConcurrentConsumers ] [ HTTP_ETAG ] [ JMS_COORELATION_ID ] [ MaxConcurrentConsumers ] [ MercurySequenceKey ] [ MercuryLastMessage ] [ FORCE_HTTP_1.0 ] [ setCharacterEncoding ] [ CHARACTER_SET_ENCODING ] Axis2 properties allow you to configure the web services engine in the ESB profile, such as specifying how to cache JMS objects, setting the minimum and maximum threads for consuming messages, and forcing outgoing HTTP/S messages to use HTTP 1.0. You can access some of these properties through the Property mediator with the scope set to axis2 or axis2-client as shown below. CacheLevel \u00b6 Name CacheLevel Possible Values none, connection, session, consumer, producer, auto Description This property determines which JMS objects should be cached. JMS objects are cached so that they can be reused in the subsequent invocations. Each caching level can be described as follows: none : No JMS object will be cached. connection : JMS connection objects will be cached. session : JMS connection and session objects will be cached. consumer : JMS connection, session and consumer objects will be cached. producer : JMS connection, session and producer objects will be cached. auto : An appropriate caching level will be used depending on the transaction strategy. Example <parameter name= \"transport.jms.CacheLevel\" > consumer </parameter> ConcurrentConsumers \u00b6 Name ConcurrentConsumers Possible Values integer Description The minimum number of threads for message consuming. The value specified for this property is the initial number of threads started. As the number of messages to be consumed increases, number of threads are also increased to match the load until the total number of threads equals the value specified for the transport.jms.MaxConcurrentConsumers property. Example <parameter name= \"transport.jms.ConcurrentConsumers\" locked=\"false\" > 50 </parameter> HTTP_ETAG \u00b6 Name HTTP_ETAG Possible Values true/false Scope axis2 Description This property determines whether the HTTP Etag should be enabled for the request or not. !!! info HTTP Etag is a mechanism provided by HTTP for Web cache validation. Example <property name= \"HTTP_ETAG\" scope= \"axis2\" type= \"BOOLEAN\" value= \"true\" /> JMS_COORELATION_ID \u00b6 Name JMS_COORELATION_ID Possible Values String Scope axis2 Description The JMS coorelation ID is used to match responses with specific requests. This property can be used to set the JMS coorrelation ID as a dynamic or a hard coded value in a request. As a result, responses with the matching JMS correlation IDs will be matched with the request. Example <property name= \"JMS_COORELATION_ID\" action= \"set\" scope= \"axis2\" expression= \"$header/wsa:MessageID\" xmlns:sam= \"http://sample.esb.org/> MaxConcurrentConsumers \u00b6 Name MaxConcurrentConsumers Possible Values integer Description The maximum number of threads that can be added for message consuming. See ConcurrentConsumers . Example <parameter name= \"transport.jms.MaxConcurrentConsumers\" locked=\"false\" > 50 </parameter> MercurySequenceKey \u00b6 Name MercurySequenceKey Possible Values integer Description Can be an identifier specifying a Mercury internal sequence key. MercuryLastMessage \u00b6 Name MercuryLastMessage Possible Values true/false Description When set to \"true\", it will make this the last message and terminate the sequence. FORCE_HTTP_1.0 \u00b6 Name FORCE_HTTP_1.0 Possible Values true/false Scope axis2-client Description Forces outgoing http/s messages to use HTTP 1.0 (instead of the default 1.1). setCharacterEncoding \u00b6 Name setCharacterEncoding Possible Values false Default Behavior By default character encoding is enabled in the ESB profile. Scope axis2 Description This property can be used to remove character encode. Note that if this property is set to 'false', the 'CHARACTER_SET_ENCODING' property cannot be used. Example <property name=\" setCharacterEncoding \" value=\"false\" scope=\"axis2\" type=\"STRING\"/> CHARACTER_SET_ENCODING \u00b6 Name CHARACTER_SET_ENCODING Possible Values Any valid encoding standard (E.g., UTF-8, UTF-16 etc.) Default Behavior N/A Scope axis2 Description Specifies the encoding type used for the content of the files processed by the transport. Note that this property cannot be used if the 'setCharacterEncoding' property is set to 'false'. Example <property name=\"CHARACTER_SET_ENCODING\" value=\"UTF-8\" scope=\"axis2\" type=\"STRING\"/>","title":"Axis2 Properties"},{"location":"references/axis2-Properties/#axis2-properties","text":"[ CacheLevel ] [ ConcurrentConsumers ] [ HTTP_ETAG ] [ JMS_COORELATION_ID ] [ MaxConcurrentConsumers ] [ MercurySequenceKey ] [ MercuryLastMessage ] [ FORCE_HTTP_1.0 ] [ setCharacterEncoding ] [ CHARACTER_SET_ENCODING ] Axis2 properties allow you to configure the web services engine in the ESB profile, such as specifying how to cache JMS objects, setting the minimum and maximum threads for consuming messages, and forcing outgoing HTTP/S messages to use HTTP 1.0. You can access some of these properties through the Property mediator with the scope set to axis2 or axis2-client as shown below.","title":"Axis2 Properties"},{"location":"references/axis2-Properties/#cachelevel","text":"Name CacheLevel Possible Values none, connection, session, consumer, producer, auto Description This property determines which JMS objects should be cached. JMS objects are cached so that they can be reused in the subsequent invocations. Each caching level can be described as follows: none : No JMS object will be cached. connection : JMS connection objects will be cached. session : JMS connection and session objects will be cached. consumer : JMS connection, session and consumer objects will be cached. producer : JMS connection, session and producer objects will be cached. auto : An appropriate caching level will be used depending on the transaction strategy. Example <parameter name= \"transport.jms.CacheLevel\" > consumer </parameter>","title":"CacheLevel"},{"location":"references/axis2-Properties/#concurrentconsumers","text":"Name ConcurrentConsumers Possible Values integer Description The minimum number of threads for message consuming. The value specified for this property is the initial number of threads started. As the number of messages to be consumed increases, number of threads are also increased to match the load until the total number of threads equals the value specified for the transport.jms.MaxConcurrentConsumers property. Example <parameter name= \"transport.jms.ConcurrentConsumers\" locked=\"false\" > 50 </parameter>","title":"ConcurrentConsumers"},{"location":"references/axis2-Properties/#http95etag","text":"Name HTTP_ETAG Possible Values true/false Scope axis2 Description This property determines whether the HTTP Etag should be enabled for the request or not. !!! info HTTP Etag is a mechanism provided by HTTP for Web cache validation. Example <property name= \"HTTP_ETAG\" scope= \"axis2\" type= \"BOOLEAN\" value= \"true\" />","title":"HTTP_ETAG"},{"location":"references/axis2-Properties/#jms95coorelation95id","text":"Name JMS_COORELATION_ID Possible Values String Scope axis2 Description The JMS coorelation ID is used to match responses with specific requests. This property can be used to set the JMS coorrelation ID as a dynamic or a hard coded value in a request. As a result, responses with the matching JMS correlation IDs will be matched with the request. Example <property name= \"JMS_COORELATION_ID\" action= \"set\" scope= \"axis2\" expression= \"$header/wsa:MessageID\" xmlns:sam= \"http://sample.esb.org/>","title":"JMS_COORELATION_ID"},{"location":"references/axis2-Properties/#maxconcurrentconsumers","text":"Name MaxConcurrentConsumers Possible Values integer Description The maximum number of threads that can be added for message consuming. See ConcurrentConsumers . Example <parameter name= \"transport.jms.MaxConcurrentConsumers\" locked=\"false\" > 50 </parameter>","title":"MaxConcurrentConsumers"},{"location":"references/axis2-Properties/#mercurysequencekey","text":"Name MercurySequenceKey Possible Values integer Description Can be an identifier specifying a Mercury internal sequence key.","title":"MercurySequenceKey"},{"location":"references/axis2-Properties/#mercurylastmessage","text":"Name MercuryLastMessage Possible Values true/false Description When set to \"true\", it will make this the last message and terminate the sequence.","title":"MercuryLastMessage"},{"location":"references/axis2-Properties/#force95http9510","text":"Name FORCE_HTTP_1.0 Possible Values true/false Scope axis2-client Description Forces outgoing http/s messages to use HTTP 1.0 (instead of the default 1.1).","title":"FORCE_HTTP_1.0"},{"location":"references/axis2-Properties/#setcharacterencoding","text":"Name setCharacterEncoding Possible Values false Default Behavior By default character encoding is enabled in the ESB profile. Scope axis2 Description This property can be used to remove character encode. Note that if this property is set to 'false', the 'CHARACTER_SET_ENCODING' property cannot be used. Example <property name=\" setCharacterEncoding \" value=\"false\" scope=\"axis2\" type=\"STRING\"/>","title":"setCharacterEncoding"},{"location":"references/axis2-Properties/#character95set95encoding","text":"Name CHARACTER_SET_ENCODING Possible Values Any valid encoding standard (E.g., UTF-8, UTF-16 etc.) Default Behavior N/A Scope axis2 Description Specifies the encoding type used for the content of the files processed by the transport. Note that this property cannot be used if the 'setCharacterEncoding' property is set to 'false'. Example <property name=\"CHARACTER_SET_ENCODING\" value=\"UTF-8\" scope=\"axis2\" type=\"STRING\"/>","title":"CHARACTER_SET_ENCODING"},{"location":"references/bean-Mediator/","text":"Bean Mediator \u00b6 Info Note Please note that this feature is deprecated. The Bean Mediator is used to manipulate a JavaBean that is bound to the Synapse message context as a property. Classes of objects manipulated by this mediator need to follow the JavaBeans specification. Info The Bean mediator is a content-aware mediator. Syntax | Configuration | Example Syntax \u00b6 <bean action=\"CREATE | REMOVE | SET_PROPERTY | GET_PROPERTY\" var=\"string\" [class=\"string\"] [property=\"string\"] [value=\"string | {xpath}\"] /> Configuration \u00b6 The parameters available to configure the Bean mediator are as follows. Field Name Description Class The class on which the action selected for the Action parameter is performed by the Beanstalks manager. Action The action to be carried out by the Bean mediator. The possible values are: CREATE : This action creates a new JavaBean. REMOVE : This action removes an existing JavaBean. SET_PROPERTY : This action sets a property of an existing JavaBean. GET_PROPERTY :This action retrieves a property of an existing JavaBean. Var The variable which is created, removed, set or retrieved for the JavaBean based on the value selected for the Action parameter. Property The name of the property used to bind the JavaBean to the Synapse client. Value The value of the property used to bind the JavaBean to the Synapse client. The property value can be entered using one of the following methods. Value : If this is selected, the property value can be entered as a static value. Expression: If this is selected, the property value can be entered as a dynamic value. You can enter the XPath expression to evaluate the relevant property value. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Target The element of the message which should be affected by execution of the Bean mediator configuration. The target element can be specified using one of the following methods. Value : If this is selected, the target element can be entered as a static value. The static value should be entered in the text field. Expression : If this is selected, the target element can be evaluated via an XPath expression. The XPath expression should be entered in the text field. Example \u00b6 In this example, the Bean mediator first creates a JavaBean with the loc as the variable. The next Bean mediator creates property named latitude within the loc variable using the SET_PROPERTY action. The third Bean mediator creates another property named longitude in the same variable using the SET_PROPERTY action. ... <bean action=\"CREATE\" class=\"org.ejb.wso2.test.bean.Location\" var=\"loc\"></bean> <bean action=\"SET_PROPERTY\" property=\"latitude\" value=\"{//latitude}\" var=\"loc\" xmlns:ns3=\"http://org.apache.synapse/xsd\" xmlns:ns=\"http://org.apache.synapse/xsd\"></bean> <bean action=\"SET_PROPERTY\" property=\"longitude\" value=\"{//longitude}\" var=\"loc\" xmlns:ns3=\"http://org.apache.synapse/xsd\" xmlns:ns=\"http://org.apache.synapse/xsd\"></bean> ...","title":"Bean Mediator"},{"location":"references/bean-Mediator/#bean-mediator","text":"Info Note Please note that this feature is deprecated. The Bean Mediator is used to manipulate a JavaBean that is bound to the Synapse message context as a property. Classes of objects manipulated by this mediator need to follow the JavaBeans specification. Info The Bean mediator is a content-aware mediator. Syntax | Configuration | Example","title":"Bean Mediator"},{"location":"references/bean-Mediator/#syntax","text":"<bean action=\"CREATE | REMOVE | SET_PROPERTY | GET_PROPERTY\" var=\"string\" [class=\"string\"] [property=\"string\"] [value=\"string | {xpath}\"] />","title":"Syntax"},{"location":"references/bean-Mediator/#configuration","text":"The parameters available to configure the Bean mediator are as follows. Field Name Description Class The class on which the action selected for the Action parameter is performed by the Beanstalks manager. Action The action to be carried out by the Bean mediator. The possible values are: CREATE : This action creates a new JavaBean. REMOVE : This action removes an existing JavaBean. SET_PROPERTY : This action sets a property of an existing JavaBean. GET_PROPERTY :This action retrieves a property of an existing JavaBean. Var The variable which is created, removed, set or retrieved for the JavaBean based on the value selected for the Action parameter. Property The name of the property used to bind the JavaBean to the Synapse client. Value The value of the property used to bind the JavaBean to the Synapse client. The property value can be entered using one of the following methods. Value : If this is selected, the property value can be entered as a static value. Expression: If this is selected, the property value can be entered as a dynamic value. You can enter the XPath expression to evaluate the relevant property value. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Target The element of the message which should be affected by execution of the Bean mediator configuration. The target element can be specified using one of the following methods. Value : If this is selected, the target element can be entered as a static value. The static value should be entered in the text field. Expression : If this is selected, the target element can be evaluated via an XPath expression. The XPath expression should be entered in the text field.","title":"Configuration"},{"location":"references/bean-Mediator/#example","text":"In this example, the Bean mediator first creates a JavaBean with the loc as the variable. The next Bean mediator creates property named latitude within the loc variable using the SET_PROPERTY action. The third Bean mediator creates another property named longitude in the same variable using the SET_PROPERTY action. ... <bean action=\"CREATE\" class=\"org.ejb.wso2.test.bean.Location\" var=\"loc\"></bean> <bean action=\"SET_PROPERTY\" property=\"latitude\" value=\"{//latitude}\" var=\"loc\" xmlns:ns3=\"http://org.apache.synapse/xsd\" xmlns:ns=\"http://org.apache.synapse/xsd\"></bean> <bean action=\"SET_PROPERTY\" property=\"longitude\" value=\"{//longitude}\" var=\"loc\" xmlns:ns3=\"http://org.apache.synapse/xsd\" xmlns:ns=\"http://org.apache.synapse/xsd\"></bean> ...","title":"Example"},{"location":"references/builder-Mediator/","text":"Builder Mediator \u00b6 The Builder Mediator can be used to build the actual SOAP message from a message coming into the ESB profile of WSO2 Enterprise Integrator (WSO2 EI) through the Binary Relay. One usage is to use this before trying to log the actual message in case of an error. Also with the Builder Mediator in the ESB can be configured to build some of the messages while passing the others along. Info In order to use the Builder mediator, BinaryRealyBuilder should be specified as the message builder in the <EI_HOME>/conf/axis2/axis2.xml file for at least one content type. The message formatter specified for the same content types should be ExpandingMessageFormatter . Unlike other message builders defined in axis2.xml, the BinaryRelayBuilder works by passing through a binary stream of the received content. The Builder mediator is used in conjunction with the BinaryRelayBuilder when we require to build the binary stream into a particular content type during mediation. We can specify the message builder that should be used to build the binary stream using the Builder mediator. By default, Builder Mediator uses the axis2 default Message builders for the content types. Users can override those by using the optional messageBuilder configuration. For more information, see Working with Message Builders and Formatters . Like in axis2.xml , a user has to specify the content type and the implementation class of the messageBuilder . Also, users can specify the message formatter for this content type. This is used by the ExpandingMessageFormatter to format the message before sending to the destination. Syntax \u00b6 <builder> <messageBuilder contentType=\"\" class=\"\" [formatterClass=\"\"]/> </builder>","title":"Builder Mediator"},{"location":"references/builder-Mediator/#builder-mediator","text":"The Builder Mediator can be used to build the actual SOAP message from a message coming into the ESB profile of WSO2 Enterprise Integrator (WSO2 EI) through the Binary Relay. One usage is to use this before trying to log the actual message in case of an error. Also with the Builder Mediator in the ESB can be configured to build some of the messages while passing the others along. Info In order to use the Builder mediator, BinaryRealyBuilder should be specified as the message builder in the <EI_HOME>/conf/axis2/axis2.xml file for at least one content type. The message formatter specified for the same content types should be ExpandingMessageFormatter . Unlike other message builders defined in axis2.xml, the BinaryRelayBuilder works by passing through a binary stream of the received content. The Builder mediator is used in conjunction with the BinaryRelayBuilder when we require to build the binary stream into a particular content type during mediation. We can specify the message builder that should be used to build the binary stream using the Builder mediator. By default, Builder Mediator uses the axis2 default Message builders for the content types. Users can override those by using the optional messageBuilder configuration. For more information, see Working with Message Builders and Formatters . Like in axis2.xml , a user has to specify the content type and the implementation class of the messageBuilder . Also, users can specify the message formatter for this content type. This is used by the ExpandingMessageFormatter to format the message before sending to the destination.","title":"Builder Mediator"},{"location":"references/builder-Mediator/#syntax","text":"<builder> <messageBuilder contentType=\"\" class=\"\" [formatterClass=\"\"]/> </builder>","title":"Syntax"},{"location":"references/cache-Mediator/","text":"Cache Mediator \u00b6 When a message enters a message flow, the Cache mediator checks whether the incoming message is similar to a previous message that was received within a specified period of time. This is done by evaluating the hash value of incoming messages. If a similar message was identified before, the Cache mediator executes the onCacheHit sequence (if specified), fetches the cached response, and prepares the ESB profile to send the response. The onCacheHit sequence can send back the response message using the Respond Mediator . If the onCacheHit sequence is not specified, the cached response is sent back to the requester and the message is not passed on. If a similar message has not been seen before, then the message is passed on. Info The Cache mediator is a content-aware mediator. Tip The Cache mediator supports only local caching. It does not support distributed caching. Syntax | Configuration | Samples | Invalidating cached responses remotely Syntax \u00b6 <cache [timeout=\"seconds\"] [collector=(true | false)] [maxMessageSize=\"in-bytes\"] > <onCacheHit [sequence=\"key\"]> (mediator)+ </onCacheHit>? <protocol type=\"http\" >? <methods>comma separated list</methods> <headersToExcludeInHash>comma separated list</headersToExcludeInHash> <responseCodes>regular expression</responseCodes> <enableCacheControl>(true | false)</enableCacheControl> <includeAgeHeader>(true | false)</includeAgeHeader> <hashGenerator>class</hashGenerator> </protocol> <implementation [maxSize=\"int\"]/> </cache> Info Note In a message flow you can use the cache mediator as a finder (in the incoming path to check the request) or as a collector (in the outgoing path to cache the response). It is not possible to have more than one cache mediator in the same message flow because mediation is terminated after the finder on a cache hit, and the response is not passed on to the next finder after a cache hit. Following is an example where the expected response from the last cache hit is not received because the response is sent once the request comes to the first finder: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"cache115\" transports=\"http https\" startOnLoad=\"true\"> <description /> <target> <inSequence> <cache collector=\"false\" timeout=\"60\"> <protocol type=\"HTTP\"> <methods>POST</methods> <headersToExcludeInHash /> <responseCodes>.*</responseCodes> <enableCacheControl>false</enableCacheControl> <includeAgeHeader>false</includeAgeHeader> <hashGenerator>org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator</hashGenerator> </protocol> </cache> <call> <endpoint> <address uri=\"http://demo0585968.mockable.io/some\" /> </endpoint> </call> <property name=\"RESPONSE\" value=\"true\" scope=\"default\" type=\"STRING\" /> <log level=\"full\" /> <cache collector=\"true\" /> <property name=\"RESPONSE\" value=\"false\" scope=\"default\" type=\"STRING\" /> <cache collector=\"false\" timeout=\"60\"> <protocol type=\"HTTP\"> <methods>POST</methods> <headersToExcludeInHash /> <responseCodes>.*</responseCodes> <hashGenerator>org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator</hashGenerator> </protocol> </cache> <call> <endpoint> <address uri=\"http://demo0585968.mockable.io/hello\" /> </endpoint> </call> <property name=\"RESPONSE\" value=\"true\" scope=\"default\" type=\"STRING\" /> <log level=\"full\" /> <cache collector=\"true\" /> <respond /> </inSequence> </target> </proxy> Configuration \u00b6 Click on the relevant tab to view the UI configuration depending on whether the cache type of the cache mediator is Finder or Collector . Finder Collector The parameters available to configure the Cache mediator are as follows. Parameter Name Description Cache Type This parameter specifies whether the Cache mediator should be in the incoming path (to check the request) or in the outgoing path (to cache the response). Possible values are as follows. Finder : If this is selected, the Cache mediator is used to search for the request hash of incoming messages. Collector : If this is selected, the Cache mediator is used to collect response messages in the cache. Cache Timeout (Seconds) The time duration that the cache should be retained specified in seconds. The cache expires once this time duration elapses. The default value is 5000 seconds. Maximum Message Size The maximum size of the message to be cached. This should be specified in bytes. Protocol Type The protocol type to be cached in the message flow. In the current implementation, HTTP is the only value that you can select. Although the only configuration supported for other protocols is the HashGenerator , you can specify the protocol type to be anything and specify a HashGenerator that you prefer. HTTP Methods A comma separated list of HTTP methods that should be cached for the HTTP protocol . The default value is * , and it caches all HTTP methods. Headers to Exclude in Hash A comma separated list of headers to ignore when hashing an incoming messages. If you want to exclude all headers when hashing an incoming message, specify *. Response Codes Specify the response codes to be cached as a regular expression. If the http status code of a response matches the regular expression, the response should be cached. The default setting is to cache any response code. Hash Generator This parameter is used to define the logic used by the Cache mediator to evaluate the hash values of incoming messages. The value specified here should be a class that implements the org.separated.carbon.mediator.cache.digest.DigestGenerator class interface. The default hash generator is org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator . If the generated hash value is found in the cache, then the Cache mediator executes the onCacheHit sequence, which can be specified inline or referenced. !!! note Note The hash generator is specific to the HTTP protocol. If you are using any other protocol, you need to write a custom hash generator or use one of the following deprecated hash generator classes: org.wso2.carbon.mediator.cache.digest.DOMHASHGenerator org.wso2.carbon.mediator.cache.digest.REQUESTHASHGenerator Enable Cache Control Headers Whether the Cache mediator should honor the Cache-Control header(no-cache, no-store, max-age headers). If you set this to the default value (i.e., false ), the Cache mediator will not consider the Cache-Control headers when caching the response or when returning the cached response. Include Age Header Whether an Age header needs to be included when returning the cached response. Maximum Size The maximum number of elements to be cached. The default size is 1000. Anonymous If this option is selected, an anonymous sequence is executed when an incoming message is identified as an equivalent to a previously received message, based on the value defined in the Hash Generator field. Sequence Reference The reference to the onCacheHit sequence to be executed when an incoming message is identified as an equivalent to a previously received message, based on the value defined in the Hash Generator field. The sequence should be created in the registry in order to be specified in this field. You can click either Configuration Registry or Governance Registry as applicable to select the required sequence from the resource tree. The parameters available to configure the Cache mediator are as follows. Parameter Name Description Cache Type This parameter specifies whether the mediator should be in the incoming path (to check the request) or in the outgoing path (to cache the response). Possible values are as follows. Finder : If this is selected, the mediator is used to search for the request hash of incoming messages. Collector : If this is selected, the mediator is used to collect response messages in the cache. Examples Following are examples of how you can use the Cache mediator. Example one \u00b6 According to this example configuration, when the first message is sent to the endpoint, the cache is not hit. The Cache mediator configured in the Out sequence caches the response to this message. When a similar message is sent to the endpoint for the second time, the previous response is directly fetched from the cache and sent to the requester. This happens because the onCacheHit sequence is not defined in this configuration. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence name=\"main\"> <in> <cache collector=\"false\" maxMessageSize=\"10000\" timeout=\"20\"> <protocol type=\"HTTP\"> <methods>POST</methods> <headersToExcludeInHash/> <responseCodes>2[0-9][0-9]</responseCodes> <enableCacheControl>false</enableCacheControl> <includeAgeHeader>false</includeAgeHeader> <hashGenerator>org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator</hashGenerator> </protocol> <implementation maxSize=\"100\"/> </cache> <send> <endpoint name=\"inlined\"> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </in> <out> <cache collector=\"true\"/> <send/> </out> </sequence> Example two \u00b6 According to this example configuration, if you define a cache collector using the cache mediator in the in sequence, you need to add the RESPONSE property to consider the message as a response message. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <api xmlns=\"http://ws.apache.org/ns/synapse\" name=\"cacheAPI\" context=\"/cache\"> <resource methods=\"POST GET\" uri-template=\"/headerapi/*\"> <inSequence> <cache collector=\"false\" timeout=\"5000\"> <protocol type=\"HTTP\"> <methods>GET, POST</methods> <headersToExcludeInHash>*</headersToExcludeInHash> <responseCodes>.*</responseCodes> <enableCacheControl>false</enableCacheControl> <includeAgeHeader>false</includeAgeHeader> <hashGenerator>org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator</hashGenerator> </protocol> </cache> <call> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </call> <property name=\"RESPONSE\" value=\"true\" scope=\"default\" type=\"STRING\"/> <enrich> <source type=\"inline\" clone=\"true\"> <ax21:newvalue xmlns:ax21=\"http://services.samples/xsd\">testsamplevalue </ax21:newvalue> </source> <target xmlns:ax21=\"http://services.samples/xsd\" xmlns:ns=\"http://services.samples\" action=\"sibling\" xpath=\"//ns:getQuoteResponse/ns:return/ax21:volume\"/> </enrich> <cache collector=\"true\"/> <respond/> </inSequence> </resource> </api> Samples \u00b6 Sample 420: Simple Response Cache in the ESB for an Actual Service Invalidating cached responses remotely \u00b6 You can invalidate all cached response remotely by using any JMX monitoring tool such as Jconsole via the exposed MBeans. You can use the invalidateTheWholeCache() operation of the org.wso2.carbon.mediatio n MBean for this as shown below. {width=\"600\" height=\"520\"}","title":"Cache Mediator"},{"location":"references/cache-Mediator/#cache-mediator","text":"When a message enters a message flow, the Cache mediator checks whether the incoming message is similar to a previous message that was received within a specified period of time. This is done by evaluating the hash value of incoming messages. If a similar message was identified before, the Cache mediator executes the onCacheHit sequence (if specified), fetches the cached response, and prepares the ESB profile to send the response. The onCacheHit sequence can send back the response message using the Respond Mediator . If the onCacheHit sequence is not specified, the cached response is sent back to the requester and the message is not passed on. If a similar message has not been seen before, then the message is passed on. Info The Cache mediator is a content-aware mediator. Tip The Cache mediator supports only local caching. It does not support distributed caching. Syntax | Configuration | Samples | Invalidating cached responses remotely","title":"Cache Mediator"},{"location":"references/cache-Mediator/#syntax","text":"<cache [timeout=\"seconds\"] [collector=(true | false)] [maxMessageSize=\"in-bytes\"] > <onCacheHit [sequence=\"key\"]> (mediator)+ </onCacheHit>? <protocol type=\"http\" >? <methods>comma separated list</methods> <headersToExcludeInHash>comma separated list</headersToExcludeInHash> <responseCodes>regular expression</responseCodes> <enableCacheControl>(true | false)</enableCacheControl> <includeAgeHeader>(true | false)</includeAgeHeader> <hashGenerator>class</hashGenerator> </protocol> <implementation [maxSize=\"int\"]/> </cache> Info Note In a message flow you can use the cache mediator as a finder (in the incoming path to check the request) or as a collector (in the outgoing path to cache the response). It is not possible to have more than one cache mediator in the same message flow because mediation is terminated after the finder on a cache hit, and the response is not passed on to the next finder after a cache hit. Following is an example where the expected response from the last cache hit is not received because the response is sent once the request comes to the first finder: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"cache115\" transports=\"http https\" startOnLoad=\"true\"> <description /> <target> <inSequence> <cache collector=\"false\" timeout=\"60\"> <protocol type=\"HTTP\"> <methods>POST</methods> <headersToExcludeInHash /> <responseCodes>.*</responseCodes> <enableCacheControl>false</enableCacheControl> <includeAgeHeader>false</includeAgeHeader> <hashGenerator>org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator</hashGenerator> </protocol> </cache> <call> <endpoint> <address uri=\"http://demo0585968.mockable.io/some\" /> </endpoint> </call> <property name=\"RESPONSE\" value=\"true\" scope=\"default\" type=\"STRING\" /> <log level=\"full\" /> <cache collector=\"true\" /> <property name=\"RESPONSE\" value=\"false\" scope=\"default\" type=\"STRING\" /> <cache collector=\"false\" timeout=\"60\"> <protocol type=\"HTTP\"> <methods>POST</methods> <headersToExcludeInHash /> <responseCodes>.*</responseCodes> <hashGenerator>org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator</hashGenerator> </protocol> </cache> <call> <endpoint> <address uri=\"http://demo0585968.mockable.io/hello\" /> </endpoint> </call> <property name=\"RESPONSE\" value=\"true\" scope=\"default\" type=\"STRING\" /> <log level=\"full\" /> <cache collector=\"true\" /> <respond /> </inSequence> </target> </proxy>","title":"Syntax"},{"location":"references/cache-Mediator/#configuration","text":"Click on the relevant tab to view the UI configuration depending on whether the cache type of the cache mediator is Finder or Collector . Finder Collector The parameters available to configure the Cache mediator are as follows. Parameter Name Description Cache Type This parameter specifies whether the Cache mediator should be in the incoming path (to check the request) or in the outgoing path (to cache the response). Possible values are as follows. Finder : If this is selected, the Cache mediator is used to search for the request hash of incoming messages. Collector : If this is selected, the Cache mediator is used to collect response messages in the cache. Cache Timeout (Seconds) The time duration that the cache should be retained specified in seconds. The cache expires once this time duration elapses. The default value is 5000 seconds. Maximum Message Size The maximum size of the message to be cached. This should be specified in bytes. Protocol Type The protocol type to be cached in the message flow. In the current implementation, HTTP is the only value that you can select. Although the only configuration supported for other protocols is the HashGenerator , you can specify the protocol type to be anything and specify a HashGenerator that you prefer. HTTP Methods A comma separated list of HTTP methods that should be cached for the HTTP protocol . The default value is * , and it caches all HTTP methods. Headers to Exclude in Hash A comma separated list of headers to ignore when hashing an incoming messages. If you want to exclude all headers when hashing an incoming message, specify *. Response Codes Specify the response codes to be cached as a regular expression. If the http status code of a response matches the regular expression, the response should be cached. The default setting is to cache any response code. Hash Generator This parameter is used to define the logic used by the Cache mediator to evaluate the hash values of incoming messages. The value specified here should be a class that implements the org.separated.carbon.mediator.cache.digest.DigestGenerator class interface. The default hash generator is org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator . If the generated hash value is found in the cache, then the Cache mediator executes the onCacheHit sequence, which can be specified inline or referenced. !!! note Note The hash generator is specific to the HTTP protocol. If you are using any other protocol, you need to write a custom hash generator or use one of the following deprecated hash generator classes: org.wso2.carbon.mediator.cache.digest.DOMHASHGenerator org.wso2.carbon.mediator.cache.digest.REQUESTHASHGenerator Enable Cache Control Headers Whether the Cache mediator should honor the Cache-Control header(no-cache, no-store, max-age headers). If you set this to the default value (i.e., false ), the Cache mediator will not consider the Cache-Control headers when caching the response or when returning the cached response. Include Age Header Whether an Age header needs to be included when returning the cached response. Maximum Size The maximum number of elements to be cached. The default size is 1000. Anonymous If this option is selected, an anonymous sequence is executed when an incoming message is identified as an equivalent to a previously received message, based on the value defined in the Hash Generator field. Sequence Reference The reference to the onCacheHit sequence to be executed when an incoming message is identified as an equivalent to a previously received message, based on the value defined in the Hash Generator field. The sequence should be created in the registry in order to be specified in this field. You can click either Configuration Registry or Governance Registry as applicable to select the required sequence from the resource tree. The parameters available to configure the Cache mediator are as follows. Parameter Name Description Cache Type This parameter specifies whether the mediator should be in the incoming path (to check the request) or in the outgoing path (to cache the response). Possible values are as follows. Finder : If this is selected, the mediator is used to search for the request hash of incoming messages. Collector : If this is selected, the mediator is used to collect response messages in the cache. Examples Following are examples of how you can use the Cache mediator.","title":"Configuration"},{"location":"references/cache-Mediator/#example-one","text":"According to this example configuration, when the first message is sent to the endpoint, the cache is not hit. The Cache mediator configured in the Out sequence caches the response to this message. When a similar message is sent to the endpoint for the second time, the previous response is directly fetched from the cache and sent to the requester. This happens because the onCacheHit sequence is not defined in this configuration. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence name=\"main\"> <in> <cache collector=\"false\" maxMessageSize=\"10000\" timeout=\"20\"> <protocol type=\"HTTP\"> <methods>POST</methods> <headersToExcludeInHash/> <responseCodes>2[0-9][0-9]</responseCodes> <enableCacheControl>false</enableCacheControl> <includeAgeHeader>false</includeAgeHeader> <hashGenerator>org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator</hashGenerator> </protocol> <implementation maxSize=\"100\"/> </cache> <send> <endpoint name=\"inlined\"> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </in> <out> <cache collector=\"true\"/> <send/> </out> </sequence>","title":"Example one"},{"location":"references/cache-Mediator/#example-two","text":"According to this example configuration, if you define a cache collector using the cache mediator in the in sequence, you need to add the RESPONSE property to consider the message as a response message. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <api xmlns=\"http://ws.apache.org/ns/synapse\" name=\"cacheAPI\" context=\"/cache\"> <resource methods=\"POST GET\" uri-template=\"/headerapi/*\"> <inSequence> <cache collector=\"false\" timeout=\"5000\"> <protocol type=\"HTTP\"> <methods>GET, POST</methods> <headersToExcludeInHash>*</headersToExcludeInHash> <responseCodes>.*</responseCodes> <enableCacheControl>false</enableCacheControl> <includeAgeHeader>false</includeAgeHeader> <hashGenerator>org.wso2.carbon.mediator.cache.digest.HttpRequestHashGenerator</hashGenerator> </protocol> </cache> <call> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </call> <property name=\"RESPONSE\" value=\"true\" scope=\"default\" type=\"STRING\"/> <enrich> <source type=\"inline\" clone=\"true\"> <ax21:newvalue xmlns:ax21=\"http://services.samples/xsd\">testsamplevalue </ax21:newvalue> </source> <target xmlns:ax21=\"http://services.samples/xsd\" xmlns:ns=\"http://services.samples\" action=\"sibling\" xpath=\"//ns:getQuoteResponse/ns:return/ax21:volume\"/> </enrich> <cache collector=\"true\"/> <respond/> </inSequence> </resource> </api>","title":"Example two"},{"location":"references/cache-Mediator/#samples","text":"Sample 420: Simple Response Cache in the ESB for an Actual Service","title":"Samples"},{"location":"references/cache-Mediator/#invalidating-cached-responses-remotely","text":"You can invalidate all cached response remotely by using any JMX monitoring tool such as Jconsole via the exposed MBeans. You can use the invalidateTheWholeCache() operation of the org.wso2.carbon.mediatio n MBean for this as shown below. {width=\"600\" height=\"520\"}","title":"Invalidating cached responses remotely"},{"location":"references/call-Mediator/","text":"Call Mediator \u00b6 The Call mediator is used to send messages out of the ESB profile to an endpoint . You can invoke services either in blocking or non-blocking manner. When you invoke a service in non-blocking mode, the underlying worker thread returns without waiting for the response. In blocking mode, the underlying worker thread gets blocked and waits for the response after sending the request to the endpoint. Call mediator in blocking mode is very much similar to the Callout mediator . In both blocking and non-blocking modes, Call mediator behaves in a synchronous manner. Hence, mediation pauses after the service invocation, and resumes from the next mediator in the sequence when the response is received. Call mediator allows you to create your configuration independent from the underlying architecture. Non-blocking mode of the Call mediator leverages the non-blocking transports for better performance. Therefore, it is recommended to use it in non-blocking mode as much as possible. However, there are scenarios where you need to use the blocking mode. For example, when you implement a scenario related to JMS transactions, it is vital to use the underlying threads in blocking mode. In blocking mode, Call mediator uses the <EI_HOME>/conf/axis2/axis2_blocking_client.xml file as the Axis2 configuration. You can obtain the service endpoint for the Call mediator as follows: Pick from message-level information Pick from a pre-defined endpoint If you do not specify an endpoint, the Call mediator tries to send the message using the WSA:TO address of the message. If you specify an endpoint, the Call mediator sends the message based on the specified endpoint. The endpoint type can be Leaf Endpoint (i.e. Address/WSDL/Default/HTTP) or Group Endpoint (i.e. Failover/Load balance/Recipient list). Group Endpoint is only supported in non-blocking mode. Info The Call mediator is a content-unaware mediator. Set up | Syntax | Configuration | Example Set up \u00b6 Enabling mutual SSL in the blocking mode \u00b6 When using the Call mediator in the blocking mode (blocking=true), enable the mediator to handle mutual SSL by adding the following JVM settings to the <EI_HOME>/bin/integrator.sh file: -Djavax.net.ssl.keyStore=\"$CARBON_HOME/repository/resources/security/wso2carbon.jks\" \\ -Djavax.net.ssl.keyStorePassword=\"wso2carbon\" \\ -Djavax.net.ssl.keyPassword=\"wso2carbon\" \\ -Drampart.axiom.parser.pool=false \\ S yntax \u00b6 <call [blocking=\"true\"] /> If the message is to be sent to one or more endpoints, use the following syntax: <call [blocking=\"true\"]> (endpointref | endpoint)+ </call> The endpointref token refers to the following: <endpoint key=\"name\"/> The endpoint token refers to an anonymous endpoint definition. Configuration \u00b6 Select one of the following options to define the endpoint to which, the message should be delivered. Parameter Name Description None Select this option if you do not want to provide an endpoint. The Call mediator will send the message using its wsa:to address. Define Inline If this is selected, the endpoint to which the message should be sent can be included within the Call mediator configuration. Click Add to add the required endpoint. For more information on Adding an endpoint, see Adding an Endpoint . Pick From Registry If this is selected, the message can be sent to a pre-defined endpoint which is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. XPath If this is selected, the endpoint to which the message should be sent will be derived via an XPath expression. You are required to enter the relevant XPath expression in the text field that appears when this option is selected. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Blocking If set to true , you can use the call mediator in blocking mode. Example \u00b6 Example 1 - Service orchestration \u00b6 In this example, t he Call mediator invokes a backend service. An Enrich mediator stores the response received for that service invocation. The Filter Mediator added after the Call mediator carries out a filter to determine whether the first call has been successful. If it is successful, second backend service is invoked. The payload of the request to the second backend is the response of the first service invocation . After a successful second backend service invocation, response of the first service is retrieved by the Enrich mediator from the property where it was formerly stored. This response is sent to the client by the Respond mediator . If it is not successful, a custom JSON error message is sent with HTTP 500. If the first call itself is not successful, the output is just sent back with the relevant error code. <target> <inSequence> <log/> <call> <endpoint> <http method=\"get\" uri-template=\"http://192.168.1.10:8088/mockaxis2service\"/> </endpoint> </call> <enrich> <source type=\"body\" clone=\"true\"/> <target type=\"property\" action=\"child\" property=\"body_of_first_call\"/> </enrich> <filter source=\"get-property('axis2', 'HTTP_SC')\" regex=\"200\"> <then> <log level=\"custom\"> <property name=\"switchlog\" value=\"Case: first call successful\"/> </log> <call> <endpoint> <http method=\"get\" uri-template=\"http://localhost:8080/MockService1\"/> </endpoint> </call> <filter source=\"get-property('axis2', 'HTTP_SC')\" regex=\"200\"> <then> <log level=\"custom\"> <property name=\"switchlog\" value=\"Case: second call successful\"/> </log> <enrich> <source type=\"property\" clone=\"true\" property=\"body_of_first_call\"/> <target type=\"body\"/> </enrich> <respond/> </then> <else> <log level=\"custom\"> <property name=\"switchlog\" value=\"Case: second call unsuccessful\"/> </log> <property name=\"HTTP_SC\" value=\"500\" scope=\"axis2\"/> <payloadFactory media-type=\"json\"> <format>{ \"status\": \"ERROR!\"}</format> <args/> </payloadFactory> <respond/> </else> </filter> </then> <else> <log level=\"custom\"> <property name=\"switchlog\" value=\"Case: first call unsuccessful\"/> </log> <respond/> </else> </filter> </inSequence> </target> Example 2 - Continuing mediation without waiting for responses \u00b6 In this example, the message will be cloned by the Clone Mediator and sent via the Call mediator. The Drop mediator drops the response so that no further mediation is carried out for the cloned message. However, since the continueParent attribute of the Clone mediator is set to true , the original message is mediated in parallel. Therefore, the Log Mediator at the end of the configuration will log the After call mediator log message without waiting for the Call mediator response. ... <log level=\"full\"/> <clone continueParent=\"true\"> <target> <sequence> <call> <endpoint> <address uri=\"http://localhost:8080/echoString\"/> </endpoint> </call> <drop/> </sequence> </target> </clone> <log level=\"custom\"> <property name=\"MESSAGE\" value=\"After call mediator\"/> </log> ... Example 3 - Call mediator in blocking mode \u00b6 In the following sample configuration, the Header Mediator is used to add the action, the PayloadFactory Mediator is used to store the the request message and the Call mediator is used to invoke a backend service. You will see that the payload of the request and header action are sent to the backend. After successful backend service invocation, you will see that the response of the service is retrieved by the EI and sent to the client as the response using the Respond Mediator . <target> <inSequence> <header name=\"Action\" value=\"urn:getQuote\" /> <payloadFactory media-type=\"xml\"> <format> <m0:getQuote xmlns:m0=\"http://services.samples\"> <m0:request> <m0:symbol>WSO2</m0:symbol> </m0:request> </m0:getQuote> </format> <args /> </payloadFactory> <call blocking=\"true\"> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\" /> </endpoint> </call> <respond /> </inSequence> </target> Example 4 - Receiving response headers in blocking mode \u00b6 If you want to receive the response message headers, when you use the Call mediator in blocking mode, add the BLOCKING_SENDER_PRESERVE_REQ_HEADERS property within the proxy service, or in a sequence as shown in the sample proxy configuration below. Info Set the value of the BLOCKING_SENDER_PRESERVE_REQ_HEADERS property to false , to receive the response message headers. If you set it to true , you cannot get the response headers, but the request headers will be preserved. <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"sample\" transports=\"https\" statistics=\"enable\" trace=\"enable\" startOnLoad=\"true\"> <target> <inSequence> <property name=\"FORCE_ERROR_ON_SOAP_FAULT\" value=\"true\" scope=\"default\" type=\"STRING\"/> <property name=\"HTTP_METHOD\" value=\"POST\" scope=\"axis2\" type=\"STRING\"/> <property name=\"messageType\" value=\"text/xml\" scope=\"axis2\" type=\"STRING\"/> <property name=\"BLOCKING_SENDER_PRESERVE_REQ_HEADERS\" value=\"false\"/> <call blocking=\"true\"> <endpoint> <address uri=\"https://localhost:8243/services/sampleBE\" trace=\"enable\" statistics=\"enable\"/> </endpoint> </call> </inSequence> <outSequence/> </target> <description/> </proxy> Samples \u00b6 For another example, see Sample 500: Call Mediator for Non-Blocking Service Invocation .","title":"Call Mediator"},{"location":"references/call-Mediator/#call-mediator","text":"The Call mediator is used to send messages out of the ESB profile to an endpoint . You can invoke services either in blocking or non-blocking manner. When you invoke a service in non-blocking mode, the underlying worker thread returns without waiting for the response. In blocking mode, the underlying worker thread gets blocked and waits for the response after sending the request to the endpoint. Call mediator in blocking mode is very much similar to the Callout mediator . In both blocking and non-blocking modes, Call mediator behaves in a synchronous manner. Hence, mediation pauses after the service invocation, and resumes from the next mediator in the sequence when the response is received. Call mediator allows you to create your configuration independent from the underlying architecture. Non-blocking mode of the Call mediator leverages the non-blocking transports for better performance. Therefore, it is recommended to use it in non-blocking mode as much as possible. However, there are scenarios where you need to use the blocking mode. For example, when you implement a scenario related to JMS transactions, it is vital to use the underlying threads in blocking mode. In blocking mode, Call mediator uses the <EI_HOME>/conf/axis2/axis2_blocking_client.xml file as the Axis2 configuration. You can obtain the service endpoint for the Call mediator as follows: Pick from message-level information Pick from a pre-defined endpoint If you do not specify an endpoint, the Call mediator tries to send the message using the WSA:TO address of the message. If you specify an endpoint, the Call mediator sends the message based on the specified endpoint. The endpoint type can be Leaf Endpoint (i.e. Address/WSDL/Default/HTTP) or Group Endpoint (i.e. Failover/Load balance/Recipient list). Group Endpoint is only supported in non-blocking mode. Info The Call mediator is a content-unaware mediator. Set up | Syntax | Configuration | Example","title":"Call Mediator"},{"location":"references/call-Mediator/#set-up","text":"","title":"Set up"},{"location":"references/call-Mediator/#enabling-mutual-ssl-in-the-blocking-mode","text":"When using the Call mediator in the blocking mode (blocking=true), enable the mediator to handle mutual SSL by adding the following JVM settings to the <EI_HOME>/bin/integrator.sh file: -Djavax.net.ssl.keyStore=\"$CARBON_HOME/repository/resources/security/wso2carbon.jks\" \\ -Djavax.net.ssl.keyStorePassword=\"wso2carbon\" \\ -Djavax.net.ssl.keyPassword=\"wso2carbon\" \\ -Drampart.axiom.parser.pool=false \\","title":"Enabling mutual SSL in the blocking mode"},{"location":"references/call-Mediator/#s-yntax","text":"<call [blocking=\"true\"] /> If the message is to be sent to one or more endpoints, use the following syntax: <call [blocking=\"true\"]> (endpointref | endpoint)+ </call> The endpointref token refers to the following: <endpoint key=\"name\"/> The endpoint token refers to an anonymous endpoint definition.","title":"S yntax"},{"location":"references/call-Mediator/#configuration","text":"Select one of the following options to define the endpoint to which, the message should be delivered. Parameter Name Description None Select this option if you do not want to provide an endpoint. The Call mediator will send the message using its wsa:to address. Define Inline If this is selected, the endpoint to which the message should be sent can be included within the Call mediator configuration. Click Add to add the required endpoint. For more information on Adding an endpoint, see Adding an Endpoint . Pick From Registry If this is selected, the message can be sent to a pre-defined endpoint which is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. XPath If this is selected, the endpoint to which the message should be sent will be derived via an XPath expression. You are required to enter the relevant XPath expression in the text field that appears when this option is selected. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Blocking If set to true , you can use the call mediator in blocking mode.","title":"Configuration"},{"location":"references/call-Mediator/#example","text":"","title":"Example"},{"location":"references/call-Mediator/#example-1-service-orchestration","text":"In this example, t he Call mediator invokes a backend service. An Enrich mediator stores the response received for that service invocation. The Filter Mediator added after the Call mediator carries out a filter to determine whether the first call has been successful. If it is successful, second backend service is invoked. The payload of the request to the second backend is the response of the first service invocation . After a successful second backend service invocation, response of the first service is retrieved by the Enrich mediator from the property where it was formerly stored. This response is sent to the client by the Respond mediator . If it is not successful, a custom JSON error message is sent with HTTP 500. If the first call itself is not successful, the output is just sent back with the relevant error code. <target> <inSequence> <log/> <call> <endpoint> <http method=\"get\" uri-template=\"http://192.168.1.10:8088/mockaxis2service\"/> </endpoint> </call> <enrich> <source type=\"body\" clone=\"true\"/> <target type=\"property\" action=\"child\" property=\"body_of_first_call\"/> </enrich> <filter source=\"get-property('axis2', 'HTTP_SC')\" regex=\"200\"> <then> <log level=\"custom\"> <property name=\"switchlog\" value=\"Case: first call successful\"/> </log> <call> <endpoint> <http method=\"get\" uri-template=\"http://localhost:8080/MockService1\"/> </endpoint> </call> <filter source=\"get-property('axis2', 'HTTP_SC')\" regex=\"200\"> <then> <log level=\"custom\"> <property name=\"switchlog\" value=\"Case: second call successful\"/> </log> <enrich> <source type=\"property\" clone=\"true\" property=\"body_of_first_call\"/> <target type=\"body\"/> </enrich> <respond/> </then> <else> <log level=\"custom\"> <property name=\"switchlog\" value=\"Case: second call unsuccessful\"/> </log> <property name=\"HTTP_SC\" value=\"500\" scope=\"axis2\"/> <payloadFactory media-type=\"json\"> <format>{ \"status\": \"ERROR!\"}</format> <args/> </payloadFactory> <respond/> </else> </filter> </then> <else> <log level=\"custom\"> <property name=\"switchlog\" value=\"Case: first call unsuccessful\"/> </log> <respond/> </else> </filter> </inSequence> </target>","title":"Example 1 - Service orchestration"},{"location":"references/call-Mediator/#example-2-continuing-mediation-without-waiting-for-responses","text":"In this example, the message will be cloned by the Clone Mediator and sent via the Call mediator. The Drop mediator drops the response so that no further mediation is carried out for the cloned message. However, since the continueParent attribute of the Clone mediator is set to true , the original message is mediated in parallel. Therefore, the Log Mediator at the end of the configuration will log the After call mediator log message without waiting for the Call mediator response. ... <log level=\"full\"/> <clone continueParent=\"true\"> <target> <sequence> <call> <endpoint> <address uri=\"http://localhost:8080/echoString\"/> </endpoint> </call> <drop/> </sequence> </target> </clone> <log level=\"custom\"> <property name=\"MESSAGE\" value=\"After call mediator\"/> </log> ...","title":"Example 2 - Continuing mediation without waiting for responses"},{"location":"references/call-Mediator/#example-3-call-mediator-in-blocking-mode","text":"In the following sample configuration, the Header Mediator is used to add the action, the PayloadFactory Mediator is used to store the the request message and the Call mediator is used to invoke a backend service. You will see that the payload of the request and header action are sent to the backend. After successful backend service invocation, you will see that the response of the service is retrieved by the EI and sent to the client as the response using the Respond Mediator . <target> <inSequence> <header name=\"Action\" value=\"urn:getQuote\" /> <payloadFactory media-type=\"xml\"> <format> <m0:getQuote xmlns:m0=\"http://services.samples\"> <m0:request> <m0:symbol>WSO2</m0:symbol> </m0:request> </m0:getQuote> </format> <args /> </payloadFactory> <call blocking=\"true\"> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\" /> </endpoint> </call> <respond /> </inSequence> </target>","title":"Example 3 - Call mediator in blocking mode"},{"location":"references/call-Mediator/#example-4-receiving-response-headers-in-blocking-mode","text":"If you want to receive the response message headers, when you use the Call mediator in blocking mode, add the BLOCKING_SENDER_PRESERVE_REQ_HEADERS property within the proxy service, or in a sequence as shown in the sample proxy configuration below. Info Set the value of the BLOCKING_SENDER_PRESERVE_REQ_HEADERS property to false , to receive the response message headers. If you set it to true , you cannot get the response headers, but the request headers will be preserved. <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"sample\" transports=\"https\" statistics=\"enable\" trace=\"enable\" startOnLoad=\"true\"> <target> <inSequence> <property name=\"FORCE_ERROR_ON_SOAP_FAULT\" value=\"true\" scope=\"default\" type=\"STRING\"/> <property name=\"HTTP_METHOD\" value=\"POST\" scope=\"axis2\" type=\"STRING\"/> <property name=\"messageType\" value=\"text/xml\" scope=\"axis2\" type=\"STRING\"/> <property name=\"BLOCKING_SENDER_PRESERVE_REQ_HEADERS\" value=\"false\"/> <call blocking=\"true\"> <endpoint> <address uri=\"https://localhost:8243/services/sampleBE\" trace=\"enable\" statistics=\"enable\"/> </endpoint> </call> </inSequence> <outSequence/> </target> <description/> </proxy>","title":"Example 4 - Receiving response headers in blocking mode"},{"location":"references/call-Mediator/#samples","text":"For another example, see Sample 500: Call Mediator for Non-Blocking Service Invocation .","title":"Samples"},{"location":"references/call-Template-Mediator/","text":"Call Template Mediator \u00b6 The Call Template mediator allows you to construct a sequence by passing values into a sequence template . Info This is currently only supported for special types of mediators such as the Iterator and Aggregate Mediators , where actual XPath operations are performed on a different SOAP message, and not on the message coming into the mediator. Syntax \u00b6 <call-template target=\"string\"> <!-- parameter values will be passed on to a sequence template --> ( <!--passing plain static values --> <with-param name=\"string\" value=\"string\" /> | <!--passing xpath expressions --> <with-param name=\"string\" value=\"{string}\" /> | <!--passing dynamic xpath expressions where values will be compiled dynamically--> <with-param name=\"string\" value=\"{{string}}\" /> | ) * <!--this is the in-line sequence of the template --> </call-template> You use the target attribute to specify the sequence template you want to use. The <with-param> element is used to parse parameter values to the target sequence template. The parameter names should be the same as the names specified in target template. The parameter value can contain a string, an XPath expression (passed in with curly braces { }), or a dynamic XPath expression (passed in with double curly braces) of which the values are compiled dynamically. Configuration \u00b6 The parameters available to configure the Call-Template mediator are as follows. Parameter Name Description Target Template The sequence template to which values should be passed. You can select a template from the Available Templates list When a target template is selected, the parameter section will be displayed as shown below if the sequence template selected has any parameters. This enables parameter values to be parsed into the sequence template selected. Parameter Name Description Parameter Name The name of the parameter. Parameter Type The type of the parameter. Possible values are as follows. Value : Select this to define the parameter value as a static value. This value should be entered in the Value / Expression parameter. Expression : Select this to define the parameter value as a dynamic value. The XPath expression to calculate the parameter value should be entered in the Value / Expression parameter. Value / Expression The parameter value. This can be a static value, or an XPath expression to calculate a dynamic value depending on the value you selected for the Parameter Type parameter. Action Click Delete to delete a parameter. Examples \u00b6 Following examples demonstrate different usecases of the Call Template mediator. Example one \u00b6 The following four Call Template mediator configurations populate a sequence template named HelloWorld_Logger with the \"hello world\" text in four different languages. <call-template target=\"HelloWorld_Logger\"> <with-param name=\"message\" value=\"HELLO WORLD!!!!!!\" /> </call-template> <call-template target=\"HelloWorld_Logger\"> <with-param name=\"message\" value=\"Bonjour tout le monde!!!!!!\" /> </call-template> <call-template target=\"HelloWorld_Logger\"> <with-param name=\"message\" value=\"Ciao a tutti!!!!!!!\" /> </call-template> <call-template target=\"HelloWorld_Logger\"> <with-param name=\"message\" value=\"???????!!!!!!!\" /> </call-template> The sequence template can be configured as follows to log any greetings message passed to it by the Call Template mediator. Thus, due to the availability of the Call Template mediator, you are not required to have the message entered in all four languages included in the sequence template configuration itself. <template name=\"HelloWorld_Logger\"> <parameter name=\"message\"/> <sequence> <log level=\"custom\"> <property expression=\"$func:message\" name=\"GREETING_MESSAGE\"/> </log> </sequence> </template> See Sequence Template for a more information about this scenario. Example two \u00b6 The following Call Template mediator configuration populates a sequence template named Testtemp with a dynamic XPath expression. <call-template target=\"Testtemp\"> <with-param name=\"message_store\" value=\"<MESSAGE_STORE_NAME>\" /> </call-template> The following Testtemp template includes a dynamic XPath expression to save messages in a Message Store, which is dynamically set via the message context. <template name=\"Testtemp\"> <parameter name=\"message_store\"/> <sequence> <log level=\"custom\"> <property expression=\"$func:message_store\" name=\"STORENAME\" xmlns:ns=\"http://org.apache.synapse/xsd\" xmlns:ns2=\"http://org.apache.synapse/xsd\" xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope\"/> </log> <store messageStore=\"{$func:message_store}\" xmlns:ns=\"http://org.apache.synapse/xsd\" xmlns:ns2=\"http://org.apache.synapse/xsd\" xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope\"/> </sequence> </template>","title":"Call Template Mediator"},{"location":"references/call-Template-Mediator/#call-template-mediator","text":"The Call Template mediator allows you to construct a sequence by passing values into a sequence template . Info This is currently only supported for special types of mediators such as the Iterator and Aggregate Mediators , where actual XPath operations are performed on a different SOAP message, and not on the message coming into the mediator.","title":"Call Template Mediator"},{"location":"references/call-Template-Mediator/#syntax","text":"<call-template target=\"string\"> <!-- parameter values will be passed on to a sequence template --> ( <!--passing plain static values --> <with-param name=\"string\" value=\"string\" /> | <!--passing xpath expressions --> <with-param name=\"string\" value=\"{string}\" /> | <!--passing dynamic xpath expressions where values will be compiled dynamically--> <with-param name=\"string\" value=\"{{string}}\" /> | ) * <!--this is the in-line sequence of the template --> </call-template> You use the target attribute to specify the sequence template you want to use. The <with-param> element is used to parse parameter values to the target sequence template. The parameter names should be the same as the names specified in target template. The parameter value can contain a string, an XPath expression (passed in with curly braces { }), or a dynamic XPath expression (passed in with double curly braces) of which the values are compiled dynamically.","title":"Syntax"},{"location":"references/call-Template-Mediator/#configuration","text":"The parameters available to configure the Call-Template mediator are as follows. Parameter Name Description Target Template The sequence template to which values should be passed. You can select a template from the Available Templates list When a target template is selected, the parameter section will be displayed as shown below if the sequence template selected has any parameters. This enables parameter values to be parsed into the sequence template selected. Parameter Name Description Parameter Name The name of the parameter. Parameter Type The type of the parameter. Possible values are as follows. Value : Select this to define the parameter value as a static value. This value should be entered in the Value / Expression parameter. Expression : Select this to define the parameter value as a dynamic value. The XPath expression to calculate the parameter value should be entered in the Value / Expression parameter. Value / Expression The parameter value. This can be a static value, or an XPath expression to calculate a dynamic value depending on the value you selected for the Parameter Type parameter. Action Click Delete to delete a parameter.","title":"Configuration"},{"location":"references/call-Template-Mediator/#examples","text":"Following examples demonstrate different usecases of the Call Template mediator.","title":"Examples"},{"location":"references/call-Template-Mediator/#example-one","text":"The following four Call Template mediator configurations populate a sequence template named HelloWorld_Logger with the \"hello world\" text in four different languages. <call-template target=\"HelloWorld_Logger\"> <with-param name=\"message\" value=\"HELLO WORLD!!!!!!\" /> </call-template> <call-template target=\"HelloWorld_Logger\"> <with-param name=\"message\" value=\"Bonjour tout le monde!!!!!!\" /> </call-template> <call-template target=\"HelloWorld_Logger\"> <with-param name=\"message\" value=\"Ciao a tutti!!!!!!!\" /> </call-template> <call-template target=\"HelloWorld_Logger\"> <with-param name=\"message\" value=\"???????!!!!!!!\" /> </call-template> The sequence template can be configured as follows to log any greetings message passed to it by the Call Template mediator. Thus, due to the availability of the Call Template mediator, you are not required to have the message entered in all four languages included in the sequence template configuration itself. <template name=\"HelloWorld_Logger\"> <parameter name=\"message\"/> <sequence> <log level=\"custom\"> <property expression=\"$func:message\" name=\"GREETING_MESSAGE\"/> </log> </sequence> </template> See Sequence Template for a more information about this scenario.","title":"Example one"},{"location":"references/call-Template-Mediator/#example-two","text":"The following Call Template mediator configuration populates a sequence template named Testtemp with a dynamic XPath expression. <call-template target=\"Testtemp\"> <with-param name=\"message_store\" value=\"<MESSAGE_STORE_NAME>\" /> </call-template> The following Testtemp template includes a dynamic XPath expression to save messages in a Message Store, which is dynamically set via the message context. <template name=\"Testtemp\"> <parameter name=\"message_store\"/> <sequence> <log level=\"custom\"> <property expression=\"$func:message_store\" name=\"STORENAME\" xmlns:ns=\"http://org.apache.synapse/xsd\" xmlns:ns2=\"http://org.apache.synapse/xsd\" xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope\"/> </log> <store messageStore=\"{$func:message_store}\" xmlns:ns=\"http://org.apache.synapse/xsd\" xmlns:ns2=\"http://org.apache.synapse/xsd\" xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope\"/> </sequence> </template>","title":"Example two"},{"location":"references/callout-Mediator/","text":"Callout Mediator \u00b6 The Callout mediator performs a blocking external service invocation during mediation. As the Callout mediator performs a blocking call, it cannot use the default non-blocking HTTP/S transports based on Java NIO. Instead, it defaults to using <EI_HOME>/conf/axis2/axis2_blocking_client.xml as the Axis2 configuration, and <EI_HOME>/repository/deployment/client as the client repository unless these are specified separately. Tip The Call mediator leverages the non-blocking transports for much greater performance than the Callout mediator, so you should use the Call mediator in most cases. However, the Callout mediator is recommended in situations where you need to execute the mediation flow in a single thread. Setup | Syntax | Configuration | Samples Setup \u00b6 Enabling mutual SSL \u00b6 Since the Callout mediator is run based on the configuration of the axis2_blocking_client.xml file, its default https transport sender is org.apache.axis2.transport.http.CommonsHTTPTransportSender . Therefore, the Callout mediator does not have access to the required key store to handle mutual SSL. To enable the Callout mediator to handle mutual SSL, the following JVM settings should be added to the <EI_HOME>/bin/integrator.sh file. -Djavax.net.ssl.keyStore=\"$CARBON_HOME/repository/resources/security/wso2carbon.jks\" \\ -Djavax.net.ssl.keyStorePassword=\"wso2carbon\" \\ -Djavax.net.ssl.keyPassword=\"wso2carbon\" \\ Disabling chunking \u00b6 The Callout mediator is not affected by the DISABLE_CHUNKING property . Instead, you can disable chunking for the Callout mediator by setting the Transfer-Encoding parameter to none in CommonsHTTPTransportSender of axis2_blocking_client.xml as follows: <parameter name=\"Transfer-Encoding\">none</parameter> For example: <transportSender name=\"http\" class=\"org.apache.axis2.transport.http.CommonsHTTPTransportSender\"> <parameter name=\"PROTOCOL\">HTTP/1.1</parameter> <parameter name=\"Transfer-Encoding\">none</parameter> <parameter name=\"cacheHttpClient\">true</parameter> <parameter name=\"defaultMaxConnectionsPerHost\">200</parameter> </transportSender> This will disable chunking for all Callout mediators present in the EI server. If you want to disable chunking for only a single Callout mediator instance, create a new axis2.xml file by copying the axis2_blocking_client.xml file, set the Transfer-Encoding parameter as shown, and then configure that Callout mediator to use this new axis2.xml file as described below. Syntax \u00b6 <callout [serviceURL=\"string\"] [action=\"string\"] [initAxis2ClientOptions=\"true|false\"] [endpointKey=\"string\"]> <configuration [axis2xml=\"string\"] [repository=\"string\"]/>? <source xpath=\"expression\" | key=\"string\" | type=\"envelope\"/> <target xpath=\"expression\" | key=\"string\"/> <enableSec policy=\"string\" | outboundPolicy=\"String\" | inboundPolicy=\"String\" />? </callout> Configuration \u00b6 There will be a slight difference in the UI depending on the option you select for the Specify as parameter. Click on the relevant tab to view the required UI. The parameters available for configuring the Callout mediator are as follows. Parameter Name Description Specify As This parameter determines whether the target external service should be configured by using either a serviceURL attribute or an endpointKey attribute. !!! note Callout mediator does not support endpoint configurations such as timeout , suspendOnFailure and markForSuspension when the endpointKey attribute is used to specify an existing endpoint. URL : Select URL if you want to call the external service by specifying its URL in the Call mediator configuration. Address Endpoint : Selected Address Endpoint if you want to call the external service via an Endpoint which is already saved in the Registry . This option should be selected if you want to make use of the WSO2 functionality related to endpoints such as format conversions, security etc. Note that only Leaf endpoint types (i.e. Address , WSDL , Default and Http ) are supported for the Callout mediator. If neither a URL or an address endpoint is specified, the To header on the request is used as the target endpoint. URL If you selected URL for the Specify As parameter, use this parameter to enter the URL of the external service that you want to call. This URL will be used as the End Point Reference (EPR) of the external service. Address Endpoint If you selected Address Endpoint for the Specify As parameter, use this parameter to enter key to access the endpoint that should be used to call the external service. Click Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. Action The SOAP action which should be appended to the service call. Axis2 Repository The path to Axis2 client repository where the services and modules are located. The purpose of this parameter is to make the Callout mediator initialize with the required client repository. Axis2 XML The path to the location of the Axis2.xml configuration file. The purpose of this parameter is to make the Callout mediator initialize with the relevant Axis2 configurations. initAxis2ClientOptions If this parameter is set to false , the existing Axis2 client options available in the Synapse message context will be reused when the Callout mediator is invoked. This is useful when you want to use NLTM authentication. The default value for this parameter is true . Source This parameter defines the payload for the request. It can be defined using one of the following options. XPath - This option allows you to specify an expression that defines the location in the message. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Property - This option allows you to specify the payload for a request via a property included in the mediation flow. Envelope - This option allows you to select the entire envelope which is available in the message flow as the source. Target The node or the property of the request message to which the payload (resulting from the value specified for the Source parameter) would be attached. The target can be specified using one of the following options. XPath - This option allows you to specify an expression that defines the location in the message. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Property - This option allows you to specify a property included in the mediation flow. WS-Security If you select the check box, WS-Security is enabled for the Callout mediator. This section would expand as shown below when you select this check box. Specify as Inbound and Outbound Policies If this check box is selected, you can define separate security policies for the inbound and outbound messages (flows). This is done by entering the required policy keys in the Outbound Policy Key and Inbound Policy Key parameters which are displayed as follows when this check box is selected. You can click Configuration Registry or Governance Registry to select a security policy saved in the Registry from the resource tree. Policy Key If the Specify as Inbound and Outbound Policies check box is not selected, this parameter is used to enter a key to access a security policy which will be applied to both inbound and outbound messages. You can click Configuration Registry or Governance Registry to select a security policy saved in the Registry from the resource tree. Examples Following examples demonstrate the usage of the Callout mediator. Example 1 - Performing a direct service invocation \u00b6 In this example, the Callout Mediator does the direct service invocation to the StockQuoteService using the client request, gets the response, and sets the response as the first child of the SOAP message body. You can then use the Send Mediator to send the message back to the client. <callout serviceURL=\"http://localhost:9000/services/SimpleStockQuoteService\" action=\"urn:getQuote\"> <source xmlns:s11=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:s12=\"http://www.w3.org/2003/05/soap-envelope\" xpath=\"s11:Body/child::*[fn:position()=1] | s12:Body/child::*[fn:position()=1]\"/> <target xmlns:s11=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:s12=\"http://www.w3.org/2003/05/soap-envelope\" xpath=\"s11:Body/child::*[fn:position()=1] | s12:Body/child::*[fn:position()=1]\"/> </callout> Example 2 - Setting an HTTP method when invoking a REST service \u00b6 The below example uses a C allout mediator to set a HTTP method when invoking a REST service. Info For this, you need to define the following property: <property name=\"HTTP_METHOD\" expression=\"$axis2:HTTP_METHOD\" scope=\"axis2-client\"/> <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"CalloutProxy\" startOnLoad=\"true\" statistics=\"disable\" trace=\"disable\" transports=\"http,https\"> <target> <inSequence> <property name=\"enableREST\" scope=\"axis2-client\" type=\"BOOLEAN\" value=\"true\"/> <property expression=\"$axis2:HTTP_METHOD\" name=\"HTTP_METHOD\" scope=\"axis2-client\"/> <callout initAxis2ClientOptions=\"false\" serviceURL=\"http://localhost:8280/callout/CalloutRESTApi\"> <source type=\"envelope\"/> <target key=\"response\"/> </callout> <log level=\"custom\"> <property expression=\"$ctx:response\" name=\"MESSAGE###########################3\"/> </log> <property expression=\"$ctx:response\" name=\"res\" type=\"OM\"/> <property action=\"remove\" name=\"NO_ENTITY_BODY\" scope=\"axis2\"/> <property name=\"RESPONSE\" value=\"true\"/> <property name=\"messageType\" scope=\"axis2\" value=\"application/xml\"/> <header action=\"remove\" name=\"To\"/> <payloadFactory media-type=\"xml\"> <format> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header/> <soapenv:Body>$1 </soapenv:Body> </soapenv:Envelope> </format> <args> <arg evaluator=\"xml\" expression=\"$ctx:res\"/> </args> </payloadFactory> <send/> </inSequence> </target> <description/> </proxy> Samples \u00b6 Sample 430: Callout Mediator for Synchronous Service Invocation .","title":"Callout Mediator"},{"location":"references/callout-Mediator/#callout-mediator","text":"The Callout mediator performs a blocking external service invocation during mediation. As the Callout mediator performs a blocking call, it cannot use the default non-blocking HTTP/S transports based on Java NIO. Instead, it defaults to using <EI_HOME>/conf/axis2/axis2_blocking_client.xml as the Axis2 configuration, and <EI_HOME>/repository/deployment/client as the client repository unless these are specified separately. Tip The Call mediator leverages the non-blocking transports for much greater performance than the Callout mediator, so you should use the Call mediator in most cases. However, the Callout mediator is recommended in situations where you need to execute the mediation flow in a single thread. Setup | Syntax | Configuration | Samples","title":"Callout Mediator"},{"location":"references/callout-Mediator/#setup","text":"","title":"Setup"},{"location":"references/callout-Mediator/#enabling-mutual-ssl","text":"Since the Callout mediator is run based on the configuration of the axis2_blocking_client.xml file, its default https transport sender is org.apache.axis2.transport.http.CommonsHTTPTransportSender . Therefore, the Callout mediator does not have access to the required key store to handle mutual SSL. To enable the Callout mediator to handle mutual SSL, the following JVM settings should be added to the <EI_HOME>/bin/integrator.sh file. -Djavax.net.ssl.keyStore=\"$CARBON_HOME/repository/resources/security/wso2carbon.jks\" \\ -Djavax.net.ssl.keyStorePassword=\"wso2carbon\" \\ -Djavax.net.ssl.keyPassword=\"wso2carbon\" \\","title":"Enabling mutual SSL"},{"location":"references/callout-Mediator/#disabling-chunking","text":"The Callout mediator is not affected by the DISABLE_CHUNKING property . Instead, you can disable chunking for the Callout mediator by setting the Transfer-Encoding parameter to none in CommonsHTTPTransportSender of axis2_blocking_client.xml as follows: <parameter name=\"Transfer-Encoding\">none</parameter> For example: <transportSender name=\"http\" class=\"org.apache.axis2.transport.http.CommonsHTTPTransportSender\"> <parameter name=\"PROTOCOL\">HTTP/1.1</parameter> <parameter name=\"Transfer-Encoding\">none</parameter> <parameter name=\"cacheHttpClient\">true</parameter> <parameter name=\"defaultMaxConnectionsPerHost\">200</parameter> </transportSender> This will disable chunking for all Callout mediators present in the EI server. If you want to disable chunking for only a single Callout mediator instance, create a new axis2.xml file by copying the axis2_blocking_client.xml file, set the Transfer-Encoding parameter as shown, and then configure that Callout mediator to use this new axis2.xml file as described below.","title":"Disabling chunking"},{"location":"references/callout-Mediator/#syntax","text":"<callout [serviceURL=\"string\"] [action=\"string\"] [initAxis2ClientOptions=\"true|false\"] [endpointKey=\"string\"]> <configuration [axis2xml=\"string\"] [repository=\"string\"]/>? <source xpath=\"expression\" | key=\"string\" | type=\"envelope\"/> <target xpath=\"expression\" | key=\"string\"/> <enableSec policy=\"string\" | outboundPolicy=\"String\" | inboundPolicy=\"String\" />? </callout>","title":"Syntax"},{"location":"references/callout-Mediator/#configuration","text":"There will be a slight difference in the UI depending on the option you select for the Specify as parameter. Click on the relevant tab to view the required UI. The parameters available for configuring the Callout mediator are as follows. Parameter Name Description Specify As This parameter determines whether the target external service should be configured by using either a serviceURL attribute or an endpointKey attribute. !!! note Callout mediator does not support endpoint configurations such as timeout , suspendOnFailure and markForSuspension when the endpointKey attribute is used to specify an existing endpoint. URL : Select URL if you want to call the external service by specifying its URL in the Call mediator configuration. Address Endpoint : Selected Address Endpoint if you want to call the external service via an Endpoint which is already saved in the Registry . This option should be selected if you want to make use of the WSO2 functionality related to endpoints such as format conversions, security etc. Note that only Leaf endpoint types (i.e. Address , WSDL , Default and Http ) are supported for the Callout mediator. If neither a URL or an address endpoint is specified, the To header on the request is used as the target endpoint. URL If you selected URL for the Specify As parameter, use this parameter to enter the URL of the external service that you want to call. This URL will be used as the End Point Reference (EPR) of the external service. Address Endpoint If you selected Address Endpoint for the Specify As parameter, use this parameter to enter key to access the endpoint that should be used to call the external service. Click Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. Action The SOAP action which should be appended to the service call. Axis2 Repository The path to Axis2 client repository where the services and modules are located. The purpose of this parameter is to make the Callout mediator initialize with the required client repository. Axis2 XML The path to the location of the Axis2.xml configuration file. The purpose of this parameter is to make the Callout mediator initialize with the relevant Axis2 configurations. initAxis2ClientOptions If this parameter is set to false , the existing Axis2 client options available in the Synapse message context will be reused when the Callout mediator is invoked. This is useful when you want to use NLTM authentication. The default value for this parameter is true . Source This parameter defines the payload for the request. It can be defined using one of the following options. XPath - This option allows you to specify an expression that defines the location in the message. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Property - This option allows you to specify the payload for a request via a property included in the mediation flow. Envelope - This option allows you to select the entire envelope which is available in the message flow as the source. Target The node or the property of the request message to which the payload (resulting from the value specified for the Source parameter) would be attached. The target can be specified using one of the following options. XPath - This option allows you to specify an expression that defines the location in the message. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Property - This option allows you to specify a property included in the mediation flow. WS-Security If you select the check box, WS-Security is enabled for the Callout mediator. This section would expand as shown below when you select this check box. Specify as Inbound and Outbound Policies If this check box is selected, you can define separate security policies for the inbound and outbound messages (flows). This is done by entering the required policy keys in the Outbound Policy Key and Inbound Policy Key parameters which are displayed as follows when this check box is selected. You can click Configuration Registry or Governance Registry to select a security policy saved in the Registry from the resource tree. Policy Key If the Specify as Inbound and Outbound Policies check box is not selected, this parameter is used to enter a key to access a security policy which will be applied to both inbound and outbound messages. You can click Configuration Registry or Governance Registry to select a security policy saved in the Registry from the resource tree. Examples Following examples demonstrate the usage of the Callout mediator.","title":"Configuration"},{"location":"references/callout-Mediator/#example-1-performing-a-direct-service-invocation","text":"In this example, the Callout Mediator does the direct service invocation to the StockQuoteService using the client request, gets the response, and sets the response as the first child of the SOAP message body. You can then use the Send Mediator to send the message back to the client. <callout serviceURL=\"http://localhost:9000/services/SimpleStockQuoteService\" action=\"urn:getQuote\"> <source xmlns:s11=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:s12=\"http://www.w3.org/2003/05/soap-envelope\" xpath=\"s11:Body/child::*[fn:position()=1] | s12:Body/child::*[fn:position()=1]\"/> <target xmlns:s11=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:s12=\"http://www.w3.org/2003/05/soap-envelope\" xpath=\"s11:Body/child::*[fn:position()=1] | s12:Body/child::*[fn:position()=1]\"/> </callout>","title":"Example 1 - Performing a direct service invocation"},{"location":"references/callout-Mediator/#example-2-setting-an-http-method-when-invoking-a-rest-service","text":"The below example uses a C allout mediator to set a HTTP method when invoking a REST service. Info For this, you need to define the following property: <property name=\"HTTP_METHOD\" expression=\"$axis2:HTTP_METHOD\" scope=\"axis2-client\"/> <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"CalloutProxy\" startOnLoad=\"true\" statistics=\"disable\" trace=\"disable\" transports=\"http,https\"> <target> <inSequence> <property name=\"enableREST\" scope=\"axis2-client\" type=\"BOOLEAN\" value=\"true\"/> <property expression=\"$axis2:HTTP_METHOD\" name=\"HTTP_METHOD\" scope=\"axis2-client\"/> <callout initAxis2ClientOptions=\"false\" serviceURL=\"http://localhost:8280/callout/CalloutRESTApi\"> <source type=\"envelope\"/> <target key=\"response\"/> </callout> <log level=\"custom\"> <property expression=\"$ctx:response\" name=\"MESSAGE###########################3\"/> </log> <property expression=\"$ctx:response\" name=\"res\" type=\"OM\"/> <property action=\"remove\" name=\"NO_ENTITY_BODY\" scope=\"axis2\"/> <property name=\"RESPONSE\" value=\"true\"/> <property name=\"messageType\" scope=\"axis2\" value=\"application/xml\"/> <header action=\"remove\" name=\"To\"/> <payloadFactory media-type=\"xml\"> <format> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header/> <soapenv:Body>$1 </soapenv:Body> </soapenv:Envelope> </format> <args> <arg evaluator=\"xml\" expression=\"$ctx:res\"/> </args> </payloadFactory> <send/> </inSequence> </target> <description/> </proxy>","title":"Example 2 - Setting an HTTP method when invoking a REST service"},{"location":"references/callout-Mediator/#samples","text":"Sample 430: Callout Mediator for Synchronous Service Invocation .","title":"Samples"},{"location":"references/class-Mediator/","text":"Class Mediator \u00b6 The Class Mediator creates an instance of a custom-specified class and sets it as a mediator. The class must implement the org.apache.synapse.api.Mediator interface. If any properties are specified, the corresponding setter methods are invoked once on the class during initialization. The Class mediator is a custom Java class, which you need to maintain by yourself. Therefore, it is recommended to use the Class mediator only for not frequently re-used custom developments and very user-specific scenarios, for which, there is no built-in mediator that already provides the required functionality. Your class mediator might not be picked up and updated if you use an existing package when creating it. For best results, use WSO2 EI Tooling for debugging Class mediators. Syntax | Configuration | Examples Syntax \u00b6 <class name=\"class-name\"> <property name=\"string\" (value=\"literal\" | expression=\"[XPath|json-eval(JSON Path)]\")/>* </class> Configuration \u00b6 Class Name : The name of the class. To load a class, enter the qualified name of the relevant class in this parameter and click Load Class . Examples \u00b6 In this configuration, the ESB profile sends the requested message to the endpoint specified via the Send mediator . This endpoint is the Axis2server running on port 9000. The response message is passed through a Class mediator before it is sent back to the client. Two parameters named variable1 and variable2 are passed to the instance mediator implementation class ( SimpleClassMediator ). Info If you want, you can pass the same variables as a value or an expression: Example for passing the variable as a value: <property name=\"variable1\" value=\"10\"/> Example for passing the variable as an expression: <property name=\"variable2\" expression=\"get-property('variable1')\"/> For more information on using the get property method, see the Property Mediator . <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"fault\"> <makefault> <code value=\"tns:Receiver\" xmlns:tns=\"http://www.w3.org/2003/05/soap-envelope\"/> <reason value=\"Mediation failed.\"/> </makefault> <send/> </sequence> <sequence name=\"main\" onError=\"fault\"> <in> <send> <endpoint name=\"stockquote\"> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </in> <out> <class name=\"samples.mediators.SimpleClassMediator\"> <property name=\"variable1\" value=\"10\"/> <property name=\"variable2\" value=\"5\"/> </class> <send/> </out> </sequence> </definitions> See the following sample Class Mediator and note the SynapseMessageContext and the full Synapse API in there. package samples.mediators; import org.apache.synapse.MessageContext; import org.apache.synapse.mediators.AbstractMediator; import org.apache.axiom.om.OMElement; import org.apache.axiom.om.OMAbstractFactory; import org.apache.axiom.om.OMFactory; import org.apache.axiom.soap.SOAPFactory; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import javax.xml.namespace.QName; public class SimpleClassMediator extends AbstractMediator { private static final Log log = LogFactory.getLog(SimpleClassMediator.class); private String variable1=\"10\"; private String variable2=\"10\"; private int variable3=0; public SimpleClassMediator(){} public boolean mediate(MessageContext mc) { // Do somthing useful.. // Note the access to the Synapse Message context return true; } public String getType() { return null; } public void setTraceState(int traceState) { traceState = 0; } public int getTraceState() { return 0; } public void setVariable1(String newValue) { variable1=newValue; } public String getVariable1() { return variable1; } public void setVariable2(String newValue){ variable2=newValue; } public String getVariable2(){ return variable2; } } Samples \u00b6 For more examples, see: Sample 380: Writing your own Custom Mediation in Java Sample 381: Class Mediator to CBR Binary Messages","title":"Class Mediator"},{"location":"references/class-Mediator/#class-mediator","text":"The Class Mediator creates an instance of a custom-specified class and sets it as a mediator. The class must implement the org.apache.synapse.api.Mediator interface. If any properties are specified, the corresponding setter methods are invoked once on the class during initialization. The Class mediator is a custom Java class, which you need to maintain by yourself. Therefore, it is recommended to use the Class mediator only for not frequently re-used custom developments and very user-specific scenarios, for which, there is no built-in mediator that already provides the required functionality. Your class mediator might not be picked up and updated if you use an existing package when creating it. For best results, use WSO2 EI Tooling for debugging Class mediators. Syntax | Configuration | Examples","title":"Class Mediator"},{"location":"references/class-Mediator/#syntax","text":"<class name=\"class-name\"> <property name=\"string\" (value=\"literal\" | expression=\"[XPath|json-eval(JSON Path)]\")/>* </class>","title":"Syntax"},{"location":"references/class-Mediator/#configuration","text":"Class Name : The name of the class. To load a class, enter the qualified name of the relevant class in this parameter and click Load Class .","title":"Configuration"},{"location":"references/class-Mediator/#examples","text":"In this configuration, the ESB profile sends the requested message to the endpoint specified via the Send mediator . This endpoint is the Axis2server running on port 9000. The response message is passed through a Class mediator before it is sent back to the client. Two parameters named variable1 and variable2 are passed to the instance mediator implementation class ( SimpleClassMediator ). Info If you want, you can pass the same variables as a value or an expression: Example for passing the variable as a value: <property name=\"variable1\" value=\"10\"/> Example for passing the variable as an expression: <property name=\"variable2\" expression=\"get-property('variable1')\"/> For more information on using the get property method, see the Property Mediator . <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"fault\"> <makefault> <code value=\"tns:Receiver\" xmlns:tns=\"http://www.w3.org/2003/05/soap-envelope\"/> <reason value=\"Mediation failed.\"/> </makefault> <send/> </sequence> <sequence name=\"main\" onError=\"fault\"> <in> <send> <endpoint name=\"stockquote\"> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </in> <out> <class name=\"samples.mediators.SimpleClassMediator\"> <property name=\"variable1\" value=\"10\"/> <property name=\"variable2\" value=\"5\"/> </class> <send/> </out> </sequence> </definitions> See the following sample Class Mediator and note the SynapseMessageContext and the full Synapse API in there. package samples.mediators; import org.apache.synapse.MessageContext; import org.apache.synapse.mediators.AbstractMediator; import org.apache.axiom.om.OMElement; import org.apache.axiom.om.OMAbstractFactory; import org.apache.axiom.om.OMFactory; import org.apache.axiom.soap.SOAPFactory; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import javax.xml.namespace.QName; public class SimpleClassMediator extends AbstractMediator { private static final Log log = LogFactory.getLog(SimpleClassMediator.class); private String variable1=\"10\"; private String variable2=\"10\"; private int variable3=0; public SimpleClassMediator(){} public boolean mediate(MessageContext mc) { // Do somthing useful.. // Note the access to the Synapse Message context return true; } public String getType() { return null; } public void setTraceState(int traceState) { traceState = 0; } public int getTraceState() { return 0; } public void setVariable1(String newValue) { variable1=newValue; } public String getVariable1() { return variable1; } public void setVariable2(String newValue){ variable2=newValue; } public String getVariable2(){ return variable2; } }","title":"Examples"},{"location":"references/class-Mediator/#samples","text":"For more examples, see: Sample 380: Writing your own Custom Mediation in Java Sample 381: Class Mediator to CBR Binary Messages","title":"Samples"},{"location":"references/clone-Mediator/","text":"Clone Mediator \u00b6 The Clone Mediator can be used to clone a message into several messages. It resembles the Scatter-Gather enterprise integration pattern . The Clone mediator is similar to the Iterate mediator . The difference between the two mediators is, the Iterate mediator splits a message into different parts, whereas the Clone mediator makes multiple identical copies of the message. Info The Clone mediator is a content-aware mediator. Syntax | Configuration Syntax \u00b6 <clone [continueParent=(true | false)]> <target [to=\"uri\"] [soapAction=\"qname\"] [sequence=\"sequence_ref\"] [endpoint=\"endpoint_ref\"]> <sequence> (mediator)+ </sequence>? <endpoint> endpoint </endpoint>? </target>+ </clone> Configuration \u00b6 The parameters available to configure the Clone mediator is as follows. Parameter Name Description Clone ID Identification of messages created by the clone mediator. This is particularly useful when aggregating responses of messages that are created using nested Clone mediators. Sequential Mediation This parameter is used to specify whether the cloned messages should be processed sequentially or not. The processing is carried based on the information relating to the sequence and endpoint specified in the target configuration . The possible values are as follows. Yes : If this is selected, the cloned messages will be processed sequentially. Note that selecting True might cause delays due to high resource consumption. No : If this is selected, the cloned messages will not be processed sequentially. This is the default value and it results in better performance. Continue Parent This parameter is used to specify whether the original message should be preserved or not. Possible values are as follows. Yes : If this is selected, the original message will be preserved. No : If this is selected, the original message will be discarded. This is the default value. Number of Clones The parameter indicates the number of targets which currently exist for the Clone mediator. Click Add Clone Target to add a new target. Each time you add a target, it will be added as a child of the Clone mediator in the mediator tree as shown below. Click Target to add the target configuration as described below. Target configuration \u00b6 The following section is displayed in the mediator page when you click Target as mentioned above. The parameters available to configure the target are as follows. Parameter Name Description SOAP Action The SOAP action of the message. To Address The target endpoint address. Sequence This parameter is used to specify whether cloned messages should be mediated via a sequence or not, and to specify the sequence if they are to be further mediated. Possible options are as follows. None : If this is selected, no further mediation will be performed for the cloned messages. Anonymous : If this is selected, you can define an anonymous sequence for the cloned messages by adding the required mediators as children to Target in the mediator tree. Pick From Registry : If this is selected, you can refer to a pre-defined sequence that is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the resource tree. Endpoint The endpoint to which the cloned messages should be sent. Possible options are as follows. None : If this is selected, the cloned messages are not sent to an endpoint . Anonymous : If this is selected, you can define an anonymous endpoint within the iterate target configuration to which the cloned messages should be sent. Click the Add link which appears after selecting this option to add the anonymous endpoint . See Adding an Endpoint for further information. Pick from Registry : If this is selected, you can refer to a pre-defined endpoint that is currently saves as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. Example In this example, the Clone Mediator clones messages and redirects them to a default endpoint and an existing sequence. <clone xmlns=\"http://ws.apache.org/ns/synapse\"> <target> <endpoint name=\"endpoint_urn_uuid_73A47733EB1E6F30812921609540392-849227072\"> <default /> </endpoint> </target> <target sequence=\"test1\" /> </clone>","title":"Clone Mediator"},{"location":"references/clone-Mediator/#clone-mediator","text":"The Clone Mediator can be used to clone a message into several messages. It resembles the Scatter-Gather enterprise integration pattern . The Clone mediator is similar to the Iterate mediator . The difference between the two mediators is, the Iterate mediator splits a message into different parts, whereas the Clone mediator makes multiple identical copies of the message. Info The Clone mediator is a content-aware mediator. Syntax | Configuration","title":"Clone Mediator"},{"location":"references/clone-Mediator/#syntax","text":"<clone [continueParent=(true | false)]> <target [to=\"uri\"] [soapAction=\"qname\"] [sequence=\"sequence_ref\"] [endpoint=\"endpoint_ref\"]> <sequence> (mediator)+ </sequence>? <endpoint> endpoint </endpoint>? </target>+ </clone>","title":"Syntax"},{"location":"references/clone-Mediator/#configuration","text":"The parameters available to configure the Clone mediator is as follows. Parameter Name Description Clone ID Identification of messages created by the clone mediator. This is particularly useful when aggregating responses of messages that are created using nested Clone mediators. Sequential Mediation This parameter is used to specify whether the cloned messages should be processed sequentially or not. The processing is carried based on the information relating to the sequence and endpoint specified in the target configuration . The possible values are as follows. Yes : If this is selected, the cloned messages will be processed sequentially. Note that selecting True might cause delays due to high resource consumption. No : If this is selected, the cloned messages will not be processed sequentially. This is the default value and it results in better performance. Continue Parent This parameter is used to specify whether the original message should be preserved or not. Possible values are as follows. Yes : If this is selected, the original message will be preserved. No : If this is selected, the original message will be discarded. This is the default value. Number of Clones The parameter indicates the number of targets which currently exist for the Clone mediator. Click Add Clone Target to add a new target. Each time you add a target, it will be added as a child of the Clone mediator in the mediator tree as shown below. Click Target to add the target configuration as described below.","title":"Configuration"},{"location":"references/clone-Mediator/#target-configuration","text":"The following section is displayed in the mediator page when you click Target as mentioned above. The parameters available to configure the target are as follows. Parameter Name Description SOAP Action The SOAP action of the message. To Address The target endpoint address. Sequence This parameter is used to specify whether cloned messages should be mediated via a sequence or not, and to specify the sequence if they are to be further mediated. Possible options are as follows. None : If this is selected, no further mediation will be performed for the cloned messages. Anonymous : If this is selected, you can define an anonymous sequence for the cloned messages by adding the required mediators as children to Target in the mediator tree. Pick From Registry : If this is selected, you can refer to a pre-defined sequence that is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the resource tree. Endpoint The endpoint to which the cloned messages should be sent. Possible options are as follows. None : If this is selected, the cloned messages are not sent to an endpoint . Anonymous : If this is selected, you can define an anonymous endpoint within the iterate target configuration to which the cloned messages should be sent. Click the Add link which appears after selecting this option to add the anonymous endpoint . See Adding an Endpoint for further information. Pick from Registry : If this is selected, you can refer to a pre-defined endpoint that is currently saves as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. Example In this example, the Clone Mediator clones messages and redirects them to a default endpoint and an existing sequence. <clone xmlns=\"http://ws.apache.org/ns/synapse\"> <target> <endpoint name=\"endpoint_urn_uuid_73A47733EB1E6F30812921609540392-849227072\"> <default /> </endpoint> </target> <target sequence=\"test1\" /> </clone>","title":"Target configuration"},{"location":"references/conditional-Router-Mediator/","text":"Conditional Router Mediator \u00b6 Info Note Please note that the Conditional Router Mediator is deprecated and will be removed from the next release. The Conditional Router Mediator specifies how a message should be routed based on given conditions. The specified target sequence is applied if the condition of the mediator evaluates to true . Info The Conditional Router mediator is a content-aware mediator. Syntax | Configuration | Example Syntax \u00b6 <conditionalRouter continueAfter=\"(true|false)\"> <route breakRoute=\"(true|false)\"> <condition ../> <target ../> </route>+ </conditionalRouter> Configuration \u00b6 The parameters available to configure the Conditional Router mediator are as follows. Parameter Name Description Continue after Routing This parameter specifies whether the mediation flow should/should not continue after executing the conditional router mediator. Possible values are as follows. Yes/True : If this is selected, mediation continues to execute (any other mediators specified) after the conditiional router mediator. No/False : If this is selected, mediation discontinues after executing the conditiional router mediator . This is the default value. Add Route The conditional route will be added as a child to the Conditional Router mediator in the mediator tree as shown below. You can add multiple conditional routes to a Conditional Router mediator by clicking on this link. Click on the conditional route in the mediator tree to configure it. The parameters available to configure a conditional route are as follows. Parameter Name Description Break after route You can specify this for each conditional route of the conditional route mediator. It specifies whether the router should/should not continue after executing the specified conditional route. Yes/True : If this is selected, a matching route would break the router, so that it does not continue to execute the next conditional route. No/False : If this is selected, the router continues to execute the next conditional route defined in the conditional router mediator. Evaluator Expression The expression to evaluate the condition based on which the target mediation sequence should be applied. Target Sequence The mediation sequence to be applied if the expression entered in the Evaluator Expression parameter evaluates to true . Example \u00b6 See Sample 157: Conditional Router for Routing Messages based on HTTP URL for an example of the Conditional Router mediator.","title":"Conditional Router Mediator"},{"location":"references/conditional-Router-Mediator/#conditional-router-mediator","text":"Info Note Please note that the Conditional Router Mediator is deprecated and will be removed from the next release. The Conditional Router Mediator specifies how a message should be routed based on given conditions. The specified target sequence is applied if the condition of the mediator evaluates to true . Info The Conditional Router mediator is a content-aware mediator. Syntax | Configuration | Example","title":"Conditional Router Mediator"},{"location":"references/conditional-Router-Mediator/#syntax","text":"<conditionalRouter continueAfter=\"(true|false)\"> <route breakRoute=\"(true|false)\"> <condition ../> <target ../> </route>+ </conditionalRouter>","title":"Syntax"},{"location":"references/conditional-Router-Mediator/#configuration","text":"The parameters available to configure the Conditional Router mediator are as follows. Parameter Name Description Continue after Routing This parameter specifies whether the mediation flow should/should not continue after executing the conditional router mediator. Possible values are as follows. Yes/True : If this is selected, mediation continues to execute (any other mediators specified) after the conditiional router mediator. No/False : If this is selected, mediation discontinues after executing the conditiional router mediator . This is the default value. Add Route The conditional route will be added as a child to the Conditional Router mediator in the mediator tree as shown below. You can add multiple conditional routes to a Conditional Router mediator by clicking on this link. Click on the conditional route in the mediator tree to configure it. The parameters available to configure a conditional route are as follows. Parameter Name Description Break after route You can specify this for each conditional route of the conditional route mediator. It specifies whether the router should/should not continue after executing the specified conditional route. Yes/True : If this is selected, a matching route would break the router, so that it does not continue to execute the next conditional route. No/False : If this is selected, the router continues to execute the next conditional route defined in the conditional router mediator. Evaluator Expression The expression to evaluate the condition based on which the target mediation sequence should be applied. Target Sequence The mediation sequence to be applied if the expression entered in the Evaluator Expression parameter evaluates to true .","title":"Configuration"},{"location":"references/conditional-Router-Mediator/#example","text":"See Sample 157: Conditional Router for Routing Messages based on HTTP URL for an example of the Conditional Router mediator.","title":"Example"},{"location":"references/creating-Custom-Mediators/","text":"Creating Custom Mediators \u00b6 The ESB profile of WSO2 EI comes with an assortment of mediators to filter, transform, route and manipulate messages. Mediators provide an easy way of extending the ESB. When you have a scenario that requires functionality not provided by the existing mediators, you can write your own custom mediators to implement your specific business requirements. Your custom mediators then must be plugged into the ESB profile. After adding them to the ESB profile, they function together with core mediators that come with the product. The custom mediators can be distributed in a packaged form that can be installed in another ESB profile instance. Writing an ESB Mediator Building the mediator Deploying the custom mediator Writing an ESB Mediator \u00b6 There are two ways of writing an ESB mediator: Using the Class Mediator - This does not allow mediator specific XML configurations. See Writing Custom Mediator Implementations for more information. Writing the mediator with factory and serialize methods - This allows mediator to have its own XML configuration. See Writing Custom Configuration Implementations for Mediators for more information. The easiest way to write a mediator is to extend your mediator class from the org.apache.synapse.mediators.AbstractMediator class. For example, you can see the following articles in the WSO2 library: Writing a Mediator in WSO2 EI - Part 1 Writing a Mediator in WSO2 EI - Part 2 Tip You can use the Class mediator and custom mediators for user-specific custom developments when there is no built-in mediator that already provides the required functionality. However, class and custom mediators incur a high maintenance overhead. Custom mediators in particular might introduce version migration complications when upgrading WSO2 EI. Therefore, avoid using them unless the scenario is frequently re-used and heavily user-specific. For best results, use WSO2 EI Tooling to debug Class and custom mediators. Building the mediator \u00b6 After you write the mediator, you must build it and make it an OSGI bundle so that it will work with the ESB. Basic approach \u00b6 Create a regular JAR that links to the Synpase core JAR and place it in the <EI_HOME>/ lib directory. The platform will automatically make it an OSGI bundle and deploy it to the server. Advanced approach \u00b6 If you want to control the way your mediator is created as an OSGI bundle, you must write the POM files so that you can export and import the packages you need, as shown in the examples below. Following is a POM file that creates the mediator using Class Mediator . <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>org.test</groupId> <artifactId>org.test</artifactId> <version>1.0.0</version> <packaging>bundle</packaging> <name>My Samples - Test mediator</name> <url>http://www.test.com</url> <repositories> <repository> <id>wso2-maven2-repository</id> <url>http://dist.wso2.org/maven2</url> </repository> <repository> <id>apache-Incubating-repo</id> <name>Maven Incubating Repository</name> <url>http://people.apache.org/repo/m2-incubating-repository</url> </repository> <repository> <id>apache-maven2-repo</id> <name>Apache Maven2 Repository</name> <url>http://repo1.maven.org/maven2/</url> </repository> </repositories> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>2.0</version> <configuration> <source>1.5</source> <target>1.5</target> </configuration> </plugin> <plugin> <groupId>org.apache.felix</groupId> <artifactId>maven-bundle-plugin</artifactId> <version>1.4.0</version> <extensions>true</extensions> <configuration> <instructions> <Bundle-SymbolicName>org.test</Bundle-SymbolicName> <Bundle-Name>org.test</Bundle-Name> <Export-Package> org.test.mediator.*, </Export-Package> <Import-Package> *; resolution:=optional </Import-Package> </instructions> </configuration> </plugin> </plugins> </build> <dependencies> <dependency> <groupId>org.apache.synapse</groupId> <artifactId>synapse-core</artifactId> <version>2.1.1-wso2v5</version> </dependency> </dependencies> </project> Info The Maven bundle plug-in was used for creating the OSGI bundle here. Make sure you export the correct package that contains the mediator code. Otherwise, your mediator will not work. If you are adding third-party libraries to the class mediator, be sure to use the maven-shade-plugin to add the dependencies. See the sample shade plugin given below. {.expand-control-image} Example maven-shade-plugin <plugin> <artifactId>maven-shade-plugin</artifactId> <version></version> <executions> <execution> <phase>package</phase> <goals> <goal>shade</goal> </goals> <configuration> <artifactSet> <includes> <include></include> <include></include> </includes> </artifactSet> </configuration> </execution> </executions> <configuration> <finalName></finalName> </configuration> </plugin> Following is a POM file that creates the mediator with its own XML configuration using the Serialize and Factory classes. <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>org.test</groupId> <artifactId>org.test</artifactId> <version>1.0.0</version> <packaging>bundle</packaging> <name>My Samples - Test mediator</name> <url>http://www.test.com</url> <repositories> <repository> <id>wso2-maven2-repository</id> <url>http://dist.wso2.org/maven2</url> </repository> <repository> <id>apache-Incubating-repo</id> <name>Maven Incubating Repository</name> <url>http://people.apache.org/repo/m2-incubating-repository</url> </repository> <repository> <id>apache-maven2-repo</id> <name>Apache Maven2 Repository</name> <url>http://repo1.maven.org/maven2/</url> </repository> </repositories> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>2.0</version> <configuration> <source>1.5</source> <target>1.5</target> </configuration> </plugin> <plugin> <groupId>org.apache.felix</groupId> <artifactId>maven-bundle-plugin</artifactId> <version>1.4.0</version> <extensions>true</extensions> <configuration> <instructions> <Bundle-SymbolicName>org.test</Bundle-SymbolicName> <Bundle-Name>org.test</Bundle-Name> <Export-Package> org.test.mediator.*, </Export-Package> <Import-Package> *; resolution:=optional </Import-Package> <Fragment-Host>synapse-core</Fragment-Host> </instructions> </configuration> </plugin> </plugins> </build> <dependencies> <dependency> <groupId>org.apache.synapse</groupId> <artifactId>synapse-core</artifactId> <version>2.1.1-wso2v5</version> </dependency> </dependencies> </project> In this case, it is necessary to make the mediator an OSGI fragment of the synapse-core bundler. To achieve this, use the <Fragment-Host>synapse-core</Fragment-Host> . Create <PROJECT_HOME>/main/resources/META-INF.services/org.apache.synapse.config.xml.MediatorFactory and <PROJECT_HOME>/main/resources/META-INF.services/org.apache.synapse.config.xml.MediatorSerializer files with the following content as shown below, to add service provider definitions to your Maven project. Content of the org.apache.synapse.config.xml.MediatorFactory file org.wso2.carbon.mediator.cache.config.xml.CacheMediatorFactory Content of the org.apache.synapse.config.xml.MediatorSerializer file org.wso2.carbon.mediator.cache.config.xml.CacheMediatorSerializer Deploying the custom mediator \u00b6 For the above example, after you create the mediator, place the JAR file in the <EI_HOME>\\dropins directory. However, there are three places inside the WSO2 EI distribution for placing the JAR file of a customer mediator. They are: <EI_HOME>\\extensions <EI_HOME>\\dropins <EI_HOME>\\lib Tip The recommended way is to copy the JAR files into the extensions directory. If you deploy a mediator through a Carbon Application (CAR file), it can only be accessed from the Sequences and Proxy Services within the same CAR file. You need to restart the server after copying the JAR file of the custom mediator into any of these directories. Extensions directory \u00b6 If you created a regular non-OSGI mediator, build it and copy the JAR file into this directory. The system will convert the JAR file of the mediator into an OSGI-JAR and deploy it into the server. This way is easy and simple. However, in this method, you cannot use any OSGI features within the mediator implementation. For example, you cannot make certain packages private or import specific versions of packages. Also, in this method, other than Class mediators, you can use a mediator, which has its own XML configuration. Dropins directory \u00b6 If your custom mediator is a Class mediator, it would be a normal bundle. Instead, if it is a mediator with an XML configuration, then it should be a fragment of the Synapse core, and thereby, you need to create an OSGI bundle for that. This requires basic knowledge about OSGI and the Maven bundle plug-in. If you created your custom mediator as an OSGI bundle, you can place its JAR file in the dropins directory. The benefit of creating the custom mediator as an OSGI bundle is that you can use the OSGI features in its implementation as preferred. Lib directory \u00b6 You can only copy JAR files of custom mediators, which are created using Class mediators into this directory. If you copy a regular JAR file into this directory, the system automatically converts it to an OSGI bundle.","title":"Creating Custom Mediators"},{"location":"references/creating-Custom-Mediators/#creating-custom-mediators","text":"The ESB profile of WSO2 EI comes with an assortment of mediators to filter, transform, route and manipulate messages. Mediators provide an easy way of extending the ESB. When you have a scenario that requires functionality not provided by the existing mediators, you can write your own custom mediators to implement your specific business requirements. Your custom mediators then must be plugged into the ESB profile. After adding them to the ESB profile, they function together with core mediators that come with the product. The custom mediators can be distributed in a packaged form that can be installed in another ESB profile instance. Writing an ESB Mediator Building the mediator Deploying the custom mediator","title":"Creating Custom Mediators"},{"location":"references/creating-Custom-Mediators/#writing-an-esb-mediator","text":"There are two ways of writing an ESB mediator: Using the Class Mediator - This does not allow mediator specific XML configurations. See Writing Custom Mediator Implementations for more information. Writing the mediator with factory and serialize methods - This allows mediator to have its own XML configuration. See Writing Custom Configuration Implementations for Mediators for more information. The easiest way to write a mediator is to extend your mediator class from the org.apache.synapse.mediators.AbstractMediator class. For example, you can see the following articles in the WSO2 library: Writing a Mediator in WSO2 EI - Part 1 Writing a Mediator in WSO2 EI - Part 2 Tip You can use the Class mediator and custom mediators for user-specific custom developments when there is no built-in mediator that already provides the required functionality. However, class and custom mediators incur a high maintenance overhead. Custom mediators in particular might introduce version migration complications when upgrading WSO2 EI. Therefore, avoid using them unless the scenario is frequently re-used and heavily user-specific. For best results, use WSO2 EI Tooling to debug Class and custom mediators.","title":"Writing an ESB Mediator"},{"location":"references/creating-Custom-Mediators/#building-the-mediator","text":"After you write the mediator, you must build it and make it an OSGI bundle so that it will work with the ESB.","title":"Building the mediator"},{"location":"references/creating-Custom-Mediators/#basic-approach","text":"Create a regular JAR that links to the Synpase core JAR and place it in the <EI_HOME>/ lib directory. The platform will automatically make it an OSGI bundle and deploy it to the server.","title":"Basic approach"},{"location":"references/creating-Custom-Mediators/#advanced-approach","text":"If you want to control the way your mediator is created as an OSGI bundle, you must write the POM files so that you can export and import the packages you need, as shown in the examples below. Following is a POM file that creates the mediator using Class Mediator . <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>org.test</groupId> <artifactId>org.test</artifactId> <version>1.0.0</version> <packaging>bundle</packaging> <name>My Samples - Test mediator</name> <url>http://www.test.com</url> <repositories> <repository> <id>wso2-maven2-repository</id> <url>http://dist.wso2.org/maven2</url> </repository> <repository> <id>apache-Incubating-repo</id> <name>Maven Incubating Repository</name> <url>http://people.apache.org/repo/m2-incubating-repository</url> </repository> <repository> <id>apache-maven2-repo</id> <name>Apache Maven2 Repository</name> <url>http://repo1.maven.org/maven2/</url> </repository> </repositories> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>2.0</version> <configuration> <source>1.5</source> <target>1.5</target> </configuration> </plugin> <plugin> <groupId>org.apache.felix</groupId> <artifactId>maven-bundle-plugin</artifactId> <version>1.4.0</version> <extensions>true</extensions> <configuration> <instructions> <Bundle-SymbolicName>org.test</Bundle-SymbolicName> <Bundle-Name>org.test</Bundle-Name> <Export-Package> org.test.mediator.*, </Export-Package> <Import-Package> *; resolution:=optional </Import-Package> </instructions> </configuration> </plugin> </plugins> </build> <dependencies> <dependency> <groupId>org.apache.synapse</groupId> <artifactId>synapse-core</artifactId> <version>2.1.1-wso2v5</version> </dependency> </dependencies> </project> Info The Maven bundle plug-in was used for creating the OSGI bundle here. Make sure you export the correct package that contains the mediator code. Otherwise, your mediator will not work. If you are adding third-party libraries to the class mediator, be sure to use the maven-shade-plugin to add the dependencies. See the sample shade plugin given below. {.expand-control-image} Example maven-shade-plugin <plugin> <artifactId>maven-shade-plugin</artifactId> <version></version> <executions> <execution> <phase>package</phase> <goals> <goal>shade</goal> </goals> <configuration> <artifactSet> <includes> <include></include> <include></include> </includes> </artifactSet> </configuration> </execution> </executions> <configuration> <finalName></finalName> </configuration> </plugin> Following is a POM file that creates the mediator with its own XML configuration using the Serialize and Factory classes. <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>org.test</groupId> <artifactId>org.test</artifactId> <version>1.0.0</version> <packaging>bundle</packaging> <name>My Samples - Test mediator</name> <url>http://www.test.com</url> <repositories> <repository> <id>wso2-maven2-repository</id> <url>http://dist.wso2.org/maven2</url> </repository> <repository> <id>apache-Incubating-repo</id> <name>Maven Incubating Repository</name> <url>http://people.apache.org/repo/m2-incubating-repository</url> </repository> <repository> <id>apache-maven2-repo</id> <name>Apache Maven2 Repository</name> <url>http://repo1.maven.org/maven2/</url> </repository> </repositories> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>2.0</version> <configuration> <source>1.5</source> <target>1.5</target> </configuration> </plugin> <plugin> <groupId>org.apache.felix</groupId> <artifactId>maven-bundle-plugin</artifactId> <version>1.4.0</version> <extensions>true</extensions> <configuration> <instructions> <Bundle-SymbolicName>org.test</Bundle-SymbolicName> <Bundle-Name>org.test</Bundle-Name> <Export-Package> org.test.mediator.*, </Export-Package> <Import-Package> *; resolution:=optional </Import-Package> <Fragment-Host>synapse-core</Fragment-Host> </instructions> </configuration> </plugin> </plugins> </build> <dependencies> <dependency> <groupId>org.apache.synapse</groupId> <artifactId>synapse-core</artifactId> <version>2.1.1-wso2v5</version> </dependency> </dependencies> </project> In this case, it is necessary to make the mediator an OSGI fragment of the synapse-core bundler. To achieve this, use the <Fragment-Host>synapse-core</Fragment-Host> . Create <PROJECT_HOME>/main/resources/META-INF.services/org.apache.synapse.config.xml.MediatorFactory and <PROJECT_HOME>/main/resources/META-INF.services/org.apache.synapse.config.xml.MediatorSerializer files with the following content as shown below, to add service provider definitions to your Maven project. Content of the org.apache.synapse.config.xml.MediatorFactory file org.wso2.carbon.mediator.cache.config.xml.CacheMediatorFactory Content of the org.apache.synapse.config.xml.MediatorSerializer file org.wso2.carbon.mediator.cache.config.xml.CacheMediatorSerializer","title":"Advanced approach"},{"location":"references/creating-Custom-Mediators/#deploying-the-custom-mediator","text":"For the above example, after you create the mediator, place the JAR file in the <EI_HOME>\\dropins directory. However, there are three places inside the WSO2 EI distribution for placing the JAR file of a customer mediator. They are: <EI_HOME>\\extensions <EI_HOME>\\dropins <EI_HOME>\\lib Tip The recommended way is to copy the JAR files into the extensions directory. If you deploy a mediator through a Carbon Application (CAR file), it can only be accessed from the Sequences and Proxy Services within the same CAR file. You need to restart the server after copying the JAR file of the custom mediator into any of these directories.","title":"Deploying the custom mediator"},{"location":"references/creating-Custom-Mediators/#extensions-directory","text":"If you created a regular non-OSGI mediator, build it and copy the JAR file into this directory. The system will convert the JAR file of the mediator into an OSGI-JAR and deploy it into the server. This way is easy and simple. However, in this method, you cannot use any OSGI features within the mediator implementation. For example, you cannot make certain packages private or import specific versions of packages. Also, in this method, other than Class mediators, you can use a mediator, which has its own XML configuration.","title":"Extensions directory"},{"location":"references/creating-Custom-Mediators/#dropins-directory","text":"If your custom mediator is a Class mediator, it would be a normal bundle. Instead, if it is a mediator with an XML configuration, then it should be a fragment of the Synapse core, and thereby, you need to create an OSGI bundle for that. This requires basic knowledge about OSGI and the Maven bundle plug-in. If you created your custom mediator as an OSGI bundle, you can place its JAR file in the dropins directory. The benefit of creating the custom mediator as an OSGI bundle is that you can use the OSGI features in its implementation as preferred.","title":"Dropins directory"},{"location":"references/creating-Custom-Mediators/#lib-directory","text":"You can only copy JAR files of custom mediators, which are created using Class mediators into this directory. If you copy a regular JAR file into this directory, the system automatically converts it to an OSGI bundle.","title":"Lib directory"},{"location":"references/creating-a-JSON-Schema-Manually/","text":"Creating a JSON Schema Manually \u00b6 You need to have two input and output JSON Schema files for a Data Mapping configuration as shown in the example below. Therefore, you can use the WSO2 Data Mapper Diagram Editor to create a JSON Schema manually by adding elements to the Data Mapper tree view as explained below. Components of a JSON Schema Adding the root element Adding a child element Setting a nullable element Editing an element Deleting an element Components of a JSON Schema \u00b6 There are the following four types of components in a JSON Schema: Arrays Objects Fields Attributes Adding the root element \u00b6 When creating the tree you need to first add the root element. The root element can be either an object or an array. Right-click on the Input or Output box and then click Add new Root Element as shown below, to add the root element. Add the following details to create the root element. Name: name of the root element Schema Type: type of the element (i.e. array or object) ID: ID of the root element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema {width=\"600\"} You view the root element added to the Input box as shown below. Adding a child element \u00b6 You can add Arrays, Objects, Fields and Attributes as child elements as explained below. Adding an Object as a Child element Adding an Array as a child element Adding a Field as a child element Adding an Attribute as a child element Right-click on the parent element and click Add new Object , to add an Object as shown below. Add the following details to create an Object as a child element. Name: name of the child element Schema Type: type of the element (i.e. object) ID: ID of the child element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema !!! info If the object has element identifiers, then select the checkbox and add the value, type, and URL of the identifier. Object holds a value: if the object holds a value or not !!! info If the object holds a value, then select the checkbox and select the data type. You view the child Object element added to the root element as shown below. Info The element identifier is added as an attribute to the element. Right-click on the parent element and click Add new Array , to add an Array as shown below. You need to add the following details to create an Array as a child element. Name: name of the child element Schema Type: type of the element (i.e. array) ID: ID of the child element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema !!! info If the array has element identifiers, then select the checkbox and add the value, type, and URL of the identifier. Object holds a value: if the object holds a value or not !!! info If the array holds a value, then select the checkbox and select the data type. {width=\"600\"} You view the child Array element added to the root element as shown below. Info The element identifier is added as an attribute to the element. Right-click on the parent element and click Add new Field , to add a Field as shown below. You need to add the following details to create a Field as a child element. Name: name of the child element Schema Type: type of the element ID: ID of the child element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema {width=\"600\"} You view the child Field element added to the root element as shown below. Right-click on the parent element and click Add new Attribute , to add an attribute as shown below. You need to add the following details to create an Attribute as a child element. Name: name of the child element Schema Type: type of the element ID: ID of the child element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema {width=\"600\"} You view the child Attribute element added to the root element as shown below. Setting a nullable element \u00b6 You can set an element as a nullable element, so that it is not required to have that element in the payload. Right-click on the parent element and click Enable Nullable , to enable an element to make it nullable as shown below. Right-click on the parent element and click Disable Nullable , to make it required in the payload as shown below. Editing an element \u00b6 Right-click on any element and click Edit Object , to edit and update the field values as required as shown below. Deleting an element \u00b6 Right-click on any element and click Delete from Model , to delete it as shown below. Info This deletes the element with its child nodes.","title":"Creating a JSON Schema Manually"},{"location":"references/creating-a-JSON-Schema-Manually/#creating-a-json-schema-manually","text":"You need to have two input and output JSON Schema files for a Data Mapping configuration as shown in the example below. Therefore, you can use the WSO2 Data Mapper Diagram Editor to create a JSON Schema manually by adding elements to the Data Mapper tree view as explained below. Components of a JSON Schema Adding the root element Adding a child element Setting a nullable element Editing an element Deleting an element","title":"Creating a JSON Schema Manually"},{"location":"references/creating-a-JSON-Schema-Manually/#components-of-a-json-schema","text":"There are the following four types of components in a JSON Schema: Arrays Objects Fields Attributes","title":"Components of a JSON Schema"},{"location":"references/creating-a-JSON-Schema-Manually/#adding-the-root-element","text":"When creating the tree you need to first add the root element. The root element can be either an object or an array. Right-click on the Input or Output box and then click Add new Root Element as shown below, to add the root element. Add the following details to create the root element. Name: name of the root element Schema Type: type of the element (i.e. array or object) ID: ID of the root element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema {width=\"600\"} You view the root element added to the Input box as shown below.","title":"Adding the root element"},{"location":"references/creating-a-JSON-Schema-Manually/#adding-a-child-element","text":"You can add Arrays, Objects, Fields and Attributes as child elements as explained below. Adding an Object as a Child element Adding an Array as a child element Adding a Field as a child element Adding an Attribute as a child element Right-click on the parent element and click Add new Object , to add an Object as shown below. Add the following details to create an Object as a child element. Name: name of the child element Schema Type: type of the element (i.e. object) ID: ID of the child element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema !!! info If the object has element identifiers, then select the checkbox and add the value, type, and URL of the identifier. Object holds a value: if the object holds a value or not !!! info If the object holds a value, then select the checkbox and select the data type. You view the child Object element added to the root element as shown below. Info The element identifier is added as an attribute to the element. Right-click on the parent element and click Add new Array , to add an Array as shown below. You need to add the following details to create an Array as a child element. Name: name of the child element Schema Type: type of the element (i.e. array) ID: ID of the child element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema !!! info If the array has element identifiers, then select the checkbox and add the value, type, and URL of the identifier. Object holds a value: if the object holds a value or not !!! info If the array holds a value, then select the checkbox and select the data type. {width=\"600\"} You view the child Array element added to the root element as shown below. Info The element identifier is added as an attribute to the element. Right-click on the parent element and click Add new Field , to add a Field as shown below. You need to add the following details to create a Field as a child element. Name: name of the child element Schema Type: type of the element ID: ID of the child element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema {width=\"600\"} You view the child Field element added to the root element as shown below. Right-click on the parent element and click Add new Attribute , to add an attribute as shown below. You need to add the following details to create an Attribute as a child element. Name: name of the child element Schema Type: type of the element ID: ID of the child element to uniquely identify it isNullable: whether the element can be a nullable (i.e. not available in the payload (optional) Namespaces: prefix and URL of the namespace (optional) Required: child elements required to be there in the payload (optional) Schema Value: custom URI of the Schema {width=\"600\"} You view the child Attribute element added to the root element as shown below.","title":"Adding a child element"},{"location":"references/creating-a-JSON-Schema-Manually/#setting-a-nullable-element","text":"You can set an element as a nullable element, so that it is not required to have that element in the payload. Right-click on the parent element and click Enable Nullable , to enable an element to make it nullable as shown below. Right-click on the parent element and click Disable Nullable , to make it required in the payload as shown below.","title":"Setting a nullable element"},{"location":"references/creating-a-JSON-Schema-Manually/#editing-an-element","text":"Right-click on any element and click Edit Object , to edit and update the field values as required as shown below.","title":"Editing an element"},{"location":"references/creating-a-JSON-Schema-Manually/#deleting-an-element","text":"Right-click on any element and click Delete from Model , to delete it as shown below. Info This deletes the element with its child nodes.","title":"Deleting an element"},{"location":"references/dB-Report-Mediator/","text":"DB Report Mediator \u00b6 The DB Report Mediator is similar to the DBLookup Mediator . The difference between the two mediators is that the DB Report mediator writes information to a database using the specified insert SQL statement. Info The DB Report mediator is a content-aware mediator. Syntax | Configuration | Examples Syntax \u00b6 The syntax of the DB Report mediator changes depending on whether you connect to the database using a connection pool, or using a data source. Click on the relevant tab to view the required syntax. Connection Pool Data source <dbreport> <connection> <pool> ( <driver/> <url/> <user/> <password/> <dsName/> <icClass/> <url/> <user/> <password/> ) <property name=\"name\" value=\"value\"/>* </pool> </connection> <statement> <sql>insert into something values(?, ?, ?, ?)</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* </statement>+ </dbreport> The syntax of the DBLookup mediator further differs based on whether the connection to the database is made using an external datasource or a Carbon datasource. Click on the relevant tab to view the required syntax. External Datasource Carbon Datasource <dbreport> <connection> <pool> <dsName/> <icClass/> <url/> <user/> <password/> <property name=\"name\" value=\"value\"/>* </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* </statement>+ </dbreport> <dbreport> <connection> <pool> <dsName/> </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* </statement>+ </dbreport> Configuration \u00b6 The configuration of the DBQuery mediator changes depending on whether you connect to the database using a connection pool, or using a data source. Click on the relevant tab to view the required UI. Pool Data Source The following UI is displayed when you select the Pool option for the Connection Information parameter, indicating that you want the connection to be made via a connection pool. The parameters available to configure the DB Report mediator are as follows. Info When specifying the DB connection using a connection pool, other than specifying parameter values inline, you can also specify following parameter values of the connection information (i.e. Driver, URL, User and password) as registry entries. The advantage of specifying a parameter value as a registry entry is that the same connection information configurations can be used in different environments simply by changing the registry entry value. To do this, give the registry path within the key attribute as shown in the example below. <dblookup xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <password key=\"conf:/repository/esb/password\"/> <driver key=\"conf:/repository/esb/driver\"/> <url key=\"conf:/repository/esb/url\"/> <user key=\"conf:/repository/esb/username\"/> </pool> </connection> </dblookup> If you need to provide the registry entry value via the Management Console, specify the registry path using the ` $registry ` prefix. Parameter Name Description Use Transaction This parameter specifies whether the database operation should be performed within a transaction or not. Click Yes or No as relevant. !!! info To include multiple database reports within the same database transaction i nside a particular message flow, set the value of this Use Transaction property to Yes . However, when you have more reports it takes more time to complete a transaction and when multiple messages flow in, then multiple transactions can become active at the same time. By default, the maximum number of active transactions is 50 as imposed by the Atomikos JTA implementation . To override this, create a file named transaction.properties by including the following property and add it to the <EI_HOME>/lib directory: com.atomikos.icatch.max_actives=1000 Specifying the value as -1 allows unlimited transactions. Change the value accordingly to limit the number of active transactions based on your environment and the concurrency level of the service. !!! note If you click Yes to perform the database operation within a transaction, you need to ensure the following: The DBReport mediator configuration must be preceded by a Transaction Mediator configuration with new as the transaction action. The DBReport mediator configuration must be followed by a Transaction Mediator configuration with commit as the transaction action. For detailed information about configuring Transaction mediators, see Transaction Mediator . Driver The class name of the database driver. Url The JDBC URL of the database that data will be written to. Set the autoReconnect parameter to true to help reconnect to the database when the connection between the client and the database is dropped. For example, <url>jdbc:mysql://<ip>:<port>/test?autoReconnect=true</url> . User The user name used to connect to the database. Password The password used to connect to the database. Adding properties to the DB Report mediator \u00b6 If you click Add Property , the page will expand to display the following parameters. The parameters available to manage properties are as follows. Parameter Name Description Name The name of the property. Value The value of the property. Action This parameter enables a property to be deleted. The available properties are as follows. Name Value Description autocommit true / false The auto-commit state of the connections created by the pool. isolation Connection.TRANSACTION_NONE / Connection.TRANSACTION_READ_COMMITTED / Connection.TRANSACTION_READ_UNCOMMITTED / Connection.TRANSACTION_REPEATABLE_READ / Connection.TRANSACTION_SERIALIZABLE The isolation state of the connections created by the pool. initialsize int The initial number of connections created when the pool is started. maxactive int The maximum number of active connections that can be allocated from this pool at a given time. When this maximum limit is reached, no more active connections will be created by the connection pool. Specify 0 or a negative value if you do not want to set a limit. maxidle int The maximum number of idle connections to be allowed in the connection pool at a given time. Specify 0 or a negative value if you want the pool to wait indefinitely. maxopenstatements int The maximum number of open statements that can be allocated from the statement pool at a given time. When this maximum limit is reached, no more new statements will be created by the statement pool. Specify 0 or a negative value if you do not want to set a limit. maxwait long The maximum number of milliseconds that the connection pool will wait for a connection to return before throwing an exception when there are no connections available in the pool. Specify 0 or a negative value if you want the pool to wait indefinitely. minidle int The minimum number of idle connections to be allowed in the connection pool at a given time. Specify 0 or a negative value if you want the pool to wait indefinitely. poolstatements true/ false If the value is true , statement pooling is enabled for the pool. testonborrow true/ false If the value is true , objects are validated before they are borrowed from the pool. An object which fails the validation test will be dropped from the pool and another object in the pool will be picked instead. testwhileidle true/ false If the value is true , the objects in the pool will be validated using an idle object evictor (if any exists). Any object which fails this validation test would be dropped from the pool. validationquery String The SQL query that will be used to validate connections from this pool before returning them to the caller. This property helps to reconnect to the database when the database connection between the client and the database is dropped. For example, <property name=\"validationquery\" value=\"select 1\"/> . The UI configuration of the DBLookup mediator further differs based on whether the connection to the database is made using an external datasource or a Carbon datasource. Click on the relevant tab to view the required UI configuration. External Carbon Datasource The following UI is displayed if you select the External option for the Datasource Type parameter, indicating that you want the connection to the database to be made using an external datasource. The parameters available to configure the DB Report mediator are as follows. Parameter Name Description Use Transaction This parameter specifies whether the database operation should be performed within a transaction or not. Click Yes or No as relevant. Initial Context The initial context factory class. The corresponding Java environment property is java.naming.factory.initial . Datasource Name The naming service provider URL . The corresponding Java environment property is java.naming.provider.url . URL The JDBC URL of the database that data will be written to. User The user name used to connect to the database. Password The password used to connect to the database. Adding properties to the DB Report mediator \u00b6 If you click Add Property , the page will expand to display the following parameters. The parameters available to manage properties are as follows. Parameter Name Description Name The name of the property. Value The value of the property. Action This parameter enables a property to be deleted. The available properties are as follows. Name Value Description autocommit true / false The auto-commit state of the connections created by the pool. isolation Connection.TRANSACTION_NONE / Connection.TRANSACTION_READ_COMMITTED / Connection.TRANSACTION_READ_UNCOMMITTED / Connection.TRANSACTION_REPEATABLE_READ / Connection.TRANSACTION_SERIALIZABLE The isolation state of the connections created by the pool. initialsize int The initial number of connections created when the pool is started. maxactive int The maximum number of active connections that can be allocated from this pool at a given time. When this maximum limit is reached, no more active connections will be created by the connection pool. Specify 0 or a negative value if you do not want to set a limit. maxidle int The maximum number of idle connections to be allowed in the connection pool at a given time. Specify 0 or a negative value if you want the pool to wait indefinitely. maxopenstatements int The maximum number of open statements that can be allocated from the statement pool at a given time. When this maximum limit is reached, no more new statements will be created by the statement pool. Specify 0 or a negative value if you do not want to set a limit. maxwait long The maximum number of milliseconds that the connection pool will wait for a connection to return before throwing an exception when there are no connections available in the pool. Specify 0 or a negative value if you want the pool to wait indefinitely. minidle int The minimum number of idle connections to be allowed in the connection pool at a given time. Specify 0 or a negative value if you want the pool to wait indefinitely. poolstatements true/ false If the value is true , statement pooling is enabled for the pool. testonborrow true/ false If the value is true , objects are validated before they are borrowed from the pool. An object which fails the validation test will be dropped from the pool and another object in the pool will be picked instead. testwhileidle true/ false If the value is true , the objects in the pool will be validated using an idle object evictor (if any exists). Any object which fails this validation test would be dropped from the pool. validationquery String The SQL query that will be used to validate connections from this pool before returning them to the caller. The following UI is displayed if you select the Carbon Datasource option for the Datasource Type parameter, indicating that you want the connection to the database to be made using an Carbon datasource. Parameter Name Description Use Transaction This parameter specifies whether the database operation should be performed within a transaction or not. Click Yes or No as relevant. Datasource This parameter is used to selected a specific Carbon datasource you want to use to make the connection. All the Carbon datasources which are currently available are included in the list. Adding SQL statements to the DB Report Mediator If you click Add Statement , the page will be expanded to display the following parameters. Parameter Name Description SQL This parameter is used to enter one or more SQL statements. Parameters This section is used to specify how the values of parameters in the SQL will be determined. A parameter value can be static or calculated at runtime based on a given expression. Parameter Type The data type of the parameter. Possible values are as follows. CHAR VARCHAR LONGVARCHAR NUMERIC DECIMAL BIT TINYINT SAMLLINT INTEGER BIGINT REAL DOUBLE DATE TIME TIMESTAMP Property Type This determines whether the parameter value should be a static value or calculated at run time via an expression. Value : If this is selected, a static value would be considered as the property value and this value should be entered in the Value/Expression parameter. Expression: If this is selected, the property value will be determined during mediation by evaluating an expression. This expression should be entered in the Value/Expression parameter. Value/Expression This parameter is used to enter the static value or the XPath expression used to determine the property value based on the option you selected for the Property Type parameter. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This allows you to delete a parameter. Examples \u00b6 Simple database write operation \u00b6 This example demonstrates simple database write operations. The DB Report mediator writes to a table using the details of the message. It updates the stock price of the company using the last quote value, which is calculated by evaluating an XPath expression against the response message. <dbreport xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <driver>org.apache.derby.jdbc.ClientDriver</driver> <url>jdbc:derby://localhost:1527/esbdb;create=false</url> <user>esb</user> <password>esb</password> </pool> </connection> <statement> <sql><![CDATA[update company set price=? where name =?]]></sql> <parameter expression=\"//m0:return/m1:last/child::text()\" type=\"DOUBLE\" xmlns:m0=\"http://services.samples\" xmlns:m1=\"http://services.samples/xsd\"/> <parameter expression=\"//m0:return/m1:symbol/child::text()\" type=\"VARCHAR\" xmlns:m0=\"http://services.samples\" xmlns:m1=\"http://services.samples/xsd\"/> </statement> </dbreport> Performing a database write operation within a transaction \u00b6 In this example, <transaction action=\"new\"/> is a Transaction Mediator configuration that starts a new transaction. The DBReport mediator configuration performs a few write operations incuding deleting records when the name matches a specific value derived via an expression as well as a few insertions. Once the database operations are complete, they are committed via <transaction action=\"commit\"/> , which is another Transaction Mediator configuration. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"myFaultHandler\"> <log level=\"custom\"> <property name=\"text\" value=\"** Rollback Transaction**\"/> </log> <transaction action=\"rollback\"/> <send/> </sequence> <sequence name=\"main\" onError=\"myFaultHandler\"> <in> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </in> <out> <transaction action=\"new\"/> <log level=\"custom\"> <property name=\"text\" value=\"** Reporting to the Database EIdb**\"/> </log> <dbreport useTransaction=\"true\" xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <dsName>java:jdbc/XADerbyDS</dsName> <icClass>org.jnp.interfaces.NamingContextFactory</icClass> <url>localhost:1099</url> <user>EI</user> <password>EI</password> </pool> </connection> <statement> <sql>delete from company where name =?</sql> <parameter expression=\"//m0:return/m1:symbol/child::text()\" xmlns:m0=\"http://services.samples\" xmlns:m1=\"http://services.samples/xsd\" type=\"VARCHAR\"/> </statement> </dbreport> <log level=\"custom\"> <property name=\"text\" value=\"** Reporting to the Database EIdb1**\"/> </log> <dbreport useTransaction=\"true\" xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <dsName>java:jdbc/XADerbyDS1</dsName> <icClass>org.jnp.interfaces.NamingContextFactory</icClass> <url>localhost:1099</url> <user>EI</user> <password>EI</password> </pool> </connection> <statement> <sql>INSERT into company values (?,'c4',?)</sql> <parameter expression=\"//m0:return/m1:symbol/child::text()\" xmlns:m1=\"http://services.samples/xsd\" xmlns:m0=\"http://services.samples\" type=\"VARCHAR\"/> <parameter expression=\"//m0:return/m1:last/child::text()\" xmlns:m1=\"http://services.samples/xsd\" xmlns:m0=\"http://services.samples\" type=\"DOUBLE\"/> </statement> </dbreport> <transaction action=\"commit\"/> <send/> </out> </sequence> </definitions> Samples \u00b6 For more examples of the DB Report mediator, see: Sample 361: Introduction to DB Report Mediator Sample 362: DB Report and DBLookup Mediators Together Sample 363: Reusable Database Connection Pools Sample 271: File Processing (moves files into a database using the VFS transport and the DB Report mediator)","title":"DB Report Mediator"},{"location":"references/dB-Report-Mediator/#db-report-mediator","text":"The DB Report Mediator is similar to the DBLookup Mediator . The difference between the two mediators is that the DB Report mediator writes information to a database using the specified insert SQL statement. Info The DB Report mediator is a content-aware mediator. Syntax | Configuration | Examples","title":"DB Report Mediator"},{"location":"references/dB-Report-Mediator/#syntax","text":"The syntax of the DB Report mediator changes depending on whether you connect to the database using a connection pool, or using a data source. Click on the relevant tab to view the required syntax. Connection Pool Data source <dbreport> <connection> <pool> ( <driver/> <url/> <user/> <password/> <dsName/> <icClass/> <url/> <user/> <password/> ) <property name=\"name\" value=\"value\"/>* </pool> </connection> <statement> <sql>insert into something values(?, ?, ?, ?)</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* </statement>+ </dbreport> The syntax of the DBLookup mediator further differs based on whether the connection to the database is made using an external datasource or a Carbon datasource. Click on the relevant tab to view the required syntax. External Datasource Carbon Datasource <dbreport> <connection> <pool> <dsName/> <icClass/> <url/> <user/> <password/> <property name=\"name\" value=\"value\"/>* </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* </statement>+ </dbreport> <dbreport> <connection> <pool> <dsName/> </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* </statement>+ </dbreport>","title":"Syntax"},{"location":"references/dB-Report-Mediator/#configuration","text":"The configuration of the DBQuery mediator changes depending on whether you connect to the database using a connection pool, or using a data source. Click on the relevant tab to view the required UI. Pool Data Source The following UI is displayed when you select the Pool option for the Connection Information parameter, indicating that you want the connection to be made via a connection pool. The parameters available to configure the DB Report mediator are as follows. Info When specifying the DB connection using a connection pool, other than specifying parameter values inline, you can also specify following parameter values of the connection information (i.e. Driver, URL, User and password) as registry entries. The advantage of specifying a parameter value as a registry entry is that the same connection information configurations can be used in different environments simply by changing the registry entry value. To do this, give the registry path within the key attribute as shown in the example below. <dblookup xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <password key=\"conf:/repository/esb/password\"/> <driver key=\"conf:/repository/esb/driver\"/> <url key=\"conf:/repository/esb/url\"/> <user key=\"conf:/repository/esb/username\"/> </pool> </connection> </dblookup> If you need to provide the registry entry value via the Management Console, specify the registry path using the ` $registry ` prefix. Parameter Name Description Use Transaction This parameter specifies whether the database operation should be performed within a transaction or not. Click Yes or No as relevant. !!! info To include multiple database reports within the same database transaction i nside a particular message flow, set the value of this Use Transaction property to Yes . However, when you have more reports it takes more time to complete a transaction and when multiple messages flow in, then multiple transactions can become active at the same time. By default, the maximum number of active transactions is 50 as imposed by the Atomikos JTA implementation . To override this, create a file named transaction.properties by including the following property and add it to the <EI_HOME>/lib directory: com.atomikos.icatch.max_actives=1000 Specifying the value as -1 allows unlimited transactions. Change the value accordingly to limit the number of active transactions based on your environment and the concurrency level of the service. !!! note If you click Yes to perform the database operation within a transaction, you need to ensure the following: The DBReport mediator configuration must be preceded by a Transaction Mediator configuration with new as the transaction action. The DBReport mediator configuration must be followed by a Transaction Mediator configuration with commit as the transaction action. For detailed information about configuring Transaction mediators, see Transaction Mediator . Driver The class name of the database driver. Url The JDBC URL of the database that data will be written to. Set the autoReconnect parameter to true to help reconnect to the database when the connection between the client and the database is dropped. For example, <url>jdbc:mysql://<ip>:<port>/test?autoReconnect=true</url> . User The user name used to connect to the database. Password The password used to connect to the database.","title":"Configuration"},{"location":"references/dB-Report-Mediator/#adding-properties-to-the-db-report-mediator","text":"If you click Add Property , the page will expand to display the following parameters. The parameters available to manage properties are as follows. Parameter Name Description Name The name of the property. Value The value of the property. Action This parameter enables a property to be deleted. The available properties are as follows. Name Value Description autocommit true / false The auto-commit state of the connections created by the pool. isolation Connection.TRANSACTION_NONE / Connection.TRANSACTION_READ_COMMITTED / Connection.TRANSACTION_READ_UNCOMMITTED / Connection.TRANSACTION_REPEATABLE_READ / Connection.TRANSACTION_SERIALIZABLE The isolation state of the connections created by the pool. initialsize int The initial number of connections created when the pool is started. maxactive int The maximum number of active connections that can be allocated from this pool at a given time. When this maximum limit is reached, no more active connections will be created by the connection pool. Specify 0 or a negative value if you do not want to set a limit. maxidle int The maximum number of idle connections to be allowed in the connection pool at a given time. Specify 0 or a negative value if you want the pool to wait indefinitely. maxopenstatements int The maximum number of open statements that can be allocated from the statement pool at a given time. When this maximum limit is reached, no more new statements will be created by the statement pool. Specify 0 or a negative value if you do not want to set a limit. maxwait long The maximum number of milliseconds that the connection pool will wait for a connection to return before throwing an exception when there are no connections available in the pool. Specify 0 or a negative value if you want the pool to wait indefinitely. minidle int The minimum number of idle connections to be allowed in the connection pool at a given time. Specify 0 or a negative value if you want the pool to wait indefinitely. poolstatements true/ false If the value is true , statement pooling is enabled for the pool. testonborrow true/ false If the value is true , objects are validated before they are borrowed from the pool. An object which fails the validation test will be dropped from the pool and another object in the pool will be picked instead. testwhileidle true/ false If the value is true , the objects in the pool will be validated using an idle object evictor (if any exists). Any object which fails this validation test would be dropped from the pool. validationquery String The SQL query that will be used to validate connections from this pool before returning them to the caller. This property helps to reconnect to the database when the database connection between the client and the database is dropped. For example, <property name=\"validationquery\" value=\"select 1\"/> . The UI configuration of the DBLookup mediator further differs based on whether the connection to the database is made using an external datasource or a Carbon datasource. Click on the relevant tab to view the required UI configuration. External Carbon Datasource The following UI is displayed if you select the External option for the Datasource Type parameter, indicating that you want the connection to the database to be made using an external datasource. The parameters available to configure the DB Report mediator are as follows. Parameter Name Description Use Transaction This parameter specifies whether the database operation should be performed within a transaction or not. Click Yes or No as relevant. Initial Context The initial context factory class. The corresponding Java environment property is java.naming.factory.initial . Datasource Name The naming service provider URL . The corresponding Java environment property is java.naming.provider.url . URL The JDBC URL of the database that data will be written to. User The user name used to connect to the database. Password The password used to connect to the database.","title":"Adding properties to the DB Report mediator"},{"location":"references/dB-Report-Mediator/#adding-properties-to-the-db-report-mediator_1","text":"If you click Add Property , the page will expand to display the following parameters. The parameters available to manage properties are as follows. Parameter Name Description Name The name of the property. Value The value of the property. Action This parameter enables a property to be deleted. The available properties are as follows. Name Value Description autocommit true / false The auto-commit state of the connections created by the pool. isolation Connection.TRANSACTION_NONE / Connection.TRANSACTION_READ_COMMITTED / Connection.TRANSACTION_READ_UNCOMMITTED / Connection.TRANSACTION_REPEATABLE_READ / Connection.TRANSACTION_SERIALIZABLE The isolation state of the connections created by the pool. initialsize int The initial number of connections created when the pool is started. maxactive int The maximum number of active connections that can be allocated from this pool at a given time. When this maximum limit is reached, no more active connections will be created by the connection pool. Specify 0 or a negative value if you do not want to set a limit. maxidle int The maximum number of idle connections to be allowed in the connection pool at a given time. Specify 0 or a negative value if you want the pool to wait indefinitely. maxopenstatements int The maximum number of open statements that can be allocated from the statement pool at a given time. When this maximum limit is reached, no more new statements will be created by the statement pool. Specify 0 or a negative value if you do not want to set a limit. maxwait long The maximum number of milliseconds that the connection pool will wait for a connection to return before throwing an exception when there are no connections available in the pool. Specify 0 or a negative value if you want the pool to wait indefinitely. minidle int The minimum number of idle connections to be allowed in the connection pool at a given time. Specify 0 or a negative value if you want the pool to wait indefinitely. poolstatements true/ false If the value is true , statement pooling is enabled for the pool. testonborrow true/ false If the value is true , objects are validated before they are borrowed from the pool. An object which fails the validation test will be dropped from the pool and another object in the pool will be picked instead. testwhileidle true/ false If the value is true , the objects in the pool will be validated using an idle object evictor (if any exists). Any object which fails this validation test would be dropped from the pool. validationquery String The SQL query that will be used to validate connections from this pool before returning them to the caller. The following UI is displayed if you select the Carbon Datasource option for the Datasource Type parameter, indicating that you want the connection to the database to be made using an Carbon datasource. Parameter Name Description Use Transaction This parameter specifies whether the database operation should be performed within a transaction or not. Click Yes or No as relevant. Datasource This parameter is used to selected a specific Carbon datasource you want to use to make the connection. All the Carbon datasources which are currently available are included in the list. Adding SQL statements to the DB Report Mediator If you click Add Statement , the page will be expanded to display the following parameters. Parameter Name Description SQL This parameter is used to enter one or more SQL statements. Parameters This section is used to specify how the values of parameters in the SQL will be determined. A parameter value can be static or calculated at runtime based on a given expression. Parameter Type The data type of the parameter. Possible values are as follows. CHAR VARCHAR LONGVARCHAR NUMERIC DECIMAL BIT TINYINT SAMLLINT INTEGER BIGINT REAL DOUBLE DATE TIME TIMESTAMP Property Type This determines whether the parameter value should be a static value or calculated at run time via an expression. Value : If this is selected, a static value would be considered as the property value and this value should be entered in the Value/Expression parameter. Expression: If this is selected, the property value will be determined during mediation by evaluating an expression. This expression should be entered in the Value/Expression parameter. Value/Expression This parameter is used to enter the static value or the XPath expression used to determine the property value based on the option you selected for the Property Type parameter. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This allows you to delete a parameter.","title":"Adding properties to the DB Report mediator"},{"location":"references/dB-Report-Mediator/#examples","text":"","title":"Examples"},{"location":"references/dB-Report-Mediator/#simple-database-write-operation","text":"This example demonstrates simple database write operations. The DB Report mediator writes to a table using the details of the message. It updates the stock price of the company using the last quote value, which is calculated by evaluating an XPath expression against the response message. <dbreport xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <driver>org.apache.derby.jdbc.ClientDriver</driver> <url>jdbc:derby://localhost:1527/esbdb;create=false</url> <user>esb</user> <password>esb</password> </pool> </connection> <statement> <sql><![CDATA[update company set price=? where name =?]]></sql> <parameter expression=\"//m0:return/m1:last/child::text()\" type=\"DOUBLE\" xmlns:m0=\"http://services.samples\" xmlns:m1=\"http://services.samples/xsd\"/> <parameter expression=\"//m0:return/m1:symbol/child::text()\" type=\"VARCHAR\" xmlns:m0=\"http://services.samples\" xmlns:m1=\"http://services.samples/xsd\"/> </statement> </dbreport>","title":"Simple database write operation"},{"location":"references/dB-Report-Mediator/#performing-a-database-write-operation-within-a-transaction","text":"In this example, <transaction action=\"new\"/> is a Transaction Mediator configuration that starts a new transaction. The DBReport mediator configuration performs a few write operations incuding deleting records when the name matches a specific value derived via an expression as well as a few insertions. Once the database operations are complete, they are committed via <transaction action=\"commit\"/> , which is another Transaction Mediator configuration. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"myFaultHandler\"> <log level=\"custom\"> <property name=\"text\" value=\"** Rollback Transaction**\"/> </log> <transaction action=\"rollback\"/> <send/> </sequence> <sequence name=\"main\" onError=\"myFaultHandler\"> <in> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </in> <out> <transaction action=\"new\"/> <log level=\"custom\"> <property name=\"text\" value=\"** Reporting to the Database EIdb**\"/> </log> <dbreport useTransaction=\"true\" xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <dsName>java:jdbc/XADerbyDS</dsName> <icClass>org.jnp.interfaces.NamingContextFactory</icClass> <url>localhost:1099</url> <user>EI</user> <password>EI</password> </pool> </connection> <statement> <sql>delete from company where name =?</sql> <parameter expression=\"//m0:return/m1:symbol/child::text()\" xmlns:m0=\"http://services.samples\" xmlns:m1=\"http://services.samples/xsd\" type=\"VARCHAR\"/> </statement> </dbreport> <log level=\"custom\"> <property name=\"text\" value=\"** Reporting to the Database EIdb1**\"/> </log> <dbreport useTransaction=\"true\" xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <dsName>java:jdbc/XADerbyDS1</dsName> <icClass>org.jnp.interfaces.NamingContextFactory</icClass> <url>localhost:1099</url> <user>EI</user> <password>EI</password> </pool> </connection> <statement> <sql>INSERT into company values (?,'c4',?)</sql> <parameter expression=\"//m0:return/m1:symbol/child::text()\" xmlns:m1=\"http://services.samples/xsd\" xmlns:m0=\"http://services.samples\" type=\"VARCHAR\"/> <parameter expression=\"//m0:return/m1:last/child::text()\" xmlns:m1=\"http://services.samples/xsd\" xmlns:m0=\"http://services.samples\" type=\"DOUBLE\"/> </statement> </dbreport> <transaction action=\"commit\"/> <send/> </out> </sequence> </definitions>","title":"Performing a database write operation within a transaction"},{"location":"references/dB-Report-Mediator/#samples","text":"For more examples of the DB Report mediator, see: Sample 361: Introduction to DB Report Mediator Sample 362: DB Report and DBLookup Mediators Together Sample 363: Reusable Database Connection Pools Sample 271: File Processing (moves files into a database using the VFS transport and the DB Report mediator)","title":"Samples"},{"location":"references/dBLookup-Mediator/","text":"DBLookup Mediator \u00b6 The DBLookup Mediator can execute an arbitrary SQL select statement and then set a resulting values as a local message property in the message context. The DB connection used may be looked up from an external data source or specified inline. Info The DBLookup mediator can set a property from one row in a result set. It cannot return multiple rows. If you need to get multiple records, or if you have a table with multiple parameters (such as URLs), you can use the WSO2 Data Services Server to create a data service and invoke that service from the ESB using the Callout mediator instead. Info The DBLookup mediator is a content-aware mediator. Syntax | Configuration | Examples | Samples Syntax \u00b6 The syntax of the DBLookup mediator changes depending on whether you connect to the database using a connection pool, or using a data source. Click on the relevant tab to view the required syntax. Connection Pool Data source <dblookup> <connection> <pool> <driver/> <url/> <user/> <password/> <property name=\"name\" value=\"value\"/>* </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* <result name=\"string\" column=\"int|string\"/>* </statement>+ </dblookup> The syntax of the DBLookup mediator further differs based on whether the connection to the database is made using an external datasource or a Carbon datasource. Click on the relevant tab to view the required syntax. External Datasource Carbon Datasource <dblookup> <connection> <pool> <dsName/> <icClass/> <url/> <user/> <password/> <property name=\"name\" value=\"value\"/>* </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* <result name=\"string\" column=\"int|string\"/>* </statement>+ </dblookup> <dblookup> <connection> <pool> <dsName/> </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* <result name=\"string\" column=\"int|string\"/>* </statement>+ </dblookup> Configuration \u00b6 The configuration of the DBLookup mediator changes depending on whether you connect to the database using a connection pool, or using a data source. Click on the relevant tab to view the required UI configuration. Connection Pool Data source The parameters available to configure the DBLookup mediator are as follows: Info When specifying the DB connection using a connection pool, other than specifying parameter values inline, you can also specify following parameter values of the connection information (i.e. Driver, URL, User and password) as registry entries. The advantage of specifying a parameter value as a registry entry is that the same connection information configurations can be used in different environments simply by changing the registry entry value. To do this, give the registry path within the key attribute as shown in the example below. <dblookup xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <password key=\"conf:/repository/esb/password\"/> <driver key=\"conf:/repository/esb/driver\"/> <url key=\"conf:/repository/esb/url\"/> <user key=\"conf:/repository/esb/username\"/> </pool> </connection> </dblookup> If you need to provide the registry entry value via the Management Console, specify the registry path using the ` $registry ` prefix. Parameter Name Description Connection Information This parameter is used to specify whether the connection should be taken from a connection pool or a datasource. Driver The class name of the database driver. URL JDBC URL of the database where the data will be looked up. User Username used to connect to the database. Password Password used to connect to the database. Adding properties to the DBLookup mediator \u00b6 If you click Add Property , the page will expand to display the following parameters. The parameters available to manage properties are as follows. Parameter Name Description Name The name of the property. Value The value of the property. Action This parameter enables a property to be deleted. The available properties are as follows. Name Value Description autocommit true / false The auto-commit state of the connections created by the pool. isolation Connection.TRANSACTION_NONE / Connection.TRANSACTION_READ_COMMITTED / Connection.TRANSACTION_READ_UNCOMMITTED / Connection.TRANSACTION_REPEATABLE_READ / Connection.TRANSACTION_SERIALIZABLE The isolation state of the connections created by the pool. initialsize int The initial number of connections created when the pool is started. maxactive int The maximum number of active connections that can be allocated from this pool at a given time. When this maximum limit is reached, no more active connections will be created by the connection pool. Specify 0 or a negative value if you do not want to set a limit. maxidle int The maximum number of idle connections allowed in the connection pool at a given time. The value should be less than the maxActive value. For high performance, tune maxIdle to match the number of average, concurrent requests to the pool. If this value is set to a large value, the pool will contain unnecessary idle connections. !!! info The enabled idle connections are checked periodically whenever a new connection is requested, and connections that are being idle for longer than minEvictableIdleTimeMillis are released , since it takes time to create a new connection. maxopenstatements int The maximum number of open statements that can be allocated from the statement pool at a given time. When this maximum limit is reached, no more new statements will be created by the statement pool. Specify 0 or a negative value if you do not want to set a limit. maxwait long The maximum number of milliseconds that the connection pool will wait for a connection to return before throwing an exception when there are no connections available in the pool. Specify 0 or a negative value if you want the pool to wait indefinitely. minidle int The minimum number of idle connections allowed in the connection pool at a given time, without extra ones being created . Default value is 0, and is derived from initialSize . The connection pool can shrink below this number if validation queries fail. !!! info This value should be similar or near to the average number of requests that will be received by the server at the same time. With this setting, you can avoid having to open and close new connections every time a request is received by the server. poolstatements true/ false If the value is true , statement pooling is enabled for the pool. testonborrow true/ false If the value is true , objects are validated before they are borrowed from the pool. An object which fails the validation test will be dropped from the pool and another object in the pool will be picked instead. testwhileidle true/ false If the value is true , the objects in the pool will be validated using an idle object evictor (if any exists). Any object which fails this validation test would be dropped from the pool. validationquery String The SQL query that will be used to validate connections from this pool before returning them to the caller. The UI configuration of the DBLookup mediator further differs based on whether the connection to the database is made using an external datasource or a Carbon datasource. Click on the relevant tab to view the required UI configuration. Carbon Datasource The parameters available to configure the DBLookup mediator are as follows. Parameter Name Description Connection Information This parameter is used to specify whether the connection should be taken from a connection pool or a datasource. Datasource Type This parameter is used to specify whether the connection to the database should be made using an external datasource or a Carbon datasource. JNDI Name The JNDI used to look up data. Adding SQL statements to the DBLookup Mediator If you click Add Statement , the page will be expanded to display the following parameters. Parameter Name Description SQL This parameter is used to enter one or more SQL statements. Parameters This section is used to specify how the values of parameters in the SQL will be determined. A parameter value can be static or calculated at runtime based on a given expression. Parameter Type The data type of the parameter. Possible values are as follows. CHAR VARCHAR LONGVARCHAR NUMERIC DECIMAL BIT TINYINT SAMLLINT INTEGER BIGINT REAL DOUBLE DATE TIME TIMESTAMP Property Type This determines whether the parameter value should be a static value or calculated at run time via an expression. Value : If this is selected, a static value would be considered as the property value and this value should be entered in the Value/Expression parameter. Expression: If this is selected, the property value will be determined during mediation by evaluating an expression. This expression should be entered in the Value/Expression parameter. Value/Expression This parameter is used to enter the static value or the XPath expression used to determine the property value based on the option you selected for the Property Type parameter. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This allows you to delete a parameter. Results This section is used to specify how to deal with the rerun result from a Database query execution. Result Name Column Action - Deletes the result. Examples \u00b6 <dblookup xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <driver>org.apache.derby.jdbc.ClientDriver</driver> <url>jdbc:derby://localhost:1527/esbdb;create=false</url> <user>esb</user> <password>esb</password> </pool> </connection> <statement> <sql><![CDATA[select * from company where name =?]]></sql> <parameter expression=\"//m0:getQuote/m0:request/m0:symbol\" type=\"VARCHAR\" xmlns:m0=\"http://services.samples/xsd\"/> <result column=\"id\" name=\"company_id\"/> </statement> </dblookup> In this example, when a message is received by a proxy service with a DBLookup mediator configuration, it opens a connection to the database and executes the SQL query. The SQL query uses ? character for attributes that will be filled at runtime. The parameters define how to calculate the value of those attributes at runtime. In this sample, the DBLookup Mediator has been used to extract the id of the company from the company database using the symbol which is evaluated using an XPath against the SOAP envelope. Samples \u00b6 Sample 360: Introduction to DBLookup Mediator Sample 362: DBReport and DBLookup Mediators Together Sample 363: Reusable Database Connection Pools","title":"DBLookup Mediator"},{"location":"references/dBLookup-Mediator/#dblookup-mediator","text":"The DBLookup Mediator can execute an arbitrary SQL select statement and then set a resulting values as a local message property in the message context. The DB connection used may be looked up from an external data source or specified inline. Info The DBLookup mediator can set a property from one row in a result set. It cannot return multiple rows. If you need to get multiple records, or if you have a table with multiple parameters (such as URLs), you can use the WSO2 Data Services Server to create a data service and invoke that service from the ESB using the Callout mediator instead. Info The DBLookup mediator is a content-aware mediator. Syntax | Configuration | Examples | Samples","title":"DBLookup Mediator"},{"location":"references/dBLookup-Mediator/#syntax","text":"The syntax of the DBLookup mediator changes depending on whether you connect to the database using a connection pool, or using a data source. Click on the relevant tab to view the required syntax. Connection Pool Data source <dblookup> <connection> <pool> <driver/> <url/> <user/> <password/> <property name=\"name\" value=\"value\"/>* </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* <result name=\"string\" column=\"int|string\"/>* </statement>+ </dblookup> The syntax of the DBLookup mediator further differs based on whether the connection to the database is made using an external datasource or a Carbon datasource. Click on the relevant tab to view the required syntax. External Datasource Carbon Datasource <dblookup> <connection> <pool> <dsName/> <icClass/> <url/> <user/> <password/> <property name=\"name\" value=\"value\"/>* </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* <result name=\"string\" column=\"int|string\"/>* </statement>+ </dblookup> <dblookup> <connection> <pool> <dsName/> </pool> </connection> <statement> <sql>select something from table where something_else = ?</sql> <parameter [value=\"\" | expression=\"\"] type=\"CHAR|VARCHAR|LONGVARCHAR|NUMERIC|DECIMAL|BIT|TINYINT|SMALLINT|INTEGER|BIGINT|REAL|FLOAT|DOUBLE|DATE|TIME|TIMESTAMP\"/>* <result name=\"string\" column=\"int|string\"/>* </statement>+ </dblookup>","title":"Syntax"},{"location":"references/dBLookup-Mediator/#configuration","text":"The configuration of the DBLookup mediator changes depending on whether you connect to the database using a connection pool, or using a data source. Click on the relevant tab to view the required UI configuration. Connection Pool Data source The parameters available to configure the DBLookup mediator are as follows: Info When specifying the DB connection using a connection pool, other than specifying parameter values inline, you can also specify following parameter values of the connection information (i.e. Driver, URL, User and password) as registry entries. The advantage of specifying a parameter value as a registry entry is that the same connection information configurations can be used in different environments simply by changing the registry entry value. To do this, give the registry path within the key attribute as shown in the example below. <dblookup xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <password key=\"conf:/repository/esb/password\"/> <driver key=\"conf:/repository/esb/driver\"/> <url key=\"conf:/repository/esb/url\"/> <user key=\"conf:/repository/esb/username\"/> </pool> </connection> </dblookup> If you need to provide the registry entry value via the Management Console, specify the registry path using the ` $registry ` prefix. Parameter Name Description Connection Information This parameter is used to specify whether the connection should be taken from a connection pool or a datasource. Driver The class name of the database driver. URL JDBC URL of the database where the data will be looked up. User Username used to connect to the database. Password Password used to connect to the database.","title":"Configuration"},{"location":"references/dBLookup-Mediator/#adding-properties-to-the-dblookup-mediator","text":"If you click Add Property , the page will expand to display the following parameters. The parameters available to manage properties are as follows. Parameter Name Description Name The name of the property. Value The value of the property. Action This parameter enables a property to be deleted. The available properties are as follows. Name Value Description autocommit true / false The auto-commit state of the connections created by the pool. isolation Connection.TRANSACTION_NONE / Connection.TRANSACTION_READ_COMMITTED / Connection.TRANSACTION_READ_UNCOMMITTED / Connection.TRANSACTION_REPEATABLE_READ / Connection.TRANSACTION_SERIALIZABLE The isolation state of the connections created by the pool. initialsize int The initial number of connections created when the pool is started. maxactive int The maximum number of active connections that can be allocated from this pool at a given time. When this maximum limit is reached, no more active connections will be created by the connection pool. Specify 0 or a negative value if you do not want to set a limit. maxidle int The maximum number of idle connections allowed in the connection pool at a given time. The value should be less than the maxActive value. For high performance, tune maxIdle to match the number of average, concurrent requests to the pool. If this value is set to a large value, the pool will contain unnecessary idle connections. !!! info The enabled idle connections are checked periodically whenever a new connection is requested, and connections that are being idle for longer than minEvictableIdleTimeMillis are released , since it takes time to create a new connection. maxopenstatements int The maximum number of open statements that can be allocated from the statement pool at a given time. When this maximum limit is reached, no more new statements will be created by the statement pool. Specify 0 or a negative value if you do not want to set a limit. maxwait long The maximum number of milliseconds that the connection pool will wait for a connection to return before throwing an exception when there are no connections available in the pool. Specify 0 or a negative value if you want the pool to wait indefinitely. minidle int The minimum number of idle connections allowed in the connection pool at a given time, without extra ones being created . Default value is 0, and is derived from initialSize . The connection pool can shrink below this number if validation queries fail. !!! info This value should be similar or near to the average number of requests that will be received by the server at the same time. With this setting, you can avoid having to open and close new connections every time a request is received by the server. poolstatements true/ false If the value is true , statement pooling is enabled for the pool. testonborrow true/ false If the value is true , objects are validated before they are borrowed from the pool. An object which fails the validation test will be dropped from the pool and another object in the pool will be picked instead. testwhileidle true/ false If the value is true , the objects in the pool will be validated using an idle object evictor (if any exists). Any object which fails this validation test would be dropped from the pool. validationquery String The SQL query that will be used to validate connections from this pool before returning them to the caller. The UI configuration of the DBLookup mediator further differs based on whether the connection to the database is made using an external datasource or a Carbon datasource. Click on the relevant tab to view the required UI configuration. Carbon Datasource The parameters available to configure the DBLookup mediator are as follows. Parameter Name Description Connection Information This parameter is used to specify whether the connection should be taken from a connection pool or a datasource. Datasource Type This parameter is used to specify whether the connection to the database should be made using an external datasource or a Carbon datasource. JNDI Name The JNDI used to look up data. Adding SQL statements to the DBLookup Mediator If you click Add Statement , the page will be expanded to display the following parameters. Parameter Name Description SQL This parameter is used to enter one or more SQL statements. Parameters This section is used to specify how the values of parameters in the SQL will be determined. A parameter value can be static or calculated at runtime based on a given expression. Parameter Type The data type of the parameter. Possible values are as follows. CHAR VARCHAR LONGVARCHAR NUMERIC DECIMAL BIT TINYINT SAMLLINT INTEGER BIGINT REAL DOUBLE DATE TIME TIMESTAMP Property Type This determines whether the parameter value should be a static value or calculated at run time via an expression. Value : If this is selected, a static value would be considered as the property value and this value should be entered in the Value/Expression parameter. Expression: If this is selected, the property value will be determined during mediation by evaluating an expression. This expression should be entered in the Value/Expression parameter. Value/Expression This parameter is used to enter the static value or the XPath expression used to determine the property value based on the option you selected for the Property Type parameter. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This allows you to delete a parameter. Results This section is used to specify how to deal with the rerun result from a Database query execution. Result Name Column Action - Deletes the result.","title":"Adding properties to the DBLookup mediator"},{"location":"references/dBLookup-Mediator/#examples","text":"<dblookup xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <driver>org.apache.derby.jdbc.ClientDriver</driver> <url>jdbc:derby://localhost:1527/esbdb;create=false</url> <user>esb</user> <password>esb</password> </pool> </connection> <statement> <sql><![CDATA[select * from company where name =?]]></sql> <parameter expression=\"//m0:getQuote/m0:request/m0:symbol\" type=\"VARCHAR\" xmlns:m0=\"http://services.samples/xsd\"/> <result column=\"id\" name=\"company_id\"/> </statement> </dblookup> In this example, when a message is received by a proxy service with a DBLookup mediator configuration, it opens a connection to the database and executes the SQL query. The SQL query uses ? character for attributes that will be filled at runtime. The parameters define how to calculate the value of those attributes at runtime. In this sample, the DBLookup Mediator has been used to extract the id of the company from the company database using the symbol which is evaluated using an XPath against the SOAP envelope.","title":"Examples"},{"location":"references/dBLookup-Mediator/#samples","text":"Sample 360: Introduction to DBLookup Mediator Sample 362: DBReport and DBLookup Mediators Together Sample 363: Reusable Database Connection Pools","title":"Samples"},{"location":"references/data-Mapper-Mediator/","text":"Data Mapper Mediator \u00b6 Data Mapper mediator is a data mapping solution that can be integrated into a mediation sequence. It converts and transforms one data format to another, or changes the structure of the data in a message. It provides WSO2 Integration Studio to create a graphical mapping configuration and generates the files required to execute this graphical mapping configuration by the WSO2 Data Mapper engine. WSO2 Data Mapper is an independent component that does not depend on any other WSO2 product. However, other products can use the Data Mapper to achieve/offer data mapping capabilities. Data Mapper Mediator is the intermediate component you need for that, which gives the data mapping capability into the ESB profile of WSO2 EI. Data Mapper mediator finds the configuration files from the Registry and configures the Data Mapper Engine with the input message type (XML/JSON/CSV) and output message type (XML/JSON/CSV). Then it takes the request message from the ESB profile message flow and uses the configured Data Mapper Engine to execute the transformation and adds the output message to the ESB profile message flow. Info The Data Mapper mediator is a content-aware mediator. Prerequisites Syntax Configuration Components of Data Mapper Data Mapper element and attribute types Data Mapper operations Examples Prerequisites \u00b6 You need to install the WSO2 Integration Studio to use the Data Mapper mediator. For instructions on installing this Plugin, see Installing WSO2 Integration Studio . Syntax \u00b6 <datamapper config=\"gov:datamapper/FoodMapping.dmc\" inputSchema=\"gov:datamapper/FoodMapping_inputSchema.json\" inputType=\"XML\" outputSchema=\"gov:datamapper/FoodMapping_outputSchema.json\" outputType=\"XML\"/> Configuration \u00b6 The parameters available for configuring the Data Mapper mediator are as follows. Parameter name Description Mapping Configuration The file, which contains the script file that is used to execute the mapping. You need to create a mapping configuration file using the WSO2 Integration Studio plugin, and store it either in the Configuration Registry or Governance Registry, to select and upload it from here. Input Schema JSON schema, which represents the input message format. You need to create an input schema file using the WSO2 Integration Studio plugin, and store it either in the Configuration Registry or Governance Registry to select and upload it from here. Output Schema JSON schema, which represents the output message format. You need to create an output schema file using the WSO2 Integration Studio plugin, and store it either in the Configuration Registry or Governance Registry to select and upload it from here. Input Type Expected input message type (XML/JSON/CSV) Output Type Target output message type (XML/JSON/CSV) Components of Data Mapper \u00b6 WSO2 Data Mapper consists of two components. They are Data Mapper Tooling and Data Mapper Engine. Data Mapper Tooling \u00b6 Data Mapper Tooling component is the interface used to create configuration files that are required by the Data Mapper Engine to execute the mapping. Following three configuration files are needed by the Data Mapper engine. Input schema file Output schema file Mapping configuration file These three files are generated by the Data Mapper Tool and saved in a Registry Resource project, which you deploy in a WSO2 server as shown in the example below. {width=\"542\" height=\"229\"} Info The .datamapper and .datamapper_diagram files as shown in the example above contain meta data related to the Data Mapper diagram. They are ignored when you deploy the project to a server to be used by the Data Mapper Engine. Only the two schema files and the .dmc (Data Mapper Configuration) get deployed. Input and output schema files \u00b6 Input and output schema files are custom-defined JSON schemas that define the input/output format of input/output messages. T he Data Mapper tool generates them when loading the input and output files as shown below. Info You can also create the input and output JSON Schemas manually using the Data Mapper Diagram Editor. For instructions, see Creating a JSON Schema Manually . {width=\"900\" height=\"192\"} {width=\"567\" height=\"250\"} You can load the following input/output message formats: Info When loading a sample input XML file, you cannot have the default namespace (i.e. without a prefix in the namespace element). Also, you need to use the same prefix in all occurrences that refer to the same namespace within one XML file. For example, see the use of the prefix axis2ns11 in the example below. {.expand-control-image} Sample input XML file <?xml version=\"1.0\" encoding=\"utf-8\"?> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header> <axis2ns11:LimitInfoHeader xmlns:axis2ns11=\"urn:partner.soap.sforce.com\"> <axis2ns11:limitInfo> <axis2ns11:current>42336</axis2ns11:current> <axis2ns11:limit>83000</axis2ns11:limit> <axis2ns11:type>API REQUESTS</axis2ns11:type> </axis2ns11:limitInfo> </axis2ns11:LimitInfoHeader> </soapenv:Header> <soapenv:Body> <axis2ns11:records xmlns:axis2ns11=\"urn:partner.soap.sforce.com\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"sf:sObject\"> <sf:type xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">Account</sf:type> <sf:Id xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">001E0000002SFO2IAO</sf:Id> <sf:CreatedDate xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">2011-03-15T00:15:00.000Z</sf:CreatedDate> <sf:Id xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">001E0000002SFO2IAO</sf:Id> <sf:Name xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">WSO2</sf:Name> </axis2ns11:records> </soapenv:Body> </soapenv:Envelope> XML: to load a sample XML file JSON: to load a sample JSON file CSV: to load a sample CSV file with column names as the first record JSONSCHEMA: to load a WSO2 Data Mapper JSON schema CONNECTOR: to use Data Mapper with WSO2 EI Connectors. Connectors will contain JSON schemas for each operation that defines the message formats to which it will respond and expect. Therefore, when you integrate connectors in a project this Connector option searches through the workspace and find the available Connectors. Then, you can select the respective Connector in the operation, so that the related JSON schema will be loaded for the Data Mapper by the tool. Mapping configuration file \u00b6 This is a JavaScript file generated by looking at the diagram you draw in the Data Mapper Diagram Editor by connecting input elements to output elements. Every operation you define in the diagram gets converted to a JavaScript operation. Data Mapper Engine \u00b6 You need the following information to configure the Data Mapper Engine: Input message type Output message type Input schema Java Scripting API Output schema Mapping configuration At the runtime, the Data Mapper Engine gets the input message and the runtime variable map object and outputs the transformed message. The D ata Mapper Engine uses the J ava Scripting API, t o execute the mapping configuration. Therefore, if your runtime is JAVA 7, it uses the Rhino JS Engine and if your runtime is JAVA 8, it uses the Nashorn JS engine. When you use JAVA 7, there are several limitations in the Rhino engine that directly affects the Data mapper Engine. There are several functions that Rhino does not support. F or example, String object functions like startsWith() and endsWith() . Therefore, the Rhino engine may have limitations in executing those when using custom functions and operators. Using product-specific runtime variables \u00b6 Also, the Data Mapper engine allows you to use runtime product-specific variables in the mapping. The intermediate component should construct a map object containing runtime product-specific variables and send it to the Data Mapper Engine, thereby, when the mapping happens in the Data Mapper Engine, these variables become available. For example, the Data Mapper mediator provides properties like axis2/transport/synapse/axis2client/operation/. . In the Data Mapper diagram, you can use the Property operator and define the scope and the property name and use it in the mapping. Then, the Data Mapper mediator will identify the required properties to execute the mapping and populate a map with the required properties and will send it to the Data Mapper Engine. Data Mapper element and attribute types \u00b6 Following are the element and attribute types that are supported by the Data Mapper. {} - represents object elements [] - represents array elements \\<> - represents primitive field values A - represents XML attribute values Data Mapper operations \u00b6 The operations palette placed in the left-hand side of the WSO2 Data Mapping Diagram Editor displays t he operations that the Data Mapper supports as shown below . {width=\"900\" height=\"349\"} You can drag and drop these operations to the Editor area. There are six categories of operations as follows: Links Common Arithmetic Conditional Boolean Type Conversion String Links \u00b6 {width=\"24\"} Data Mapping Link: maps elements with other operators and elements. Common \u00b6 {width=\"24\"} Constant: defines String, number or boolean constant values. {width=\"24\"} Custom Function: defines custom functions to use in the mapping. {width=\"24\"} Properties: uses product-specific runtime variables. {width=\"24\"} Global Variable: instantiates global variables that you can access from anywhere. {width=\"24\"} Compare: compares two inputs in the mapping. Arithmetic \u00b6 {width=\"24\"} Add: adds two numbers. {width=\"24\"} Subtract: subtracts two or more numbers. {width=\"24\"} Multiply: multiplies two or more numbers. {width=\"24\"} Divide: divides two numbers. {width=\"24\"} Ceiling: derives the ceiling value of a number (closest larger integer value). {width=\"24\"} Floor: derives the floor value of a number (closest lower integer value). {width=\"24\"} Round: derives the nearest integer value. {width=\"24\"} Set Precision: formats a number into a specified length. {width=\"24\"} Absolute Value: derives the absolute value of a rational number. {width=\"24\"} Min: derives the minimum number from given inputs {width=\"24\"} Max: derives the maximum number from given inputs Conditional \u00b6 {width=\"24\"} IfElse: uses a condition and selects one input from given two. Boolean \u00b6 {width=\"24\"} AND: performs the boolean AND operation on inputs. {width=\"24\"} OR: performs the boolean OR operation on inputs. {width=\"24\"} NOT: performs the boolean NOT operation on inputs. Type conversion \u00b6 {width=\"24\"} StringToNumber: converts a String value to number (\u201c0\u201d -> 0). {width=\"24\"} StringToBoolean: converts a String value to boolean (\u201ctrue\u201d -> true). {width=\"24\"} ToString: converts a number or a boolean value to String. String \u00b6 {width=\"24\"} Concat: concatenates two or more Strings. {width=\"24\"} Split: splits a String by a matching String value. {width=\"24\"} Uppercase: converts a String to uppercase letters. {width=\"24\"} Lowercase: converts a String to lowercase letters. {width=\"24\"} String Length: gets the length of the String. {width=\"24\"} StartsWith: checks whether a String starts with a specific value. (This is not supported in Java 7.) {width=\"24\"} EndsWith: checks whether String ends with a specific value. (This is not supported in Java 7.) {width=\"24\"} Substring: extracts a part of the String value. {width=\"24\"} Trim: removes white spaces from the beginning and end of a String. {width=\"24\"} Replace: replaces the first occurrence of a target String with another. {width=\"24\"} Match \u2013 check whether the input match with a (JS) Regular Expression Examples \u00b6 Example 1 - Creating a SOAP payload with namespaces \u00b6 This example creates a Salesforce login SOAP payload using a JSON payload. The login payload consists of XML namespaces. Even though the JSON payload does not contain any namespace information, the output JSON schema will be generated with XML namespace information using the provided SOAP payload. {width=\"900\" height=\"196\"} The sample input JSON payload is as follows. { \"name\":\"Watson\", \"password\":\"watson@123\" } The sample output XML is as follows. <soapenv:Envelope xmlns:urn=\"urn:enterprise.soap.sforce.com\" xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope/\"> <soapenv:Body> <urn:login> <urn:username><b>user@domain.com</b></urn:username> <urn:password><b>secret</b></urn:password> </urn:login> </soapenv:Body> </soapenv:Envelope> Example 2 - M apping SOAP header elements \u00b6 This example demonstrates how to map SOAP header elements along with SOAP body elements to create a certain SOAP payload, by creating a Salesforce convertLead SOAP payload using a JSON payload. The Convert Lead SOAP payload needs mapping SOAP header information. E.g. <urn:sessionId>QwWsHJyTPW.1pd0_jXlNKOSU</urn:sessionId> {width=\"900\" height=\"400\"} The sample input JSON payload is as follows. { \"owner\":{ \"ID\":\"005D0000000nVYVIA2\", \"name\":\"Smith\", \"city\":\"CA\", \"code\":\"94041\", \"country\":\"US\" }, \"lead\":{ \"ID\":\"00QD000000FP14JMAT\", \"name\":\"Carl\", \"city\":\"NC\", \"code\":\"97788\", \"country\":\"US\" }, \"sendNotificationEmail\":\"true\", \"convertedStatus\":\"Qualified\", \"doNotCreateOpportunity\":\"true\", \"opportunityName\":\"Partner Opportunity\", \"overwriteLeadSource\":\"true\", \"sessionId\":\"QwWsHJyTPW.1pd0_jXlNKOSU\" } The sample o utput XML is as follows. <?xml version=\"1.0\" encoding=\"utf-8\"?> <soapenv:Envelope xmlns:urn=\"urn:enterprise.soap.sforce.com\" xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope/\"> <soapenv:Header> <urn:SessionHeader> <urn:sessionId>QwWsHJyTPW.1pd0_jXlNKOSU</urn:sessionId> </urn:SessionHeader> </soapenv:Header> <soapenv:Body> <urn:convertLead > <urn:leadConverts> <!-- Zero or more repetitions --> <urn:convertedStatus>Qualified</urn:convertedStatus> <urn:doNotCreateOpportunity>false</urn:doNotCreateOpportunity> <urn:leadId>00QD000000FP14JMAT</urn:leadId> <urn:opportunityName>Partner Opportunity</urn:opportunityName> <urn:overwriteLeadSource>true</urn:overwriteLeadSource> <urn:ownerId>005D0000000nVYVIA2</urn:ownerId> <urn:sendNotificationEmail>true</urn:sendNotificationEmail> </urn:leadConverts> </urn:convertLead> </soapenv:Body> </soapenv:Envelope> Example 3 - Mapping primitive types \u00b6 This example demonstrates how you can map an XML payload with integer, boolean etc. values, into a JSON payload with required primitive types, by specifying the required primitive type in the JSON schema. {width=\"900\"} The sample input XML payload is as follows. <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <name>app_name</name> <version>version</version> <manifest_version>2</manifest_version> <description>description_text</description> <container>GOOGLE_DRIVE</container> <api_console_project_id>YOUR_APP_ID</api_console_project_id> <gdrive_mime_types> <http://drive.google.com/intents/opendrivedoc> <type>image/png</type> <type>image/jpeg</type> <type>image/gif</type> <type>application/vnd.google.drive.ext-type.png</type> <type>application/vnd.google.drive.ext-type.jpg</type> <type>application/vnd.google.drive.ext-type.gif</type> <href>http://your_web_url/</href> <title>Open</title> <disposition>window</disposition> </http://drive.google.com/intents/opendrivedoc> </gdrive_mime_types> <icons> <128>icon_128.png</128> </icons> <app> <launch> <web_url>http://yoursite.com</web_url> </launch> </app> The sample o utput JSON is as follows. { \"name\" : \"app_name\", \"version\" : \"version\", \"manifest_version\" : 2, \"description\" : \"description_text\", \"container\" : \"GOOGLE_DRIVE\", \"api_console_project_id\" : \"YOUR_APP_ID\", \"gdrive_mime_types\": { \"http://drive.google.com/intents/opendrivedoc\": [ { \"type\": [\"image/png\", \"image/jpeg\", \"image/gif\", \"application/vnd.google.drive.ext-type.png\", \"application/vnd.google.drive.ext-type.jpg\",\"application/vnd.google.drive.ext-type.gif\"], \"href\": \"http://your_web_url/\", \"title\" : \"Open\", \"disposition\" : \"window\" } ] }, \"icons\": { \"128\": \"icon_128.png\" }, \"app\" : { \"launch\" : { \"web_url\" : \"http://yoursite.com\" } } } Example 4 - Mapping XML to CSV \u00b6 This example demonstrates how you can map an XML payload to CSV format. Info If you specify special characters (e.g., & , &amp; ) within the <text> tag w hen converting from CSV to CSV , they will be displayed as follows by default. & -> &amp; &amp; -> &amp;amp; < -> &lt; &lt; -> &lt;lt; To avoid this and to display the exact special characters as text in the returned output, add the following properties in the Synapse configuration. <property name=\"messageType\" value=\"text/plain\" scope=\"axis2\"/> <property name=\"ContentType\" value=\"text/plain\" scope=\"axis2\"/> {width=\"900\" height=\"217\"} The sample in put XML payload is as follows. <?xml version=\"1.0\"?> <PurchaseOrder PurchaseOrderNumber=\"001\"> <Address> <Name>James Yee</Name> <Street>Downtown Bartow</Street> <City>Old Town</City> <State>PA</State> <Zip>95819</Zip> <Country>USA</Country> </Address> <Address> <Name>Elen Smith</Name> <Street>123 Maple Street</Street> <City>Mill Valley</City> <State>CA</State> <Zip>10999</Zip> <Country>USA</Country> </Address> <DeliveryNotes>Please leave packages in shed by driveway.</DeliveryNotes> </PurchaseOrder> The sample out put CSV is as follows. Name,Street,City,State,Zip,Country James Yee,Downtown Bartow,Old Town,PA,95819,USA Ellen Smith,123 Maple Street,Mill Valley,CA,10999,USA Example 5 - Mapping XSD to JSON \u00b6 This example demonstrates how you can map an XSD payload to JSON format. {width=\"900\"} The sample in put XSD payload is as follows. <xs:schema attributeFormDefault=\"unqualified\" elementFormDefault=\"qualified\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"> <xs:element name=\"books\"> <xs:complexType> <xs:sequence> <xs:element name=\"book\"> <xs:complexType> <xs:sequence> <xs:element type=\"xs:string\" name=\"id\"/> <xs:element type=\"xs:string\" name=\"author\"/> <xs:element type=\"xs:string\" name=\"title\"/> <xs:element type=\"xs:float\" name=\"price\"/> </xs:sequence> </xs:complexType> </xs:element> </xs:sequence> </xs:complexType> </xs:element> </xs:schema> The sample out put JSON is as follows. { \"books\": { \"book\": { \"id\": \"001\", \"author\": \"Writer\", \"title\": \"Great book on nature\", \"price\": \"44.95\" } } }","title":"Data Mapper Mediator"},{"location":"references/data-Mapper-Mediator/#data-mapper-mediator","text":"Data Mapper mediator is a data mapping solution that can be integrated into a mediation sequence. It converts and transforms one data format to another, or changes the structure of the data in a message. It provides WSO2 Integration Studio to create a graphical mapping configuration and generates the files required to execute this graphical mapping configuration by the WSO2 Data Mapper engine. WSO2 Data Mapper is an independent component that does not depend on any other WSO2 product. However, other products can use the Data Mapper to achieve/offer data mapping capabilities. Data Mapper Mediator is the intermediate component you need for that, which gives the data mapping capability into the ESB profile of WSO2 EI. Data Mapper mediator finds the configuration files from the Registry and configures the Data Mapper Engine with the input message type (XML/JSON/CSV) and output message type (XML/JSON/CSV). Then it takes the request message from the ESB profile message flow and uses the configured Data Mapper Engine to execute the transformation and adds the output message to the ESB profile message flow. Info The Data Mapper mediator is a content-aware mediator. Prerequisites Syntax Configuration Components of Data Mapper Data Mapper element and attribute types Data Mapper operations Examples","title":"Data Mapper Mediator"},{"location":"references/data-Mapper-Mediator/#prerequisites","text":"You need to install the WSO2 Integration Studio to use the Data Mapper mediator. For instructions on installing this Plugin, see Installing WSO2 Integration Studio .","title":"Prerequisites"},{"location":"references/data-Mapper-Mediator/#syntax","text":"<datamapper config=\"gov:datamapper/FoodMapping.dmc\" inputSchema=\"gov:datamapper/FoodMapping_inputSchema.json\" inputType=\"XML\" outputSchema=\"gov:datamapper/FoodMapping_outputSchema.json\" outputType=\"XML\"/>","title":"Syntax"},{"location":"references/data-Mapper-Mediator/#configuration","text":"The parameters available for configuring the Data Mapper mediator are as follows. Parameter name Description Mapping Configuration The file, which contains the script file that is used to execute the mapping. You need to create a mapping configuration file using the WSO2 Integration Studio plugin, and store it either in the Configuration Registry or Governance Registry, to select and upload it from here. Input Schema JSON schema, which represents the input message format. You need to create an input schema file using the WSO2 Integration Studio plugin, and store it either in the Configuration Registry or Governance Registry to select and upload it from here. Output Schema JSON schema, which represents the output message format. You need to create an output schema file using the WSO2 Integration Studio plugin, and store it either in the Configuration Registry or Governance Registry to select and upload it from here. Input Type Expected input message type (XML/JSON/CSV) Output Type Target output message type (XML/JSON/CSV)","title":"Configuration"},{"location":"references/data-Mapper-Mediator/#components-of-data-mapper","text":"WSO2 Data Mapper consists of two components. They are Data Mapper Tooling and Data Mapper Engine.","title":"Components of Data Mapper"},{"location":"references/data-Mapper-Mediator/#data-mapper-tooling","text":"Data Mapper Tooling component is the interface used to create configuration files that are required by the Data Mapper Engine to execute the mapping. Following three configuration files are needed by the Data Mapper engine. Input schema file Output schema file Mapping configuration file These three files are generated by the Data Mapper Tool and saved in a Registry Resource project, which you deploy in a WSO2 server as shown in the example below. {width=\"542\" height=\"229\"} Info The .datamapper and .datamapper_diagram files as shown in the example above contain meta data related to the Data Mapper diagram. They are ignored when you deploy the project to a server to be used by the Data Mapper Engine. Only the two schema files and the .dmc (Data Mapper Configuration) get deployed.","title":"Data Mapper Tooling"},{"location":"references/data-Mapper-Mediator/#input-and-output-schema-files","text":"Input and output schema files are custom-defined JSON schemas that define the input/output format of input/output messages. T he Data Mapper tool generates them when loading the input and output files as shown below. Info You can also create the input and output JSON Schemas manually using the Data Mapper Diagram Editor. For instructions, see Creating a JSON Schema Manually . {width=\"900\" height=\"192\"} {width=\"567\" height=\"250\"} You can load the following input/output message formats: Info When loading a sample input XML file, you cannot have the default namespace (i.e. without a prefix in the namespace element). Also, you need to use the same prefix in all occurrences that refer to the same namespace within one XML file. For example, see the use of the prefix axis2ns11 in the example below. {.expand-control-image} Sample input XML file <?xml version=\"1.0\" encoding=\"utf-8\"?> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header> <axis2ns11:LimitInfoHeader xmlns:axis2ns11=\"urn:partner.soap.sforce.com\"> <axis2ns11:limitInfo> <axis2ns11:current>42336</axis2ns11:current> <axis2ns11:limit>83000</axis2ns11:limit> <axis2ns11:type>API REQUESTS</axis2ns11:type> </axis2ns11:limitInfo> </axis2ns11:LimitInfoHeader> </soapenv:Header> <soapenv:Body> <axis2ns11:records xmlns:axis2ns11=\"urn:partner.soap.sforce.com\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"sf:sObject\"> <sf:type xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">Account</sf:type> <sf:Id xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">001E0000002SFO2IAO</sf:Id> <sf:CreatedDate xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">2011-03-15T00:15:00.000Z</sf:CreatedDate> <sf:Id xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">001E0000002SFO2IAO</sf:Id> <sf:Name xmlns:sf=\"urn:sobject.partner.soap.sforce.com\">WSO2</sf:Name> </axis2ns11:records> </soapenv:Body> </soapenv:Envelope> XML: to load a sample XML file JSON: to load a sample JSON file CSV: to load a sample CSV file with column names as the first record JSONSCHEMA: to load a WSO2 Data Mapper JSON schema CONNECTOR: to use Data Mapper with WSO2 EI Connectors. Connectors will contain JSON schemas for each operation that defines the message formats to which it will respond and expect. Therefore, when you integrate connectors in a project this Connector option searches through the workspace and find the available Connectors. Then, you can select the respective Connector in the operation, so that the related JSON schema will be loaded for the Data Mapper by the tool.","title":"Input and output schema files"},{"location":"references/data-Mapper-Mediator/#mapping-configuration-file","text":"This is a JavaScript file generated by looking at the diagram you draw in the Data Mapper Diagram Editor by connecting input elements to output elements. Every operation you define in the diagram gets converted to a JavaScript operation.","title":"Mapping configuration file"},{"location":"references/data-Mapper-Mediator/#data-mapper-engine","text":"You need the following information to configure the Data Mapper Engine: Input message type Output message type Input schema Java Scripting API Output schema Mapping configuration At the runtime, the Data Mapper Engine gets the input message and the runtime variable map object and outputs the transformed message. The D ata Mapper Engine uses the J ava Scripting API, t o execute the mapping configuration. Therefore, if your runtime is JAVA 7, it uses the Rhino JS Engine and if your runtime is JAVA 8, it uses the Nashorn JS engine. When you use JAVA 7, there are several limitations in the Rhino engine that directly affects the Data mapper Engine. There are several functions that Rhino does not support. F or example, String object functions like startsWith() and endsWith() . Therefore, the Rhino engine may have limitations in executing those when using custom functions and operators.","title":"Data Mapper Engine"},{"location":"references/data-Mapper-Mediator/#using-product-specific-runtime-variables","text":"Also, the Data Mapper engine allows you to use runtime product-specific variables in the mapping. The intermediate component should construct a map object containing runtime product-specific variables and send it to the Data Mapper Engine, thereby, when the mapping happens in the Data Mapper Engine, these variables become available. For example, the Data Mapper mediator provides properties like axis2/transport/synapse/axis2client/operation/. . In the Data Mapper diagram, you can use the Property operator and define the scope and the property name and use it in the mapping. Then, the Data Mapper mediator will identify the required properties to execute the mapping and populate a map with the required properties and will send it to the Data Mapper Engine.","title":"Using product-specific runtime variables"},{"location":"references/data-Mapper-Mediator/#data-mapper-element-and-attribute-types","text":"Following are the element and attribute types that are supported by the Data Mapper. {} - represents object elements [] - represents array elements \\<> - represents primitive field values A - represents XML attribute values","title":"Data Mapper element and attribute types"},{"location":"references/data-Mapper-Mediator/#data-mapper-operations","text":"The operations palette placed in the left-hand side of the WSO2 Data Mapping Diagram Editor displays t he operations that the Data Mapper supports as shown below . {width=\"900\" height=\"349\"} You can drag and drop these operations to the Editor area. There are six categories of operations as follows: Links Common Arithmetic Conditional Boolean Type Conversion String","title":"Data Mapper operations"},{"location":"references/data-Mapper-Mediator/#links","text":"{width=\"24\"} Data Mapping Link: maps elements with other operators and elements.","title":"Links"},{"location":"references/data-Mapper-Mediator/#common","text":"{width=\"24\"} Constant: defines String, number or boolean constant values. {width=\"24\"} Custom Function: defines custom functions to use in the mapping. {width=\"24\"} Properties: uses product-specific runtime variables. {width=\"24\"} Global Variable: instantiates global variables that you can access from anywhere. {width=\"24\"} Compare: compares two inputs in the mapping.","title":"Common"},{"location":"references/data-Mapper-Mediator/#arithmetic","text":"{width=\"24\"} Add: adds two numbers. {width=\"24\"} Subtract: subtracts two or more numbers. {width=\"24\"} Multiply: multiplies two or more numbers. {width=\"24\"} Divide: divides two numbers. {width=\"24\"} Ceiling: derives the ceiling value of a number (closest larger integer value). {width=\"24\"} Floor: derives the floor value of a number (closest lower integer value). {width=\"24\"} Round: derives the nearest integer value. {width=\"24\"} Set Precision: formats a number into a specified length. {width=\"24\"} Absolute Value: derives the absolute value of a rational number. {width=\"24\"} Min: derives the minimum number from given inputs {width=\"24\"} Max: derives the maximum number from given inputs","title":"Arithmetic"},{"location":"references/data-Mapper-Mediator/#conditional","text":"{width=\"24\"} IfElse: uses a condition and selects one input from given two.","title":"Conditional"},{"location":"references/data-Mapper-Mediator/#boolean","text":"{width=\"24\"} AND: performs the boolean AND operation on inputs. {width=\"24\"} OR: performs the boolean OR operation on inputs. {width=\"24\"} NOT: performs the boolean NOT operation on inputs.","title":"Boolean"},{"location":"references/data-Mapper-Mediator/#type-conversion","text":"{width=\"24\"} StringToNumber: converts a String value to number (\u201c0\u201d -> 0). {width=\"24\"} StringToBoolean: converts a String value to boolean (\u201ctrue\u201d -> true). {width=\"24\"} ToString: converts a number or a boolean value to String.","title":"Type conversion"},{"location":"references/data-Mapper-Mediator/#string","text":"{width=\"24\"} Concat: concatenates two or more Strings. {width=\"24\"} Split: splits a String by a matching String value. {width=\"24\"} Uppercase: converts a String to uppercase letters. {width=\"24\"} Lowercase: converts a String to lowercase letters. {width=\"24\"} String Length: gets the length of the String. {width=\"24\"} StartsWith: checks whether a String starts with a specific value. (This is not supported in Java 7.) {width=\"24\"} EndsWith: checks whether String ends with a specific value. (This is not supported in Java 7.) {width=\"24\"} Substring: extracts a part of the String value. {width=\"24\"} Trim: removes white spaces from the beginning and end of a String. {width=\"24\"} Replace: replaces the first occurrence of a target String with another. {width=\"24\"} Match \u2013 check whether the input match with a (JS) Regular Expression","title":"String"},{"location":"references/data-Mapper-Mediator/#examples","text":"","title":"Examples"},{"location":"references/data-Mapper-Mediator/#example-1-creating-a-soap-payload-with-namespaces","text":"This example creates a Salesforce login SOAP payload using a JSON payload. The login payload consists of XML namespaces. Even though the JSON payload does not contain any namespace information, the output JSON schema will be generated with XML namespace information using the provided SOAP payload. {width=\"900\" height=\"196\"} The sample input JSON payload is as follows. { \"name\":\"Watson\", \"password\":\"watson@123\" } The sample output XML is as follows. <soapenv:Envelope xmlns:urn=\"urn:enterprise.soap.sforce.com\" xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope/\"> <soapenv:Body> <urn:login> <urn:username><b>user@domain.com</b></urn:username> <urn:password><b>secret</b></urn:password> </urn:login> </soapenv:Body> </soapenv:Envelope>","title":"Example 1 - Creating a SOAP payload with namespaces"},{"location":"references/data-Mapper-Mediator/#example-2-m-apping-soap-header-elements","text":"This example demonstrates how to map SOAP header elements along with SOAP body elements to create a certain SOAP payload, by creating a Salesforce convertLead SOAP payload using a JSON payload. The Convert Lead SOAP payload needs mapping SOAP header information. E.g. <urn:sessionId>QwWsHJyTPW.1pd0_jXlNKOSU</urn:sessionId> {width=\"900\" height=\"400\"} The sample input JSON payload is as follows. { \"owner\":{ \"ID\":\"005D0000000nVYVIA2\", \"name\":\"Smith\", \"city\":\"CA\", \"code\":\"94041\", \"country\":\"US\" }, \"lead\":{ \"ID\":\"00QD000000FP14JMAT\", \"name\":\"Carl\", \"city\":\"NC\", \"code\":\"97788\", \"country\":\"US\" }, \"sendNotificationEmail\":\"true\", \"convertedStatus\":\"Qualified\", \"doNotCreateOpportunity\":\"true\", \"opportunityName\":\"Partner Opportunity\", \"overwriteLeadSource\":\"true\", \"sessionId\":\"QwWsHJyTPW.1pd0_jXlNKOSU\" } The sample o utput XML is as follows. <?xml version=\"1.0\" encoding=\"utf-8\"?> <soapenv:Envelope xmlns:urn=\"urn:enterprise.soap.sforce.com\" xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope/\"> <soapenv:Header> <urn:SessionHeader> <urn:sessionId>QwWsHJyTPW.1pd0_jXlNKOSU</urn:sessionId> </urn:SessionHeader> </soapenv:Header> <soapenv:Body> <urn:convertLead > <urn:leadConverts> <!-- Zero or more repetitions --> <urn:convertedStatus>Qualified</urn:convertedStatus> <urn:doNotCreateOpportunity>false</urn:doNotCreateOpportunity> <urn:leadId>00QD000000FP14JMAT</urn:leadId> <urn:opportunityName>Partner Opportunity</urn:opportunityName> <urn:overwriteLeadSource>true</urn:overwriteLeadSource> <urn:ownerId>005D0000000nVYVIA2</urn:ownerId> <urn:sendNotificationEmail>true</urn:sendNotificationEmail> </urn:leadConverts> </urn:convertLead> </soapenv:Body> </soapenv:Envelope>","title":"Example 2 - M apping SOAP header elements"},{"location":"references/data-Mapper-Mediator/#example-3-mapping-primitive-types","text":"This example demonstrates how you can map an XML payload with integer, boolean etc. values, into a JSON payload with required primitive types, by specifying the required primitive type in the JSON schema. {width=\"900\"} The sample input XML payload is as follows. <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <name>app_name</name> <version>version</version> <manifest_version>2</manifest_version> <description>description_text</description> <container>GOOGLE_DRIVE</container> <api_console_project_id>YOUR_APP_ID</api_console_project_id> <gdrive_mime_types> <http://drive.google.com/intents/opendrivedoc> <type>image/png</type> <type>image/jpeg</type> <type>image/gif</type> <type>application/vnd.google.drive.ext-type.png</type> <type>application/vnd.google.drive.ext-type.jpg</type> <type>application/vnd.google.drive.ext-type.gif</type> <href>http://your_web_url/</href> <title>Open</title> <disposition>window</disposition> </http://drive.google.com/intents/opendrivedoc> </gdrive_mime_types> <icons> <128>icon_128.png</128> </icons> <app> <launch> <web_url>http://yoursite.com</web_url> </launch> </app> The sample o utput JSON is as follows. { \"name\" : \"app_name\", \"version\" : \"version\", \"manifest_version\" : 2, \"description\" : \"description_text\", \"container\" : \"GOOGLE_DRIVE\", \"api_console_project_id\" : \"YOUR_APP_ID\", \"gdrive_mime_types\": { \"http://drive.google.com/intents/opendrivedoc\": [ { \"type\": [\"image/png\", \"image/jpeg\", \"image/gif\", \"application/vnd.google.drive.ext-type.png\", \"application/vnd.google.drive.ext-type.jpg\",\"application/vnd.google.drive.ext-type.gif\"], \"href\": \"http://your_web_url/\", \"title\" : \"Open\", \"disposition\" : \"window\" } ] }, \"icons\": { \"128\": \"icon_128.png\" }, \"app\" : { \"launch\" : { \"web_url\" : \"http://yoursite.com\" } } }","title":"Example 3 - Mapping primitive types"},{"location":"references/data-Mapper-Mediator/#example-4-mapping-xml-to-csv","text":"This example demonstrates how you can map an XML payload to CSV format. Info If you specify special characters (e.g., & , &amp; ) within the <text> tag w hen converting from CSV to CSV , they will be displayed as follows by default. & -> &amp; &amp; -> &amp;amp; < -> &lt; &lt; -> &lt;lt; To avoid this and to display the exact special characters as text in the returned output, add the following properties in the Synapse configuration. <property name=\"messageType\" value=\"text/plain\" scope=\"axis2\"/> <property name=\"ContentType\" value=\"text/plain\" scope=\"axis2\"/> {width=\"900\" height=\"217\"} The sample in put XML payload is as follows. <?xml version=\"1.0\"?> <PurchaseOrder PurchaseOrderNumber=\"001\"> <Address> <Name>James Yee</Name> <Street>Downtown Bartow</Street> <City>Old Town</City> <State>PA</State> <Zip>95819</Zip> <Country>USA</Country> </Address> <Address> <Name>Elen Smith</Name> <Street>123 Maple Street</Street> <City>Mill Valley</City> <State>CA</State> <Zip>10999</Zip> <Country>USA</Country> </Address> <DeliveryNotes>Please leave packages in shed by driveway.</DeliveryNotes> </PurchaseOrder> The sample out put CSV is as follows. Name,Street,City,State,Zip,Country James Yee,Downtown Bartow,Old Town,PA,95819,USA Ellen Smith,123 Maple Street,Mill Valley,CA,10999,USA","title":"Example 4 - Mapping XML to CSV"},{"location":"references/data-Mapper-Mediator/#example-5-mapping-xsd-to-json","text":"This example demonstrates how you can map an XSD payload to JSON format. {width=\"900\"} The sample in put XSD payload is as follows. <xs:schema attributeFormDefault=\"unqualified\" elementFormDefault=\"qualified\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"> <xs:element name=\"books\"> <xs:complexType> <xs:sequence> <xs:element name=\"book\"> <xs:complexType> <xs:sequence> <xs:element type=\"xs:string\" name=\"id\"/> <xs:element type=\"xs:string\" name=\"author\"/> <xs:element type=\"xs:string\" name=\"title\"/> <xs:element type=\"xs:float\" name=\"price\"/> </xs:sequence> </xs:complexType> </xs:element> </xs:sequence> </xs:complexType> </xs:element> </xs:schema> The sample out put JSON is as follows. { \"books\": { \"book\": { \"id\": \"001\", \"author\": \"Writer\", \"title\": \"Great book on nature\", \"price\": \"44.95\" } } }","title":"Example 5 - Mapping XSD to JSON"},{"location":"references/debugging-Mediation/","text":"Debugging Mediation \u00b6 Message mediation mode is one of the operational modes of the ESB profile where the ESB profile functions as an intermediate message router. W hen operating in this mode, i t can filter, transform, drop or forward messages to an endpoint based on the given parameters. A unit of the mediation flow is a mediator. Sequences define the message mediation behavior of the ESB profile. A sequence is a series of mediators, where each mediator is a unit entity that can input a message, carry out a predefined processing task on the message, and output the message for further processing. What is debugging with respect to mediation Instant debugging using Micro Integrator Debugging with external WSO2 EI server Creating the artifact Enabling mediation debugging Information provided by the Debugger Tool Changing the property values Viewing wire logs What is debugging with respect to mediation \u00b6 Debugging is where you want to know if these units, which function as separate entities are operating as intended, or if a combination of these units are operating as a whole as intended. The ESB profile packs the Mediation debugger that enables you to debug the ESB profile message mediation flow in the server. Tooling support for the Mediation debugger is provided by the WSO2 Integration Studio Plugin which comes out of the box with WSO2 Integration Studio. First f ollow the steps below to create a sample the ESB profile artifact, which you will debug . Install the plugin and run it. For instructions, see Installing WSO2 Integration Studio . Create the ESB profile artifact using the WSO2 Integration Studio . For instructions, see WSO2 Integration Studio . There are two ways to debug a developed mediation flow. Instant debugging using Micro Integrator ( a light version of ESB profile) packaged with WSO2 Integration Studio. Deploy artifacts to an external WSO2 EI server and debug Above two approaches are discussed in detail below. Instant debugging using Micro Integrator \u00b6 When project artifacts are ready, on the project panel select the project you want to debug and click on Run > Debug . {width=\"200\"} It will ask to choose the artifacts those needs to be deployed to Embedded Micro EI server. Internally WSO2 Integration Studio will generate a CAR application with choosed artifacts and deploy. {width=\"500\"} On the console of WSO2 Integration Sturdio, notice that Micro Integrator is started with artifacts deployed. HTTP traffic is listened on 8290 port. Add some breakpoints in the flow as below. You can mark a particular mediator as a breakpoint. {width=\"800\"} Invoke the service using SOAP UI or some external client. As soon as a request comes to the proxy service fisrt brak point will hit. {width=\"800\" height=\"431\"} Note that you can view the payload that comes into the mediator and the properties that you can access on the message context. Click on \" Continue \" button. Then the message will be sent to the backend by call mediator and next breakpoint, the log mediator will hit. {width=\"800\" height=\"424\"} Note that response can be viewed on Message Envelope tab. The property set before calling the endpoint is also accessible in the context. Click on \" Continue \" button again. Response will be received by the client. {width=\"800\"} Debugging with external WSO2 EI server \u00b6 Creating the artifact \u00b6 Deploy the artifact you created on the ESB profile. For instructions, see Packaging your Artifacts into Composite Applications . Enabling mediation debugging \u00b6 Follow the steps below to enable debugging with respect to mediation. Click Run in the top menu of the WSO2 Integration Studio, and then click Debug Configurations . {width=\"200\"} Enter the details to create a new configuration as shown in the example below. You need to define two port numbers and a hostname t o connect the ESB profile with WSO2 Integration Studio in the mediation debug mode. Note that you need to specify debug mode as Remote . {width=\"500\"} Add the new configuration to debug menu as below. Then you can access the configuation easily. {width=\"500\"} Execute the following commands to s tart WSO2 EI server in the debug mode by passing a system variable at start up: On Windows: <EI_HOME>\\bin\\integrator.bat --run -Desb.debug=true On Linux/Solaris: sh <EI_HOME>/bin/ integrator .sh -Desb.debug=true Click downward arrow beside Debug in the WSO2 Integration Studio, and select the new profile above created when the Console indicates the following. !!! note You have approximately a one-minute time span to connect WSO2 Integration Studio with the EI server for the execution of the above created debug configuration. Otherwise, the EI server will stop listening and start without connecting with the debugger tool. {width=\"900\"} {width=\"200\"} In WSO2 Integration Studio, r ight-click and add breakpoints or skip points on the desired mediators to start debugging as shown in the example below. {height=\"250\"} !!! info You can add the following debugging options on the mediators using the right click context menu. - **Toggle Breakpoint:** Adds a breakpoint to the selected mediator - **Toggle Skip Point:** Adds a skip point to the selected mediator - **Resend EI Debug Points:** I f you re-start the the ESB profile, or i f you re-deploy the proxy service after changing its Synapse configuration, you need to re-send the information on breakpoints to the WSO2 EI server. T his re-s ends all registered debugging points to the EI Server. - **Delete All EI Debug Points:** Deletes all registered debug points from the EI Server and WSO2 Integration Studio. Now you can send a request to the external WSO2 EI server and debug the flow as discussed under \"Instant debugging using Micro Integrator\". Information provided by the Debugger Tool \u00b6 When your target artifact gets a request message and when the mediation flow reaches a mediator marked as a breakpoint, the message mediation process suspends at that point. A tool tip message of the suspended mediator displays the message envelope of the message payload at that point as shown in the example below . {width=\"650\"} You can view the message payload at that point of the message flow also in the Message Envelope tab as shown below. {width=\"650\"} Also, you can view the message mediation properties in the Variables view as shown in the example below. The Variable view contains properties of the following property scopes. Axis2-Client Scope Properties Axis2 Scope Properties Operation Scope Properties Synapse Scope Properties Transport Scope Properties You can have a list of selected properties out of the above, in the properties table of the Message Envelope tab, and view information on the property keys and values of them as shown below. {width=\"650\"} Click Add Property , specify the context and name of the property, and then click OK , to add that property to the properties table in the Message Envelope tab as shown below. Tip Click Clear Property , to remove a property from the properties table. {width=\"650\"} Changing the property values \u00b6 There are three operations that you can perform on message mediation property values as described below. Injecting new properties \u00b6 Follow the steps below to inject new properties to the ESB profile while debugging. Right click on the Variable view, click Inject/Clear Property , and then click Inject Property as shown below. {width=\"650\"} Enter the details about the property you prefer to add as shown in the example below. {width=\"500\"} Click OK . When the next debug point is hit, you will see the property is set to the specified context. {height=\"250\"} Clearing a property \u00b6 Follow the steps below to c lear an existing property from the the ESB profile. Right click on the Variable view, click Inject/Clear Property , and then click Clear Property as shown below. {width=\"650\"} Enter the details about the property you want to clear as shown in the example below. {width=\"500\"} Click OK . Modifying a property \u00b6 Click on the value section of the preferred property and change the value in the Variable view as shown in the example below, to modify it . {width=\"650\"} You will see the property is changed on the property view. {width=\"650\"} Viewing wire logs \u00b6 While debugging a Synapse flow, you can view the the actual HTTP messages at the entry point of the ESB profile via wire logs. For example, you can view wire logs of the incoming flow and the final response of a proxy service. Also, you can view wire logs for points, where it goes out from the ESB profile. For example, you can see the outgoing and incoming wire logs for specific mediators (i.e. Call mediator, Send mediator etc.). Wire logs are useful to troubleshoot unexpected issues, which occurr while integrating miscallaneous systems. You can use wire logs to verify whether the message payload is properly going out from the server, whether the HTTP headers such as the content-type is properly set in the outgoing message etc. Note It is recommended to enable wire logs only for troubleshooting purposes. Running productions systems with wire logs enabled is not recommended. Enabling wire logs \u00b6 The passthrough HTTP transport is the main transport, which handles HTTP/HTTPS messages in WSO2 EI. Un-comment the following entry in the <EI_HOME/conf/log4j.properties file to enable wire logs for the passthrough HTTP transport: log4j.logger.org.apache.synapse.transport.http.wire=DEBUG Info Callout mediator uses the Axis2 CommonsHTTPSender to invoke services. It does not leverage the non-blocking NHTTP/passthrough transports. Therefore, you need to add the following entries to the <EI_HOME/conf/log4j.properties file to enable wire logs for the callout mediator. log4j.logger.httpclient.wire.header=DEBUG log4j.logger.httpclient.wire.content=DEBUG Following is a sample wirelog. [2013-09-22 19:47:57,797] DEBUG - wire >> \"POST /services/StockQuoteProxy HTTP/1.1[\\r][\\n]\" [2013-09-22 19:47:57,798] DEBUG - wire >> \"Content-Type: text/xml; charset=UTF-8[\\r][\\n]\" [2013-09-22 19:47:57,798] DEBUG - wire >> \"SOAPAction: \"urn:getQuote\"[\\r][\\n]\" [2013-09-22 19:47:57,799] DEBUG - wire >> \"User-Agent: Axis2[\\r][\\n]\" [2013-09-22 19:47:57,799] DEBUG - wire >> \"Host: localhost:8280[\\r][\\n]\" [2013-09-22 19:47:57,799] DEBUG - wire >> \"Transfer-Encoding: chunked[\\r][\\n]\" [2013-09-22 19:47:57,800] DEBUG - wire >> \"[\\r][\\n]\" [2013-09-22 19:47:57,800] DEBUG - wire >> \"215[\\r][\\n]\" [2013-09-22 19:47:57,800] DEBUG - wire >> \"http://localhost:8280/services/StockQuoteProxyurn:uuid:9e1b0def-a24b-4fa2-8016-86cf3b458f67urn:getQuoteIBM[\\r][\\n]\" [2013-09-22 19:47:57,801] DEBUG - wire >> \"0[\\r][\\n]\" [2013-09-22 19:47:57,801] DEBUG - wire >> \"[\\r][\\n]\" [2013-09-22 19:47:57,846] INFO - TimeoutHandler This engine will expire all callbacks after : 120 seconds, irrespective of the timeout action, after the specified or optional timeout [2013-09-22 19:47:57,867] DEBUG - wire << \"POST /services/SimpleStockQuoteService HTTP/1.1[\\r][\\n]\" [2013-09-22 19:47:57,867] DEBUG - wire << \"Content-Type: text/xml; charset=UTF-8[\\r][\\n]\" [2013-09-22 19:47:57,867] DEBUG - wire << \"SOAPAction: \"urn:getQuote\"[\\r][\\n]\" [2013-09-22 19:47:57,867] DEBUG - wire << \"Transfer-Encoding: chunked[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"Host: localhost:9000[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"Connection: Keep-Alive[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"User-Agent: Synapse-PT-HttpComponents-NIO[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"215[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"http://localhost:8280/services/StockQuoteProxyurn:uuid:9e1b0def-a24b-4fa2-8016-86cf3b458f67urn:getQuoteIBM[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"0[\\r][\\n]\" [2013-09-22 19:47:57,869] DEBUG - wire << \"[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"HTTP/1.1 200 OK[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"Content-Type: text/xml; charset=UTF-8[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"Date: Sun, 22 Sep 2013 14:17:57 GMT[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"Transfer-Encoding: chunked[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"Connection: Keep-Alive[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"[\\r][\\n]\" [2013-09-22 19:47:58,014] DEBUG - wire << \"HTTP/1.1 200 OK[\\r][\\n]\" [2013-09-22 19:47:58,015] DEBUG - wire << \"Content-Type: text/xml; charset=UTF-8[\\r][\\n]\" [2013-09-22 19:47:58,015] DEBUG - wire << \"Date: Sun, 22 Sep 2013 14:17:58 GMT[\\r][\\n]\" [2013-09-22 19:47:58,015] DEBUG - wire << \"Server: WSO2-PassThrough-HTTP[\\r][\\n]\" [2013-09-22 19:47:58,016] DEBUG - wire << \"Transfer-Encoding: chunked[\\r][\\n]\" [2013-09-22 19:47:58,016] DEBUG - wire << \"[\\r][\\n]\" [2013-09-22 19:47:58,016] DEBUG - wire >> \"4d8[\\r][\\n]\" [2013-09-22 19:47:58,017] DEBUG - wire >> \"urn:getQuoteResponseurn:uuid:9e1b0def-a24b-4fa2-8016-86cf3b458f673.827143922330303-8.819296796724336-170.50810412063595170.73218944560944Sun Sep 22 19:47:57 IST 2013-170.472077024782785.562077973231586E7IBM Company178.0616712932281324.9438904049222641.9564266653777567195.61908401976004IBM6216[\\r][\\n]\" [2013-09-22 19:47:58,017] DEBUG - wire >> \"0[\\r][\\n]\" [2013-09-22 19:47:58,018] DEBUG - wire >> \"[\\r][\\n]\" [2013-09-22 19:47:58,021] DEBUG - wire << \"4d8[\\r][\\n]\" [2013-09-22 19:47:58,022] DEBUG - wire << \"urn:getQuoteResponseurn:uuid:9e1b0def-a24b-4fa2-8016-86cf3b458f673.827143922330303-8.819296796724336-170.50810412063595170.73218944560944Sun Sep 22 19:47:57 IST 2013-170.472077024782785.562077973231586E7IBM Company178.0616712932281324.9438904049222641.9564266653777567195.61908401976004IBM6216[\\r][\\n]\" [2013-09-22 19:47:58,022] DEBUG - wire << \"0[\\r][\\n]\" [2013-09-22 19:47:58,022] DEBUG - wire << \"[\\r][\\n] There are two incoming messages and two outgoing messages in the above log. First part of the wire logs of a message contains the HTTP headers and it is followed by the message payload. You need to identify the message direction as shown below to read wire logs. DEBUG - wire >> - This represents a message, which is coming into WSO2 EI from the wire DEBUG - wire << - This represents a message, which goes to the wire from WSO2 EI Viewing wire logs of a specific mediator \u00b6 You need to put a debug point to the mediator, to view wire logs of it. When debugging is finished (or while debugging), right click on the mediator, and click Show WireLogs , to view wire logs for a specific mediator. Info You can only view wire logs for a whole proxy service, call mediator, send mediator, or other API resources . However, you cannot view a wire log of a Synapse config (e.g. sequences), because there would not be anything written to wire, when the flow comes to the sequence etc. Hence, you can only view them in wire entry points. {width=\"400\"} Viewing wire logs while debugging \u00b6 If you view wire logs while debugging, you view only the wire logs of mediators, whose execution is already completed as shown in the example below. {width=\"800\"} Viewing wire logs of a mediator after debugging execution of it \u00b6 When you view wire logs of a mediator (e.g. send mediator) after debugging, you can view the request and response wire logs as shown in the example below. {width=\"800\"} Viewing wire logs of a proxy service after debugging \u00b6 If you view wire logs of a proxy service after debugging finished, you view the request wire log and final response wire log of that proxy as shown in the example below. {width=\"800\"}","title":"Debugging Mediation"},{"location":"references/debugging-Mediation/#debugging-mediation","text":"Message mediation mode is one of the operational modes of the ESB profile where the ESB profile functions as an intermediate message router. W hen operating in this mode, i t can filter, transform, drop or forward messages to an endpoint based on the given parameters. A unit of the mediation flow is a mediator. Sequences define the message mediation behavior of the ESB profile. A sequence is a series of mediators, where each mediator is a unit entity that can input a message, carry out a predefined processing task on the message, and output the message for further processing. What is debugging with respect to mediation Instant debugging using Micro Integrator Debugging with external WSO2 EI server Creating the artifact Enabling mediation debugging Information provided by the Debugger Tool Changing the property values Viewing wire logs","title":"Debugging Mediation"},{"location":"references/debugging-Mediation/#what-is-debugging-with-respect-to-mediation","text":"Debugging is where you want to know if these units, which function as separate entities are operating as intended, or if a combination of these units are operating as a whole as intended. The ESB profile packs the Mediation debugger that enables you to debug the ESB profile message mediation flow in the server. Tooling support for the Mediation debugger is provided by the WSO2 Integration Studio Plugin which comes out of the box with WSO2 Integration Studio. First f ollow the steps below to create a sample the ESB profile artifact, which you will debug . Install the plugin and run it. For instructions, see Installing WSO2 Integration Studio . Create the ESB profile artifact using the WSO2 Integration Studio . For instructions, see WSO2 Integration Studio . There are two ways to debug a developed mediation flow. Instant debugging using Micro Integrator ( a light version of ESB profile) packaged with WSO2 Integration Studio. Deploy artifacts to an external WSO2 EI server and debug Above two approaches are discussed in detail below.","title":"What is debugging with respect to mediation"},{"location":"references/debugging-Mediation/#instant-debugging-using-micro-integrator","text":"When project artifacts are ready, on the project panel select the project you want to debug and click on Run > Debug . {width=\"200\"} It will ask to choose the artifacts those needs to be deployed to Embedded Micro EI server. Internally WSO2 Integration Studio will generate a CAR application with choosed artifacts and deploy. {width=\"500\"} On the console of WSO2 Integration Sturdio, notice that Micro Integrator is started with artifacts deployed. HTTP traffic is listened on 8290 port. Add some breakpoints in the flow as below. You can mark a particular mediator as a breakpoint. {width=\"800\"} Invoke the service using SOAP UI or some external client. As soon as a request comes to the proxy service fisrt brak point will hit. {width=\"800\" height=\"431\"} Note that you can view the payload that comes into the mediator and the properties that you can access on the message context. Click on \" Continue \" button. Then the message will be sent to the backend by call mediator and next breakpoint, the log mediator will hit. {width=\"800\" height=\"424\"} Note that response can be viewed on Message Envelope tab. The property set before calling the endpoint is also accessible in the context. Click on \" Continue \" button again. Response will be received by the client. {width=\"800\"}","title":"Instant debugging using Micro Integrator"},{"location":"references/debugging-Mediation/#debugging-with-external-wso2-ei-server","text":"","title":"Debugging with external WSO2 EI server"},{"location":"references/debugging-Mediation/#creating-the-artifact","text":"Deploy the artifact you created on the ESB profile. For instructions, see Packaging your Artifacts into Composite Applications .","title":"Creating the artifact"},{"location":"references/debugging-Mediation/#enabling-mediation-debugging","text":"Follow the steps below to enable debugging with respect to mediation. Click Run in the top menu of the WSO2 Integration Studio, and then click Debug Configurations . {width=\"200\"} Enter the details to create a new configuration as shown in the example below. You need to define two port numbers and a hostname t o connect the ESB profile with WSO2 Integration Studio in the mediation debug mode. Note that you need to specify debug mode as Remote . {width=\"500\"} Add the new configuration to debug menu as below. Then you can access the configuation easily. {width=\"500\"} Execute the following commands to s tart WSO2 EI server in the debug mode by passing a system variable at start up: On Windows: <EI_HOME>\\bin\\integrator.bat --run -Desb.debug=true On Linux/Solaris: sh <EI_HOME>/bin/ integrator .sh -Desb.debug=true Click downward arrow beside Debug in the WSO2 Integration Studio, and select the new profile above created when the Console indicates the following. !!! note You have approximately a one-minute time span to connect WSO2 Integration Studio with the EI server for the execution of the above created debug configuration. Otherwise, the EI server will stop listening and start without connecting with the debugger tool. {width=\"900\"} {width=\"200\"} In WSO2 Integration Studio, r ight-click and add breakpoints or skip points on the desired mediators to start debugging as shown in the example below. {height=\"250\"} !!! info You can add the following debugging options on the mediators using the right click context menu. - **Toggle Breakpoint:** Adds a breakpoint to the selected mediator - **Toggle Skip Point:** Adds a skip point to the selected mediator - **Resend EI Debug Points:** I f you re-start the the ESB profile, or i f you re-deploy the proxy service after changing its Synapse configuration, you need to re-send the information on breakpoints to the WSO2 EI server. T his re-s ends all registered debugging points to the EI Server. - **Delete All EI Debug Points:** Deletes all registered debug points from the EI Server and WSO2 Integration Studio. Now you can send a request to the external WSO2 EI server and debug the flow as discussed under \"Instant debugging using Micro Integrator\".","title":"Enabling mediation debugging"},{"location":"references/debugging-Mediation/#information-provided-by-the-debugger-tool","text":"When your target artifact gets a request message and when the mediation flow reaches a mediator marked as a breakpoint, the message mediation process suspends at that point. A tool tip message of the suspended mediator displays the message envelope of the message payload at that point as shown in the example below . {width=\"650\"} You can view the message payload at that point of the message flow also in the Message Envelope tab as shown below. {width=\"650\"} Also, you can view the message mediation properties in the Variables view as shown in the example below. The Variable view contains properties of the following property scopes. Axis2-Client Scope Properties Axis2 Scope Properties Operation Scope Properties Synapse Scope Properties Transport Scope Properties You can have a list of selected properties out of the above, in the properties table of the Message Envelope tab, and view information on the property keys and values of them as shown below. {width=\"650\"} Click Add Property , specify the context and name of the property, and then click OK , to add that property to the properties table in the Message Envelope tab as shown below. Tip Click Clear Property , to remove a property from the properties table. {width=\"650\"}","title":"Information provided by the Debugger Tool"},{"location":"references/debugging-Mediation/#changing-the-property-values","text":"There are three operations that you can perform on message mediation property values as described below.","title":"Changing the property values"},{"location":"references/debugging-Mediation/#injecting-new-properties","text":"Follow the steps below to inject new properties to the ESB profile while debugging. Right click on the Variable view, click Inject/Clear Property , and then click Inject Property as shown below. {width=\"650\"} Enter the details about the property you prefer to add as shown in the example below. {width=\"500\"} Click OK . When the next debug point is hit, you will see the property is set to the specified context. {height=\"250\"}","title":"Injecting new properties"},{"location":"references/debugging-Mediation/#clearing-a-property","text":"Follow the steps below to c lear an existing property from the the ESB profile. Right click on the Variable view, click Inject/Clear Property , and then click Clear Property as shown below. {width=\"650\"} Enter the details about the property you want to clear as shown in the example below. {width=\"500\"} Click OK .","title":"Clearing a property"},{"location":"references/debugging-Mediation/#modifying-a-property","text":"Click on the value section of the preferred property and change the value in the Variable view as shown in the example below, to modify it . {width=\"650\"} You will see the property is changed on the property view. {width=\"650\"}","title":"Modifying a property"},{"location":"references/debugging-Mediation/#viewing-wire-logs","text":"While debugging a Synapse flow, you can view the the actual HTTP messages at the entry point of the ESB profile via wire logs. For example, you can view wire logs of the incoming flow and the final response of a proxy service. Also, you can view wire logs for points, where it goes out from the ESB profile. For example, you can see the outgoing and incoming wire logs for specific mediators (i.e. Call mediator, Send mediator etc.). Wire logs are useful to troubleshoot unexpected issues, which occurr while integrating miscallaneous systems. You can use wire logs to verify whether the message payload is properly going out from the server, whether the HTTP headers such as the content-type is properly set in the outgoing message etc. Note It is recommended to enable wire logs only for troubleshooting purposes. Running productions systems with wire logs enabled is not recommended.","title":"Viewing wire logs"},{"location":"references/debugging-Mediation/#enabling-wire-logs","text":"The passthrough HTTP transport is the main transport, which handles HTTP/HTTPS messages in WSO2 EI. Un-comment the following entry in the <EI_HOME/conf/log4j.properties file to enable wire logs for the passthrough HTTP transport: log4j.logger.org.apache.synapse.transport.http.wire=DEBUG Info Callout mediator uses the Axis2 CommonsHTTPSender to invoke services. It does not leverage the non-blocking NHTTP/passthrough transports. Therefore, you need to add the following entries to the <EI_HOME/conf/log4j.properties file to enable wire logs for the callout mediator. log4j.logger.httpclient.wire.header=DEBUG log4j.logger.httpclient.wire.content=DEBUG Following is a sample wirelog. [2013-09-22 19:47:57,797] DEBUG - wire >> \"POST /services/StockQuoteProxy HTTP/1.1[\\r][\\n]\" [2013-09-22 19:47:57,798] DEBUG - wire >> \"Content-Type: text/xml; charset=UTF-8[\\r][\\n]\" [2013-09-22 19:47:57,798] DEBUG - wire >> \"SOAPAction: \"urn:getQuote\"[\\r][\\n]\" [2013-09-22 19:47:57,799] DEBUG - wire >> \"User-Agent: Axis2[\\r][\\n]\" [2013-09-22 19:47:57,799] DEBUG - wire >> \"Host: localhost:8280[\\r][\\n]\" [2013-09-22 19:47:57,799] DEBUG - wire >> \"Transfer-Encoding: chunked[\\r][\\n]\" [2013-09-22 19:47:57,800] DEBUG - wire >> \"[\\r][\\n]\" [2013-09-22 19:47:57,800] DEBUG - wire >> \"215[\\r][\\n]\" [2013-09-22 19:47:57,800] DEBUG - wire >> \"http://localhost:8280/services/StockQuoteProxyurn:uuid:9e1b0def-a24b-4fa2-8016-86cf3b458f67urn:getQuoteIBM[\\r][\\n]\" [2013-09-22 19:47:57,801] DEBUG - wire >> \"0[\\r][\\n]\" [2013-09-22 19:47:57,801] DEBUG - wire >> \"[\\r][\\n]\" [2013-09-22 19:47:57,846] INFO - TimeoutHandler This engine will expire all callbacks after : 120 seconds, irrespective of the timeout action, after the specified or optional timeout [2013-09-22 19:47:57,867] DEBUG - wire << \"POST /services/SimpleStockQuoteService HTTP/1.1[\\r][\\n]\" [2013-09-22 19:47:57,867] DEBUG - wire << \"Content-Type: text/xml; charset=UTF-8[\\r][\\n]\" [2013-09-22 19:47:57,867] DEBUG - wire << \"SOAPAction: \"urn:getQuote\"[\\r][\\n]\" [2013-09-22 19:47:57,867] DEBUG - wire << \"Transfer-Encoding: chunked[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"Host: localhost:9000[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"Connection: Keep-Alive[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"User-Agent: Synapse-PT-HttpComponents-NIO[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"215[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"http://localhost:8280/services/StockQuoteProxyurn:uuid:9e1b0def-a24b-4fa2-8016-86cf3b458f67urn:getQuoteIBM[\\r][\\n]\" [2013-09-22 19:47:57,868] DEBUG - wire << \"0[\\r][\\n]\" [2013-09-22 19:47:57,869] DEBUG - wire << \"[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"HTTP/1.1 200 OK[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"Content-Type: text/xml; charset=UTF-8[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"Date: Sun, 22 Sep 2013 14:17:57 GMT[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"Transfer-Encoding: chunked[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"Connection: Keep-Alive[\\r][\\n]\" [2013-09-22 19:47:58,002] DEBUG - wire >> \"[\\r][\\n]\" [2013-09-22 19:47:58,014] DEBUG - wire << \"HTTP/1.1 200 OK[\\r][\\n]\" [2013-09-22 19:47:58,015] DEBUG - wire << \"Content-Type: text/xml; charset=UTF-8[\\r][\\n]\" [2013-09-22 19:47:58,015] DEBUG - wire << \"Date: Sun, 22 Sep 2013 14:17:58 GMT[\\r][\\n]\" [2013-09-22 19:47:58,015] DEBUG - wire << \"Server: WSO2-PassThrough-HTTP[\\r][\\n]\" [2013-09-22 19:47:58,016] DEBUG - wire << \"Transfer-Encoding: chunked[\\r][\\n]\" [2013-09-22 19:47:58,016] DEBUG - wire << \"[\\r][\\n]\" [2013-09-22 19:47:58,016] DEBUG - wire >> \"4d8[\\r][\\n]\" [2013-09-22 19:47:58,017] DEBUG - wire >> \"urn:getQuoteResponseurn:uuid:9e1b0def-a24b-4fa2-8016-86cf3b458f673.827143922330303-8.819296796724336-170.50810412063595170.73218944560944Sun Sep 22 19:47:57 IST 2013-170.472077024782785.562077973231586E7IBM Company178.0616712932281324.9438904049222641.9564266653777567195.61908401976004IBM6216[\\r][\\n]\" [2013-09-22 19:47:58,017] DEBUG - wire >> \"0[\\r][\\n]\" [2013-09-22 19:47:58,018] DEBUG - wire >> \"[\\r][\\n]\" [2013-09-22 19:47:58,021] DEBUG - wire << \"4d8[\\r][\\n]\" [2013-09-22 19:47:58,022] DEBUG - wire << \"urn:getQuoteResponseurn:uuid:9e1b0def-a24b-4fa2-8016-86cf3b458f673.827143922330303-8.819296796724336-170.50810412063595170.73218944560944Sun Sep 22 19:47:57 IST 2013-170.472077024782785.562077973231586E7IBM Company178.0616712932281324.9438904049222641.9564266653777567195.61908401976004IBM6216[\\r][\\n]\" [2013-09-22 19:47:58,022] DEBUG - wire << \"0[\\r][\\n]\" [2013-09-22 19:47:58,022] DEBUG - wire << \"[\\r][\\n] There are two incoming messages and two outgoing messages in the above log. First part of the wire logs of a message contains the HTTP headers and it is followed by the message payload. You need to identify the message direction as shown below to read wire logs. DEBUG - wire >> - This represents a message, which is coming into WSO2 EI from the wire DEBUG - wire << - This represents a message, which goes to the wire from WSO2 EI","title":"Enabling wire logs"},{"location":"references/debugging-Mediation/#viewing-wire-logs-of-a-specific-mediator","text":"You need to put a debug point to the mediator, to view wire logs of it. When debugging is finished (or while debugging), right click on the mediator, and click Show WireLogs , to view wire logs for a specific mediator. Info You can only view wire logs for a whole proxy service, call mediator, send mediator, or other API resources . However, you cannot view a wire log of a Synapse config (e.g. sequences), because there would not be anything written to wire, when the flow comes to the sequence etc. Hence, you can only view them in wire entry points. {width=\"400\"}","title":"Viewing wire logs of a specific mediator"},{"location":"references/debugging-Mediation/#viewing-wire-logs-while-debugging","text":"If you view wire logs while debugging, you view only the wire logs of mediators, whose execution is already completed as shown in the example below. {width=\"800\"}","title":"Viewing wire logs while debugging"},{"location":"references/debugging-Mediation/#viewing-wire-logs-of-a-mediator-after-debugging-execution-of-it","text":"When you view wire logs of a mediator (e.g. send mediator) after debugging, you can view the request and response wire logs as shown in the example below. {width=\"800\"}","title":"Viewing wire logs of a mediator\u00a0after debugging execution of it"},{"location":"references/debugging-Mediation/#viewing-wire-logs-of-a-proxy-service-after-debugging","text":"If you view wire logs of a proxy service after debugging finished, you view the request wire log and final response wire log of that proxy as shown in the example below. {width=\"800\"}","title":"Viewing wire logs of a proxy service after debugging"},{"location":"references/deleting-a-Priority-Executor/","text":"Deleting a Priority Executor \u00b6 Info Note Please note that this feature is deprecated. can be used to execute sequences with a given priority. This allows user to control the resources allocated to executing sequences and prevent high priority messages from getting delayed and dropped. If there is no need to use the existing priority order, you can delete a priority executor in WSO2 EI. Follow the instructions below to delete a priority executor from the WSO2 EI. 1. Sign in. Enter your user name and password to log on to the EI Management Console. 2. Click on \"Main\" in the left menu to access the \"Manage\" menu. 3. In the \"Manage\" menu, click on \"Priority Executors\" under \"Service Bus.\" 4. In the \"Priority Executors\" window, click on the \"Delete\" link in the \"Actions\" column. 5. Confirm your request in the \"WSO2 Carbon\" window.","title":"Deleting a Priority Executor"},{"location":"references/deleting-a-Priority-Executor/#deleting-a-priority-executor","text":"Info Note Please note that this feature is deprecated. can be used to execute sequences with a given priority. This allows user to control the resources allocated to executing sequences and prevent high priority messages from getting delayed and dropped. If there is no need to use the existing priority order, you can delete a priority executor in WSO2 EI. Follow the instructions below to delete a priority executor from the WSO2 EI. 1. Sign in. Enter your user name and password to log on to the EI Management Console. 2. Click on \"Main\" in the left menu to access the \"Manage\" menu. 3. In the \"Manage\" menu, click on \"Priority Executors\" under \"Service Bus.\" 4. In the \"Priority Executors\" window, click on the \"Delete\" link in the \"Actions\" column. 5. Confirm your request in the \"WSO2 Carbon\" window.","title":"Deleting a Priority Executor"},{"location":"references/drop-Mediator/","text":"Drop Mediator \u00b6 The Drop Mediator stops the processing of the current message . This mediator is useful for ensuring that the message is sent only once and then dropped by the ESB profile . If you have any mediators defined after the <drop/> element, they will not be executed, because <drop/> is considered to be the end of the message flow. When the Drop mediator is within the In sequence, it sends an HTTP 202 Accepted response to the client when it stops the message flow. When the Drop mediator is within the Out sequence before the Send mediator, no response is sent to the client. Info The Drop mediator is a content-unaware mediator. Syntax | Configuration | Example Syntax \u00b6 The drop token refers to a \\< drop > element, which is used to stop further processing of a message: <drop/> Configuration \u00b6 As with other mediators, after adding the drop mediator to a sequence, you can click its up and down arrows to move its location in the sequence. Example \u00b6 You can use the drop mediator for messages that do not meet the filter criteria in case the client is waiting for a response to ensure the message was received by the ESB profile . For example: html/xml <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"main\"> <in> <!-- filtering of messages with XPath and regex matches --> <filter source=\"get-property('To')\" regex=\".*/StockQuote.*\"> <then> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </then> <else> <drop/> </else> </filter> ... In this scenario, if the message doesn't meet the filter condition, it is dropped, and the HTTP 202 Accepted response is sent to the client. If you did not include the drop mediator, the client would not receive any response.","title":"Drop Mediator"},{"location":"references/drop-Mediator/#drop-mediator","text":"The Drop Mediator stops the processing of the current message . This mediator is useful for ensuring that the message is sent only once and then dropped by the ESB profile . If you have any mediators defined after the <drop/> element, they will not be executed, because <drop/> is considered to be the end of the message flow. When the Drop mediator is within the In sequence, it sends an HTTP 202 Accepted response to the client when it stops the message flow. When the Drop mediator is within the Out sequence before the Send mediator, no response is sent to the client. Info The Drop mediator is a content-unaware mediator. Syntax | Configuration | Example","title":"Drop Mediator"},{"location":"references/drop-Mediator/#syntax","text":"The drop token refers to a \\< drop > element, which is used to stop further processing of a message: <drop/>","title":"Syntax"},{"location":"references/drop-Mediator/#configuration","text":"As with other mediators, after adding the drop mediator to a sequence, you can click its up and down arrows to move its location in the sequence.","title":"Configuration"},{"location":"references/drop-Mediator/#example","text":"You can use the drop mediator for messages that do not meet the filter criteria in case the client is waiting for a response to ensure the message was received by the ESB profile . For example: html/xml <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"main\"> <in> <!-- filtering of messages with XPath and regex matches --> <filter source=\"get-property('To')\" regex=\".*/StockQuote.*\"> <then> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </then> <else> <drop/> </else> </filter> ... In this scenario, if the message doesn't meet the filter condition, it is dropped, and the HTTP 202 Accepted response is sent to the client. If you did not include the drop mediator, the client would not receive any response.","title":"Example"},{"location":"references/eJB-Mediator/","text":"EJB Mediator \u00b6 The EJB mediator calls an external Enterprise JavaBean(EJB) and stores the result in the message payload or in a message context property. Currently, this mediator supports EJB3 Stateless Session Beans and Stateful Session Beans. Info The EJB mediator is a content-aware mediator. Syntax | Configuration | Example Syntax \u00b6 <ejb beanstalk=\"string\" class=\"string\" [sessionId=\"string\"] [remove=\"true | false\"] [method=\"string\"] [target=\"string | {xpath}\"] [jndiName=\"string\"] /> <args> <arg (value=\"string | {xpath}\")/>* </args> </ejb> Configuration \u00b6 Parameter Name Description Beanstalk ID Reference to the application server specific connection source information, which is defined at the synapse.properties. Class This required the remote interface definition provided in the EJB 3.0 (EJB service invocation remote/home interface). Session ID When the EJB context is invoked in the form state-full bean then the related ejb session status specified will be stored in here. Possible values are as follows. Value : If this is selected, the session ID can be entered as a static value. Expression : If this is selected, an XPath expression can be entered to evaluate the session ID. Remove This parameter specifies whether the Enterprise Entity Manager should remove the EJB context related parameters once the state full/stateless session is invoked. Target If a particular EJB method returns, then the return object can be saved against the the name provided in the target at the synapse property context. JNDI Name The Java Naming and Directory Interface (JNDI) is an application programming interface (API) for accessing different kinds of naming and directory services. JNDI is not specific to a particular naming or directory service. It can be used to access many different kinds of systems including file systems; distributed objects systems such as CORBA, Java RMI, and EJB; and directory services such as LDAP, Novell NetWare, and NIS+. Add Argument Can be used to define the arguments which is required for the particular ejb method to be invoked Expression/Value. Info Tip You can click the \"Namespaces\" link to add namespaces if you are providing an expression. You will be provided another panel named \"Namespace Editor\" where you can provide any number of namespace prefixes and the URL used in the XPath expression. Example \u00b6 <ejb beanstalk=\"jack\" class=\"org.ejb.wso2.test.StoreRegister\" method=\"getStoreById\" target=\"store\" jndiName=\"ejb:/EJBDemo/StoreRegsiterBean!org.ejb.wso2.test.StoreRegister\"> <args> <arg xmlns:ns=\"http://org.apache.synapse/xsd\" xmlns:ns3=\"http://org.apache.synapse/xsd\" value=\"{get-property('loc_id')}\"/> </args> </ejb> In this example, the EJB Mediator does the EJB service invocation by calling getStoreById pubished at the application server and exposed via ejb:/EJBDemo/StoreRegsiterBean!org.ejb.wso2.test.StoreRegister , then response will be assigned to the target specified (variable/expression).","title":"EJB Mediator"},{"location":"references/eJB-Mediator/#ejb-mediator","text":"The EJB mediator calls an external Enterprise JavaBean(EJB) and stores the result in the message payload or in a message context property. Currently, this mediator supports EJB3 Stateless Session Beans and Stateful Session Beans. Info The EJB mediator is a content-aware mediator. Syntax | Configuration | Example","title":"EJB Mediator"},{"location":"references/eJB-Mediator/#syntax","text":"<ejb beanstalk=\"string\" class=\"string\" [sessionId=\"string\"] [remove=\"true | false\"] [method=\"string\"] [target=\"string | {xpath}\"] [jndiName=\"string\"] /> <args> <arg (value=\"string | {xpath}\")/>* </args> </ejb>","title":"Syntax"},{"location":"references/eJB-Mediator/#configuration","text":"Parameter Name Description Beanstalk ID Reference to the application server specific connection source information, which is defined at the synapse.properties. Class This required the remote interface definition provided in the EJB 3.0 (EJB service invocation remote/home interface). Session ID When the EJB context is invoked in the form state-full bean then the related ejb session status specified will be stored in here. Possible values are as follows. Value : If this is selected, the session ID can be entered as a static value. Expression : If this is selected, an XPath expression can be entered to evaluate the session ID. Remove This parameter specifies whether the Enterprise Entity Manager should remove the EJB context related parameters once the state full/stateless session is invoked. Target If a particular EJB method returns, then the return object can be saved against the the name provided in the target at the synapse property context. JNDI Name The Java Naming and Directory Interface (JNDI) is an application programming interface (API) for accessing different kinds of naming and directory services. JNDI is not specific to a particular naming or directory service. It can be used to access many different kinds of systems including file systems; distributed objects systems such as CORBA, Java RMI, and EJB; and directory services such as LDAP, Novell NetWare, and NIS+. Add Argument Can be used to define the arguments which is required for the particular ejb method to be invoked Expression/Value. Info Tip You can click the \"Namespaces\" link to add namespaces if you are providing an expression. You will be provided another panel named \"Namespace Editor\" where you can provide any number of namespace prefixes and the URL used in the XPath expression.","title":"Configuration"},{"location":"references/eJB-Mediator/#example","text":"<ejb beanstalk=\"jack\" class=\"org.ejb.wso2.test.StoreRegister\" method=\"getStoreById\" target=\"store\" jndiName=\"ejb:/EJBDemo/StoreRegsiterBean!org.ejb.wso2.test.StoreRegister\"> <args> <arg xmlns:ns=\"http://org.apache.synapse/xsd\" xmlns:ns3=\"http://org.apache.synapse/xsd\" value=\"{get-property('loc_id')}\"/> </args> </ejb> In this example, the EJB Mediator does the EJB service invocation by calling getStoreById pubished at the application server and exposed via ejb:/EJBDemo/StoreRegsiterBean!org.ejb.wso2.test.StoreRegister , then response will be assigned to the target specified (variable/expression).","title":"Example"},{"location":"references/eSB-Mediators/","text":"ESB Mediators \u00b6 A mediator is the basic message processing unit and a fundamental part the ESB profile . A mediator can take a message, carry out some predefined actions on it, and output the modified message. For example, the Clone mediator splits a message into several clones, the Send mediator sends the messages, and the Aggregate mediator collects and merges the responses before sending them back to the client. The ESB profile ships with a range of mediators capable of carrying out various tasks on input messages, including functionality to match incompatible protocols, data formats and interaction patterns across different resources. Data can be split, cloned, aggregated, and enriched, allowing the ESB profile to match the different capabilities of services. XQuery and XSLT allow rich transformations on the messages. Rule-based message mediation allows users to cope with the uncertainty of business logic. Content-based routing using XPath filtering is supported in different flavors, allowing users to get the most convenient configuration experience. Built-in capability to handle Transactions allows message mediation to be done transactionally inside the ESB profile . With the eventing capabilities of the ESB profile , EDA based components can be easily interconnected, allowing the ESB profile to be used in the front-end of an organisation's SOA infrastructure. A mediator is a full-powered processing unit in the ESB profile of WSO2 EI. At run-time, a mediator has access to all the parts of the ESB profile along with the current message and can do virtually anything with the message. At the run-time, a message is injected in to the mediator with the ESB profile information. Then this mediator can do virtually anything with the message. A user can write a mediator and put it into the ESB profile . This custom mediator and any other built-in mediator will be exactly the same as the API and the privileges (Refer to more information in Creating Custom Mediators ). A mediation sequence , commonly called a \"sequence\", is a list of mediators. That means, it can hold other mediators and execute them. It is part of the ESB profile 's core and message mediation cannot live without this mediator. When a message is delivered to a sequence, it sends the message through all its child mediators. For more information, see Mediation Sequences . The Process of message mediation is depicted in the diagram below. {width=\"600\"} In case an error occurs in the main sequence while processing, the message goes to the fault sequence. {width=\"600\"} When adding a mediator to a sequence , you can configure the mediator in design view or in source view. Usually, a mediator is configured using XML. Source view provides the XML representation of the configurations done in the UI. When you edit the source XML all changes immediately reflect in the design view as well. Each mediator has its own XML configuration. You can change the following mediators using their source view: Aggregate Mediator Cache Mediator Filter Mediator In Mediator Iterate Mediator Out Mediator Sequence Mediator Synapse Mediator Template Mediator Validate Mediator Tip It is possible to comment out lines of code in the Synapse definition as well as in the source view of the complete EI configuration. Mediators in a sequence can be one of the following types: Node mediators - Contains child mediators. Leaf mediators - Does not hold any other child mediators. Mediators are classified as follows based on whether or not they access the message's content: Content-aware mediators : These mediators always access the message content when mediating messages (e.g., Enrich mediator ). Content-unaware mediators : These mediators never access the message content when mediating messages (e.g., Send mediator ). Conditionally content-aware mediators : These mediators could be either content-aware or content-unaware depending on their exact instance configuration. For an example a simple Log Mediator instance (i.e. configured as <log/> ) is content-unaware. However a log mediator configured as <log level=\u201dfull\u201d/> would be content-aware since it is expected to log the message payload. Mediators are considered to be one of the main mechanisms for extending an EI. You can create custom mediators and add them to the ESB profile . This custom mediator and any other built-in mediator will be exactly the same as the API and the privileges. The standard mediators in the ESB profile are listed in the table below. Click a link for details on that mediator. There are also many samples that demonstrate how to use mediators. The WSO2 EI mediator catalog \u00b6 Category Name Description Core Call Invoke a service in non blocking synchronous manner Enqueue (deprecated) Uses a priority executor to ensure high-priority messages are not dropped Send Sends a message Loopback Moves the message from the In flow to the Out flow, skipping all remaining configuration in the In flow Sequence Inserts a reference to a sequence Respond Stops processing on the message and sends it back to the client Event Sends event notifications to an event source, publishes messages to predefined topics Drop Drops a message Call Template Constructs a sequence by passing values into a sequence template Enrich Enriches a message Property Sets or remove properties associated with the message Log Logs a message Filter Filter Filters a message using XPath, if-else kind of logic Out (deprecated) Applies to messages that are in the Out path of the ESB profile In (deprecated) Applies to messages that are in the In path of the ESB profile Validate Validates XML messages against a specified schema. Switch Filters messages using XPath, switch logic Conditional Router (deprecated) Implements complex routing rules (Header based routing, content based routing and other rules) Transform XSLT Performs XSLT transformations on the XML payload FastXSLT Performs XSLT transformations on the message stream URLRewrite Modifies and rewrites URLs or URL fragments XQuery Performs XQuery transformation Header Sets or removes SOAP headers Fault (also called Makefault) Create SOAP Faults PayloadFactory Transforms or replaces message content in between the client and the backend server Advanced Cache Evaluates messages based on whether the same message came to the ESB profile ForEach Splits a message into a number of different messages by finding matching elements in an XPath expression of the original message. Clone Clones a message Store Stores messages in a predefined message store Iterate Splits a message Aggregate Combines a message Callout Blocks web services calls Transaction Executes a set of mediators transactionally Throttle Limits the number of messages DBReport Writes data to database DBLookup Retrieves information from database EJB Calls an external Enterprise JavaBean(EJB) and stores the result in the message payload or in a message context property. Rule Executes rules Builder Builds the actual SOAP message, from a message, which is coming into the ESB profile through the Binary Relay. Entitlement Evaluates user actions against a XACML policy OAuth 2-legged OAuth support Smooks Used to apply lightweight transformations on messages in an efficient manner. Data Mapper Converts and transforms one data format to another, or changes the structure of the data in a message. Extension Bean (deprecated) Manipulates JavaBeans Class Creates and executes a custom mediator POJOCommand (deprecated) Executes a custom command Script Executes a mediator written in Scripting language Spring (deprecated) Creates a mediator managed by Spring Agent Publish Event Constructs events and publishes them to different systems such as WSO2 BAM/DAS/CEP/SP via event sinks.","title":"Clone Mediator"},{"location":"references/eSB-Mediators/#esb-mediators","text":"A mediator is the basic message processing unit and a fundamental part the ESB profile . A mediator can take a message, carry out some predefined actions on it, and output the modified message. For example, the Clone mediator splits a message into several clones, the Send mediator sends the messages, and the Aggregate mediator collects and merges the responses before sending them back to the client. The ESB profile ships with a range of mediators capable of carrying out various tasks on input messages, including functionality to match incompatible protocols, data formats and interaction patterns across different resources. Data can be split, cloned, aggregated, and enriched, allowing the ESB profile to match the different capabilities of services. XQuery and XSLT allow rich transformations on the messages. Rule-based message mediation allows users to cope with the uncertainty of business logic. Content-based routing using XPath filtering is supported in different flavors, allowing users to get the most convenient configuration experience. Built-in capability to handle Transactions allows message mediation to be done transactionally inside the ESB profile . With the eventing capabilities of the ESB profile , EDA based components can be easily interconnected, allowing the ESB profile to be used in the front-end of an organisation's SOA infrastructure. A mediator is a full-powered processing unit in the ESB profile of WSO2 EI. At run-time, a mediator has access to all the parts of the ESB profile along with the current message and can do virtually anything with the message. At the run-time, a message is injected in to the mediator with the ESB profile information. Then this mediator can do virtually anything with the message. A user can write a mediator and put it into the ESB profile . This custom mediator and any other built-in mediator will be exactly the same as the API and the privileges (Refer to more information in Creating Custom Mediators ). A mediation sequence , commonly called a \"sequence\", is a list of mediators. That means, it can hold other mediators and execute them. It is part of the ESB profile 's core and message mediation cannot live without this mediator. When a message is delivered to a sequence, it sends the message through all its child mediators. For more information, see Mediation Sequences . The Process of message mediation is depicted in the diagram below. {width=\"600\"} In case an error occurs in the main sequence while processing, the message goes to the fault sequence. {width=\"600\"} When adding a mediator to a sequence , you can configure the mediator in design view or in source view. Usually, a mediator is configured using XML. Source view provides the XML representation of the configurations done in the UI. When you edit the source XML all changes immediately reflect in the design view as well. Each mediator has its own XML configuration. You can change the following mediators using their source view: Aggregate Mediator Cache Mediator Filter Mediator In Mediator Iterate Mediator Out Mediator Sequence Mediator Synapse Mediator Template Mediator Validate Mediator Tip It is possible to comment out lines of code in the Synapse definition as well as in the source view of the complete EI configuration. Mediators in a sequence can be one of the following types: Node mediators - Contains child mediators. Leaf mediators - Does not hold any other child mediators. Mediators are classified as follows based on whether or not they access the message's content: Content-aware mediators : These mediators always access the message content when mediating messages (e.g., Enrich mediator ). Content-unaware mediators : These mediators never access the message content when mediating messages (e.g., Send mediator ). Conditionally content-aware mediators : These mediators could be either content-aware or content-unaware depending on their exact instance configuration. For an example a simple Log Mediator instance (i.e. configured as <log/> ) is content-unaware. However a log mediator configured as <log level=\u201dfull\u201d/> would be content-aware since it is expected to log the message payload. Mediators are considered to be one of the main mechanisms for extending an EI. You can create custom mediators and add them to the ESB profile . This custom mediator and any other built-in mediator will be exactly the same as the API and the privileges. The standard mediators in the ESB profile are listed in the table below. Click a link for details on that mediator. There are also many samples that demonstrate how to use mediators.","title":"ESB Mediators"},{"location":"references/eSB-Mediators/#the-wso2-ei-mediator-catalog","text":"Category Name Description Core Call Invoke a service in non blocking synchronous manner Enqueue (deprecated) Uses a priority executor to ensure high-priority messages are not dropped Send Sends a message Loopback Moves the message from the In flow to the Out flow, skipping all remaining configuration in the In flow Sequence Inserts a reference to a sequence Respond Stops processing on the message and sends it back to the client Event Sends event notifications to an event source, publishes messages to predefined topics Drop Drops a message Call Template Constructs a sequence by passing values into a sequence template Enrich Enriches a message Property Sets or remove properties associated with the message Log Logs a message Filter Filter Filters a message using XPath, if-else kind of logic Out (deprecated) Applies to messages that are in the Out path of the ESB profile In (deprecated) Applies to messages that are in the In path of the ESB profile Validate Validates XML messages against a specified schema. Switch Filters messages using XPath, switch logic Conditional Router (deprecated) Implements complex routing rules (Header based routing, content based routing and other rules) Transform XSLT Performs XSLT transformations on the XML payload FastXSLT Performs XSLT transformations on the message stream URLRewrite Modifies and rewrites URLs or URL fragments XQuery Performs XQuery transformation Header Sets or removes SOAP headers Fault (also called Makefault) Create SOAP Faults PayloadFactory Transforms or replaces message content in between the client and the backend server Advanced Cache Evaluates messages based on whether the same message came to the ESB profile ForEach Splits a message into a number of different messages by finding matching elements in an XPath expression of the original message. Clone Clones a message Store Stores messages in a predefined message store Iterate Splits a message Aggregate Combines a message Callout Blocks web services calls Transaction Executes a set of mediators transactionally Throttle Limits the number of messages DBReport Writes data to database DBLookup Retrieves information from database EJB Calls an external Enterprise JavaBean(EJB) and stores the result in the message payload or in a message context property. Rule Executes rules Builder Builds the actual SOAP message, from a message, which is coming into the ESB profile through the Binary Relay. Entitlement Evaluates user actions against a XACML policy OAuth 2-legged OAuth support Smooks Used to apply lightweight transformations on messages in an efficient manner. Data Mapper Converts and transforms one data format to another, or changes the structure of the data in a message. Extension Bean (deprecated) Manipulates JavaBeans Class Creates and executes a custom mediator POJOCommand (deprecated) Executes a custom command Script Executes a mediator written in Scripting language Spring (deprecated) Creates a mediator managed by Spring Agent Publish Event Constructs events and publishes them to different systems such as WSO2 BAM/DAS/CEP/SP via event sinks.","title":"The WSO2 EI mediator catalog"},{"location":"references/editing-a-Priority-Executor/","text":"Editing a Priority Executor \u00b6 Info Note Please note that this feature is deprecated. can be used to execute sequences with a given priority. This allows user to control the resources allocated to executing sequences and prevent high priority messages from getting delayed and dropped. Once adding a priority executor, you can edit it options according to new requirements. Follow the instructions below to edit a priority executor in the WSO2 EI. 1. Sign in. Enter your user name and password to log on to the EI Management Console. 2. Click on \"Main\" in the left menu to access the \"Manage\" menu. 3. In the \"Manage\" menu, click on \"Priority Executors\" under \"Service Bus.\" 4. In the \"Priority Executors\" window, click on the \"Edit\" link in the \"Actions\" column. 5. The \"Priority Executor Design\" window appears. 6. Edit the options of the priority executor. For the detailed information about the executors options see Adding a Priority Executor . 7. Click on the \"Save\" button.","title":"Editing a Priority Executor"},{"location":"references/editing-a-Priority-Executor/#editing-a-priority-executor","text":"Info Note Please note that this feature is deprecated. can be used to execute sequences with a given priority. This allows user to control the resources allocated to executing sequences and prevent high priority messages from getting delayed and dropped. Once adding a priority executor, you can edit it options according to new requirements. Follow the instructions below to edit a priority executor in the WSO2 EI. 1. Sign in. Enter your user name and password to log on to the EI Management Console. 2. Click on \"Main\" in the left menu to access the \"Manage\" menu. 3. In the \"Manage\" menu, click on \"Priority Executors\" under \"Service Bus.\" 4. In the \"Priority Executors\" window, click on the \"Edit\" link in the \"Actions\" column. 5. The \"Priority Executor Design\" window appears. 6. Edit the options of the priority executor. For the detailed information about the executors options see Adding a Priority Executor . 7. Click on the \"Save\" button.","title":"Editing a Priority Executor"},{"location":"references/ei_config_catalog/","text":"Configuration Catalog \u00b6 This document describes all the configuration parameters that are used in WSO2 Enterprise Integrator. Instructions for use \u00b6 Select the configuration sections, parameters, and values that are required for your use and add them to the .toml file. See the example .toml file given below. # This is an example .toml file. [server] pattern=\"value\" enable_port_forward=true [[custom_transport.listener]] class=\"value\" protocol = \"value\" Configuring the default deployment settings [server] Required This toml header groups the parameters that are used for identifying a server node. You need need to update these values when you set up a deployment . hostname string Required \"localhost\" The hostname of the WSO2 EI server instance. node_ip string Required \"10.100.1.80\" The IP address of the server node. enable_mtom boolean false Use this paramater to enable MTOM (Message Transmission Optimization Mechanism) for the product server. enable_swa boolean false Use this paramater to enable SwA (SOAP with Attachments) for the product server. When SwA is enabled, the ESB will process the files attached to SOAP messages. offset integer 0 Port offset allows you to run multiple WSO2 products, multiple instances of a WSO2 product, or multiple WSO2 product clusters on the same server or virtual machine (VM). Port offset defines the number by which all ports defined in the runtime such as the HTTP/S ports will be offset. For example, if the default HTTP port is 9443 and the portOffset is 1, the effective HTTP port will be 9444. Therefore, for each additional WSO2 product instance, set the port offset to a unique value (the default is 0) so that they can all run on the same server without any port conflicts. [server] hostname=\"localhost\" node_ip = \"10.100.1.80\" base_path = \"127.0.0.1\" enable_mtom=false enable_swa=false userAgent = \"WSO2 ${product.key} ${product.version}\" serverDetails = \"WSO2 ${product.key} ${product.version}\" offset = 0 proxy_context_path = \"\" Configuring the cluster settings [clustering] Required This config heading groups the parameters that connects the server to a clustered deployment . members string Required [\"10.100.5.86:4000\",\"10.100.5.86:4001\"] Specify the well-known members in the cluster in both nodes as shown below. For example, when you configure one ESB node, you need to specify the other nodes in the cluster as well-known members as shown below. The port value for the WKA node must be the same value as it's localMemberPort (in this case it is 4000). Note You can also use IP address ranges for the hostname (e.g., 192.168.1.2-10). However, you can define a range only for the last portion of the IP address. Smaller the range, faster the time it takes to discover members since each node has to scan a lesser number of potential members. The best practice is to add all the members (including itself) in all the nodes to avoid any conflicts in configurations. local_member_port integer Required 4000 The port that is assigned to the server node in the deployment. Note This port number is not affected by the port offset value specified under the [server] section. If this port number is already assigned to another server, the clustering framework automatically increments this port number. However, if there are two servers running on the same machine, ensure that a unique port is set for each server. For example, you can have port 4000 for node 1 and port 4001 for node 2. local_member_host string Required \"10.100.5.86\" The hostname of the server node. When you have multiple nodes in the deployment, this hostname will be used to identify the node during inter-node communications. membership_scheme string 'wka' Add this parameter to change the default membership scheme. By default, the well-known address registration method (WKA) is used, which ensures that each node sends cluster initiation messages to the WKA members. domain string \"wso2.carbon.domain\" Add this parameter to change the default cluster (domain name) to which the server node joins. By default, wso2.carbon.domain is set. [clustering] members = [\"10.100.5.86:4000\",\"10.100.5.86:4001\"] local_member_port = 4000 local_member_host = \"10.100.5.86\" membership_scheme = 'wka' domain = \"wso2.carbon.domain\" Connecting to the primary data store [database.shared_db] Required This config heading groups the parameters connecting the server to the primary database. This database stores the user permissions that apply to user roles. This also stores the users and roles unless a separate user store is configured with the [user_store] config section. Read more about how to use databases for your WSO2 EI deployment . type string Required \"H2\" \"MySQL\" , \"Oracle\" , \"Postgre\" . The type of database that is used for the primary database. All the database types that are supported by this product is listed below as possible values. Warning \"H2\" (not recommended for production environments). So please change when going on production. url string Required \"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" The connection URL of the database. Note that this is specific to the database type you are using. Use the URL pattern for the database type. username string Required wso2carbon The user name for connecting to the database. password string Required wso2carbon The password for connecting to the database. [database.shared_db] type=\"H2\" url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" username=wso2carbon password=wso2carbon Tuning the primary data store connection [database.shared_db.pool_options] This config heading in the ei.toml file groups the performance tuning parameters of the primary database that you configured using the [database.shared_db] section. Read more about tuning database performance in WSO2 EI . max_active integer \"50\" \"MySQL\" The maximum number of active connections that can be allocated at the same time from the connection pool. If there are too many active connections in the connection pool, some of them would be idle, which incurs an unnecessary system overhead. Depending on your environment, enter any negative value to denote an unlimited number of active connections. max_wait integer \"6000\" The maximum number of milliseconds that the pool will wait (when there are no available connections) for a connection to be returned before throwing an exception. Depending on your environment, enter zero or a negative value to wait indefinitely. test_on_borrow boolean true true , false Determines whether objects are validated before being borrowed from the pool. If the object fails to validate, it will be dropped from the pool, and another attempt will be made to borrow another. validation_interval integer \"30000\" Connections are validated (at the most) at this frequency (time in milliseconds). If a connection is due for validation but has been validated previously within this interval, it will not be validated again. This avoids access validation. default_auto_commit boolean true true , false Specifies whether each SQL statement should be automatically committed when the operation is completed. It is recommended to disable auto committing. [database.shared_db_options] maxActive=\"50\" maxWait=\"6000\" testOnBorrow=true validationInterval=\"30000\" defaultAutoCommit=true Connecting to the user stores [user_store] Required This config heading in the ei.toml connects the server to the user store. The user store holds the users and roles defined for the system. Read more about how to set up user stores for your WSO2 EI deployment . type string Required \"database\" \"database\" , \"JDBC\" , \"RW LDAP\" , \"RO LDAP\" , \"AD\" , \"Custom\" The type of user store: \"database\" : Refers the embedded H2 database with the settings defined under [database.shared_db]. \"JDBC\" : Refers an external RDBMS. \"RW LDAP\" : Refers an external LDAP with both read/write access. \"RO LDAP\" : Refers an external LDAP with read only access. \"AD\" : Rrefers an active directory user store. \"Custom\" : Refers a custom user store implementation. connection_url string Required \"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" Add this parameter if you have changed the default user store type. You need to change the default connection URL by specifying the connection URL of your new user store (LDAP or AD user store). connection_name string Required wso2carbon Add this parameter if you have changed the default user store type. You need to change the default connection name (user name for connecting to the data store) by specifying the connection name of your new user store (LDAP or AD user store). connection_password string Required wso2carbon Add this parameter if you have changed the default user store type. You need to change the default password (password for connecting to the data store) by specifying the connection password of your new user store (LDAP or AD user store). base_dn string The..... [user_store] type=\"database\" connection_url=\"\" connection_name=\"uid=admin,ou=wso2ei\" connection_password=\"$secret{ldap_password}\" base_dn=\"dc=example,dc=com\" Configuring the system administrator [super_admin] Required The config heading in the ei.toml file that groups the parameters defining the system administrator . username string Required \"admin\" The user name of the system administrator. This user has all permissions enabled by default. password string Required \"admin\" The password of the system adminsitrator. You can use a plain text password such as 'admin'. However, if you want to use an encrypted password that is listed under the [secrets] configuration section, use the ... expression to include the encrypted password. Read more about encrypting plain text passwords. create_admin_account boolean Required true true or false Sets whether the system administrator credentials should be created in the system at the time of starting the server. [super_admin] username=\"admin\" password=\"admin\" create_admin_account=true Configuring the TLS keystore [keystore.tls] This config heading in the ei.toml file groups the parameters that connect the server to the primary keystore. This keystore is used for SSL handshaking (when the server communicates with another server) and for encrypting plain text information in configuration files. By default, this keystore is also used for encrypted data in internal datastores, unless you have configured a separate keystore for internal data encryption. Read more about configuring the primary keystore . file_name string \"wso2carbon.jks\" The name of the keystore file that is used for SSL communication and for encrypting/decrypting data in configuration files. type string \"JKS\" \"JKS\" or \"PKCS12\" The type of the keystore file. password string \"wso2carbon\" The password of the keystore file that is used for SSL communication and for encrypting/decrypting data in configuration files. The keystore password is used when accessing the keys in the keystore. alias string \"wso2carbon\" The alias of the public key corresponding to the private key that is included in the keystore. The public key is used for encrypting data in the ESB server, which only the corresponding private key can decrypt. The public key is embedded in a digital certificate, and this certificate can be shared over the internet by storing it in a separate trust store file. key_password string \"wso2carbon\" The password of the private key that is included in the keystore. The private key is used to decrypt the data that has been encrypted using the keystore's public key. [keystore.tls] file_name=\"wso2carbon.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"wso2carbon\" key_password=\"wso2carbon\" Configuring the internal keystore [keystore.internal] Add this config heading to the ei.toml file to group the parameters that connect the server to the keystore used for encrypting/decrypting data in internal data stores. You may sometimes choose to configure a separate keystore for this purpose because the primary keystore that is used by the [keystore.tls] configuration needs to renew certificates frequently. However, for encrypting information in internal data stores, the keystore certificates should not be changed frequently because the data that is already encrypted will become unusable every time the certificate changes. Read more about configuring the internal keystore . file_name string \"wso2carbon.jks\" The name of the keystore file that is used for data encryption/decryption in internal data stores. type string \"JKS\" \"JKS\" or \"PKCS12\" The type of the keystore file. password string \"wso2carbon\" The password of the keystore file that is used for data encryption/decryption in internal data stores. The keystore password is used when accessing the keys in the keystore. alias string \"wso2carbon\" The alias of the public key corresponding to the private key that is included in the keystore. The public key is used for encrypting data in the ESB server, which only the corresponding private key can decrypt. The public key is embedded in a digital certificate, and this certificate can be shared over the internet by storing it in a separate trust store file. key_password string \"wso2carbon\" The password of the private key that is included in the keystore. The private key is used to decrypt the data that has been encrypted using the keystore's public key. [keystore.internal] file_name=\"wso2carbon.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"wso2carbon\" key_password=\"wso2carbon\" Configuring the trust store [truststore] Add this config heading to the ei.toml file to group the parameters that connect the server to the keystore file (trust store) that is used to store the digital certificates that the server trusts for SSL communication. All keystore files used by this product should be stored in the EI_HOME/repository/resources/security/ directory. The product is configured to use the default trust store (wso2truststore.jks), which contains the self-signed digital certificate of the default keystore. Read more about asymetric encryption in WSO2 EI. Read more about configuring the truststore . file_name string \"wso2truststore.jks\" The name of the keystore file that is used for storing the trusted digital certificates. type string \"JKS\" \"JKS\" or \"PKCS12\" The type of the keystore file that is used as the trust store. password string \"wso2carbon\" The password of the keystore file that is used as the trust store. alias string \"symmetric.key.value\" The alias is the password of the digital certificate (which holds the public key) that is included in the trustore. algorithm string ..... The..... [truststore] file_name=\"wso2truststore.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"symmetric.key.value\" algorithm=\"\" Configuring the server request processor [[server.get_request_processor]] Add this config section to the ei.toml file to add processors that process special HTTP GET requests such as ?wsdl, ?policy etc. In order to plug in a processor to handle a special request, simply add an entry to this section. item string \"swagger.json\" The value of the Item element is the first parameter in the query string(e.g. ?wsdl) which needs special processing. class string \"org.wso2.appcloud.api.swagger.processors.json.SwaggerJsonProcessor\" The value of the Class element is a class that implements org.wso2.carbon.transport.HttpGetRequestProcessor. [[server.get_request_processor]] item = \"swagger.json\" class = \"org.wso2.appcloud.api.swagger.processors.json.SwaggerJsonProcessor\" Configuring the HTTP transport [transport.http] Add this config heading to the ei.toml file to group the parameters for configuring the HTTP/S transports in the product. socket_timeout integer \"3m\" The... core_worker_pool_size integer 400 The... max_worker_pool_size integer 400 The... worker_pool_queue_length integer -1 The... io_buffer_size integer 16384 The... max_http_connection_per_host_port integer 32767 The... preserve_http_user_agent boolean false The... preserve_http_server_name boolean true The... preserve_http_headers string [\"Content-Type\"] The... disable_connection_keepalive boolean false The... enable_message_size_validation boolean false The... max_message_size_bytes integer 81920 The... max_open_connections integer -1 The... force_xml_validation boolean false The... force_json_validation boolean false The... listener.port integer \"8280\" The port on which this transport receiver should listen for incoming messages. listener.wsdl_epr_prefix string \"$ref{server.hostname}\" A URL prefix which will be added to all service EPRs and EPRs in WSDLs etc. listener.bind_address string \"$ref{server.hostname}\" ....... listener.secured_port integer \"8243\" The secured port on which this transport receiver should listen for incoming messages. listener.secured_wsdl_epr_prefix string \"$ref{server.hostname}\" A URL prefix which will be added to all service EPRs and EPRs in WSDLs etc. listener.secured_bind_address string \"$ref{server.hostname}\" ......... listener.secured_protocols string \"TLSv1,TLSv1.1,TLSv1.2\" ...... listener.verify_client string \"require\" ... listener.ssl_profile.file_path string \"conf/sslprofiles/listenerprofiles.xml\" ... listener.ssl_profile.read_interval string \"1h\" ... listener.preferred_ciphers string \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\" ... listener.keystore.file_name string \"$ref{keystore.tls.file_name}\" ... listener.keystore.type string \"$ref{keystore.tls.type}\" ... listener.keystore.password string \"$ref{keystore.tls.password}\" ... listener.keystore.key_password string \"$ref{keystore.tls.key_password}\" ... listener.truststore.file_name string \"$ref{truststore.file_name}\" ... listener.truststore.type string \"$ref{truststore.type}\" ... listener.truststore.password string \"$ref{truststore.password}\" ... sender.warn_on_http_500 string If the outgoing messages should be sent through an HTTP proxy server, use this parameter to specify the target proxy. sender.proxy_host string \"${deployement.node_ip}\" If the outgoing messages should be sent through an HTTP proxy server, use this parameter to specify the target proxy. sender.proxy_port integer \"3128\" The port through which the target proxy accepts HTTP traffic. sender.non_proxy_hosts string [\"$ref{server.hostname}\"] The list of hosts to which the HTTP traffic should be sent directly without going through the proxy. When trying to add multiple hostnames along with an asterisk in order to define a set of sub-domains for non-proxy hosts, you need to add a period before the asterisk when configuring proxy server. sender.hostname_verifier string \"AllowAll\" The list of hosts to which the HTTP traffic should be sent directly without going through the proxy. When trying to add multiple hostnames along with an asterisk in order to define a set of sub-domains for non-proxy hosts, you need to add a period before the asterisk when configuring proxy server. sender.keystore.file_name string \"$ref{keystore.tls.file_name}\" ... sender.keystore.type string \"$ref{keystore.tls.type}\" ... sender.keystore.password string \"$ref{keystore.tls.password}\" ... sender.keystore.key_password string \"$ref{keystore.tls.key_password}\" ... sender.truststore.file_name string \"$ref{truststore.file_name}\" ... sender.truststore.type string \"$ref{truststore.type}\" ... sender.truststore.password string \"$ref{truststore.password}\" ... blocking_sender.enable_client_caching boolean true true or false This parameter is used to specify whether the HTTP client should save cache entries and the cached responses in the JVM memory or not. blocking_sender.transfer_encoding string \"chunked\" \"chunked\" or false This parameter enables you to specify whether the data sent should be chunked. It can be used instead of the Content-Length header if you want to upload data without having to know the amount of data to be uploaded in advance. blocking_sender.default_connections_per_host boolean true true or false . The maximum number of connections that will be created per host server by the client. If the backend server is slow, the connections in use at a given time will take a long time to be released and added back to the connection pool. As a result, connections may not be available for some requests. In such situations, it is recommended to increase the value for this parameter. blocking_sender.omit_soap12_action boolean true true or false . If following is set to 'true', optional action part of the Content-Type will not be added to the SOAP 1.2 messages. blocking_sender.so_timeout boolean \"1m\" true or false . If following is set to 'true', optional action part of the Content-Type will not be added to the SOAP 1.2 messages. [transport.http] socket_timeout = \"3m\" core_worker_pool_size = 400 max_worker_pool_size = 400 worker_pool_queue_length = -1 io_buffer_size = 16384 max_http_connection_per_host_port = 32767 preserve_http_user_agent = false preserve_http_server_name = true preserve_http_headers = [\"Content-Type\"] disable_connection_keepalive = false enable_message_size_validation = false max_message_size_bytes = 81920 max_open_connections = -1 force_xml_validation = false force_json_validation = false listener.port = 8280 #inferred default: 8280 listener.wsdl_epr_prefix =\"$ref{server.hostname}\" listener.bind_address = \"$ref{server.hostname}\" listener.secured_port = 8243 listener.secured_wsdl_epr_prefix = \"$ref{server.hostname}\" listener.secured_bind_address = \"$ref{server.hostname}\" listener.secured_protocols = \"TLSv1,TLSv1.1,TLSv1.2\" listener.verify_client = \"require\" listener.ssl_profile.file_path = \"conf/sslprofiles/listenerprofiles.xml\" listener.ssl_profile.read_interval = \"1h\" listener.preferred_ciphers = \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\" listener.keystore.file_name =\"$ref{keystore.tls.file_name}\" listener.keystore.type = \"$ref{keystore.tls.type}\" listener.keystore.password = \"$ref{keystore.tls.password}\" listener.keystore.key_password = \"$ref{keystore.tls.key_password}\" listener.truststore.file_name = \"$ref{truststore.file_name}\" listener.truststore.type = \"$ref{truststore.type}\" listener.truststore.password = \"$ref{truststore.password}\" sender.warn_on_http_500 = \"*\" sender.proxy_host = \"$ref{server.hostname}\" sender.proxy_port = 3128 sender.non_proxy_hosts = [\"$ref{server.hostname}\"] sender.hostname_verifier = \"AllowAll\" sender.keystore.file_name =\"$ref{keystore.tls.file_name}\" sender.keystore.type = \"$ref{keystore.tls.type}\" sender.keystore.password = \"$ref{keystore.tls.password}\" sender.keystore.key_password = \"$ref{keystore.tls.key_password}\" sender.truststore.file_name = \"$ref{truststore.file_name}\" sender.truststore.type = \"$ref{truststore.type}\" sender.truststore.password = \"$ref{truststore.password}\" sender.ssl_profile.file_path = \"conf/sslprofiles/senderprofiles.xml\" sender.ssl_profile.read_interval = \"30s\" # common for http/https blocking_sender.enable_client_caching = true blocking_sender.transfer_encoding = \"chunked\" blocking_sender.default_connections_per_host = 200 blocking_sender.omit_soap12_action = true blocking_sender.so_timeout = \"1m\" Configuring the HTTP proxy profile [[transport.http.proxy_profile]] This..... target_hosts string \"\" true or \"\" The..... proxy_hosts string \"\" \"\" or \"\" . The parameter for enabling the local transport sender. proxy_port integer \"\" \"\" or \"\" . The parameter for enabling the local transport sender. proxy_username string \"\" \"\" or \"\" . The parameter for enabling the local transport sender. proxy_password string \"\" \"\" or \"\" . The parameter for enabling the local transport sender. bypass_hosts string \"\" \"\" or \"\" . The parameter for enabling the local transport sender. [[transport.http.proxy_profile]] target_hosts = [\"\"] proxy_host = \"\" proxy_port = \"\" proxy_username = \"\" proxy_password = \"\" bypass_hosts = [\"\"] Configuring the Local transport [transport.local] This parameter is used to enable the listeners and senders when the ESB server communicates through the local transport. listener.enabled boolean false true or false The parameter for enabling the local transport listener. sender.enabled boolean false true or false . The parameter for enabling the local transport sender. [transport.local] listener.enabled=false sender.enabled=false Configuring the VFS transport [transport.vfs] Required Add this config heading to the ei.toml file to group the parameters that configure the ESB server to communicate through the VFS transport. Read more about file transfering in the ESB. listener.enabled boolean true true or false The parameter for enabling the VFS transport listener. listener.keystore.file_name string \"$ref{keystore.tls.file_name}\" ... listener.keystore.type string \"$ref{keystore.tls.type}\" ... listener.keystore.password string \"$ref{keystore.tls.password}\" ... listener.keystore.key_password string \"$ref{keystore.tls.key_password}\" ... listener.keystore.alias string \"$ref{keystore.tls.alias}\" ... sender.enabled boolean true true or false . The parameter for enabling the VFS transport sender. [transport.vfs] listener.enable = true listener.keystore.file_name = \"$ref{keystore.tls.file_name}\" listener.keystore.type = \"$ref{keystore.tls.type}\" listener.keystore.password = \"$ref{keystore.tls.password}\" listener.keystore.key_password = \"$ref{keystore.tls.key_password}\" listener.keystore.alias = \"$ref{keystore.tls.alias}\" sender.enable = true Configuring the MAIL transport listener [transport.mail.listener] Add this config heading to the ei.toml file to group the parameters that configure the ESB server to communicate through the MAIL transport. Read more about using the MAIL transport. enabled boolean true true or false The parameter for enabling the MAIL transport listener in the ESB. name string \"mailto\" The parameter for enabling the MAIL transport listener in the ESB. [transport.mail.listener] enable = true name = \"mailto\" Configuring the MAIL transport sender [[transport.mail.sender]] Add this config heading to the ei.toml file to group the parameters that configure the ESB server to communicate through the MAIL transport. Read more about using the MAIL transport. name string \"mailto\" The parameter for enabling the MAIL transport sender in the ESB. parameter.hostname string \"smtp.gmail.com\" The...... parameter.port integer 587 The...... parameter.enable_tls boolean true The...... parameter.auth boolean true The...... parameter.username string \"demo_user\" The...... parameter.password string \"mailpassword\" The...... parameter.from string \"demo_user@wso2.com\" The...... [[transport.mail.sender]] name = \"mailto\" parameter.hostname = \"smtp.gmail.com\" parameter.port = \"587\" parameter.enable_tls = true parameter.auth = true parameter.username = \"demo_user\" parameter.password = \"mailpassword\" parameter.from = \"demo_user@wso2.com\" Configuring the JMS transport listener [[transport.jms.listener]] Add this config heading to the ei.toml file to group the parameters that configure the ESB server to communicate through the JMS transport. Read more about using the JMS transport. name string \"myTopicListener\" The ...... parameter.initial_naming_factory string \"org.apache.activemq.artemis.jndi.ActiveMQInitialContextFactory\" The...... parameter.broker_name string \"artemis\" The...... parameter.provider_url string \"tcp://localhost:61616\" The...... parameter.connection_factory_name string \"TopicConnectionFactory\" The...... parameter.connection_factory_type string \"topic\" The...... parameter.cache_level string \"consumer\" The...... parameter.naming_security_principal string \"\" The...... parameter.naming_security_credential string \"\" The...... parameter.transactionality string \"\" The...... parameter.transaction_jndi_name string \"\" The...... parameter.cache_user_transaction boolean true The...... parameter.session_transaction boolean true The...... parameter.session_acknowledgement string \"AUTO_ACKNOWLEDGE\" The...... parameter.jms_spec_version string \"1.1\" The...... parameter.username string \"\" The...... parameter.password string \"\" The...... parameter.destination string \"\" The...... parameter.destination_type string \"queue\" The...... parameter.default_reply_destination string \"\" The...... parameter.default_destination_type string \"queue\" The...... parameter.message_selector string \"\" The...... parameter.subscription_durable boolean \"\" The...... parameter.durable_subscriber_client_id string \"\" The...... parameter.durable_subscriber_name string \"\" The...... parameter.pub_sub_local boolean false The...... parameter.receive_timeout string \"1s\" The...... parameter.concurrent_consumer integer 1 The...... parameter.max_concurrent_consumer integer 1 The...... parameter.idle_task_limit integer 10 The...... parameter.max_message_per_task integer -1 The...... parameter.initial_reconnection_duration string \"10s\" The...... parameter.reconnect_progress_factor integer 2 The...... parameter.max_reconnect_duration string \"1h\" The...... parameter.reconnect_interval string \"1h\" The...... parameter.max_jsm_connection integer 10 The...... parameter.max_consumer_error_retrieve_before_delay integer 20 The...... parameter.consume_error_delay string \"100ms\" The...... parameter.consume_error_progression string \"2.0\" The...... [[transport.jms.listener]] name = \"myTopicListener\" parameter.initial_naming_factory = \"org.apache.activemq.artemis.jndi.ActiveMQInitialContextFactory\" parameter.broker_name = \"artemis\" parameter.provider_url = \"tcp://localhost:61616\" parameter.connection_factory_name = \"TopicConnectionFactory\" parameter.connection_factory_type = \"topic\" parameter.cache_level = \"consumer\" parameter.naming_security_principal = \"\" parameter.naming_security_credential = \"\" parameter.transactionality = \"\" parameter.transaction_jndi_name = \"\" parameter.cache_user_transaction = true parameter.session_transaction = true parameter.session_acknowledgement = \"AUTO_ACKNOWLEDGE\" parameter.jms_spec_version = \"1.1\" parameter.username = \"\" parameter.password = \"\" parameter.destination = \"\" parameter.destination_type = \"queue\" parameter.default_reply_destination = \"\" parameter.default_destination_type = \"queue\" parameter.message_selector = \"\" parameter.subscription_durable = false parameter.durable_subscriber_client_id = \"\" parameter.durable_subscriber_name = \"\" parameter.pub_sub_local = false parameter.receive_timeout = \"1s\" parameter.concurrent_consumer = 1 parameter.max_concurrent_consumer = 1 parameter.idle_task_limit = 10 parameter.max_message_per_task = -1 parameter.initial_reconnection_duration = \"10s\" parameter.reconnect_progress_factor = 2 parameter.max_reconnect_duration = \"1h\" parameter.reconnect_interval = \"1h\" parameter.max_jsm_connection = 10 parameter.max_consumer_error_retrieve_before_delay = 20 parameter.consume_error_delay = \"100ms\" parameter.consume_error_progression = \"2.0\" Configuring the JMS transport sender [[transport.jms.sender]] Add this config heading to the ei.toml file to group the parameters that configure the ESB server to communicate through the JMS transport. Read more about using the JMS transport. enable boolean true The ...... name string \"myTopicSender\" The...... parameter.initial_naming_factory string \"org.apache.activemq.artemis.jndi.ActiveMQInitialContextFactory\" The...... parameter.broker_name string \"artemis\" The...... parameter.provider_url string \"TopicConnectionFactory\" The...... parameter.connection_factory_type string \"topic\" The...... parameter.cache_level string \"producer\" The...... parameter.naming_security_principal string \"\" The...... parameter.naming_security_credential string \"\" The...... parameter.transactionality string \"\" The...... parameter.transaction_jndi_name string \"\" The...... parameter.cache_user_transaction boolean true The...... parameter.session_transaction boolean true The...... parameter.session_acknowledgement string \"AUTO_ACKNOWLEDGE\" The...... parameter.jms_spec_version string \"1.1\" The...... parameter.username string \"\" The...... parameter.password string \"\" The...... parameter.destination string \"\" The...... parameter.destination_type string \"queue\" The...... parameter.default_reply_destination string \"\" The...... parameter.default_destination_type string \"queue\" The...... parameter.message_selector string \"\" The...... parameter.subscription_durable boolean \"\" The...... parameter.durable_subscriber_client_id string \"\" The...... parameter.durable_subscriber_name string \"\" The...... parameter.pub_sub_local boolean false The...... parameter.receive_timeout string \"1s\" The...... parameter.concurrent_consumer integer 1 The...... parameter.max_concurrent_consumer integer 1 The...... parameter.idle_task_limit integer 10 The...... parameter.max_message_per_task integer -1 The...... parameter.initial_reconnection_duration string \"10s\" The...... parameter.reconnect_progress_factor integer 2 The...... parameter.max_reconnect_duration string \"1h\" The...... parameter.reconnect_interval string \"1h\" The...... parameter.max_jsm_connection integer 10 The...... parameter.max_consumer_error_retrieve_before_delay integer 20 The...... parameter.consume_error_delay string \"100ms\" The...... parameter.consume_error_progression string \"2.0\" The...... parameter.vender_class_loader boolean false The...... [[transport.jms.sender]] enable = true name = \"myTopicSender\" parameter.initial_naming_factory = \"org.apache.activemq.artemis.jndi.ActiveMQInitialContextFactory\" parameter.broker_name = \"artemis\" parameter.provider_url = \"tcp://localhost:61616\" parameter.connection_factory_name = \"TopicConnectionFactory\" parameter.connection_factory_type = \"topic\" parameter.cache_level = \"producer\" parameter.naming_security_principal = \"\" parameter.naming_security_credential = \"\" parameter.transactionality = \"\" parameter.transaction_jndi_name = \"\" parameter.cache_user_transaction = true parameter.session_transaction = true parameter.session_acknowledgement = \"AUTO_ACKNOWLEDGE\" parameter.jms_spec_version = \"1.1\" parameter.username = \"\" parameter.password = \"\" parameter.destination = \"\" parameter.destination_type = \"queue\" # queue/topic parameter.default_reply_destination = \"\" parameter.default_destination_type = \"queue\" parameter.message_selector = \"\" parameter.subscription_durable = false parameter.durable_subscriber_client_id = \"\" parameter.durable_subscriber_name = \"\" parameter.pub_sub_local = false parameter.receive_timeout = \"1s\" parameter.concurrent_consumer = 1 parameter.max_concurrent_consumer = 1 parameter.idle_task_limit = 10 parameter.max_message_per_task = -1 parameter.initial_reconnection_duration = \"10s\" parameter.reconnect_progress_factor = 2 parameter.max_reconnect_duration = \"1h\" parameter.reconnect_interval = \"1h\" parameter.max_jsm_connection = 10 parameter.max_consumer_error_retrieve_before_delay = 20 parameter.consume_error_delay = \"100ms\" parameter.consume_error_progression = \"2.0\" parameter.vender_class_loader = false Configuring the JNDI connection factories [transport.jndi.connection_factories] This... QueueConnectionFactory string \"amqp://admin:admin@clientID/carbon?brokerlist='tcp://localhost:5675'\" The..... TopicConnectionFactory string \"amqp://admin:admin@clientID/carbon?brokerlist='tcp://localhost:5675'\" The...... [transport.jndi.connection_factories] QueueConnectionFactory = \"amqp://admin:admin@clientID/carbon?brokerlist='tcp://localhost:5675'\" TopicConnectionFactory = \"amqp://admin:admin@clientID/carbon?brokerlist='tcp://localhost:5675'\" Configuring the JNDI queues [transport.jndi.queue] This... JMSMS string \"JMSMS\" The..... StockQuotesQueue string \"StockQuotesQueue\" The...... [transport.jndi.queue] JMSMS = \"JMSMS\" StockQuotesQueue = \"StockQuotesQueue\" Configuring the JNDI topics [transport.jndi.topic] This... MyTopic string \"example.MyTopic\" The...... [transport.jndi.topic] MyTopic = \"example.MyTopic\" Configuring the RabbitMQ listener [[transport.rabbitmq.listener]] Add... name string true The ...... parameter.hostname string \"localhost\" The...... parameter.port integer 5672 The...... parameter.username string \"guest\" The...... parameter.password string \"guest\" The...... parameter.connection_factory string \"\" The...... parameter.exchange_name string \"amq.direct\" The...... parameter.queue_name string \"MyQueue\" The...... parameter.queue_auto_ack boolean false The...... parameter.consumer_tag string \"\" The...... parameter.channel_consumer_qos string \"\" The...... parameter.durable string \"\" The...... parameter.queue_exclusive string \"\" The...... parameter.queue_auto_delete string \"\" The...... parameter.queue_routing_key string \"\" The...... parameter.queue_auto_declare string \"\" The...... parameter.exchange_auto_declare string \"\" The...... parameter.exchange_type string \"\" The...... parameter.exchange_durable string \"\" The...... parameter.exchange_auto_delete string \"\" The...... parameter.default_destination_type string \"\" The...... parameter.retry_interval string \"10s\" The...... parameter.retry_count integer 5 The...... parameter.ssl_enable boolean true The...... parameter.ssl_version string \"SSL\" The...... parameter.keystore_file_name string \"$ref{keystore.tls.file_name}\" The...... parameter.keystore_type string \"$ref{keystore.tls.type}\" The...... parameter.keystore_password string \"$ref{keystore.tls.password}\" The...... parameter.truststore_file_name string \"$ref{truststore.file_name}\" The...... parameter.truststore_type string \"$ref{truststore.type}\" The...... parameter.truststore_password string \"$ref{truststore.password}\" The...... [[transport.rabbitmq.listener]] name = \"rabbitMQListener\" parameter.hostname = \"localhost\" parameter.port = 5672 parameter.username = \"guest\" parameter.password = \"guest\" parameter.connection_factory = \"\" parameter.exchange_name = \"amq.direct\" parameter.queue_name = \"MyQueue\" parameter.queue_auto_ack = false parameter.consumer_tag = \"\" parameter.channel_consumer_qos = \"\" parameter.durable = \"\" parameter.queue_exclusive = \"\" parameter.queue_auto_delete = \"\" parameter.queue_routing_key = \"\" parameter.queue_auto_declare = \"\" parameter.exchange_auto_declare = \"\" parameter.exchange_type = \"\" parameter.exchange_durable = \"\" parameter.exchange_auto_delete = \"\" parameter.message_content_type = \"\" parameter.retry_interval = \"10s\" parameter.retry_count = 5 parameter.ssl_enable = true parameter.ssl_version = \"SSL\" parameter.keystore_file_name =\"$ref{keystore.tls.file_name}\" parameter.keystore_type = \"$ref{keystore.tls.type}\" parameter.keystore_password = \"$ref{keystore.tls.password}\" parameter.truststore_file_name =\"$ref{truststore.file_name}\" parameter.truststore_type = \"$ref{truststore.type}\" parameter.truststore_password = \"$ref{truststore.password}\" Configuring the RabbitMQ sender [[transport.rabbitmq.sender]] Add... name string \"rabbitMQSender\" The ...... parameter.hostname string \"localhost\" The...... parameter.port integer 5672 The...... parameter.username string \"guest\" The...... parameter.password string \"guest\" The...... parameter.exchange_name string \"amq.direct\" The...... parameter.routing_key string \"MyQueue\" The...... parameter.queue_name string \"MyQueue\" The...... parameter.reply_to_name string The...... parameter.queue_delivery_mode integer 1 The...... parameter.exchange_type string \"\" The...... parameter.queue_name string \"MyQueue\" The...... parameter.reply_to_name string \"\" The...... parameter.queue_delivery_mode integer 1 The...... parameter.exchange_type string \"\" The...... parameter.queue_durable boolean false The...... parameter.queue_exclusive boolean false The...... parameter.queue_auto_delete boolean \"\" The...... parameter.exchange_durable string \"\" The...... parameter.queue_auto_declare string \"\" The...... parameter.exchange_auto_declare string \"\" The...... [[transport.rabbitmq.sender]] name = \"rabbitMQSender\" parameter.hostname = \"localhost\" parameter.port = 5672 parameter.username = \"guest\" parameter.password = \"guest\" parameter.exchange_name = \"amq.direct\" parameter.routing_key = \"MyQueue\" parameter.reply_to_name = \"\" parameter.queue_delivery_mode = 1 # 1/2 parameter.exchange_type = \"\" parameter.queue_name = \"MyQueue\" parameter.queue_durable = false parameter.queue_exclusive = false parameter.queue_auto_delete = false parameter.exchange_durable = \"\" parameter.queue_auto_declare = \"\" parameter.exchange_auto_declare = \"\" Configuring the FIX transport [transport.fix] Add this config heading to the ei.toml to group the parameters that configure the ESB server to communicate through the FIX transport. Read more about how the ESB uses FIX communication. listener.enabled boolean false true or false The parameter for enabling the FIX transport listener. sender.enabled boolean false true or false . The parameter for enabling the FIX transport sender. [transport.vfs] listener.enabled=false sender.enabled=false Configuring the MQTT transport [transport.mqtt] This.... listener.enabled boolean false true or false The parameter for enabling the MQTT transport listener. listener.parameter.hostname string \"$ref{server.hostname}\" The.... listener.parameter.connection_factory string \"mqttConFactory\" The.... listener.parameter.server_port integer 1883 The.... listener.parameter.client_id string \"client-id-1234\" The.... listener.parameter.topic_name string \"esb.test\" The.... listener.parameter.subscription_qos integer 0 The.... listener.parameter.session_clean boolean false The.... listener.parameter.enable_ssl boolean false The.... listener.parameter.subscription_username string \"\" The.... listener.parameter.subscription_password string \"\" The.... listener.parameter.temporary_store_directory string \"\" The.... listener.parameter.blocking_sender boolean false The.... listener.parameter.connect_type string \"text/plain\" The.... listener.parameter.message_retained boolean false The.... sender.enable boolean false The.... [transport.mqtt] listener.enable = false listener.parameter.hostname = \"$ref{server.hostname}\" listener.parameter.connection_factory = \"mqttConFactory\" listener.parameter.server_port = 1883 listener.parameter.client_id = \"client-id-1234\" listener.parameter.topic_name = \"esb.test\" # not reqired parameter list listener.parameter.subscription_qos = 0 listener.parameter.session_clean = false listener.parameter.enable_ssl = false listener.parameter.subscription_username = \"\" listener.parameter.subscription_password = \"\" listener.parameter.temporary_store_directory = \"\" listener.parameter.blocking_sender = false listener.parameter.connect_type = \"text/plain\" listener.parameter.message_retained = false sender.enable = false Configuring the HL7 transport [transport.hl7] Add this config heading to group the parameters that configure the ESB server communicate through the HL7 transport. Read more about HL7 communication in the ESB. listener.enabled boolean false true or false The parameter for enabling the HL7 transport listener. listener.port integer 9292 The port of the HL7 transport listener. sender.enabled boolean false true or false . The parameter for enabling the HL7 transport sender. sender.non_blocking boolean true true or false . The... [transport.hl7] listener.enable = false listener.port = 9292 sender.enable = false sender.non_blocking = true Configuring the SAP transport [transport.sap] Add this config heading to the ei.toml file to group the parameters that configure the ESB to communicate with a SAP system. Read more about SAP integration of the ESB. listener.enabled boolean false true or false The parameter for enabling SAP transport listener. listener.idoc.class string \"org.wso2.carbon.transports.sap.SAPTransportListener\" The.... listener.bapi.class string \"org.wso2.carbon.transports.sap.SAPTransportListener\" The.... sender.enabled boolean false true or false . The parameter for enabling the SAP transport sender. sender.idoc.class string \"org.wso2.carbon.transports.sap.SAPTransportSender\" The.... sender.bapi.class string \"org.wso2.carbon.transports.sap.SAPTransportSender\" The.... [transport.sap] listener.enabled=false sender.enabled=false Configuring the MSMQ transport [transport.msmq] Add..... listener.enabled boolean false true or false The parameter for enabling MSMQ transport listener. listener.hostname string \"$ref{server.hostname}\" The.... sender.enable boolean false The.... [transport.msmq] listener.enable = false listener.hostname = \"$ref{server.hostname}\" sender.enable = false Configuring the TCP transport [transport.tcp] Add..... listener.enabled boolean false true or false The parameter for enabling the TCP transport listener. listener.port integer 8000 The..... listener.hostname string \"$ref{server.hostname}\" The..... listener.content_type string [\"application/xml\"] The..... listener.response_client boolean true The..... sender.enabled boolean false true or false The parameter for enabling the TCP transport sender. [transport.tcp] listener.enable = false listener.port = 8000 listener.hostname = \"$ref{server.hostname}\" listener.content_type = [\"application/xml\"] listener.response_client = true sender.enable = true Configuring the Websocket transport [transport.ws] Add..... sender.enable boolean false true or false The parameter for enabling the websocket transport listener. sender.parameter.outflow_dispatch_sequence string \"outflowDispatchSeq\" The..... sender.parameter.outflow_dispatch_fault_sequence string \"outflowFaultSeq\" The..... [transport.ws] sender.enable = false sender.parameter.outflow_dispatch_sequence = \"outflowDispatchSeq\" sender.parameter.outflow_dispatch_fault_sequence = \"outflowFaultSeq\" Configuring the Websocket secured transport [transport.wss] Add..... sender.enable boolean false true or false The parameter for enabling the websocket secured transport sender. sender.parameter.outflow_dispatch_sequence string \"outflowDispatchSeq\" The..... sender.parameter.outflow_dispatch_fault_sequence string \"outflowFaultSeq\" The..... sender.truststore_location string \"$ref{truststore.file_name}\" The..... sender.truststore_password string \"$ref{truststore.password}\" The..... [transport.wss] sender.enable = false sender.parameter.outflow_dispatch_sequence = \"outflowDispatchSeq\" sender.parameter.outflow_dispatch_fault_sequence = \"outflowFaultSeq\" sender.truststore_location = \"$ref{truststore.file_name}\" sender.truststore_password = \"$ref{truststore.password}\" Configuring the UDP transport [transport.udp] Add..... listener.enabled boolean false true or false The parameter for enabling the UDP transport listener. sender.enabled boolean false true or false The parameter for enabling the UDP transport sender. [transport.udp] listener.enable = false sender.enable =false Configuring a custom transport listener [[custom_transport.listener]] This config heading is used to group the parameters for a custom transport implementation that you want to use in your product. class string \"\" The ..... protocol string \"ISO8583\" ...... parameter.port integer 8081 ....... parameter.hostname string \"$ref{server.node_ip}\" ....... parameter.non-blocking boolean true ....... parameter.bind-address string \"$ref{server.hostname}\" ....... parameter.WSDLEPRPrefix string \"$ref{server.hostname}\" ....... keystore.file_name string \"$ref{keystore.tls.file_name}\" ... keystore.type string \"$ref{keystore.tls.type}\" ... keystore.password string \"$ref{keystore.tls.password}\" ... keystore.key_password string \"$ref{keystore.tls.key_password}\" ... truststore.file_name string \"$ref{truststore.file_name}\" ... truststore.type string \"$ref{truststore.type}\" ... truststore.password string \"$ref{truststore.password}\" ... ssl_profile.file_path string \"conf/sslprofiles/listenerprofiles.xml\" ... ssl_profile.read_interval string \"30s\" ... [[custom_transport.listener]] class=\"\" protocol = \"ISO8583\" parameter.port = 8081 parameter.hostname = \"$ref{server.node_ip}\" parameter.non-blocking = true parameter.bind-address = \"$ref{server.hostname}\" parameter.WSDLEPRPrefix = \"$ref{server.hostname}\" # inferred keystore.file_name = \"$ref{keystore.tls.file_name}\" keystore.type = \"$ref{keystore.tls.type}\" keystore.password = \"$ref{keystore.tls.password}\" keystore.key_password = \"$ref{keystore.tls.key_password}\" truststore.file_name = \"$ref{truststore.file_name}\" truststore.type = \"$ref{truststore.type}\" truststore.password = \"$ref{truststore.password}\" ssl_profile.file_path= \"conf/sslprofiles/listenerprofiles.xml\" ssl_profile.read_interval = \"30s\" Configuring a custom transport sender [[custom_transport.sender]] This config heading is used to group the parameters for a custom transport implementation that you want to use in your product. class string \"\" The ..... protocol string \"ISO8583\" ...... parameter.port integer 8081 ....... parameter.hostname string \"$ref{server.node_ip}\" ....... parameter.non-blocking boolean true ....... parameter.non_proxy_host string \"\" ....... keystore.file_name string \"$ref{keystore.tls.file_name}\" ... keystore.type string \"$ref{keystore.tls.type}\" ... keystore.password string \"$ref{keystore.tls.password}\" ... keystore.key_password string \"$ref{keystore.tls.key_password}\" ... truststore.file_name string \"$ref{truststore.file_name}\" ... truststore.type string \"$ref{truststore.type}\" ... truststore.password string \"$ref{truststore.password}\" ... ssl_profile.file_path string \"conf/sslprofiles/senderprofiles.xml\" ... ssl_profile.read_interval string \"30s\" ... [[custom_transport.sender]] class=\"\" protocol = \"ISO8583\" parameter.hostname = \"\" parameter.port = \"\" parameter.non_proxy_host = \"\" parameter.non_blocking = true keystore.file_name = \"$ref{keystore.tls.file_name}\" keystore.type = \"$ref{keystore.tls.type}\" keystore.password = \"$ref{keystore.tls.password}\" keystore.key_password = \"$ref{keystore.tls.key_password}\" truststore.file_name = \"$ref{truststore.file_name}\" truststore.type = \"$ref{truststore.type}\" truststore.password = \"$ref{truststore.password}\" ssl_profile.file_path = \"conf/sslprofiles/senderprofiles.xml\" ssl_profile.read_interval = \"30s\" Configuring the mediation process [mediation] Required Add this config heading to the ei.toml file to group the parameters for tuning the mediation process (Synapse engine) of the ESB. These parameters are mainly used when mediators such as Iterate and Clone (which leverage on internal thread pools) are used. synapse.core_threads integer \"20\" The initial number of synapse threads in the pool. This parameter is applicable only if the Iterate or the Clone mediator is used to handle a higher load. The number of threads specified for this parameter should be increased as required to balance an increased load. synapse.max_threads integer \"100\" The maximum number of synapse threads in the pool. This parameter is applicable only if the Iterate or the Clone mediator is used to handle a higher load. The number of threads specified for this parameter should be increased as required to balance an increased load. synapse.threads_queue_length integer \"10\" The length of the queue that is used to hold the runnable tasks to be executed by the pool. This parameter is applicable only if the Iterate or the Clone mediator is used to handle a higher load. synapse.global_timeout_interval_millis integer \"120000\" The maximum number of milliseconds within which a response for the request should be received. A response which arrives after the specified number of seconds cannot be correlated with the request. Hence, a warning all be logged and the request will be dropped. This parameter is also referred to as the time-out handler. synapse.enable_xpath_dom_failover boolean true If this parameter is set to true , it will be possible for ESB to switch to xpath 2.0. The default value for this parameter is false since xpath 2.0 evaluations can cause performance degradation. Warning WSO2 EI uses the Saxon Home Edition in implementing XPATH 2.0 functionalities, and thus supports all the functions that are shipped with it. For more information on the supported functions, see Saxon Documentation. synapse.temp_data_chunk_size integer 3072 The message size that can be processed by the ESB server. synapse.command_debugger_port integer 9005 The..... synapse.event_debugger_port integer 9006 The..... synapse.script_mediator_pool_size integer 15 When using externally referenced scripts, this parameter is used to specify the size of the script engine pool to be used per script mediator. The script engines from this pool are used for externally referenced script execution where updates to external scripts on an engine currently in use may otherwise not be thread safe. It is recommended to keep this value at a reasonable size since there will be a pool per externally referenced script. synapse.enable_xml_nil boolean false The..... synapse.disable_auto_primitive_regex string \"^-?(0|[1-9][0-9]*)(\\\\.[0-9]+)?([eE][+-]?[0-9]+)?$\" The..... synapse.disable_custom_replace_regex string \"@@@\" The..... synapse.enable_namespace_declaration boolean false The..... synapse.build_valid_nc_name boolean false The..... synapse.enable_auto_primitive boolean false The..... synapse.json_out_auto_array boolean false The..... synapse.preserve_namespace_on_xml_to_json boolean false Preserves the namespace declarations in the JSON output in XML to JSON transformations. flow.statistics.enable boolean false Set this property to true and enable statistics for the required ESB artifact, to record information such as the following. The time spent on each mediator. The time spent to process each message. The fault count of a single message flow. flow.statistics.capture_all boolean false Set this property to true and set the mediation.flow.statistics.enable property also to true, to enable mediation statistics for all the artifacts in the ESB profile by default. Note If you set this property to false, you need to set the mediation.flow.statistics.enable property to true and manually enable statistics for the required ESB artifact. statistics.enable_clean boolean true If this parameter is set to true, all the existing statistics would be cleared before processing a request. This is recommended if you want to increase the processing speed. statistics.clean_interval string \"1000ms\" The..... flow.statistics.tracer.collect_payload boolean false Set this property to true and enable tracing for the required ESB artifact, to record the message payload before and after the mediation performed by individual mediators. flow.statistics.tracer.collect_properties boolean false Set this property to true and enable tracing for the required ESB artifact, to record the following information. Message context properties. Message transport-scope properties. inbound.core_threads integer 20 The..... inbound.max_threads integer 100 The..... [mediation] synapse.core_threads = 20 synapse.max_threads = 100 synapse.threads_queue_length = 10 synapse.global_timeout_interval = \"120000ms\" synapse.enable_xpath_dom_failover=true synapse.temp_data_chunk_size=3072 synapse.command_debugger_port=9005 synapse.event_debugger_port=9006 synapse.script_mediator_pool_size=15 synapse.enable_xml_nil=false synapse.disable_auto_primitive_regex = \"^-?(0|[1-9][0-9]*)(\\\\.[0-9]+)?([eE][+-]?[0-9]+)?$\" synapse.disable_custom_replace_regex = \"@@@\" synapse.enable_namespace_declaration = false synapse.build_valid_nc_name = false synapse.enable_auto_primitive = false synapse.json_out_auto_array = false synapse.preserve_namespace_on_xml_to_json=false flow.statistics.enable=false flow.statistics.capture_all=false statistics.enable_clean=true statistics.clean_interval = \"1000ms\" flow.statistics.tracer.collect_payloads=false flow.statistics.tracer.collect_properties=false inbound.core_threads = 20 inbound.max_threads = 100","title":"Configuration Catalog"},{"location":"references/ei_config_catalog/#configuration-catalog","text":"This document describes all the configuration parameters that are used in WSO2 Enterprise Integrator.","title":"Configuration Catalog"},{"location":"references/ei_config_catalog/#instructions-for-use","text":"Select the configuration sections, parameters, and values that are required for your use and add them to the .toml file. See the example .toml file given below. # This is an example .toml file. [server] pattern=\"value\" enable_port_forward=true [[custom_transport.listener]] class=\"value\" protocol = \"value\"","title":"Instructions for use"},{"location":"references/enqueue-Mediator/","text":"Enqueue Mediator \u00b6 Info Note Please note that this feature is deprecated. The Enqueue mediator uses a priority executor to handle messages. Priority executors are used in high-load scenarios when you want to execute different sequences for messages with different priorities. This approach allows you to control the resources allocated to executing sequences and to prevent high-priority messages from getting delayed and dropped. For example, if there are two priorities with value 10 and 1, messages with priority 10 will get 10 times more resources than messages with priority 1. Info The Enqueue mediator is a content-unaware mediator. Syntax | Configuration | Example Syntax \u00b6 <enqueue xmlns=\"http://ws.apache.org/ns/synapse\" executor=\"[Priority Executor Name]\" priority=\"[Priority]\" sequence=\"[Sequence Name]\"> </enqueue> Configuration \u00b6 The parameters available to configure the Enqueue mediator are as follows. Parameter Name Description Executor The priority executor that should be used to process the messages. Priority The priority of messages that this mediator will handle. This priority level should also be defined in the priority executor. Sequence The sequence that should be used to process messages with the specified priority. This sequence should be saved in the registry before it can be selected here. Click either Configuration Registry or the Governance Registry to select the required sequence from the resource tree. Example \u00b6 In this example, two Enqueue mediator configurations use the priority executor One based on which requests are prioritized. A sequence named Send applies to requests with priority 2 , and a sequence named LogSend applies to priority 1 . <inSequence xmlns=\"http://ws.apache.org/ns/synapse\"> <enqueue executor=\"One\" priority=\"2\" sequence=\"conf:/Send\"></enqueue> <enqueue executor=\"One\" priority=\"1\" sequence=\"conf:/LogSend\"></enqueue> </inSequence> Samples \u00b6 For another example, see Sample 652: Priority Based Message Mediation .","title":"Enqueue Mediator"},{"location":"references/enqueue-Mediator/#enqueue-mediator","text":"Info Note Please note that this feature is deprecated. The Enqueue mediator uses a priority executor to handle messages. Priority executors are used in high-load scenarios when you want to execute different sequences for messages with different priorities. This approach allows you to control the resources allocated to executing sequences and to prevent high-priority messages from getting delayed and dropped. For example, if there are two priorities with value 10 and 1, messages with priority 10 will get 10 times more resources than messages with priority 1. Info The Enqueue mediator is a content-unaware mediator. Syntax | Configuration | Example","title":"Enqueue Mediator"},{"location":"references/enqueue-Mediator/#syntax","text":"<enqueue xmlns=\"http://ws.apache.org/ns/synapse\" executor=\"[Priority Executor Name]\" priority=\"[Priority]\" sequence=\"[Sequence Name]\"> </enqueue>","title":"Syntax"},{"location":"references/enqueue-Mediator/#configuration","text":"The parameters available to configure the Enqueue mediator are as follows. Parameter Name Description Executor The priority executor that should be used to process the messages. Priority The priority of messages that this mediator will handle. This priority level should also be defined in the priority executor. Sequence The sequence that should be used to process messages with the specified priority. This sequence should be saved in the registry before it can be selected here. Click either Configuration Registry or the Governance Registry to select the required sequence from the resource tree.","title":"Configuration"},{"location":"references/enqueue-Mediator/#example","text":"In this example, two Enqueue mediator configurations use the priority executor One based on which requests are prioritized. A sequence named Send applies to requests with priority 2 , and a sequence named LogSend applies to priority 1 . <inSequence xmlns=\"http://ws.apache.org/ns/synapse\"> <enqueue executor=\"One\" priority=\"2\" sequence=\"conf:/Send\"></enqueue> <enqueue executor=\"One\" priority=\"1\" sequence=\"conf:/LogSend\"></enqueue> </inSequence>","title":"Example"},{"location":"references/enqueue-Mediator/#samples","text":"For another example, see Sample 652: Priority Based Message Mediation .","title":"Samples"},{"location":"references/enrich-Mediator/","text":"Enrich Mediator \u00b6 The Enrich Mediator can process a message based on a given source configuration and then perform the specified action on the message by using the target configuration. It gets an OMElement using the configuration specified in the source and then modifies the message by putting it on the current message using the configuration in the target. Info The Enrich mediator is a content-aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <enrich> <source [clone=true|false] [type=custom|envelope|body|property|inline] xpath | json-eval(JSON-Path)=\"\" property=\"\" /> <target [action=replace|child|sibiling] [type=custom|envelope|body|property|inline] xpath | json-eval(JSON-Path)=\"\" property=\"\" /> </enrich> Configuration \u00b6 The main properties of the Enrich Mediator available are: Source Target Source configuration \u00b6 The following properties are available: Clone - By setting the clone configuration, the message can be cloned or used as a reference during enriching. The default value is true. True False Type - The type that the mediator uses from the original message to enrich the modified message that passes through the mediator. Custom - Custom XPath value. Envelope - Envelope of the original message used for enriching. Body - Body of the original message used for enriching. Property - Specifies a property. For information on how you can use the Property mediator to specify properties, see Property Mediator . XPath Expression - This field is used to specify the custom XPath value if you selected custom for the Type field. Info Tip You can click the Namespaces link to add namespaces if you are providing an expression. You will be provided another panel named \"Namespace Editor,\" where you can provide any number of namespace prefixes and URL that you have used in the XPath expression. Target Configuration \u00b6 The following properties are available: Action - By specifying the action type, the relevant action can be applied to outgoing messages. Replace - Replace is the default value of Action . It will be used if a specific value for Action is not given. This replaces the XML message based on the target type specified on the target configuration. Child - Adding as a child of the specified target type. Sibling - Adding as a sibling of the specified target type. Info For the target type ' envelope ', the action type should be 'replace '. Herein, action type ' child ' is not acceptable because it adds an envelope within an envelope, and action type ' sibling ' is also not acceptable because there will be two envelopes in a message if you use it. Type and ****XPath Expression**** - Refer the Source configuration above. !!! info The target type depends on the source type. For the valid and invalid combinations of source and target types, see below table. Examples \u00b6 Example 1: Setting the property symbol \u00b6 In this example, you are setting the property symbol. Later, you can log it using the Log Mediator . <enrich xmlns=\"http://ws.apache.org/ns/synapse\"> <source clone=\"false\" type=\"envelope\"/> <target type=\"property\" property=\"payload\" /> </enrich> #### Example 2: Adding a child object to a property In this example, you add a child property named Lamborghini to a property named Cars. The configuration for this is as follows: ``` html/xml Lamborghini #### Example 3 - Adding a SOAPEnvelope type object as a property to a message In this example, you add the SOAP envelope in a SOAP request as a property to a message. The Enrich mediator is useful in this scenario since adding the property directly using the [Property mediator](_Property_Mediator_) results in the ` SOAPEnvelope ` object being created as an ` OM ` type object. The ` OM ` type object created cannot be converted back to a ` SOAPEnvelope ` object. ``` xml <enrich> <source type=\"envelope\" clone=\"true\"/> <target type=\"property\" property=\"ExtractedEnvelope\"/> </enrich> #### Example 4 - Preserving the original payload In this example, you copy the original payload to a property using the Enrich mediator. <enrich> <source clone=\"false\" type=\"body\"/> <target action=\"replace\" type=\"property\" property=\"ORGINAL_PAYLOAD\"/> </enrich> Then whenever you need the original payload, you replace the message body with this property value using the Enrich mediator as follows: <enrich> <source clone=\"false\" type=\"property\" property=\"ORIGINAL_PAYLOAD\"/> <target action=\"replace\" type=\"body\"/> </enrich> For other example using the Enrich mediator, see [Sample 15: Using the Enrich Mediator for Message Copying and Content Enrichment](https://docs.wso2.com/display/EI6xx/Sample+15%3A+Using+the+Enrich+Mediator+for+Message+Copying+and+Content+Enrichment) and [Sample 440: Converting JSON to XML Using XSLT](https://docs.wso2.com/display/EI6xx/Sample+440%3A+Converting+JSON+to+XML+Using+XSLT) .","title":"Enrich Mediator"},{"location":"references/enrich-Mediator/#enrich-mediator","text":"The Enrich Mediator can process a message based on a given source configuration and then perform the specified action on the message by using the target configuration. It gets an OMElement using the configuration specified in the source and then modifies the message by putting it on the current message using the configuration in the target. Info The Enrich mediator is a content-aware mediator. Syntax | Configuration | Examples","title":"Enrich Mediator"},{"location":"references/enrich-Mediator/#syntax","text":"<enrich> <source [clone=true|false] [type=custom|envelope|body|property|inline] xpath | json-eval(JSON-Path)=\"\" property=\"\" /> <target [action=replace|child|sibiling] [type=custom|envelope|body|property|inline] xpath | json-eval(JSON-Path)=\"\" property=\"\" /> </enrich>","title":"Syntax"},{"location":"references/enrich-Mediator/#configuration","text":"The main properties of the Enrich Mediator available are: Source Target","title":"Configuration"},{"location":"references/enrich-Mediator/#source-configuration","text":"The following properties are available: Clone - By setting the clone configuration, the message can be cloned or used as a reference during enriching. The default value is true. True False Type - The type that the mediator uses from the original message to enrich the modified message that passes through the mediator. Custom - Custom XPath value. Envelope - Envelope of the original message used for enriching. Body - Body of the original message used for enriching. Property - Specifies a property. For information on how you can use the Property mediator to specify properties, see Property Mediator . XPath Expression - This field is used to specify the custom XPath value if you selected custom for the Type field. Info Tip You can click the Namespaces link to add namespaces if you are providing an expression. You will be provided another panel named \"Namespace Editor,\" where you can provide any number of namespace prefixes and URL that you have used in the XPath expression.","title":"Source configuration"},{"location":"references/enrich-Mediator/#target-configuration","text":"The following properties are available: Action - By specifying the action type, the relevant action can be applied to outgoing messages. Replace - Replace is the default value of Action . It will be used if a specific value for Action is not given. This replaces the XML message based on the target type specified on the target configuration. Child - Adding as a child of the specified target type. Sibling - Adding as a sibling of the specified target type. Info For the target type ' envelope ', the action type should be 'replace '. Herein, action type ' child ' is not acceptable because it adds an envelope within an envelope, and action type ' sibling ' is also not acceptable because there will be two envelopes in a message if you use it. Type and ****XPath Expression**** - Refer the Source configuration above. !!! info The target type depends on the source type. For the valid and invalid combinations of source and target types, see below table.","title":"Target Configuration"},{"location":"references/enrich-Mediator/#examples","text":"","title":"Examples"},{"location":"references/enrich-Mediator/#example-1-setting-the-property-symbol","text":"In this example, you are setting the property symbol. Later, you can log it using the Log Mediator . <enrich xmlns=\"http://ws.apache.org/ns/synapse\"> <source clone=\"false\" type=\"envelope\"/> <target type=\"property\" property=\"payload\" /> </enrich> #### Example 2: Adding a child object to a property In this example, you add a child property named Lamborghini to a property named Cars. The configuration for this is as follows: ``` html/xml Lamborghini #### Example 3 - Adding a SOAPEnvelope type object as a property to a message In this example, you add the SOAP envelope in a SOAP request as a property to a message. The Enrich mediator is useful in this scenario since adding the property directly using the [Property mediator](_Property_Mediator_) results in the ` SOAPEnvelope ` object being created as an ` OM ` type object. The ` OM ` type object created cannot be converted back to a ` SOAPEnvelope ` object. ``` xml <enrich> <source type=\"envelope\" clone=\"true\"/> <target type=\"property\" property=\"ExtractedEnvelope\"/> </enrich> #### Example 4 - Preserving the original payload In this example, you copy the original payload to a property using the Enrich mediator. <enrich> <source clone=\"false\" type=\"body\"/> <target action=\"replace\" type=\"property\" property=\"ORGINAL_PAYLOAD\"/> </enrich> Then whenever you need the original payload, you replace the message body with this property value using the Enrich mediator as follows: <enrich> <source clone=\"false\" type=\"property\" property=\"ORIGINAL_PAYLOAD\"/> <target action=\"replace\" type=\"body\"/> </enrich> For other example using the Enrich mediator, see [Sample 15: Using the Enrich Mediator for Message Copying and Content Enrichment](https://docs.wso2.com/display/EI6xx/Sample+15%3A+Using+the+Enrich+Mediator+for+Message+Copying+and+Content+Enrichment) and [Sample 440: Converting JSON to XML Using XSLT](https://docs.wso2.com/display/EI6xx/Sample+440%3A+Converting+JSON+to+XML+Using+XSLT) .","title":"Example 1: Setting the property symbol"},{"location":"references/entitlement-Mediator/","text":"Entitlement Mediator \u00b6 The Entitlement Mediator intercepts requests and evaluates the actions performed by a user against an eXtensible Access Control Markup Language (XACML) policy. This supports XACML 2.0 and 3.0. WSO2 Identity Server can be used as the XACML Policy Decision Point (PDP) where the policy is set, and the ESB profile of WSO2 EI serves as the XACML Policy Enforcement Point (PEP) where the policy is enforced. Syntax | Configuration | Example Syntax \u00b6 <entitlementService remoteServiceUrl=\"\" remoteServiceUserName=\"\" remoteServicePassword=\"\" callbackClass=\"org.wso2.carbon.identity.entitlement.mediator.callback.[UTEntitlementCallbackHandler|X509EntitlementCallbackHandler|SAMLEntitlementCallbackHandler|KerberosEntitlementCallbackHandler]\" client=\"soap|basicAuth|thrift|wsXacml\"> <onReject/> <onAccept/> <advice/> <obligations/> </entitlementService> Configuration \u00b6 When you add the Entitlement mediator to a sequence, the Entitlement mediator node appears as follows with four sub elements. These sub elements are used to define a mediation sequence to be applied based on the entitlement result. The following are descriptions for the four sub elements of the Entitlement mediator. Parameter Name Description OnAccept The sequence to execute when the result returned by the Entitlement mediator is Permit . For example, you can configure the sequence to direct the request to the back end server as requested. OnReject The sequence to execute when the result returned by the Entitlement mediator is Deny , Not Applicable or Indeterminate . For example, you can configure the sequence to respond to the client with the message Unauthorized Request. Obligations The sequence to execute when the XACML response contains an obligation statement. When this response is received, the Entitlement mediator clones the current message context, creates a new message context, adds the obligation statement to the SOAP body and then executes the sequence. Since the Obligations sequence is executed synchronously, the Entitlement mediator waits for a response. If the sequence returns a true value, the sequence defined for the OnAccept sub element is applied. If the sequence returns a false value, the sequence defined for the OnReject sub element is applied. Advice The sequence to execute when the XACML response contains an advice statement. When this response is received, the Entitlement mediator clones the current message context, creates a new message context, adds the advice statement to the SOAP body and then executes the sequence. Since the Advice sequence is executed asynchronously, the Entitlement mediator does not wait for a response. The Entitlement mediator configuration screen appears below the tree as shown below. The parameters available for configuring the Entitlement mediator are as follows. Parameter Name Description Entitlement Server Server URL of the WSO2 Identity Server that acts as the PDP (e.g., https://localhost:9443/services ). User Name This user should have permissions to log in and manage configurations in the WSO2 Identity Server. Password The password of the username entered in the User Name parameter. Entitlement Callback Handler The handler that should be used to get the subject (user name) for the XACML request. !!! tip You need to secure the proxy service , which uses the Entitlement mediator using one of the following methods and select the Entitlement Callback Handler based on the method you used. UT : This class looks for the subject name in the Axis2 message context under the username property. This is useful when the UsernameToken security is enabled in the ESB profile for a proxy service, because when the user is authenticated for such a proxy service, the username would be set in the Axis2 message context. As a result, the Entitlement mediator would automatically get the subject value for the XACML request from there. This is the default callback class. X509 : Specify this class if the proxy is secured with X509 certificates. SAML : Specify this class if the proxy is secured with WS-Trust. Kerberos : Specify this class if the proxy is secured with Kerberos. Custom : This allows you to specify a custom entitlement callback handler class. !!! info You can also set properties that control how the subject is retrieved; see Advanced Callback Properties . Entitlement Service Client The method of communication to use between the PEP and the PDP. For SOAP, choose whether to use Basic Authentication (available with WSO2 Identify Server 4.0.0 and later) OR the AuthenticationAdmin service, which authenticates with the Entitlement service in Identity Server 3.2.3 and earlier. Thrift uses its own authentication service over TCP. WS-XACML uses Basic Authentication. !!! info The XAMCL standard refrains from specifying which method should be used to communicate from the PEP to the PDP, and many vendors have implemented a proprietary approach. There is a standard called \u201cWeb Services Profile of XACML (WS-XACML) Version 1.0\u2033, but it has not been widely adopted because of its bias toward SOAP and the performance implications from XML signatures. However, the benefit of adopting a standard is the elimination of vendor locking, because it will allow your current PEP to work even if you move to a PDP from another vendor (as long as the new PDP also supports this standard). Otherwise you may need to modify your existing PEP to adopt to the new PDP. WSO2 Identity Server has its proprietary SOAP API, Thrift API, and basic support for WS-XACML. Thrift Host The host used to establish a Thrift connection with the Entitlement service when the Entitlement Service Client is set to Thrift. Thrift Port The port used to establish a Thrift connection with the Entitlement service when the Entitlement Service Client is set to Thrift. The default port is 10500. You will now define the sequences you want to run for the entitlement results. If you want to specify an existing sequence for a result, click Referring Sequence for that result and select the sequence from the registry. If you want to define the sequence in the tree, leave In-Lined Sequence selected. Click Update . In the tree, click the first result node for which you want to define the sequence, and then add the appropriate mediators to create the sequence. Repeat for each result node. Example \u00b6 In the following example, the WSO2 Identity Server (with log in URL https://localhost:9443/service s) is see to authenticate the user invoking the secured backend service. If the authorization test performed on a request sent to this URL fails, the Fault mediator converts the request into a fault message giving Unauthorized as the reason for the request to be rejected and XACML Authorization Failed as the detail. Then the Respond mediator sends the converted message back to the client. If the user is successfully authenticated, the request is sent using the Send Mediator to the endpoint with the http://localhost:8281/services/echo\"/ URL. html/xml <entitlementService remoteServiceUrl=\"https://localhost:9443/services\" remoteServiceUserName=\"admin\" remoteServicePassword= \"enc:kuv2MubUUveMyv6GeHrXr9il59ajJIqUI4eoYHcgGKf/BBFOWn96NTjJQI+wYbWjKW6r79S7L7ZzgYeWx7DlGbff5X3pBN2Gh9yV0BHP1E93QtFqR7uTWi141Tr7V7ZwScwNqJbiNoV+vyLbsqKJE7T3nP8Ih9Y6omygbcLcHzg=\" callbackClass=\"org.wso2.carbon.identity.entitlement.mediator.callback.UTEntitlementCallbackHandler\" client=\"basicAuth\"> <onReject> <makefault version=\"soap12\"> <code xmlns:soap12Env=\"http://www.w3.org/2003/05/soap-envelope\" value=\"soap12Env:Receiver\"/> <reason value=\"UNAUTHORIZED\"/> <node/> <role/> <detail>XACML Authorization Failed</detail> </makefault> <respond/> </onReject> <onAccept> <send> <endpoint> <address uri=\"http://localhost:8281/services/echo\"/> </endpoint> </send> </onAccept> <obligations/> <advice/> </entitlementService>","title":"Entitlement Mediator"},{"location":"references/entitlement-Mediator/#entitlement-mediator","text":"The Entitlement Mediator intercepts requests and evaluates the actions performed by a user against an eXtensible Access Control Markup Language (XACML) policy. This supports XACML 2.0 and 3.0. WSO2 Identity Server can be used as the XACML Policy Decision Point (PDP) where the policy is set, and the ESB profile of WSO2 EI serves as the XACML Policy Enforcement Point (PEP) where the policy is enforced. Syntax | Configuration | Example","title":"Entitlement Mediator"},{"location":"references/entitlement-Mediator/#syntax","text":"<entitlementService remoteServiceUrl=\"\" remoteServiceUserName=\"\" remoteServicePassword=\"\" callbackClass=\"org.wso2.carbon.identity.entitlement.mediator.callback.[UTEntitlementCallbackHandler|X509EntitlementCallbackHandler|SAMLEntitlementCallbackHandler|KerberosEntitlementCallbackHandler]\" client=\"soap|basicAuth|thrift|wsXacml\"> <onReject/> <onAccept/> <advice/> <obligations/> </entitlementService>","title":"Syntax"},{"location":"references/entitlement-Mediator/#configuration","text":"When you add the Entitlement mediator to a sequence, the Entitlement mediator node appears as follows with four sub elements. These sub elements are used to define a mediation sequence to be applied based on the entitlement result. The following are descriptions for the four sub elements of the Entitlement mediator. Parameter Name Description OnAccept The sequence to execute when the result returned by the Entitlement mediator is Permit . For example, you can configure the sequence to direct the request to the back end server as requested. OnReject The sequence to execute when the result returned by the Entitlement mediator is Deny , Not Applicable or Indeterminate . For example, you can configure the sequence to respond to the client with the message Unauthorized Request. Obligations The sequence to execute when the XACML response contains an obligation statement. When this response is received, the Entitlement mediator clones the current message context, creates a new message context, adds the obligation statement to the SOAP body and then executes the sequence. Since the Obligations sequence is executed synchronously, the Entitlement mediator waits for a response. If the sequence returns a true value, the sequence defined for the OnAccept sub element is applied. If the sequence returns a false value, the sequence defined for the OnReject sub element is applied. Advice The sequence to execute when the XACML response contains an advice statement. When this response is received, the Entitlement mediator clones the current message context, creates a new message context, adds the advice statement to the SOAP body and then executes the sequence. Since the Advice sequence is executed asynchronously, the Entitlement mediator does not wait for a response. The Entitlement mediator configuration screen appears below the tree as shown below. The parameters available for configuring the Entitlement mediator are as follows. Parameter Name Description Entitlement Server Server URL of the WSO2 Identity Server that acts as the PDP (e.g., https://localhost:9443/services ). User Name This user should have permissions to log in and manage configurations in the WSO2 Identity Server. Password The password of the username entered in the User Name parameter. Entitlement Callback Handler The handler that should be used to get the subject (user name) for the XACML request. !!! tip You need to secure the proxy service , which uses the Entitlement mediator using one of the following methods and select the Entitlement Callback Handler based on the method you used. UT : This class looks for the subject name in the Axis2 message context under the username property. This is useful when the UsernameToken security is enabled in the ESB profile for a proxy service, because when the user is authenticated for such a proxy service, the username would be set in the Axis2 message context. As a result, the Entitlement mediator would automatically get the subject value for the XACML request from there. This is the default callback class. X509 : Specify this class if the proxy is secured with X509 certificates. SAML : Specify this class if the proxy is secured with WS-Trust. Kerberos : Specify this class if the proxy is secured with Kerberos. Custom : This allows you to specify a custom entitlement callback handler class. !!! info You can also set properties that control how the subject is retrieved; see Advanced Callback Properties . Entitlement Service Client The method of communication to use between the PEP and the PDP. For SOAP, choose whether to use Basic Authentication (available with WSO2 Identify Server 4.0.0 and later) OR the AuthenticationAdmin service, which authenticates with the Entitlement service in Identity Server 3.2.3 and earlier. Thrift uses its own authentication service over TCP. WS-XACML uses Basic Authentication. !!! info The XAMCL standard refrains from specifying which method should be used to communicate from the PEP to the PDP, and many vendors have implemented a proprietary approach. There is a standard called \u201cWeb Services Profile of XACML (WS-XACML) Version 1.0\u2033, but it has not been widely adopted because of its bias toward SOAP and the performance implications from XML signatures. However, the benefit of adopting a standard is the elimination of vendor locking, because it will allow your current PEP to work even if you move to a PDP from another vendor (as long as the new PDP also supports this standard). Otherwise you may need to modify your existing PEP to adopt to the new PDP. WSO2 Identity Server has its proprietary SOAP API, Thrift API, and basic support for WS-XACML. Thrift Host The host used to establish a Thrift connection with the Entitlement service when the Entitlement Service Client is set to Thrift. Thrift Port The port used to establish a Thrift connection with the Entitlement service when the Entitlement Service Client is set to Thrift. The default port is 10500. You will now define the sequences you want to run for the entitlement results. If you want to specify an existing sequence for a result, click Referring Sequence for that result and select the sequence from the registry. If you want to define the sequence in the tree, leave In-Lined Sequence selected. Click Update . In the tree, click the first result node for which you want to define the sequence, and then add the appropriate mediators to create the sequence. Repeat for each result node.","title":"Configuration"},{"location":"references/entitlement-Mediator/#example","text":"In the following example, the WSO2 Identity Server (with log in URL https://localhost:9443/service s) is see to authenticate the user invoking the secured backend service. If the authorization test performed on a request sent to this URL fails, the Fault mediator converts the request into a fault message giving Unauthorized as the reason for the request to be rejected and XACML Authorization Failed as the detail. Then the Respond mediator sends the converted message back to the client. If the user is successfully authenticated, the request is sent using the Send Mediator to the endpoint with the http://localhost:8281/services/echo\"/ URL. html/xml <entitlementService remoteServiceUrl=\"https://localhost:9443/services\" remoteServiceUserName=\"admin\" remoteServicePassword= \"enc:kuv2MubUUveMyv6GeHrXr9il59ajJIqUI4eoYHcgGKf/BBFOWn96NTjJQI+wYbWjKW6r79S7L7ZzgYeWx7DlGbff5X3pBN2Gh9yV0BHP1E93QtFqR7uTWi141Tr7V7ZwScwNqJbiNoV+vyLbsqKJE7T3nP8Ih9Y6omygbcLcHzg=\" callbackClass=\"org.wso2.carbon.identity.entitlement.mediator.callback.UTEntitlementCallbackHandler\" client=\"basicAuth\"> <onReject> <makefault version=\"soap12\"> <code xmlns:soap12Env=\"http://www.w3.org/2003/05/soap-envelope\" value=\"soap12Env:Receiver\"/> <reason value=\"UNAUTHORIZED\"/> <node/> <role/> <detail>XACML Authorization Failed</detail> </makefault> <respond/> </onReject> <onAccept> <send> <endpoint> <address uri=\"http://localhost:8281/services/echo\"/> </endpoint> </send> </onAccept> <obligations/> <advice/> </entitlementService>","title":"Example"},{"location":"references/event-Mediator/","text":"Event Mediator \u00b6 Info Note Please note that this feature is deprecated. The Event Mediator redirects incoming events to the specified event topic. For more information, see Working with Topics and Events . Info The Event mediator is a content-aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <event xmlns=\"http://ws.apache.org/ns/synapse\" topic=\"\" [expression=\"\"] /> Configuration \u00b6 The parameters available for configuring the Event mediator are as follows: Parameter Name Description Topic Type The type of topic. The available options are as follows. Static : If this is selected, the topic to which the events are published is a static value. Dynamic : If this is selected, the topic to which the events are published is a dynamic value that should be derived via an XPath expression. Topic The topic to which the events should be published. You can enter a static value or an XPath expression based on the option you selected for the Topic Type parameter. !!! info Tip You can click NameSpaces to add namespaces when you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Expression The XPath expression used to build the message to be published to the specified topic. !!! info Tip You can click NameSpaces to add namespaces when you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Examples \u00b6 In this example, when an event notification comes to the EventingProxy proxy service, they are processed by the PublicEventSource sequence, which logs the messages and publishes them to the topic SampleEventSource . Services that subscribe to the topic SampleEventSource will then receive these messages. <!-- Simple Eventing configuration --> <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"PublicEventSource\" > <log level=\"full\"/> <event topic=\"SampleEventSource\"/> </sequence> <proxy name=\"EventingProxy\"> <target inSequence=\"PublicEventSource\" /> </proxy> </definitions> Sample \u00b6 See the following sample for another example. Sample 460: Introduction to Eventing and Event Mediator .","title":"Event Mediator"},{"location":"references/event-Mediator/#event-mediator","text":"Info Note Please note that this feature is deprecated. The Event Mediator redirects incoming events to the specified event topic. For more information, see Working with Topics and Events . Info The Event mediator is a content-aware mediator. Syntax | Configuration | Examples","title":"Event Mediator"},{"location":"references/event-Mediator/#syntax","text":"<event xmlns=\"http://ws.apache.org/ns/synapse\" topic=\"\" [expression=\"\"] />","title":"Syntax"},{"location":"references/event-Mediator/#configuration","text":"The parameters available for configuring the Event mediator are as follows: Parameter Name Description Topic Type The type of topic. The available options are as follows. Static : If this is selected, the topic to which the events are published is a static value. Dynamic : If this is selected, the topic to which the events are published is a dynamic value that should be derived via an XPath expression. Topic The topic to which the events should be published. You can enter a static value or an XPath expression based on the option you selected for the Topic Type parameter. !!! info Tip You can click NameSpaces to add namespaces when you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Expression The XPath expression used to build the message to be published to the specified topic. !!! info Tip You can click NameSpaces to add namespaces when you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression.","title":"Configuration"},{"location":"references/event-Mediator/#examples","text":"In this example, when an event notification comes to the EventingProxy proxy service, they are processed by the PublicEventSource sequence, which logs the messages and publishes them to the topic SampleEventSource . Services that subscribe to the topic SampleEventSource will then receive these messages. <!-- Simple Eventing configuration --> <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"PublicEventSource\" > <log level=\"full\"/> <event topic=\"SampleEventSource\"/> </sequence> <proxy name=\"EventingProxy\"> <target inSequence=\"PublicEventSource\" /> </proxy> </definitions>","title":"Examples"},{"location":"references/event-Mediator/#sample","text":"See the following sample for another example. Sample 460: Introduction to Eventing and Event Mediator .","title":"Sample"},{"location":"references/fastXSLT-Mediator/","text":"FastXSLT Mediator \u00b6 The FastXSLT Mediator is similar to the XSLT mediator , but it uses the Streaming XPath Parser and applies the XSLT transformation to the message stream instead of to the XML message payload. The result is a faster transformation, but you cannot specify the source, properties, features, or resources as you can with the XSLT mediator. Therefore, the FastXSLT mediator is intended to be used to gain performance in cases where the original message remains unmodified. Any pre-processing performed on the message payload will not be visible to the FastXSLT mediator, because the transformation logic is applied on the original message stream instead of the message payload. In cases where the message payload needs to be pre-processed, use the XSLT mediator instead of the FastXSLT mediator. Note The streaming XPath parser used in the Fast XSLT mediator does not support Xpath functions specified with the prefix \" fn: \". Examples are \" fn:contains \", \" fn:count \", and \" fn:concat \". For example, if you are using the VFS transport to handle files, you might want to read the content of the file as a stream and directly send the content for XSLT transformation. If you need to pre-process the message payload, such as adding or removing properties, use the XSLT mediator instead. In summary, following are the key differences between the XSLT and FastXSLT mediators: XSLT Mediator FastXSLT Mediator Performs XSLT transformations on the message payload . Performs XSLT transformations on the message stream . The message is built before processing. Therefore, you can pre-process the message payload before the XSLT transformation. The message is not built before processing. Therefore, any pre-processing on the message will not be reflected in the XSLT transformation. The performance is slower than the FastXSLT mediator. The performance is faster than the XSLT mediator. Note To enable the FastXSLT mediator, your XSLT script must include the following parameter in the XSL output. omit-xml-declaration=\"yes\" For example: <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" encoding=\"UTF-8\" indent=\"yes\"/> If you do not include this parameter in your XSLT when using the FastXSLT mediator, you will get the following error. ERROR XSLTMediator Error creating XSLT transformer Info The FastXSLT mediator is a conditionally content-aware mediator. Syntax | Configuration | Example Syntax \u00b6 <fastXSLT key=\"string\"/> For example, specify the XSLT by the key transform/example.xslt , which is used to transform the message stream as shown below. <fastxslt xmlns=\"http://ws.apache.org/ns/synapse\" key=\"transform/example.xslt\"/> Configuration \u00b6 The parameters available to configure the FastXSLT mediator are as follows. Parameter Name Description Key Type You can select one of the following options. Static Key : If this is selected, an existing key can be selected from the registry for the Key parameter. Dynamic Key : If this is selected, the key can be entered dynamically in the Key parameter. Key This specifies the registry key to refer the XSLT to. This supports static and dynamic keys. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Example \u00b6 The following example applies a simple XSLT stylesheet to a message payload via the FastXSLT mediator. The FastXSLT mediator reads values from the current XML payload using XPath and populates them into the stylesheet to create a new or different payload as the response. The ESB configuration of the API of this example is as follows: <api xmlns=\"http://ws.apache.org/ns/synapse\" context=\"/xslt\" name=\"XSLTAPI\"> <resource methods=\"POST\"> <inSequence> <fastXSLT key=\"conf:myresources/discountPayment.xsl\" /> <log level=\"full\" /> <respond /> </inSequence> <outSequence /> <faultSequence /> </resource> </api> Follow the steps below to specify the stylesheet as a Registry entry in the above API. Double click on the API and click the following link in the Properties tab. Click Create & point to a new resource... link. {height=\"250\"} Enter the following details to create the empty XSL file in which you enter the stylesheet, in the Registry. Double-click the stylesheet file in the Project Explorer , and add the following stylesheet as the content of the XSL file. discountPayment.xsl <xsl:stylesheet xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:fn=\"http://www.w3.org/2005/02/xpath-functions\" xmlns:m0=\"http://services.samples\" version=\"2.0\" exclude-result-prefixes=\"m0 fn\"> <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" indent=\"yes\" /> <xsl:template match=\"/\"> <Payment> <xsl:for-each select=\"//order/lunch[contains(drinkName, 'Coffee')]\"> <discount> <xsl:value-of select=\"drinkPrice\" /> </discount> </xsl:for-each> </Payment> </xsl:template> </xsl:stylesheet> Pass the following XML payload using SOAP UI. Info You pass this payload into the XSLT mediator specifying a certain drinkName as a parameter to the style sheet. For example, the following payload passes the drinkName as 'Coffee'. The style sheet traverses through the incoming payload and finds the <lunch> elements, which contains 'Coffee' as drinkName . When it finds matching ectries, it adds the prices of those elements under a new <Payment> element. Therefore, when the message flow comes out of XSLT mediator, the payload changes the <Payment> entry, where it contains the drinkPrice values of matching elements. Tip You can get the URI of the REST API from the Management Console as shown below. {height=\"250\"} <order> <lunch> <meal> Rice and Curry </meal> <mealPrice> USD 10 </mealPrice> <drinkName> Dark Coffee </drinkName> <drinkPrice> USD 1.8 </drinkPrice> </lunch> <lunch> <meal> Sandwiches </meal> <mealPrice> USD 4 </mealPrice> <drinkName> Milk Shake </drinkName> <drinkPrice> USD 2.6 </drinkPrice> </lunch> <lunch> <meal> Chicken Burger </meal> <mealPrice> USD 5 </mealPrice> <drinkName> Iced Coffee </drinkName> <drinkPrice> USD 1.5 </drinkPrice> </lunch> <lunch> <meal> Noodles </meal> <mealPrice> USD 8 </mealPrice> <drinkName> Bottled Water </drinkName> <drinkPrice> USD 2.5 </drinkPrice> </lunch> </order> You receive the response as shown below.","title":"FastXSLT Mediator"},{"location":"references/fastXSLT-Mediator/#fastxslt-mediator","text":"The FastXSLT Mediator is similar to the XSLT mediator , but it uses the Streaming XPath Parser and applies the XSLT transformation to the message stream instead of to the XML message payload. The result is a faster transformation, but you cannot specify the source, properties, features, or resources as you can with the XSLT mediator. Therefore, the FastXSLT mediator is intended to be used to gain performance in cases where the original message remains unmodified. Any pre-processing performed on the message payload will not be visible to the FastXSLT mediator, because the transformation logic is applied on the original message stream instead of the message payload. In cases where the message payload needs to be pre-processed, use the XSLT mediator instead of the FastXSLT mediator. Note The streaming XPath parser used in the Fast XSLT mediator does not support Xpath functions specified with the prefix \" fn: \". Examples are \" fn:contains \", \" fn:count \", and \" fn:concat \". For example, if you are using the VFS transport to handle files, you might want to read the content of the file as a stream and directly send the content for XSLT transformation. If you need to pre-process the message payload, such as adding or removing properties, use the XSLT mediator instead. In summary, following are the key differences between the XSLT and FastXSLT mediators: XSLT Mediator FastXSLT Mediator Performs XSLT transformations on the message payload . Performs XSLT transformations on the message stream . The message is built before processing. Therefore, you can pre-process the message payload before the XSLT transformation. The message is not built before processing. Therefore, any pre-processing on the message will not be reflected in the XSLT transformation. The performance is slower than the FastXSLT mediator. The performance is faster than the XSLT mediator. Note To enable the FastXSLT mediator, your XSLT script must include the following parameter in the XSL output. omit-xml-declaration=\"yes\" For example: <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" encoding=\"UTF-8\" indent=\"yes\"/> If you do not include this parameter in your XSLT when using the FastXSLT mediator, you will get the following error. ERROR XSLTMediator Error creating XSLT transformer Info The FastXSLT mediator is a conditionally content-aware mediator. Syntax | Configuration | Example","title":"FastXSLT Mediator"},{"location":"references/fastXSLT-Mediator/#syntax","text":"<fastXSLT key=\"string\"/> For example, specify the XSLT by the key transform/example.xslt , which is used to transform the message stream as shown below. <fastxslt xmlns=\"http://ws.apache.org/ns/synapse\" key=\"transform/example.xslt\"/>","title":"Syntax"},{"location":"references/fastXSLT-Mediator/#configuration","text":"The parameters available to configure the FastXSLT mediator are as follows. Parameter Name Description Key Type You can select one of the following options. Static Key : If this is selected, an existing key can be selected from the registry for the Key parameter. Dynamic Key : If this is selected, the key can be entered dynamically in the Key parameter. Key This specifies the registry key to refer the XSLT to. This supports static and dynamic keys. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression.","title":"Configuration"},{"location":"references/fastXSLT-Mediator/#example","text":"The following example applies a simple XSLT stylesheet to a message payload via the FastXSLT mediator. The FastXSLT mediator reads values from the current XML payload using XPath and populates them into the stylesheet to create a new or different payload as the response. The ESB configuration of the API of this example is as follows: <api xmlns=\"http://ws.apache.org/ns/synapse\" context=\"/xslt\" name=\"XSLTAPI\"> <resource methods=\"POST\"> <inSequence> <fastXSLT key=\"conf:myresources/discountPayment.xsl\" /> <log level=\"full\" /> <respond /> </inSequence> <outSequence /> <faultSequence /> </resource> </api> Follow the steps below to specify the stylesheet as a Registry entry in the above API. Double click on the API and click the following link in the Properties tab. Click Create & point to a new resource... link. {height=\"250\"} Enter the following details to create the empty XSL file in which you enter the stylesheet, in the Registry. Double-click the stylesheet file in the Project Explorer , and add the following stylesheet as the content of the XSL file. discountPayment.xsl <xsl:stylesheet xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:fn=\"http://www.w3.org/2005/02/xpath-functions\" xmlns:m0=\"http://services.samples\" version=\"2.0\" exclude-result-prefixes=\"m0 fn\"> <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" indent=\"yes\" /> <xsl:template match=\"/\"> <Payment> <xsl:for-each select=\"//order/lunch[contains(drinkName, 'Coffee')]\"> <discount> <xsl:value-of select=\"drinkPrice\" /> </discount> </xsl:for-each> </Payment> </xsl:template> </xsl:stylesheet> Pass the following XML payload using SOAP UI. Info You pass this payload into the XSLT mediator specifying a certain drinkName as a parameter to the style sheet. For example, the following payload passes the drinkName as 'Coffee'. The style sheet traverses through the incoming payload and finds the <lunch> elements, which contains 'Coffee' as drinkName . When it finds matching ectries, it adds the prices of those elements under a new <Payment> element. Therefore, when the message flow comes out of XSLT mediator, the payload changes the <Payment> entry, where it contains the drinkPrice values of matching elements. Tip You can get the URI of the REST API from the Management Console as shown below. {height=\"250\"} <order> <lunch> <meal> Rice and Curry </meal> <mealPrice> USD 10 </mealPrice> <drinkName> Dark Coffee </drinkName> <drinkPrice> USD 1.8 </drinkPrice> </lunch> <lunch> <meal> Sandwiches </meal> <mealPrice> USD 4 </mealPrice> <drinkName> Milk Shake </drinkName> <drinkPrice> USD 2.6 </drinkPrice> </lunch> <lunch> <meal> Chicken Burger </meal> <mealPrice> USD 5 </mealPrice> <drinkName> Iced Coffee </drinkName> <drinkPrice> USD 1.5 </drinkPrice> </lunch> <lunch> <meal> Noodles </meal> <mealPrice> USD 8 </mealPrice> <drinkName> Bottled Water </drinkName> <drinkPrice> USD 2.5 </drinkPrice> </lunch> </order> You receive the response as shown below.","title":"Example"},{"location":"references/fault-Mediator/","text":"Fault Mediator \u00b6 The Fault Mediator (also called the Makefault Mediator ) transforms the current message into a fault message. However, this mediator does not send the converted message. The Send Mediator needs to be invoked to send a fault message created via the Fault mediator. The fault message's To header is set to the Fault-To of the original message (if such a header exists in the original message). You can create the fault message as a SOAP 1.1, SOAP 1.2, or plain-old XML (POX) fault. For more information on faults and errors, see Error Handling . Syntax | UI Configuration | Examples | Samples Syntax \u00b6 <makefault [version=\"soap11|soap12|pox\"]> <code (value=\"literal\" | expression=\"xpath\")/> <reason (value=\"literal\" | expression=\"xpath\")> <node>? <role>? <detail>? </makefault> UI Configuration \u00b6 Click on the relevant tab to view the required UI configuration pending on whether you want to create the fault message as a SOAP 1.1 fault, SOAP 1.2 fault or a plain-old XML (POX) fault. SOAP 1.1 SOAP 1.2 Plain-old XML (POX) The parameters available to configure the Fault mediator to create a SOAP 1.1 fault are as follows. Parameter Name Description Fault Code This parameter is used to select the fault code for which the fault string should be defined. Possible values are as follows. versionMismatch : Select this to specify the fault string for a SOAP version mismatch. mustUnderstand : Select this to specify the fault string for the mustUnderstand error in SOAP. Client : Select this to specify the fault string for client side errors. Server : Select this to specify the fault string for server side errors. Fault String The detailed fault string of the fault code. The following options are available. value : If this option is selected, the fault string is specified as a string value. expression : If this option is selected, the fault string is specified as an expression. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Fault Actor The element of the SOAP fault message which is used to capture the party which caused the fault. Detail This parameter is used to enter a custom description of the error. The parameters available to configure the Fault mediator to create a SOAP 1.2 fault are as follows. Parameter Name Description Code This parameter is used to select the fault code for which the reason should be defined. Possible values are as follows. versionMismatch : Select this to specify the reason for a SOAP version mismatch. mustUnderstand : Select this to specify the reason for the mustUnderstand error in SOAP. dataEncodingUnknown : Select this to specify the reason for a SOAP encoding error. Sender : Select this ti specify the reason for a sender-side error. Receiver : Select this to specify the reason for a receiver-side error. Reason This parameter is used to specify the reason for the error code selected in the Code parameter. The following options are available. value : If this option is selected, the reason is specified as a string value. expression : If this option is selected, the reason is specified as an expression. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Role The SOAP 1.1 role name. Node The SOAP 1.2 node name. Detail This parameter is used to enter a custom description of the error. The parameters available to configure the Fault mediator to create a plain-old XML (POX) fault are as follows. Parameter Name Description Reason This parameter is used to enter a custom fault message. The following options are available. value : If this option is selected, the fault message is specified as a string value. expression : If this option is selected, the fault message is specified as an expression. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Detail This parameter is used to enter details for the fault message. The following options are available. value : If this option is selected, the detail is specified as a string value. expression : If this option is selected, the detail is specified as an expression. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Examples \u00b6 Following are examples of different usages of the Fault Mediator. Example one \u00b6 In the following example, the testmessage string value is given as the reason for the SOAP error versionMismatch . <makefault xmlns=\"http://ws.apache.org/ns/synapse\" version=\"soap11\"> <code xmlns:soap11Env=\"http://schemas.xmlsoap.org/soap/envelope/\" value=\"soap11Env:VersionMismatch\" /> <reason value=\"test message \" /> <role></role> </makefault> Example two \u00b6 The following sample proxy validates the content type using the Filter Mediator based on the Content-Type header property. If the result is true, it sends an exception back to the client using the Fault Mediator. Else, if the result is false, it continues the flow. <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"CheckContentType\" transports=\"https http\" startOnLoad=\"true\" trace=\"disable\"> <description/> <target> <inSequence> <log level=\"custom\"> <property name=\"_______Content-Type\" expression=\"get-property('transport','Content-Type')\"/> </log> <filter source=\"get-property('transport','Content-Type')\" regex=\"application/xhtml\\+xml\"> <then> <log> <property name=\"Content-Type\" expression=\"get-property('transport','Content-Type')\"/> <property name=\"Decision\" value=\"Exception, due to unexpected Content-Type.\"/> </log> <makefault version=\"soap11\"> <code xmlns:soap11Env=\"http://schemas.xmlsoap.org/soap/envelope/\" value=\"soap11Env:Client\"/> <reason value=\"Content-Type Error\"/> <role/> <detail>Content-Type: application/xhtml+xml is not a valid content type.</detail> </makefault> <header name=\"To\" scope=\"default\" action=\"remove\"/> <send/> </then> <else> <log> <property name=\"Content-Type\" expression=\"get-property('transport','Content-Type')\"/> <property name=\"Decision\" value=\"Continue the mediation flow...\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </else> </filter> </inSequence> <outSequence> <send/> </outSequence> </target> <publishWSDL uri=\"http://localhost:9000/services/SimpleStockQuoteService?wsdl\"/> </proxy> Samples \u00b6 Sample 5: Creating SOAP Fault Messages and Changing the Direction of a Message .","title":"Fault Mediator"},{"location":"references/fault-Mediator/#fault-mediator","text":"The Fault Mediator (also called the Makefault Mediator ) transforms the current message into a fault message. However, this mediator does not send the converted message. The Send Mediator needs to be invoked to send a fault message created via the Fault mediator. The fault message's To header is set to the Fault-To of the original message (if such a header exists in the original message). You can create the fault message as a SOAP 1.1, SOAP 1.2, or plain-old XML (POX) fault. For more information on faults and errors, see Error Handling . Syntax | UI Configuration | Examples | Samples","title":"Fault Mediator"},{"location":"references/fault-Mediator/#syntax","text":"<makefault [version=\"soap11|soap12|pox\"]> <code (value=\"literal\" | expression=\"xpath\")/> <reason (value=\"literal\" | expression=\"xpath\")> <node>? <role>? <detail>? </makefault>","title":"Syntax"},{"location":"references/fault-Mediator/#ui-configuration","text":"Click on the relevant tab to view the required UI configuration pending on whether you want to create the fault message as a SOAP 1.1 fault, SOAP 1.2 fault or a plain-old XML (POX) fault. SOAP 1.1 SOAP 1.2 Plain-old XML (POX) The parameters available to configure the Fault mediator to create a SOAP 1.1 fault are as follows. Parameter Name Description Fault Code This parameter is used to select the fault code for which the fault string should be defined. Possible values are as follows. versionMismatch : Select this to specify the fault string for a SOAP version mismatch. mustUnderstand : Select this to specify the fault string for the mustUnderstand error in SOAP. Client : Select this to specify the fault string for client side errors. Server : Select this to specify the fault string for server side errors. Fault String The detailed fault string of the fault code. The following options are available. value : If this option is selected, the fault string is specified as a string value. expression : If this option is selected, the fault string is specified as an expression. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Fault Actor The element of the SOAP fault message which is used to capture the party which caused the fault. Detail This parameter is used to enter a custom description of the error. The parameters available to configure the Fault mediator to create a SOAP 1.2 fault are as follows. Parameter Name Description Code This parameter is used to select the fault code for which the reason should be defined. Possible values are as follows. versionMismatch : Select this to specify the reason for a SOAP version mismatch. mustUnderstand : Select this to specify the reason for the mustUnderstand error in SOAP. dataEncodingUnknown : Select this to specify the reason for a SOAP encoding error. Sender : Select this ti specify the reason for a sender-side error. Receiver : Select this to specify the reason for a receiver-side error. Reason This parameter is used to specify the reason for the error code selected in the Code parameter. The following options are available. value : If this option is selected, the reason is specified as a string value. expression : If this option is selected, the reason is specified as an expression. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Role The SOAP 1.1 role name. Node The SOAP 1.2 node name. Detail This parameter is used to enter a custom description of the error. The parameters available to configure the Fault mediator to create a plain-old XML (POX) fault are as follows. Parameter Name Description Reason This parameter is used to enter a custom fault message. The following options are available. value : If this option is selected, the fault message is specified as a string value. expression : If this option is selected, the fault message is specified as an expression. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Detail This parameter is used to enter details for the fault message. The following options are available. value : If this option is selected, the detail is specified as a string value. expression : If this option is selected, the detail is specified as an expression. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression.","title":"UI Configuration"},{"location":"references/fault-Mediator/#examples","text":"Following are examples of different usages of the Fault Mediator.","title":"Examples"},{"location":"references/fault-Mediator/#example-one","text":"In the following example, the testmessage string value is given as the reason for the SOAP error versionMismatch . <makefault xmlns=\"http://ws.apache.org/ns/synapse\" version=\"soap11\"> <code xmlns:soap11Env=\"http://schemas.xmlsoap.org/soap/envelope/\" value=\"soap11Env:VersionMismatch\" /> <reason value=\"test message \" /> <role></role> </makefault>","title":"Example one"},{"location":"references/fault-Mediator/#example-two","text":"The following sample proxy validates the content type using the Filter Mediator based on the Content-Type header property. If the result is true, it sends an exception back to the client using the Fault Mediator. Else, if the result is false, it continues the flow. <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"CheckContentType\" transports=\"https http\" startOnLoad=\"true\" trace=\"disable\"> <description/> <target> <inSequence> <log level=\"custom\"> <property name=\"_______Content-Type\" expression=\"get-property('transport','Content-Type')\"/> </log> <filter source=\"get-property('transport','Content-Type')\" regex=\"application/xhtml\\+xml\"> <then> <log> <property name=\"Content-Type\" expression=\"get-property('transport','Content-Type')\"/> <property name=\"Decision\" value=\"Exception, due to unexpected Content-Type.\"/> </log> <makefault version=\"soap11\"> <code xmlns:soap11Env=\"http://schemas.xmlsoap.org/soap/envelope/\" value=\"soap11Env:Client\"/> <reason value=\"Content-Type Error\"/> <role/> <detail>Content-Type: application/xhtml+xml is not a valid content type.</detail> </makefault> <header name=\"To\" scope=\"default\" action=\"remove\"/> <send/> </then> <else> <log> <property name=\"Content-Type\" expression=\"get-property('transport','Content-Type')\"/> <property name=\"Decision\" value=\"Continue the mediation flow...\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </else> </filter> </inSequence> <outSequence> <send/> </outSequence> </target> <publishWSDL uri=\"http://localhost:9000/services/SimpleStockQuoteService?wsdl\"/> </proxy>","title":"Example two"},{"location":"references/fault-Mediator/#samples","text":"Sample 5: Creating SOAP Fault Messages and Changing the Direction of a Message .","title":"Samples"},{"location":"references/filter-Mediator/","text":"Filter Mediator \u00b6 The Filter Mediator can be used for filtering messages based on an XPath, JSONPath or a regular expression. If the test succeeds, the Filter mediator executes the other mediators enclosed in the sequence. The Filter Mediator closely resembles the \"If-else\" control structure. Info The Filter mediator is a conditionally content aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <filter (source=\"[XPath|json-eval(JSONPath)]\" regex=\"string\") | xpath=\"[XPath|json-eval(JSONPath)]\"> mediator+ </filter> This mediator could also be used to handle a scenario where two different sequences are applied to messages that meet the filter criteria and messages that do not meet the filter criteria. <filter (source=\"[XPath|json-eval(JSONPath)]\" regex=\"string\") | xpath=\"[XPath|json-eval(JSONPath)]\"> <then [sequence=\"string\"]> mediator+ </then> <else [sequence=\"string\"]> mediator+ </else> </filter> In this case, the Filter condition remains the same. The messages that match the filter criteria will be mediated using the set of mediators enclosed in the then element. The messages that do not match the filter criteria will be mediated using the set of mediators enclosed in the else element. Configuration \u00b6 The parameters available for configuring the Filter mediator are as follows: Parameter Name Description Specify As This is used to specify whether you want to specify the filter criteria via an XPath expression or a regular expression. XPath : If this option is selected, the Filter mediator tests the given XPath/JSONPath expression as a Boolean expression. When specifying a JSONPath, use the format json-eval(<JSON_PATH>) , such as json-eval(getQuote.request.symbol) . For more information on using JSON with the ESB profile , see Working with JSON Message Payloads . Source and Regular Expression : If this option is selected, the Filter mediator matches the evaluation result of a source XPath/JSONPath expression as a string against the given regular expression. Source The expression to locate the value that matches the regular expression that you can define in the Regex parameter. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Regex The regular expression to match the source value. Examples \u00b6 Example 1: Sending only messages matching the filter criteria \u00b6 In this example, the Filter will get the To header value and match it against the given regular expression. If this evaluation returns true , it will send the message. If the evaluation returns false , it will drop the message. <filter source=\"get-property('To')\" regex=\".*/StockQuote.*\"> <then> <send/> </then> <else> <drop/> </else> </filter> Example 2: Applying separate sequences \u00b6 In this example, the Log mediator is used to log information from a service named Bus Services via a property when the request matches the filter criteria. When the request does not match the filter criteria, another log mediator configuration is used log information from a service named Train Service in a similar way. <filter source=\"get-property('Action')\" regex=\".*getBusNo\"> <then> <log level=\"custom\"> <property name=\"service\" value=\"Bus Services is called\"/> </log> </then> <else> <log level=\"custom\"> <property name=\"service\" value=\"Train Service is called\"/> </log> </else> </filter>","title":"Filter Mediator"},{"location":"references/filter-Mediator/#filter-mediator","text":"The Filter Mediator can be used for filtering messages based on an XPath, JSONPath or a regular expression. If the test succeeds, the Filter mediator executes the other mediators enclosed in the sequence. The Filter Mediator closely resembles the \"If-else\" control structure. Info The Filter mediator is a conditionally content aware mediator. Syntax | Configuration | Examples","title":"Filter Mediator"},{"location":"references/filter-Mediator/#syntax","text":"<filter (source=\"[XPath|json-eval(JSONPath)]\" regex=\"string\") | xpath=\"[XPath|json-eval(JSONPath)]\"> mediator+ </filter> This mediator could also be used to handle a scenario where two different sequences are applied to messages that meet the filter criteria and messages that do not meet the filter criteria. <filter (source=\"[XPath|json-eval(JSONPath)]\" regex=\"string\") | xpath=\"[XPath|json-eval(JSONPath)]\"> <then [sequence=\"string\"]> mediator+ </then> <else [sequence=\"string\"]> mediator+ </else> </filter> In this case, the Filter condition remains the same. The messages that match the filter criteria will be mediated using the set of mediators enclosed in the then element. The messages that do not match the filter criteria will be mediated using the set of mediators enclosed in the else element.","title":"Syntax"},{"location":"references/filter-Mediator/#configuration","text":"The parameters available for configuring the Filter mediator are as follows: Parameter Name Description Specify As This is used to specify whether you want to specify the filter criteria via an XPath expression or a regular expression. XPath : If this option is selected, the Filter mediator tests the given XPath/JSONPath expression as a Boolean expression. When specifying a JSONPath, use the format json-eval(<JSON_PATH>) , such as json-eval(getQuote.request.symbol) . For more information on using JSON with the ESB profile , see Working with JSON Message Payloads . Source and Regular Expression : If this option is selected, the Filter mediator matches the evaluation result of a source XPath/JSONPath expression as a string against the given regular expression. Source The expression to locate the value that matches the regular expression that you can define in the Regex parameter. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Regex The regular expression to match the source value.","title":"Configuration"},{"location":"references/filter-Mediator/#examples","text":"","title":"Examples"},{"location":"references/filter-Mediator/#example-1-sending-only-messages-matching-the-filter-criteria","text":"In this example, the Filter will get the To header value and match it against the given regular expression. If this evaluation returns true , it will send the message. If the evaluation returns false , it will drop the message. <filter source=\"get-property('To')\" regex=\".*/StockQuote.*\"> <then> <send/> </then> <else> <drop/> </else> </filter>","title":"Example 1: Sending only messages matching the filter criteria"},{"location":"references/filter-Mediator/#example-2-applying-separate-sequences","text":"In this example, the Log mediator is used to log information from a service named Bus Services via a property when the request matches the filter criteria. When the request does not match the filter criteria, another log mediator configuration is used log information from a service named Train Service in a similar way. <filter source=\"get-property('Action')\" regex=\".*getBusNo\"> <then> <log level=\"custom\"> <property name=\"service\" value=\"Bus Services is called\"/> </log> </then> <else> <log level=\"custom\"> <property name=\"service\" value=\"Train Service is called\"/> </log> </else> </filter>","title":"Example 2: Applying separate sequences"},{"location":"references/forEach-Mediator/","text":"ForEach Mediator \u00b6 The ForEach mediator requires an XPath expression and a sequence (inline or referred). It splits the message into a number of different messages derived from the original message by finding matching elements for the XPath expression specified. Based on the matching elements, new messages are created for each iteration and processed sequentially. The processing is carried out based on a specified sequence. The behaviour of ForEach mediator is similar to a generic loop. After mediation, the sub-messages are merged back to their original parent element in the original message sequentially. The ForEach mediator creates the following properties during mediation. Property Description FOREACH_ORIGINAL_MESSAGE This contains the original envelop of the messages split by the ForEach mediator. FOREACH_COUNTER This contains the count of the messages processed. The message count increases during each iteration. Note Iterate Mediator is quite similar to the ForEach mediator. You can use complex XPath expressions to conditionally select elements to iterate over in both mediators. Following are the main difference between ForEach and Iterate mediators: Use the ForEach mediator only for message transformations. If you need to make back-end calls from each iteration, then use the iterate mediator. ForEach supports modifying the original payload. You can use Iterate for situations where you send the split messages to a target and collect them by an Aggregate in a different flow You need to always accompany an Iterate with an Aggregate mediator. ForEach loops over the sub-messages and merges them back to the same parent element of the message. In Iterate you need to send the split messages to an endpoint to continue the message flow. However, ForEach does not allow using Call , Send and Callout mediators in the sequence. ForEach does not split the message flow, unlike Iterate Mediator. It guarantees to execute in the same thread until all iterations are complete. When you use ForEach mediator, you can only loop through segments of the message and do changes to a particular segment. For example, you can change the payload using payload factory mediator. But you cannot send the split message out to a service. Once you exit from the ForEach loop, it automatically aggregates the split segments. This replaces the ForEach function of the complex XSLT mediators using a ForEach mediator and a Payload Factory mediator. However, to implement the split-aggregate pattern, you still need to use Iterate mediator. Syntax | Configuration | Examples | Sample Syntax \u00b6 <foreach expression=\"xpath\" [sequence=\"sequence_ref\"] [id=\"foreach_id\"] > <sequence> (mediator)+ </sequence>? </foreach> Configuration \u00b6 The parameters available to configure the ForEach mediator are as follows. Parameter Name Description ForEach ID If a value is entered for this parameter, it will be used as the prefix for the FOREACH_ORIGINAL_MESSAGE and FOREACH_COUNTER properties created during mediation. This is an optional parameter. However, it is recommended to define a ForEach ID in nested ForEach scenarios to avoid the properties mentioned from being overwritten. Expression The XPath expression with which different messages are derived by splitting the parent message. This expression should have matching elements based on which the splitting is carried out. !!! tip You can click NameSpaces to add namespaces when you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Sequence The mediation sequence that should be applied to the messages derived from the parent message. ForEach mediator is used only for transformations, thereby, you should not include Call , Send and Callout mediators, which are used to invoke endpoints, in t his sequence. You can select one of the following options. Anonymous : This allows you to define an anonymous sequence to be applied to the split messages by adding the required mediators as children of the ForEach mediator in the mediator tree. Pick from Registry : This allows you to pick an existing mediation sequence that is saved in the Registry. Click either Configuration Registry or Governance Registry as relevant to select the required mediation sequence from the Resource Tree. Examples \u00b6 In this configuration, the //m0:getQuote/m0:request\" XPath expression evaluates the split messages to be derived from the parent message. Then the split messages pass through a sequence which includes a Log mediator with the log level set to full . <foreach id=\"foreach_1\" expression=\"//m0:getQuote/m0:request\" xmlns:m0=\"http://services.samples\"> <sequence> <log level=\"full\"/> </sequence> </foreach> Sample \u00b6 See Sample 18: Transforming a Message Using ForEach Mediator .","title":"ForEach Mediator"},{"location":"references/forEach-Mediator/#foreach-mediator","text":"The ForEach mediator requires an XPath expression and a sequence (inline or referred). It splits the message into a number of different messages derived from the original message by finding matching elements for the XPath expression specified. Based on the matching elements, new messages are created for each iteration and processed sequentially. The processing is carried out based on a specified sequence. The behaviour of ForEach mediator is similar to a generic loop. After mediation, the sub-messages are merged back to their original parent element in the original message sequentially. The ForEach mediator creates the following properties during mediation. Property Description FOREACH_ORIGINAL_MESSAGE This contains the original envelop of the messages split by the ForEach mediator. FOREACH_COUNTER This contains the count of the messages processed. The message count increases during each iteration. Note Iterate Mediator is quite similar to the ForEach mediator. You can use complex XPath expressions to conditionally select elements to iterate over in both mediators. Following are the main difference between ForEach and Iterate mediators: Use the ForEach mediator only for message transformations. If you need to make back-end calls from each iteration, then use the iterate mediator. ForEach supports modifying the original payload. You can use Iterate for situations where you send the split messages to a target and collect them by an Aggregate in a different flow You need to always accompany an Iterate with an Aggregate mediator. ForEach loops over the sub-messages and merges them back to the same parent element of the message. In Iterate you need to send the split messages to an endpoint to continue the message flow. However, ForEach does not allow using Call , Send and Callout mediators in the sequence. ForEach does not split the message flow, unlike Iterate Mediator. It guarantees to execute in the same thread until all iterations are complete. When you use ForEach mediator, you can only loop through segments of the message and do changes to a particular segment. For example, you can change the payload using payload factory mediator. But you cannot send the split message out to a service. Once you exit from the ForEach loop, it automatically aggregates the split segments. This replaces the ForEach function of the complex XSLT mediators using a ForEach mediator and a Payload Factory mediator. However, to implement the split-aggregate pattern, you still need to use Iterate mediator. Syntax | Configuration | Examples | Sample","title":"ForEach Mediator"},{"location":"references/forEach-Mediator/#syntax","text":"<foreach expression=\"xpath\" [sequence=\"sequence_ref\"] [id=\"foreach_id\"] > <sequence> (mediator)+ </sequence>? </foreach>","title":"Syntax"},{"location":"references/forEach-Mediator/#configuration","text":"The parameters available to configure the ForEach mediator are as follows. Parameter Name Description ForEach ID If a value is entered for this parameter, it will be used as the prefix for the FOREACH_ORIGINAL_MESSAGE and FOREACH_COUNTER properties created during mediation. This is an optional parameter. However, it is recommended to define a ForEach ID in nested ForEach scenarios to avoid the properties mentioned from being overwritten. Expression The XPath expression with which different messages are derived by splitting the parent message. This expression should have matching elements based on which the splitting is carried out. !!! tip You can click NameSpaces to add namespaces when you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Sequence The mediation sequence that should be applied to the messages derived from the parent message. ForEach mediator is used only for transformations, thereby, you should not include Call , Send and Callout mediators, which are used to invoke endpoints, in t his sequence. You can select one of the following options. Anonymous : This allows you to define an anonymous sequence to be applied to the split messages by adding the required mediators as children of the ForEach mediator in the mediator tree. Pick from Registry : This allows you to pick an existing mediation sequence that is saved in the Registry. Click either Configuration Registry or Governance Registry as relevant to select the required mediation sequence from the Resource Tree.","title":"Configuration"},{"location":"references/forEach-Mediator/#examples","text":"In this configuration, the //m0:getQuote/m0:request\" XPath expression evaluates the split messages to be derived from the parent message. Then the split messages pass through a sequence which includes a Log mediator with the log level set to full . <foreach id=\"foreach_1\" expression=\"//m0:getQuote/m0:request\" xmlns:m0=\"http://services.samples\"> <sequence> <log level=\"full\"/> </sequence> </foreach>","title":"Examples"},{"location":"references/forEach-Mediator/#sample","text":"See Sample 18: Transforming a Message Using ForEach Mediator .","title":"Sample"},{"location":"references/generic-Properties/","text":"Generic Properties \u00b6 [ PRESERVE_WS_ADDRESSING ] [ RESPONSE ] [ OUT_ONLY ] [ ERROR_CODE ] [ ERROR_MESSAGE ] [ ERROR_DETAIL ] [ ERROR_EXCEPTION ] [ TRANSPORT_HEADERS ] [ messageType ] [ ContentType ] [ disableAddressingForOutMessages ] [ DISABLE_SMOOKS_RESULT_PAYLOAD ] [ ClientApiNonBlocking ] [ transportNonBlocking ] [ TRANSPORT_IN_NAME ] [ preserveProcessedHeaders ] [ SERVER_IP ] [ FORCE_ERROR_ON_SOAP_FAULT ] [ QUOTE_STRING_IN_PAYLOAD_FACTORY_JSON ] Generic properties allow you to configure messages as they're processed by the ESB profile, such as marking a message as out-only (no response message will be expected), adding a custom error message or code to the message, and disabling WS-Addressing headers. PRESERVE_WS_ADDRESSING \u00b6 Name PRESERVE_WS_ADDRESSING Possible Values \"true\", \"false\" Default Behavior none Scope synapse Description By default, the ESB profile adds a new set of WS-Addressing headers to the messages forwarded from the ESB profile. If this property is set to \" true \" on a message, the ESB profile will forward it without altering its existing WS-Addressing headers. Example <property name= \"PRESERVE_WS_ADDRESSING\" value= \"true\" /> RESPONSE \u00b6 Name RESPONSE Possible Values \"true\", \"false\" Default Behavior none Scope synapse Description Once this property is set to 'true' on a message, the ESB profile will start treating it as a response message. It is generally used to route a request message back to its source as the response. Example <property name= \"RESPONSE\" value= \"true\" /> OUT_ONLY \u00b6 Name OUT_ONLY Possible Values \"true\", \"false\" Default Behavior none Scope synapse Description Set this property to \"true\" on a message to indicate that no response message is expected for it once it is forwarded from the ESB profile. In other words, the ESB profile will do an out-only invocation with such messages. It is very important to set this property on messages that are involved in out-only invocations to prevent the ESB profile from registering unnecessary callbacks for response handling and eventually running out of memory. Description for value=\"true\" Set this property to \"true\" on a message to indicate that no response message is expected for it once it is forwarded from the ESB profile. In other words, the ESB profile will do an out-only invocation with such messages. It is very important to set this property on messages that are involved in out-only invocations to prevent the ESB profile from registering unnecessary callbacks for response handling and eventually running out of memory. <property name= \"OUT_ONLY\" value= \"true\" /> Description for value=\"false\" Set this property to \"false\" to call the endpoint and get a response once it is forwarded from the ESB. <property name= \"OUT_ONLY\" value= \"false\" /> ERROR_CODE \u00b6 Name ERROR_CODE Possible Values string Default Behavior none Scope synapse Description Use this property to set a custom error code on a message which can be later processed by a Synapse fault handler. If the Synapse encounters an error during mediation or routing, this property will be automatically populated. Example <property name= \"ERROR_CODE\" value= \"100100\" /> ERROR_MESSAGE \u00b6 Name ERROR_MESSAGE Possible Values string Default Behavior none Scope synapse Description Use this property to set a custom error message on a message which can be later processed by a Synapse fault handler. If the Synapse encounters an error during mediation or routing, this property will be automatically populated. Example <log level= \"custom\" > <property name= \"Cause\" expression= \"get-property('ERROR_MESSAGE')\" /> </log> ERROR_DETAIL \u00b6 Name ERROR_DETAIL Possible Values string Default Behavior none Scope synapse Description Use this property to set the exception stacktrace in case of an error. If the ESB profile encounters an error during mediation or routing, this property will be automatically populated. Example <log level= \"custom\" > <property name= \"Trace\" expression= \"get-property('ERROR_DETAIL')\" /> </log> ERROR_EXCEPTION \u00b6 Name ERROR_EXCEPTION Possible Values java.lang.Exception Default Behavior none Scope synapse Description Contains the actual exception thrown in case of a runtime error. TRANSPORT_HEADERS \u00b6 Name TRANSPORT_HEADERS Possible Values java.util.Map Default Behavior Populated with the transport headers of the incoming request. Scope axis2 Description Contains the map of transport headers. Automatically populated. Individual values of this map can be accessed using the property mediator in the transport scope. Example <property name= \"TRANSPORT_HEADERS\" action= \"remove\" scope= \"axis2\" /> messageType \u00b6 Name messageType Possible Values string Default Behavior Content type of incoming request. Scope axis2 Description Message formatter is selected based on this property. This property should have the content type, such as text/xml, application/xml, or application/json. Example <property name= \"messageType\" value= \"text/xml\" scope= \"axis2\" /> ContentType \u00b6 Name ContentType Possible Values string Default Behavior Value of the Content-type header of the incoming request. Scope axis2 Description This property will be in effect only if the messageType property is set. If the messageType is set, the value of Content-Type HTTP header of the outgoing request will be chosen based on this property. Note that this property is required to be set only if the message formatter seeks it in the message formatter implementation. Example <property name= \"ContentType\" value= \"text/xml\" scope= \"axis2\" /> disableAddressingForOutMessages \u00b6 Name disableAddressingForOutMessages Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description Set this property to \"true\" if you do not want the ESB profile to add WS-Addressing headers to outgoing messages. This property can affect messages sent to backend services as well as the responses routed back to clients. Example <property name= \"disableAddressingForOutMessages\" value= \"true\" scope= \"axis2\" /> DISABLE_SMOOKS_RESULT_PAYLOAD \u00b6 Name DISABLE_SMOOKS_RESULT_PAYLOAD Possible Values \"true\", \"false\" Default Behavior false Scope synapse Description If this property is set to true , the result of file content processing carried out by the Smooks Mediator will not be loaded into the message context. This is useful in situations where you want to avoid large memory growth/out of heap space issue that may occur when large files processed by the Smooks mediator are reprocessed. See VFS Transport for a proxy service configuration where this property is used. Example <property name= \"DISABLE_SMOOKS_RESULT_PAYLOAD\" value= \"true\" scope= \"default\" type= \"STRING\" /> ClientApiNonBlocking \u00b6 Name ClientApiNonBlocking Possible Values \"true\", \"false\" Default Behavior true Scope axis2 Description By default, Axis2 spawns a new thread to handle each outgoing message. This property holds the primary thread until a VFS proxy writes to a VFS endpoint. You need to r emove this property from the message to change this behavior when queuing transports like JMS are involved. Example <property name= \"ClientApiNonBlocking\" action= \"remove\" scope= \"axis2\" /> transportNonBlocking \u00b6 Name transportNonBlocking Possible Values \"true\", \"false\" Default Behavior true Scope axis2 Description This property works the same way as ClientApiNonBlocking . It is recommended to use ClientApiNonBlocking for this purpose instead of transportNonBlocking since the former uses the latest axis2 translations. Example <property name= \"transportNonBlocking\" action= \"remove\" scope= \"axis2\" value= \"true\" /> TRANSPORT_IN_NAME \u00b6 Name TRANSPORT_IN_NAME Scope synapse Description Mediation logic can read incoming transport name using this property (since WSO2 ESB 4.7.0) Example <log level= \"custom\" > <property name= \"INCOMING_TRANSPORT\" expression= \"get-property('TRANSPORT_IN_NAME')\" /> </log> preserveProcessedHeaders \u00b6 Name preserveProcessedHeaders Possible Values \"true\", \"false\" Default Behavior Preserving SOAP headers Scope synapse(default) Description By default, Synapse removes the SOAP headers of incoming requests that have been processed. If we set this property to 'true', Synapse preserves the SOAP headers. Example <property name= \"preserveProcessedHeaders\" value= \"true\" scope= \"default\" /> SERVER_IP \u00b6 Name SERVER_IP Possible Values IP address or hostname of the ESB profile host Default Behavior Set automatically by the mediation engine upon startup Scope synapse FORCE_ERROR_ON_SOAP_FAULT \u00b6 Name FORCE_ERROR_ON_SOAP_FAULT Possible Values \"true\", \"false\" Default Behavior true Scope synapse(default) Description When a SOAP error occurs in a response, the SOAPFault sent from the back end is received by the out sequence as a usual response by default. If this property is set to true , the SOAPFault is redirected to a fault sequence. Note that when this property is true , only properties in the 'operation' scope will be passed to the error handler, and other properties in the axis2 or default scopes will not be passed to the error handler. Example <property name= \"FORCE_ERROR_ON_SOAP_FAULT\" value= \"true\" scope= \"default\" type= \"STRING\" ></property> QUOTE_STRING_IN_PAYLOAD_FACTORY_JSON \u00b6 Name QUOTE_STRING_IN_PAYLOAD_FACTORY_JSON Possible Values \"true\", \"false\" Default Behavior none Scope synapse Description When you create a JSON payload using the PayloadFactory mediator, a string value evaluated from an argument is replaced as it is. If you want to force double quotes to be added to a string value evaluated from an argument, set this property to true . !!! info Note Double quotes are added only if the value evaluated from an argument is string. If the value is a valid JSON number, boolean value or null, double quotes are not added. Example <property name= \"QUOTE_STRING_IN_PAYLOAD_FACTORY_JSON\" value= \"true\" />","title":"Generic Properties"},{"location":"references/generic-Properties/#generic-properties","text":"[ PRESERVE_WS_ADDRESSING ] [ RESPONSE ] [ OUT_ONLY ] [ ERROR_CODE ] [ ERROR_MESSAGE ] [ ERROR_DETAIL ] [ ERROR_EXCEPTION ] [ TRANSPORT_HEADERS ] [ messageType ] [ ContentType ] [ disableAddressingForOutMessages ] [ DISABLE_SMOOKS_RESULT_PAYLOAD ] [ ClientApiNonBlocking ] [ transportNonBlocking ] [ TRANSPORT_IN_NAME ] [ preserveProcessedHeaders ] [ SERVER_IP ] [ FORCE_ERROR_ON_SOAP_FAULT ] [ QUOTE_STRING_IN_PAYLOAD_FACTORY_JSON ] Generic properties allow you to configure messages as they're processed by the ESB profile, such as marking a message as out-only (no response message will be expected), adding a custom error message or code to the message, and disabling WS-Addressing headers.","title":"Generic Properties"},{"location":"references/generic-Properties/#preserve95ws95addressing","text":"Name PRESERVE_WS_ADDRESSING Possible Values \"true\", \"false\" Default Behavior none Scope synapse Description By default, the ESB profile adds a new set of WS-Addressing headers to the messages forwarded from the ESB profile. If this property is set to \" true \" on a message, the ESB profile will forward it without altering its existing WS-Addressing headers. Example <property name= \"PRESERVE_WS_ADDRESSING\" value= \"true\" />","title":"PRESERVE_WS_ADDRESSING"},{"location":"references/generic-Properties/#response","text":"Name RESPONSE Possible Values \"true\", \"false\" Default Behavior none Scope synapse Description Once this property is set to 'true' on a message, the ESB profile will start treating it as a response message. It is generally used to route a request message back to its source as the response. Example <property name= \"RESPONSE\" value= \"true\" />","title":"RESPONSE"},{"location":"references/generic-Properties/#out95only","text":"Name OUT_ONLY Possible Values \"true\", \"false\" Default Behavior none Scope synapse Description Set this property to \"true\" on a message to indicate that no response message is expected for it once it is forwarded from the ESB profile. In other words, the ESB profile will do an out-only invocation with such messages. It is very important to set this property on messages that are involved in out-only invocations to prevent the ESB profile from registering unnecessary callbacks for response handling and eventually running out of memory. Description for value=\"true\" Set this property to \"true\" on a message to indicate that no response message is expected for it once it is forwarded from the ESB profile. In other words, the ESB profile will do an out-only invocation with such messages. It is very important to set this property on messages that are involved in out-only invocations to prevent the ESB profile from registering unnecessary callbacks for response handling and eventually running out of memory. <property name= \"OUT_ONLY\" value= \"true\" /> Description for value=\"false\" Set this property to \"false\" to call the endpoint and get a response once it is forwarded from the ESB. <property name= \"OUT_ONLY\" value= \"false\" />","title":"OUT_ONLY"},{"location":"references/generic-Properties/#error95code","text":"Name ERROR_CODE Possible Values string Default Behavior none Scope synapse Description Use this property to set a custom error code on a message which can be later processed by a Synapse fault handler. If the Synapse encounters an error during mediation or routing, this property will be automatically populated. Example <property name= \"ERROR_CODE\" value= \"100100\" />","title":"ERROR_CODE"},{"location":"references/generic-Properties/#error95message","text":"Name ERROR_MESSAGE Possible Values string Default Behavior none Scope synapse Description Use this property to set a custom error message on a message which can be later processed by a Synapse fault handler. If the Synapse encounters an error during mediation or routing, this property will be automatically populated. Example <log level= \"custom\" > <property name= \"Cause\" expression= \"get-property('ERROR_MESSAGE')\" /> </log>","title":"ERROR_MESSAGE"},{"location":"references/generic-Properties/#error95detail","text":"Name ERROR_DETAIL Possible Values string Default Behavior none Scope synapse Description Use this property to set the exception stacktrace in case of an error. If the ESB profile encounters an error during mediation or routing, this property will be automatically populated. Example <log level= \"custom\" > <property name= \"Trace\" expression= \"get-property('ERROR_DETAIL')\" /> </log>","title":"ERROR_DETAIL"},{"location":"references/generic-Properties/#error95exception","text":"Name ERROR_EXCEPTION Possible Values java.lang.Exception Default Behavior none Scope synapse Description Contains the actual exception thrown in case of a runtime error.","title":"ERROR_EXCEPTION"},{"location":"references/generic-Properties/#transport95headers","text":"Name TRANSPORT_HEADERS Possible Values java.util.Map Default Behavior Populated with the transport headers of the incoming request. Scope axis2 Description Contains the map of transport headers. Automatically populated. Individual values of this map can be accessed using the property mediator in the transport scope. Example <property name= \"TRANSPORT_HEADERS\" action= \"remove\" scope= \"axis2\" />","title":"TRANSPORT_HEADERS"},{"location":"references/generic-Properties/#messagetype","text":"Name messageType Possible Values string Default Behavior Content type of incoming request. Scope axis2 Description Message formatter is selected based on this property. This property should have the content type, such as text/xml, application/xml, or application/json. Example <property name= \"messageType\" value= \"text/xml\" scope= \"axis2\" />","title":"messageType"},{"location":"references/generic-Properties/#contenttype","text":"Name ContentType Possible Values string Default Behavior Value of the Content-type header of the incoming request. Scope axis2 Description This property will be in effect only if the messageType property is set. If the messageType is set, the value of Content-Type HTTP header of the outgoing request will be chosen based on this property. Note that this property is required to be set only if the message formatter seeks it in the message formatter implementation. Example <property name= \"ContentType\" value= \"text/xml\" scope= \"axis2\" />","title":"ContentType"},{"location":"references/generic-Properties/#disableaddressingforoutmessages","text":"Name disableAddressingForOutMessages Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description Set this property to \"true\" if you do not want the ESB profile to add WS-Addressing headers to outgoing messages. This property can affect messages sent to backend services as well as the responses routed back to clients. Example <property name= \"disableAddressingForOutMessages\" value= \"true\" scope= \"axis2\" />","title":"disableAddressingForOutMessages"},{"location":"references/generic-Properties/#disable95smooks95result95payload","text":"Name DISABLE_SMOOKS_RESULT_PAYLOAD Possible Values \"true\", \"false\" Default Behavior false Scope synapse Description If this property is set to true , the result of file content processing carried out by the Smooks Mediator will not be loaded into the message context. This is useful in situations where you want to avoid large memory growth/out of heap space issue that may occur when large files processed by the Smooks mediator are reprocessed. See VFS Transport for a proxy service configuration where this property is used. Example <property name= \"DISABLE_SMOOKS_RESULT_PAYLOAD\" value= \"true\" scope= \"default\" type= \"STRING\" />","title":"DISABLE_SMOOKS_RESULT_PAYLOAD"},{"location":"references/generic-Properties/#clientapinonblocking","text":"Name ClientApiNonBlocking Possible Values \"true\", \"false\" Default Behavior true Scope axis2 Description By default, Axis2 spawns a new thread to handle each outgoing message. This property holds the primary thread until a VFS proxy writes to a VFS endpoint. You need to r emove this property from the message to change this behavior when queuing transports like JMS are involved. Example <property name= \"ClientApiNonBlocking\" action= \"remove\" scope= \"axis2\" />","title":"ClientApiNonBlocking"},{"location":"references/generic-Properties/#transportnonblocking","text":"Name transportNonBlocking Possible Values \"true\", \"false\" Default Behavior true Scope axis2 Description This property works the same way as ClientApiNonBlocking . It is recommended to use ClientApiNonBlocking for this purpose instead of transportNonBlocking since the former uses the latest axis2 translations. Example <property name= \"transportNonBlocking\" action= \"remove\" scope= \"axis2\" value= \"true\" />","title":"transportNonBlocking"},{"location":"references/generic-Properties/#transport95in95name","text":"Name TRANSPORT_IN_NAME Scope synapse Description Mediation logic can read incoming transport name using this property (since WSO2 ESB 4.7.0) Example <log level= \"custom\" > <property name= \"INCOMING_TRANSPORT\" expression= \"get-property('TRANSPORT_IN_NAME')\" /> </log>","title":"TRANSPORT_IN_NAME"},{"location":"references/generic-Properties/#preserveprocessedheaders","text":"Name preserveProcessedHeaders Possible Values \"true\", \"false\" Default Behavior Preserving SOAP headers Scope synapse(default) Description By default, Synapse removes the SOAP headers of incoming requests that have been processed. If we set this property to 'true', Synapse preserves the SOAP headers. Example <property name= \"preserveProcessedHeaders\" value= \"true\" scope= \"default\" />","title":"preserveProcessedHeaders"},{"location":"references/generic-Properties/#server95ip","text":"Name SERVER_IP Possible Values IP address or hostname of the ESB profile host Default Behavior Set automatically by the mediation engine upon startup Scope synapse","title":"SERVER_IP"},{"location":"references/generic-Properties/#force95error95on95soap95fault","text":"Name FORCE_ERROR_ON_SOAP_FAULT Possible Values \"true\", \"false\" Default Behavior true Scope synapse(default) Description When a SOAP error occurs in a response, the SOAPFault sent from the back end is received by the out sequence as a usual response by default. If this property is set to true , the SOAPFault is redirected to a fault sequence. Note that when this property is true , only properties in the 'operation' scope will be passed to the error handler, and other properties in the axis2 or default scopes will not be passed to the error handler. Example <property name= \"FORCE_ERROR_ON_SOAP_FAULT\" value= \"true\" scope= \"default\" type= \"STRING\" ></property>","title":"FORCE_ERROR_ON_SOAP_FAULT"},{"location":"references/generic-Properties/#quote95string95in95payload95factory95json","text":"Name QUOTE_STRING_IN_PAYLOAD_FACTORY_JSON Possible Values \"true\", \"false\" Default Behavior none Scope synapse Description When you create a JSON payload using the PayloadFactory mediator, a string value evaluated from an argument is replaced as it is. If you want to force double quotes to be added to a string value evaluated from an argument, set this property to true . !!! info Note Double quotes are added only if the value evaluated from an argument is string. If the value is a valid JSON number, boolean value or null, double quotes are not added. Example <property name= \"QUOTE_STRING_IN_PAYLOAD_FACTORY_JSON\" value= \"true\" />","title":"QUOTE_STRING_IN_PAYLOAD_FACTORY_JSON"},{"location":"references/hTTP-Transport-Properties/","text":"HTTP Transport Properties \u00b6 [ POST_TO_URI ] [ FORCE_SC_ACCEPTED ] [ DISABLE_CHUNKING ] [ NO_ENTITY_BODY ] [ FORCE_HTTP_1.0 ] [ HTTP_SC ] [ HTTP_SC_DESC ] [ FAULTS_AS_HTTP_200 ] [ NO_KEEPALIVE ] [ REST_URL_POSTFIX ] [ REQUEST_HOST_HEADER ] [ FORCE_POST_PUT_NOBODY ] [ FORCE_HTTP_CONTENT_LENGTH ] [ COPY_CONTENT_LENGTH_FROM_INCOMING ] HTTP transport properties allow you to configure how the HTTP transport processes messages, such as forcing a 202 HTTP response to the client so that it stops waiting for a response, setting the HTTP status code, and appending a context to the target URL in RESTful invocations. POST_TO_URI \u00b6 Name POST_TO_URI Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description This property makes the request URL that is sent from the ESB profile a complete URL. When set to false only the context path will be included in the request URL that is sent. It is important that this property is set to true when the ESB profile needs to communicate with the back-end service through a proxy server. Example <property name= \"POST_TO_URI\" scope= \"axis2\" value= \"true\" /> FORCE_SC_ACCEPTED \u00b6 Name FORCE_SC_ACCEPTED Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description When set to true, this property forces a 202 HTTP response to the client immediately after the ESB profile receives the message so that the client stops waiting for a response. Example <property name= \"FORCE_SC_ACCEPTED\" value= \"true\" scope= \"axis2\" /> DISABLE_CHUNKING \u00b6 Name DISABLE_CHUNKING Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description If you set this to true, it disables HTTP chunking for outgoing messages. Instead, the ESB profile builds the message to calculate the content length and then sends the particular message to the backend with the content length (e.g., Content-Length: 25 ). You can use this parameter if the client sends the request with HTTP chunking (i.e., with Transfer Encoding:chunked ) although you need to send the message without HTTP chunking to the backend, or if you need to modify the message payload, which the client receives before sending it to the backend. !!! warning This property might decrease performance since the messages get built per each invocation. Also, this property does not affect Callout mediators, whose chunking must be disabled separately . Example <property name= \"DISABLE_CHUNKING\" value= \"true\" scope= \"axis2\" /> NO_ENTITY_BODY \u00b6 Name NO_ENTITY_BODY Possible Values \"true\", \"false\" Default Behavior In case of GET requests this property is set to true. Scope Axis2 Description Set this property if you want to do the following: check if an incoming request to the ESB mediation flow has an entity body or not check if an outgoing request/response generated from the ESB mediation flow has an entity body or not !!! info If using the PayloadFactory mediator , this property does not need to be manually set since it is done automatically by the mediator. Example <property name= \"NO_ENTITY_BODY\" value= \"true\" scope= \"axis2\" type= \"BOOLEAN\" /> FORCE_HTTP_1.0 \u00b6 Name FORCE_HTTP_1.0 Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description Force HTTP 1.0 for outgoing HTTP messages. Example <property name= \"FORCE_HTTP_1.0\" value= \"true\" scope= \"axis2\" /> HTTP_SC \u00b6 Name HTTP_SC Possible Values HTTP status code number Default Behavior none Scope axis2 Description Set the HTTP status code. Example <property name= \"HTTP_SC\" value= \"500\" scope= \"axis2\" /> HTTP_SC_DESC \u00b6 Name HTTP_SC_DESC Possible Values HTTP response's Reason- Phrase that is sent by the backend. For example, if the backend sends the response's status as HTTP/1.1 200 OK, then the value of HTTP_SC_DESC is OK. Default Behavior none Scope axis2 Description Set the HTTP status message ( Reason-Phrase ). Example <property name= \"HTTP_SC_DESC\" value= \"Your description here\" scope= \"axis2\" /> FAULTS_AS_HTTP_200 \u00b6 Name FAULTS_AS_HTTP_200 Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description When WSO2 EI receives a soap fault as a HTTP 500 message, the ESB profile will forward this fault to client with status code 200. Example <property name= \"FAULTS_AS_HTTP_200\" value= \"true\" scope= \"axis2\" /> NO_KEEPALIVE \u00b6 Name NO_KEEPALIVE Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description Disables HTTP keep alive for outgoing requests. Example <property name= \"NO_KEEPALIVE\" value= \"true\" scope= \"axis2\" /> REST_URL_POSTFIX \u00b6 Name REST_URL_POSTFIX Possible Values A URL fragment starting with \"/\" Default Behavior In the case of GET requests through an address endpoint, this contains the query string. Scope axis2 Description The value of this property will be appended to the target URL when sending messages out in a RESTful manner through an address endpoint. This is useful when you need to append a context to the target URL in case of RESTful invocations. If you are using an HTTP endpoint instead of an address endpoint, specify variables in the format of \"uri.var.*\" instead of using this property. Example <property name= \"REST_URL_POSTFIX\" value= \"/context\" scope= \"axis2\" /> REQUEST_HOST_HEADER \u00b6 Name REQUEST_HOST_HEADER Possible Values string Default Behavior The ESB profile will set hostname of target endpoint and port as the HTTP host header Scope axis2 Description The value of this property will be set as the HTTP host header of outgoing request Example <property name= \"REQUEST_HOST_HEADER\" value= \"www.wso2.org\" scope= \"axis2\" /> FORCE_POST_PUT_NOBODY \u00b6 Name FORCE_POST_PUT_NOBODY Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description This property allows to send a request without a body for POST and PUT HTTP methods. Applicable only for HTTP Passthrough transport. Example <property name= \"FORCE_POST_PUT_NOBODY\" value= \"true\" scope= \"axis2\" type= \"BOOLEAN\" /> FORCE_HTTP_CONTENT_LENGTH \u00b6 Name FORCE_HTTP_CONTENT_LENGTH Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description If the request sent by the client contains the \u2018Content-Length\u2019 header, this property allows the ESB profile to send the request with the content length (without HTTP chunking) to the back end server. You should set this to true in scenarios where the backend server is not able to accept chunked content. For example, in a scenario where a pass-through proxy is defined and the backend does not accept chunked content, this property should be used together with the COPY_CONTENT_LENGTH_FROM_INCOMING property, to simply add the content length without chunking. When HTTP 1.1 is used, this property disables chunking and sends the content length. When HTTP 1.0 is used, the property only sends the content length. !!! warning This property can cause performance degradation, and thereby, you should only use it with message relay. If you set this to true, the ESB profile forwards the content length coming from the client request to the backend without building the message and calculating the content length. Since the message doesn\u2019t get build, using these properties will perform better than using DISABLE_CHUNKING . However, if you change the receiving payload before sending it to the backend, then having this property will result in an error due to a content length mismatch. Example <property name= \"FORCE_HTTP_CONTENT_LENGTH\" scope= \"axis2\" value= \"true\" ></property> COPY_CONTENT_LENGTH_FROM_INCOMING \u00b6 Name COPY_CONTENT_LENGTH_FROM_INCOMING Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description This property allows the HTTP content length to be copied from an incoming message. It is only valid when the FORCE_HTTP_CONTENT_LENGTH property is used. The COPY_CONTENT_LENGTH_FROM_INCOMING avoids buffering the message in memory for calculating the content length, thus reducing the risk of performance degradation. Example <property name= \"COPY_CONTENT_LENGTH_FROM_INCOMING\" value= \"true\" scope= \"axis2\" />","title":"HTTP Transport Properties"},{"location":"references/hTTP-Transport-Properties/#http-transport-properties","text":"[ POST_TO_URI ] [ FORCE_SC_ACCEPTED ] [ DISABLE_CHUNKING ] [ NO_ENTITY_BODY ] [ FORCE_HTTP_1.0 ] [ HTTP_SC ] [ HTTP_SC_DESC ] [ FAULTS_AS_HTTP_200 ] [ NO_KEEPALIVE ] [ REST_URL_POSTFIX ] [ REQUEST_HOST_HEADER ] [ FORCE_POST_PUT_NOBODY ] [ FORCE_HTTP_CONTENT_LENGTH ] [ COPY_CONTENT_LENGTH_FROM_INCOMING ] HTTP transport properties allow you to configure how the HTTP transport processes messages, such as forcing a 202 HTTP response to the client so that it stops waiting for a response, setting the HTTP status code, and appending a context to the target URL in RESTful invocations.","title":"HTTP Transport Properties"},{"location":"references/hTTP-Transport-Properties/#post95to95uri","text":"Name POST_TO_URI Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description This property makes the request URL that is sent from the ESB profile a complete URL. When set to false only the context path will be included in the request URL that is sent. It is important that this property is set to true when the ESB profile needs to communicate with the back-end service through a proxy server. Example <property name= \"POST_TO_URI\" scope= \"axis2\" value= \"true\" />","title":"POST_TO_URI"},{"location":"references/hTTP-Transport-Properties/#force95sc95accepted","text":"Name FORCE_SC_ACCEPTED Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description When set to true, this property forces a 202 HTTP response to the client immediately after the ESB profile receives the message so that the client stops waiting for a response. Example <property name= \"FORCE_SC_ACCEPTED\" value= \"true\" scope= \"axis2\" />","title":"FORCE_SC_ACCEPTED"},{"location":"references/hTTP-Transport-Properties/#disable95chunking","text":"Name DISABLE_CHUNKING Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description If you set this to true, it disables HTTP chunking for outgoing messages. Instead, the ESB profile builds the message to calculate the content length and then sends the particular message to the backend with the content length (e.g., Content-Length: 25 ). You can use this parameter if the client sends the request with HTTP chunking (i.e., with Transfer Encoding:chunked ) although you need to send the message without HTTP chunking to the backend, or if you need to modify the message payload, which the client receives before sending it to the backend. !!! warning This property might decrease performance since the messages get built per each invocation. Also, this property does not affect Callout mediators, whose chunking must be disabled separately . Example <property name= \"DISABLE_CHUNKING\" value= \"true\" scope= \"axis2\" />","title":"DISABLE_CHUNKING"},{"location":"references/hTTP-Transport-Properties/#no95entity95body","text":"Name NO_ENTITY_BODY Possible Values \"true\", \"false\" Default Behavior In case of GET requests this property is set to true. Scope Axis2 Description Set this property if you want to do the following: check if an incoming request to the ESB mediation flow has an entity body or not check if an outgoing request/response generated from the ESB mediation flow has an entity body or not !!! info If using the PayloadFactory mediator , this property does not need to be manually set since it is done automatically by the mediator. Example <property name= \"NO_ENTITY_BODY\" value= \"true\" scope= \"axis2\" type= \"BOOLEAN\" />","title":"NO_ENTITY_BODY"},{"location":"references/hTTP-Transport-Properties/#force95http9510","text":"Name FORCE_HTTP_1.0 Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description Force HTTP 1.0 for outgoing HTTP messages. Example <property name= \"FORCE_HTTP_1.0\" value= \"true\" scope= \"axis2\" />","title":"FORCE_HTTP_1.0"},{"location":"references/hTTP-Transport-Properties/#http95sc","text":"Name HTTP_SC Possible Values HTTP status code number Default Behavior none Scope axis2 Description Set the HTTP status code. Example <property name= \"HTTP_SC\" value= \"500\" scope= \"axis2\" />","title":"HTTP_SC"},{"location":"references/hTTP-Transport-Properties/#http95sc95desc","text":"Name HTTP_SC_DESC Possible Values HTTP response's Reason- Phrase that is sent by the backend. For example, if the backend sends the response's status as HTTP/1.1 200 OK, then the value of HTTP_SC_DESC is OK. Default Behavior none Scope axis2 Description Set the HTTP status message ( Reason-Phrase ). Example <property name= \"HTTP_SC_DESC\" value= \"Your description here\" scope= \"axis2\" />","title":"HTTP_SC_DESC"},{"location":"references/hTTP-Transport-Properties/#faults95as95http95200","text":"Name FAULTS_AS_HTTP_200 Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description When WSO2 EI receives a soap fault as a HTTP 500 message, the ESB profile will forward this fault to client with status code 200. Example <property name= \"FAULTS_AS_HTTP_200\" value= \"true\" scope= \"axis2\" />","title":"FAULTS_AS_HTTP_200"},{"location":"references/hTTP-Transport-Properties/#no95keepalive","text":"Name NO_KEEPALIVE Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description Disables HTTP keep alive for outgoing requests. Example <property name= \"NO_KEEPALIVE\" value= \"true\" scope= \"axis2\" />","title":"NO_KEEPALIVE"},{"location":"references/hTTP-Transport-Properties/#rest95url95postfix","text":"Name REST_URL_POSTFIX Possible Values A URL fragment starting with \"/\" Default Behavior In the case of GET requests through an address endpoint, this contains the query string. Scope axis2 Description The value of this property will be appended to the target URL when sending messages out in a RESTful manner through an address endpoint. This is useful when you need to append a context to the target URL in case of RESTful invocations. If you are using an HTTP endpoint instead of an address endpoint, specify variables in the format of \"uri.var.*\" instead of using this property. Example <property name= \"REST_URL_POSTFIX\" value= \"/context\" scope= \"axis2\" />","title":"REST_URL_POSTFIX"},{"location":"references/hTTP-Transport-Properties/#request95host95header","text":"Name REQUEST_HOST_HEADER Possible Values string Default Behavior The ESB profile will set hostname of target endpoint and port as the HTTP host header Scope axis2 Description The value of this property will be set as the HTTP host header of outgoing request Example <property name= \"REQUEST_HOST_HEADER\" value= \"www.wso2.org\" scope= \"axis2\" />","title":"REQUEST_HOST_HEADER"},{"location":"references/hTTP-Transport-Properties/#force95post95put95nobody","text":"Name FORCE_POST_PUT_NOBODY Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description This property allows to send a request without a body for POST and PUT HTTP methods. Applicable only for HTTP Passthrough transport. Example <property name= \"FORCE_POST_PUT_NOBODY\" value= \"true\" scope= \"axis2\" type= \"BOOLEAN\" />","title":"FORCE_POST_PUT_NOBODY"},{"location":"references/hTTP-Transport-Properties/#force95http95content95length","text":"Name FORCE_HTTP_CONTENT_LENGTH Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description If the request sent by the client contains the \u2018Content-Length\u2019 header, this property allows the ESB profile to send the request with the content length (without HTTP chunking) to the back end server. You should set this to true in scenarios where the backend server is not able to accept chunked content. For example, in a scenario where a pass-through proxy is defined and the backend does not accept chunked content, this property should be used together with the COPY_CONTENT_LENGTH_FROM_INCOMING property, to simply add the content length without chunking. When HTTP 1.1 is used, this property disables chunking and sends the content length. When HTTP 1.0 is used, the property only sends the content length. !!! warning This property can cause performance degradation, and thereby, you should only use it with message relay. If you set this to true, the ESB profile forwards the content length coming from the client request to the backend without building the message and calculating the content length. Since the message doesn\u2019t get build, using these properties will perform better than using DISABLE_CHUNKING . However, if you change the receiving payload before sending it to the backend, then having this property will result in an error due to a content length mismatch. Example <property name= \"FORCE_HTTP_CONTENT_LENGTH\" scope= \"axis2\" value= \"true\" ></property>","title":"FORCE_HTTP_CONTENT_LENGTH"},{"location":"references/hTTP-Transport-Properties/#copy95content95length95from95incoming","text":"Name COPY_CONTENT_LENGTH_FROM_INCOMING Possible Values \"true\", \"false\" Default Behavior false Scope axis2 Description This property allows the HTTP content length to be copied from an incoming message. It is only valid when the FORCE_HTTP_CONTENT_LENGTH property is used. The COPY_CONTENT_LENGTH_FROM_INCOMING avoids buffering the message in memory for calculating the content length, thus reducing the risk of performance degradation. Example <property name= \"COPY_CONTENT_LENGTH_FROM_INCOMING\" value= \"true\" scope= \"axis2\" />","title":"COPY_CONTENT_LENGTH_FROM_INCOMING"},{"location":"references/header-Mediator/","text":"Header Mediator \u00b6 The Header Mediator allows you to manipulate SOAP and HTTP headers. Info The Header mediator is a conditionally content aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <header name=\u201dstring\u201d (value=\u201dstring|{property}\u201d | expression=\u201dxpath\u201d) [scope=default|transport] [action=set|remove]/> The optional action attribute specifies whether the mediator should set or remove the header. If no value is specified, the header is set by default. Configuration \u00b6 The parameters available to configure the Header mediator are as follows. Paramater Name Description Name The name of the header element. You can specify the namespace used in the header element by clicking the Namespaces link next to the text field. Action Select Set if you want to set the header as a new header. Select Remove if you want to remove the header from the incoming message. Value/Expression A static value or an XPath expression that will be executed on the message to set the header value. Inline XML Header This parameter allows you to directly input any XML syntax related to the Header mediator (specifically for SOAP headers). For example, to achieve the following configuration, you should enter the lastTradeTimestamp element in the Inline XML Header parameter. <header> <urn:lastTradeTimestamp xmlns:urn= \" http://synapse.apache.org/ \" > Mon May 13 13:52:17 IST 2013 </urn:lastTradeTimestamp> </header> Scope Select Synapse if you want to manipulate SOAP headers. Select Transport if you want to manipulate HTTP headers. Namespaces You can click this link to add namespaces if you are providing an expression. The Namespace Editor panel would appear. You can enter any number of namespace prefixes and URL that you have used in the XPath expression in this panel. Examples \u00b6 This section covers the following scenarios in which the Header mediator can be used. Example 1 - SOAP headers Example 2 - HTTP headers Example 3 - Handling headers with complex XML Example 4 - Adding a dynamic SOAP header Example 5 - Setting the endpoint URL dynamically Example 1 - SOAP headers \u00b6 In the following example, the value for P1 code should be included in the SOAP header of the message sent from the client to the ESB profile . To do this, the header mediator is added to the in sequence of the proxy configuration as shown below. To get a response with Hello World in the SOAP header, the header mediator is also added to the out sequence. <inSequence> <header> <p1:Code xmlns:p1=\"http://www.XYZ.com/XSD\">XYZ</p1:Code> </header> <send> <endpoint> <address uri=\"http://localhost:8899/services/SimpleStockQuoteService?wsdl\"/> </endpoint> </send> </inSequence> <outSequence> <header> <p2:Header xmlns:p2=\"http://www.ABC.com/XSD\"> <p2:Hello>World</p2:Hello> </p2:Header> </header> <send/> </outSequence> Example 2 - HTTP headers \u00b6 The following example makes the ESB profile add the HTTP header Accept with the value image/jpeg to the HTTP request made to the endpoint. ``` html/xml If you have [enabled wire logs](https://docs.wso2.com/display/EI620/Debugging+Mediation#DebuggingMediation-Viewingwirelogs) , you will view the following output. ``` text << GET /people/eric+cooke HTTP/1.1 << Accept: image/jpeg << Host: localhost:9763 << Connection: Keep-Alive Example 3 - Handling headers with complex XML \u00b6 A header can contain XML structured values by embedding XML content within the <header> element as shown below. <header> <m:complexHeader xmlns:m=\"http://org.synapse.example\"> <property key=\"k1\" value=\"v1\" /> <property key=\"k2\" value=\"v2\" /> </m:complexHeader> </header> Example 4 - Adding a dynamic SOAP header \u00b6 The following configuration takes the value of an element named symbol in the message body (the namespace http://services.samples/xsd ), and adds it as a SOAP header named header1 . <header xmlns:m=\"http://org.synapse.example\" xmlns:sym=\"http://services.samples/xsd\" name=\"m:header1\" scope=\"default\" expression=\"//sym:symbol\"/> Example 5 - Setting the endpoint URL dynamically \u00b6 In this example, the Header mediator allows the endpoint URL to which the message is sent to be set dynamically. It specifies the default address to which the message is sent dynamically by deriving the To header of the message via an XPath expression. Then the Send mediator sends the message to a Default Endpoint . A Default Endpoint sends the message to the default address of the message (i.e. address specified in the To header). Therefore, in this scenario, selecting the Default Endpoint results in the message being sent to relevant URL calculated via the fn:concat('http://localhost:9764/services/Axis2SampleService_',get-property('epr')) expression. <header name=\"To\" expression=\"fn:concat('http://localhost:9764/services/Axis2SampleService_',get-property('epr'))\"/> <send> <endpoint> <default/> </endpoint> </send>","title":"Header Mediator"},{"location":"references/header-Mediator/#header-mediator","text":"The Header Mediator allows you to manipulate SOAP and HTTP headers. Info The Header mediator is a conditionally content aware mediator. Syntax | Configuration | Examples","title":"Header Mediator"},{"location":"references/header-Mediator/#syntax","text":"<header name=\u201dstring\u201d (value=\u201dstring|{property}\u201d | expression=\u201dxpath\u201d) [scope=default|transport] [action=set|remove]/> The optional action attribute specifies whether the mediator should set or remove the header. If no value is specified, the header is set by default.","title":"Syntax"},{"location":"references/header-Mediator/#configuration","text":"The parameters available to configure the Header mediator are as follows. Paramater Name Description Name The name of the header element. You can specify the namespace used in the header element by clicking the Namespaces link next to the text field. Action Select Set if you want to set the header as a new header. Select Remove if you want to remove the header from the incoming message. Value/Expression A static value or an XPath expression that will be executed on the message to set the header value. Inline XML Header This parameter allows you to directly input any XML syntax related to the Header mediator (specifically for SOAP headers). For example, to achieve the following configuration, you should enter the lastTradeTimestamp element in the Inline XML Header parameter. <header> <urn:lastTradeTimestamp xmlns:urn= \" http://synapse.apache.org/ \" > Mon May 13 13:52:17 IST 2013 </urn:lastTradeTimestamp> </header> Scope Select Synapse if you want to manipulate SOAP headers. Select Transport if you want to manipulate HTTP headers. Namespaces You can click this link to add namespaces if you are providing an expression. The Namespace Editor panel would appear. You can enter any number of namespace prefixes and URL that you have used in the XPath expression in this panel.","title":"Configuration"},{"location":"references/header-Mediator/#examples","text":"This section covers the following scenarios in which the Header mediator can be used. Example 1 - SOAP headers Example 2 - HTTP headers Example 3 - Handling headers with complex XML Example 4 - Adding a dynamic SOAP header Example 5 - Setting the endpoint URL dynamically","title":"Examples"},{"location":"references/header-Mediator/#example-1-soap-headers","text":"In the following example, the value for P1 code should be included in the SOAP header of the message sent from the client to the ESB profile . To do this, the header mediator is added to the in sequence of the proxy configuration as shown below. To get a response with Hello World in the SOAP header, the header mediator is also added to the out sequence. <inSequence> <header> <p1:Code xmlns:p1=\"http://www.XYZ.com/XSD\">XYZ</p1:Code> </header> <send> <endpoint> <address uri=\"http://localhost:8899/services/SimpleStockQuoteService?wsdl\"/> </endpoint> </send> </inSequence> <outSequence> <header> <p2:Header xmlns:p2=\"http://www.ABC.com/XSD\"> <p2:Hello>World</p2:Hello> </p2:Header> </header> <send/> </outSequence>","title":"Example 1 - \u00a0SOAP headers"},{"location":"references/header-Mediator/#example-2-http-headers","text":"The following example makes the ESB profile add the HTTP header Accept with the value image/jpeg to the HTTP request made to the endpoint. ``` html/xml If you have [enabled wire logs](https://docs.wso2.com/display/EI620/Debugging+Mediation#DebuggingMediation-Viewingwirelogs) , you will view the following output. ``` text << GET /people/eric+cooke HTTP/1.1 << Accept: image/jpeg << Host: localhost:9763 << Connection: Keep-Alive","title":"Example 2 - HTTP headers"},{"location":"references/header-Mediator/#example-3-handling-headers-with-complex-xml","text":"A header can contain XML structured values by embedding XML content within the <header> element as shown below. <header> <m:complexHeader xmlns:m=\"http://org.synapse.example\"> <property key=\"k1\" value=\"v1\" /> <property key=\"k2\" value=\"v2\" /> </m:complexHeader> </header>","title":"Example 3 - Handling headers with complex XML"},{"location":"references/header-Mediator/#example-4-adding-a-dynamic-soap-header","text":"The following configuration takes the value of an element named symbol in the message body (the namespace http://services.samples/xsd ), and adds it as a SOAP header named header1 . <header xmlns:m=\"http://org.synapse.example\" xmlns:sym=\"http://services.samples/xsd\" name=\"m:header1\" scope=\"default\" expression=\"//sym:symbol\"/>","title":"Example 4 - Adding a dynamic SOAP header"},{"location":"references/header-Mediator/#example-5-setting-the-endpoint-url-dynamically","text":"In this example, the Header mediator allows the endpoint URL to which the message is sent to be set dynamically. It specifies the default address to which the message is sent dynamically by deriving the To header of the message via an XPath expression. Then the Send mediator sends the message to a Default Endpoint . A Default Endpoint sends the message to the default address of the message (i.e. address specified in the To header). Therefore, in this scenario, selecting the Default Endpoint results in the message being sent to relevant URL calculated via the fn:concat('http://localhost:9764/services/Axis2SampleService_',get-property('epr')) expression. <header name=\"To\" expression=\"fn:concat('http://localhost:9764/services/Axis2SampleService_',get-property('epr'))\"/> <send> <endpoint> <default/> </endpoint> </send>","title":"Example 5 - Setting the endpoint URL dynamically"},{"location":"references/in-and-Out-Mediators/","text":"In and Out Mediators \u00b6 Info Note Please note that this feature is deprecated. The In and Out Mediators act as predefined filters. Messages that are in the In path of the EI will traverse through the child mediators of the In Mediator. Messages that are in the Out path of the ESB profile will traverse through the child mediators of the Out Mediator. Info Note Do not use these mediators in proxy service sequences, as proxy services have the predefined sequences \\<inSequence> and \\<outSequence> for this purpose. Syntax | Configuration | Example Syntax \u00b6 In <in> mediator+ </in> Out <out> mediator+ </out> Configuration \u00b6 After adding an In or Out mediator, you add and configure its child mediators. Example \u00b6 In the following example, the In mediator has a Log mediator and a Filter mediator as child mediators. The Log mediator logs the messages in the In path. Then the Filter mediator filters these messages and sends the messages which match the filter criteria to http://localhost:9000 . The messages are sent via the Send mediator which is added as a child to the Filter mediator . The messages in the Out path are sent via the Send mediator which is added as a child to the Out mediator. <sequence name=\"main\" xmlns=\"http://ws.apache.org/ns/synapse\"> <in> <log level=\"full\"/> <filter source=\"get-property('To')\" regex=\"http://localhost:9000.*\"> <send/> </filter> </in> <out> <send/> </out> </sequence>","title":"In and Out Mediators"},{"location":"references/in-and-Out-Mediators/#in-and-out-mediators","text":"Info Note Please note that this feature is deprecated. The In and Out Mediators act as predefined filters. Messages that are in the In path of the EI will traverse through the child mediators of the In Mediator. Messages that are in the Out path of the ESB profile will traverse through the child mediators of the Out Mediator. Info Note Do not use these mediators in proxy service sequences, as proxy services have the predefined sequences \\<inSequence> and \\<outSequence> for this purpose. Syntax | Configuration | Example","title":"In and Out Mediators"},{"location":"references/in-and-Out-Mediators/#syntax","text":"In <in> mediator+ </in> Out <out> mediator+ </out>","title":"Syntax"},{"location":"references/in-and-Out-Mediators/#configuration","text":"After adding an In or Out mediator, you add and configure its child mediators.","title":"Configuration"},{"location":"references/in-and-Out-Mediators/#example","text":"In the following example, the In mediator has a Log mediator and a Filter mediator as child mediators. The Log mediator logs the messages in the In path. Then the Filter mediator filters these messages and sends the messages which match the filter criteria to http://localhost:9000 . The messages are sent via the Send mediator which is added as a child to the Filter mediator . The messages in the Out path are sent via the Send mediator which is added as a child to the Out mediator. <sequence name=\"main\" xmlns=\"http://ws.apache.org/ns/synapse\"> <in> <log level=\"full\"/> <filter source=\"get-property('To')\" regex=\"http://localhost:9000.*\"> <send/> </filter> </in> <out> <send/> </out> </sequence>","title":"Example"},{"location":"references/iterate-Mediator/","text":"Iterate Mediator \u00b6 The Iterate Mediator implements the Splitter enterprise integration pattern and splits the message into a number of different messages derived from the parent message. The Iterate mediator is similar to the Clone mediator . The difference between the two mediators is, the Iterate mediator splits a message into different parts, whereas the Clone mediator makes multiple identical copies of the message. Info The Iterate mediator is a content aware mediator. Note Iterate Mediator is quite similar to the ForEach mediator . You can use complex XPath expressions or JSON expressions to conditionally select elements to iterate over in both mediators. Following are the main difference between ForEach and Iterate mediators: Use the ForEach mediator only for message transformations. If you need to make back-end calls from each iteration, then use the iterate mediator. ForEach supports modifying the original payload. You can use Iterate for situations where you send the split messages to a target and collect them by an Aggregate in a different flow You need to always accompany an Iterate with an Aggregate mediator. ForEach loops over the sub-messages and merges them back to the same parent element of the message. In Iterate you need to send the split messages to an endpoint to continue the message flow. However, ForEach does not allow using Call , Send and Callout mediators in the sequence. ForEach does not split the message flow, unlike Iterate Mediator. It guarantees to execute in the same thread until all iterations are complete. When you use ForEach mediator, you can only loop through segments of the message and do changes to a particular segment. For example, you can change the payload using payload factory mediator. But you cannot send the split message out to a service. Once you exit from the for-each loop, it automatically aggregates the split segments. This replaces the for-each function of the complex XSLT mediators using a ForEach mediator and a Payload Factory mediator. However, to implement the split-aggregate pattern, you still need to use Iterate mediator. Syntax | Configuration | Examples Syntax \u00b6 <iterate [sequential=(true | false)] [continueParent=(true | false)] [preservePayload=(true | false)] [(attachPath=\"XPath|json-eval(JSON Path)\")? expression=\"XPath|json-eval(JSON Path)\"]> <target [to=\"uri\"] [soapAction=\"qname\"] [sequence=\"sequence_ref\"] [endpoint=\"endpoint_ref\"]> <sequence> (mediator)+ </sequence>? <endpoint> endpoint </endpoint>? </target>+ </iterate> Configuration \u00b6 The parameters available to configure the Iterate mediator are as follows. Parameter Name Description Iterate ID The iterate ID can be used to identify messages created by the iterate mediator. This is particularly useful when aggregating responses of messages that are created using nested iterate mediators. Sequential Mediation This parameter is used to specify whether the split messages should be processed sequentially or not. The processing is carried based on the information relating to the sequence and endpoint specified in the target configuration . The possible values are as follows. True : If this is selected, the split messages will be processed sequentially. Note that selecting True might cause delays due to high resource consumption. False : If this is selected, the split messages will not be processed sequentially. This is the default value and it results in better performance. !!! info The responses will not necessarily be aggregated in the same order that the requests were sent, even if the sequential Mediation parameter is set to true . Continue Parent This parameter is used to specify whether the original message should be preserved or not. Possible values are as follows. True : If this is selected, the original message will be preserved. False : If this is selected, the original message will be discarded. This is the default value. Preserve Payload This parameter is used to specify whether the original message payload should be used as a template when creating split messages. Possible values are as follows. True : If this is selected, the original message payload will be used as a template. False : If this is selected, the original message payload will not be used as a template. This is the default value. Iterate Expression The XPath expression used to split the message.. This expression selects the set of XML elements from the request payload that are applied to the mediation defined within the iterate target. E ach iteration of the iterate mediator will get one element from that set. New messages are created for each and every matching element and processed in parallel or in sequence based on the value specified for the Sequential Mediation parameter. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Attach Path To form new messages, you can specify an XPath expression or a JSONPath expression to identify the parent element to which the split elements are attached (as expressed in Iterate expression). !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Each Iterate mediator has its own target by default. It appears in the mediation tree as shown below once you configure the above parameters and save them. The parameters available to configure the target configuration are as follows. Parameter Name Description SOAP Action The SOAP action of the message. To Address The target endpoint address. Sequence This parameter is used to specify whether split messages should be mediated via a sequence or not, and to specify the sequence if they are to be further mediated. Possible options are as follows. None : If this is selected, no further mediation will be performed for the split messages. Anonymous : If this is selected, you can define an anonymous sequence for the split messages by adding the required mediators as children to Target in the mediator tree. Pick From Registry : If this is selected, you can refer to a pre-defined sequence that is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the resource tree. Endpoint The endpoint to which the split messages should be sent. Possible options are as follows. None : If this is selected, the split messages are not sent to an endpoint . Anonymous : If this is selected, you can define an anonymous endpoint within the iterate target configuration to which the split messages should be sent. Click the Add link which appears after selecting this option to add the anonymous endpoint . See Adding an Endpoint for further information. Pick from Registry : If this is selected, you can refer to a pre-defined endpoint that is currently saves as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. Examples \u00b6 In these examples, the Iterate mediator splits the messages into parts and processes them asynchronously. Also see Splitting Messages into Parts and Processing in Parallel (Iterate/Aggregate) . Using an XPath expression Using a JSONpath expression <iterate expression=\"//m0:getQuote/m0:request\" preservePayload=\"true\" attachPath=\"//m0:getQuote\" xmlns:m0=\"http://services.samples\"> <target> <sequence> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </sequence> </target> </iterate> <iterate id=\"jsonIterator\" preservePayload=\"true\" attachPath=\"json-eval($.placeHolder)\" expression=\"json-eval($.students.studentlist)\"> <target> <sequence> <send> <endpoint> <http method=\"POST\" uri-template=\"http://localhost:8280/iteratesample/echojson\"/> </endpoint> </send> </sequence> </target> </iterate> Sample \u00b6 See Sample 400: Message Splitting and Aggregating the Responses for another example.","title":"Iterate Mediator"},{"location":"references/iterate-Mediator/#iterate-mediator","text":"The Iterate Mediator implements the Splitter enterprise integration pattern and splits the message into a number of different messages derived from the parent message. The Iterate mediator is similar to the Clone mediator . The difference between the two mediators is, the Iterate mediator splits a message into different parts, whereas the Clone mediator makes multiple identical copies of the message. Info The Iterate mediator is a content aware mediator. Note Iterate Mediator is quite similar to the ForEach mediator . You can use complex XPath expressions or JSON expressions to conditionally select elements to iterate over in both mediators. Following are the main difference between ForEach and Iterate mediators: Use the ForEach mediator only for message transformations. If you need to make back-end calls from each iteration, then use the iterate mediator. ForEach supports modifying the original payload. You can use Iterate for situations where you send the split messages to a target and collect them by an Aggregate in a different flow You need to always accompany an Iterate with an Aggregate mediator. ForEach loops over the sub-messages and merges them back to the same parent element of the message. In Iterate you need to send the split messages to an endpoint to continue the message flow. However, ForEach does not allow using Call , Send and Callout mediators in the sequence. ForEach does not split the message flow, unlike Iterate Mediator. It guarantees to execute in the same thread until all iterations are complete. When you use ForEach mediator, you can only loop through segments of the message and do changes to a particular segment. For example, you can change the payload using payload factory mediator. But you cannot send the split message out to a service. Once you exit from the for-each loop, it automatically aggregates the split segments. This replaces the for-each function of the complex XSLT mediators using a ForEach mediator and a Payload Factory mediator. However, to implement the split-aggregate pattern, you still need to use Iterate mediator. Syntax | Configuration | Examples","title":"Iterate Mediator"},{"location":"references/iterate-Mediator/#syntax","text":"<iterate [sequential=(true | false)] [continueParent=(true | false)] [preservePayload=(true | false)] [(attachPath=\"XPath|json-eval(JSON Path)\")? expression=\"XPath|json-eval(JSON Path)\"]> <target [to=\"uri\"] [soapAction=\"qname\"] [sequence=\"sequence_ref\"] [endpoint=\"endpoint_ref\"]> <sequence> (mediator)+ </sequence>? <endpoint> endpoint </endpoint>? </target>+ </iterate>","title":"Syntax"},{"location":"references/iterate-Mediator/#configuration","text":"The parameters available to configure the Iterate mediator are as follows. Parameter Name Description Iterate ID The iterate ID can be used to identify messages created by the iterate mediator. This is particularly useful when aggregating responses of messages that are created using nested iterate mediators. Sequential Mediation This parameter is used to specify whether the split messages should be processed sequentially or not. The processing is carried based on the information relating to the sequence and endpoint specified in the target configuration . The possible values are as follows. True : If this is selected, the split messages will be processed sequentially. Note that selecting True might cause delays due to high resource consumption. False : If this is selected, the split messages will not be processed sequentially. This is the default value and it results in better performance. !!! info The responses will not necessarily be aggregated in the same order that the requests were sent, even if the sequential Mediation parameter is set to true . Continue Parent This parameter is used to specify whether the original message should be preserved or not. Possible values are as follows. True : If this is selected, the original message will be preserved. False : If this is selected, the original message will be discarded. This is the default value. Preserve Payload This parameter is used to specify whether the original message payload should be used as a template when creating split messages. Possible values are as follows. True : If this is selected, the original message payload will be used as a template. False : If this is selected, the original message payload will not be used as a template. This is the default value. Iterate Expression The XPath expression used to split the message.. This expression selects the set of XML elements from the request payload that are applied to the mediation defined within the iterate target. E ach iteration of the iterate mediator will get one element from that set. New messages are created for each and every matching element and processed in parallel or in sequence based on the value specified for the Sequential Mediation parameter. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Attach Path To form new messages, you can specify an XPath expression or a JSONPath expression to identify the parent element to which the split elements are attached (as expressed in Iterate expression). !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Each Iterate mediator has its own target by default. It appears in the mediation tree as shown below once you configure the above parameters and save them. The parameters available to configure the target configuration are as follows. Parameter Name Description SOAP Action The SOAP action of the message. To Address The target endpoint address. Sequence This parameter is used to specify whether split messages should be mediated via a sequence or not, and to specify the sequence if they are to be further mediated. Possible options are as follows. None : If this is selected, no further mediation will be performed for the split messages. Anonymous : If this is selected, you can define an anonymous sequence for the split messages by adding the required mediators as children to Target in the mediator tree. Pick From Registry : If this is selected, you can refer to a pre-defined sequence that is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the resource tree. Endpoint The endpoint to which the split messages should be sent. Possible options are as follows. None : If this is selected, the split messages are not sent to an endpoint . Anonymous : If this is selected, you can define an anonymous endpoint within the iterate target configuration to which the split messages should be sent. Click the Add link which appears after selecting this option to add the anonymous endpoint . See Adding an Endpoint for further information. Pick from Registry : If this is selected, you can refer to a pre-defined endpoint that is currently saves as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree.","title":"Configuration"},{"location":"references/iterate-Mediator/#examples","text":"In these examples, the Iterate mediator splits the messages into parts and processes them asynchronously. Also see Splitting Messages into Parts and Processing in Parallel (Iterate/Aggregate) . Using an XPath expression Using a JSONpath expression <iterate expression=\"//m0:getQuote/m0:request\" preservePayload=\"true\" attachPath=\"//m0:getQuote\" xmlns:m0=\"http://services.samples\"> <target> <sequence> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </sequence> </target> </iterate> <iterate id=\"jsonIterator\" preservePayload=\"true\" attachPath=\"json-eval($.placeHolder)\" expression=\"json-eval($.students.studentlist)\"> <target> <sequence> <send> <endpoint> <http method=\"POST\" uri-template=\"http://localhost:8280/iteratesample/echojson\"/> </endpoint> </send> </sequence> </target> </iterate>","title":"Examples"},{"location":"references/iterate-Mediator/#sample","text":"See Sample 400: Message Splitting and Aggregating the Responses for another example.","title":"Sample"},{"location":"references/log-Mediator/","text":"Log Mediator \u00b6 The Log mediator is used to log mediated messages. For more information on logging, see Monitoring Logs in WSO2 Administration Guide. Info The Log mediator is a conditionally content aware mediator. Syntax | Configuration | Examples Syntax \u00b6 The log token refers to a <log> element, which may be used to log messages being mediated. <log [level=\"string\"] [separator=\"string\"]> <property name=\"string\" (value=\"literal\" | expression=\"[XPath|json-eval(JSON Path)]\")/>* </log> Configuration \u00b6 The general parameters available to configure the Log mediator are as follows. Parameter Name Description Log Category This parameter is used to specify the log category. Possible values are as follows. Following log levels correspond to the ESB profile service level logs. TRACE - This designates fine-grained informational events than the DEBUG. DEBUG - This designates fine-grained informational events that are most useful to debug an application. INFO - This designates informational messages that highlight the progress of the application at coarse-grained level. WARN - This designates potentially harmful situations. ERROR - This designates error events that might still allow the application to continue running. FATAL - This designate s very severe error events that will presumably lead the application to abort. Log Level This parameter is used to specify the log level. The possible values are as follows. Full : If this is selected, all the standard headers logged at the Simple level as well as the full payload of the message will be logged. This log level causes the message content to be parsed and hence incurs a performance overhead. Simple : If this is selected, the standard headers (i.e. To , From , WSAction , SOAPAction , ReplyTo , and MessageID ) will be logged. Headers : If this is selected, all the SOAP header blocks are logged. Custom : If this is selected, only the properties added to the Log mediator configuration will be logged. !!! info The properties included in the Log mediator configuration will be logged regardless of the log level selected. Log Separator This parameter is used to specify a value to be used in the log to separate attributes. The , comma is default. !!! note Use only the Source View to add a tab (i.e., by defining the separator=\"&#x9;\" parameter in the syntax) or a new line (i.e., by defining the separator=\"&#xA;\" parameter in the syntax ) as the Log Separator , since the Design View does not support this. Properties to be logged by the Log mediator can be added by clicking Add Property . The parameters available to configure a property are as follows. Parameter Name Description Property Name The name of the property to be logged. Property Value The possible values for this parameter are as follows: Value : If this is selected, a static value would be considered as the property value and this value should be entered in the Value/Expression parameter. Expression : If this is selected, the property value will be determined during mediation by evaluating an expression. This expression should be entered in the Value/ Expression parameter. Value/Expression This parameter is used to enter a status value as the property value, or to enter an expression to evaluate the property value based on the what you entered for the Property Value parameter. When specifying a JSONPath, use the format json-eval(<JSON_PATH>) , such as json-eval(getQuote.request.symbol) . For more information on using JSON with the the ESB profile , see Working with JSON Message Payloads . !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This parameter allows the property to be deleted. Examples \u00b6 Example 1 - Full log \u00b6 In this example, everything is logged including the complete SOAP message. <log level=\"full\" xmlns=\"http://ws.apache.org/ns/synapse\"/> Example 2 - Custom logs \u00b6 In this example, the log level is custom . A property with an XPath expression which is used to get a stock price from a message is included. This results in logging the stock price which is a dynamic value. html/xml <log level=\"custom\" xmlns=\"http://ws.apache.org/ns/synapse\"> <property name=\"text\" expression=\"fn:concat('Stock price - ',get-property('stock_price'))\"/> </log>","title":"Log Mediator"},{"location":"references/log-Mediator/#log-mediator","text":"The Log mediator is used to log mediated messages. For more information on logging, see Monitoring Logs in WSO2 Administration Guide. Info The Log mediator is a conditionally content aware mediator. Syntax | Configuration | Examples","title":"Log Mediator"},{"location":"references/log-Mediator/#syntax","text":"The log token refers to a <log> element, which may be used to log messages being mediated. <log [level=\"string\"] [separator=\"string\"]> <property name=\"string\" (value=\"literal\" | expression=\"[XPath|json-eval(JSON Path)]\")/>* </log>","title":"Syntax"},{"location":"references/log-Mediator/#configuration","text":"The general parameters available to configure the Log mediator are as follows. Parameter Name Description Log Category This parameter is used to specify the log category. Possible values are as follows. Following log levels correspond to the ESB profile service level logs. TRACE - This designates fine-grained informational events than the DEBUG. DEBUG - This designates fine-grained informational events that are most useful to debug an application. INFO - This designates informational messages that highlight the progress of the application at coarse-grained level. WARN - This designates potentially harmful situations. ERROR - This designates error events that might still allow the application to continue running. FATAL - This designate s very severe error events that will presumably lead the application to abort. Log Level This parameter is used to specify the log level. The possible values are as follows. Full : If this is selected, all the standard headers logged at the Simple level as well as the full payload of the message will be logged. This log level causes the message content to be parsed and hence incurs a performance overhead. Simple : If this is selected, the standard headers (i.e. To , From , WSAction , SOAPAction , ReplyTo , and MessageID ) will be logged. Headers : If this is selected, all the SOAP header blocks are logged. Custom : If this is selected, only the properties added to the Log mediator configuration will be logged. !!! info The properties included in the Log mediator configuration will be logged regardless of the log level selected. Log Separator This parameter is used to specify a value to be used in the log to separate attributes. The , comma is default. !!! note Use only the Source View to add a tab (i.e., by defining the separator=\"&#x9;\" parameter in the syntax) or a new line (i.e., by defining the separator=\"&#xA;\" parameter in the syntax ) as the Log Separator , since the Design View does not support this. Properties to be logged by the Log mediator can be added by clicking Add Property . The parameters available to configure a property are as follows. Parameter Name Description Property Name The name of the property to be logged. Property Value The possible values for this parameter are as follows: Value : If this is selected, a static value would be considered as the property value and this value should be entered in the Value/Expression parameter. Expression : If this is selected, the property value will be determined during mediation by evaluating an expression. This expression should be entered in the Value/ Expression parameter. Value/Expression This parameter is used to enter a status value as the property value, or to enter an expression to evaluate the property value based on the what you entered for the Property Value parameter. When specifying a JSONPath, use the format json-eval(<JSON_PATH>) , such as json-eval(getQuote.request.symbol) . For more information on using JSON with the the ESB profile , see Working with JSON Message Payloads . !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This parameter allows the property to be deleted.","title":"Configuration"},{"location":"references/log-Mediator/#examples","text":"","title":"Examples"},{"location":"references/log-Mediator/#example-1-full-log","text":"In this example, everything is logged including the complete SOAP message. <log level=\"full\" xmlns=\"http://ws.apache.org/ns/synapse\"/>","title":"Example 1 - Full log"},{"location":"references/log-Mediator/#example-2-custom-logs","text":"In this example, the log level is custom . A property with an XPath expression which is used to get a stock price from a message is included. This results in logging the stock price which is a dynamic value. html/xml <log level=\"custom\" xmlns=\"http://ws.apache.org/ns/synapse\"> <property name=\"text\" expression=\"fn:concat('Stock price - ',get-property('stock_price'))\"/> </log>","title":"Example 2 - Custom logs"},{"location":"references/loopback-Mediator/","text":"Loopback Mediator \u00b6 The Loopback Mediator moves messages from the in flow (request path) to the out flow (response path). All the configuration included in the in sequence that appears after the Loopback mediator is skipped. Info The messages that have already been passed from the In sequence to the Out sequence cannot be moved to the Out sequence again via the Loopback mediator. Info The Loopback mediator is a content-unaware mediator. Syntax | Configuration | Example Syntax \u00b6 The loopback token refers to a \\< loopback > element, which is used to skip the rest of the in flow and move the message to the out flow. <loopback/> Configuration \u00b6 As with other mediators, after adding the Loopback mediator to a sequence, you can click its up and down arrows to move its location in the sequence. Example \u00b6 This example is a main sequence configuration with two PayloadFactory mediators . Assume you only want to use the first factory but need to keep the second factory in the configuration for future reference. The Loopback mediator is added after the first PayloadFactory mediator configuration to skip the second PayloadFactory mediator configuration. This configuration will cause the message to be processed with the first payload factory and then immediately move to the out flow, skipping the second payload factory in the in flow. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"main\"> <in> <payloadFactory> <format> <m:messageBeforeLoopBack xmlns:m=\"http://services.samples\"> <m:messageBeforeLoopBackSymbol> <m:symbolBeforeLoopBack>$1</m:symbolBeforeLoopBack> </m:messageBeforeLoopBackSymbol> </m:messageBeforeLoopBack> </format> <args> <arg xmlns:m0=\"http://services.samples\" evaluator=\"xml\" expression=\"//m0:symbol/text()\"/> </args> </payloadFactory> <loopback/> <payloadFactory> <format> <m:messageAfterLoopBack xmlns:m=\"http://services.samples\"> <m:messageAfterLoopBackSymbol> <m:symbolAfterLoopBack>$1</m:symbolAfterLoopBack> </m:messageAfterLoopBackSymbol> </m:messageAfterLoopBack> </format> <args> <arg xmlns:m0=\"http://services.samples\" evaluator=\"xml\" expression=\"//m0:symbolBeforeLoopBack/text()\"/> </args> </payloadFactory> </in> <out> <send/> </out> </sequence> </definitions>","title":"Loopback Mediator"},{"location":"references/loopback-Mediator/#loopback-mediator","text":"The Loopback Mediator moves messages from the in flow (request path) to the out flow (response path). All the configuration included in the in sequence that appears after the Loopback mediator is skipped. Info The messages that have already been passed from the In sequence to the Out sequence cannot be moved to the Out sequence again via the Loopback mediator. Info The Loopback mediator is a content-unaware mediator. Syntax | Configuration | Example","title":"Loopback Mediator"},{"location":"references/loopback-Mediator/#syntax","text":"The loopback token refers to a \\< loopback > element, which is used to skip the rest of the in flow and move the message to the out flow. <loopback/>","title":"Syntax"},{"location":"references/loopback-Mediator/#configuration","text":"As with other mediators, after adding the Loopback mediator to a sequence, you can click its up and down arrows to move its location in the sequence.","title":"Configuration"},{"location":"references/loopback-Mediator/#example","text":"This example is a main sequence configuration with two PayloadFactory mediators . Assume you only want to use the first factory but need to keep the second factory in the configuration for future reference. The Loopback mediator is added after the first PayloadFactory mediator configuration to skip the second PayloadFactory mediator configuration. This configuration will cause the message to be processed with the first payload factory and then immediately move to the out flow, skipping the second payload factory in the in flow. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"main\"> <in> <payloadFactory> <format> <m:messageBeforeLoopBack xmlns:m=\"http://services.samples\"> <m:messageBeforeLoopBackSymbol> <m:symbolBeforeLoopBack>$1</m:symbolBeforeLoopBack> </m:messageBeforeLoopBackSymbol> </m:messageBeforeLoopBack> </format> <args> <arg xmlns:m0=\"http://services.samples\" evaluator=\"xml\" expression=\"//m0:symbol/text()\"/> </args> </payloadFactory> <loopback/> <payloadFactory> <format> <m:messageAfterLoopBack xmlns:m=\"http://services.samples\"> <m:messageAfterLoopBackSymbol> <m:symbolAfterLoopBack>$1</m:symbolAfterLoopBack> </m:messageAfterLoopBackSymbol> </m:messageAfterLoopBack> </format> <args> <arg xmlns:m0=\"http://services.samples\" evaluator=\"xml\" expression=\"//m0:symbolBeforeLoopBack/text()\"/> </args> </payloadFactory> </in> <out> <send/> </out> </sequence> </definitions>","title":"Example"},{"location":"references/mediation-Sequences/","text":"Mediation Sequences \u00b6 A mediation sequence , commonly called a sequence , is a tree of mediators that you can use in your mediation workflow. When a message is delivered to a sequence, the sequence sends it through all its mediators. When you want to work with mediation sequences, you can use the EI tooling plug-in to create a new sequence as well as to import an existing sequence, or you can add, edit, and delete sequences via the Management Console. Configuring a mediation sequence \u00b6 You can define mediation sequences using the Management Console as described in Adding a Mediation Sequence . The underlying Synapse configuration uses the following syntax: ``` html/xml mediator* You can list the mediators right in the sequence definition (referred to as an **in-line sequence** ) and refer to other sequences by name. For example: ``` html/xml <sequence name=\"foo\"> <log/> <property name=\"test\" value=\"test value\"/> <sequence key=\"other_sequence\"/> <send/> </sequence> This sequence specifies three mediators in-line: the log mediator , property mediator , and the send mediator . It also references the named sequence \"other_sequence\" and therefore uses all the mediators defined in that sequence. In addition to mediators and other sequences, you can configure the following: Create a dynamic sequence by referring to an entry in the registry, in which case the sequence will change as the registry entry changes. Activate statistics collection by setting the statistics attribute to enable. In this mode the sequence will keep track of the number of messages processed and their processing times. For more information, see Monitoring WSO2 EI Using WSO2 Analytics . Activate trace collection by setting the trace attribute to enable. If tracing is enabled on a sequence, all messages being processed through the sequence will write tracing information through each mediation step. Use the onError attribute to define a custom error handler sequence. If an error occurs while executing the sequence, this error handler will be called. If you do not specify an error handler, the fault sequence will be used, as described in the next section. About the main and fault sequences \u00b6 A mediation configuration holds two special sequences named main and fault . All messages that are not destined for proxy services are sent through the main sequence. By default, the main sequence simply sends a message without mediation, so to add message mediation, you add mediators and/or named sequences in the main sequence. By default, the fault sequence will log the message, the payload, and any error/exception encountered, and the drop mediator stops further processing. You should configure the fault sequence with the correct error handling instead of simply dropping messages. For more information, see Error Handling .","title":"Mediation Sequences"},{"location":"references/mediation-Sequences/#mediation-sequences","text":"A mediation sequence , commonly called a sequence , is a tree of mediators that you can use in your mediation workflow. When a message is delivered to a sequence, the sequence sends it through all its mediators. When you want to work with mediation sequences, you can use the EI tooling plug-in to create a new sequence as well as to import an existing sequence, or you can add, edit, and delete sequences via the Management Console.","title":"Mediation Sequences"},{"location":"references/mediation-Sequences/#configuring-a-mediation-sequence","text":"You can define mediation sequences using the Management Console as described in Adding a Mediation Sequence . The underlying Synapse configuration uses the following syntax: ``` html/xml mediator* You can list the mediators right in the sequence definition (referred to as an **in-line sequence** ) and refer to other sequences by name. For example: ``` html/xml <sequence name=\"foo\"> <log/> <property name=\"test\" value=\"test value\"/> <sequence key=\"other_sequence\"/> <send/> </sequence> This sequence specifies three mediators in-line: the log mediator , property mediator , and the send mediator . It also references the named sequence \"other_sequence\" and therefore uses all the mediators defined in that sequence. In addition to mediators and other sequences, you can configure the following: Create a dynamic sequence by referring to an entry in the registry, in which case the sequence will change as the registry entry changes. Activate statistics collection by setting the statistics attribute to enable. In this mode the sequence will keep track of the number of messages processed and their processing times. For more information, see Monitoring WSO2 EI Using WSO2 Analytics . Activate trace collection by setting the trace attribute to enable. If tracing is enabled on a sequence, all messages being processed through the sequence will write tracing information through each mediation step. Use the onError attribute to define a custom error handler sequence. If an error occurs while executing the sequence, this error handler will be called. If you do not specify an error handler, the fault sequence will be used, as described in the next section.","title":"Configuring a mediation sequence"},{"location":"references/mediation-Sequences/#about-the-main-and-fault-sequences","text":"A mediation configuration holds two special sequences named main and fault . All messages that are not destined for proxy services are sent through the main sequence. By default, the main sequence simply sends a message without mediation, so to add message mediation, you add mediators and/or named sequences in the main sequence. By default, the fault sequence will log the message, the payload, and any error/exception encountered, and the drop mediator stops further processing. You should configure the fault sequence with the correct error handling instead of simply dropping messages. For more information, see Error Handling .","title":"About the main and fault sequences"},{"location":"references/mediator-catalog/","text":"","title":"Mediator catalog"},{"location":"references/ndex/","text":"EI650 (WSO2 Enterprise Integrator 6.5.0) \u00b6 Available Pages: \u00b6 Working with Mediators Working with Mediators via Tooling ESB Mediators Aggregate Mediator Bean Mediator Builder Mediator Cache Mediator Call Mediator Call Template Mediator Callout Mediator Class Mediator Clone Mediator Conditional Router Mediator Data Mapper Mediator Creating a JSON Schema Manually WSO2 ESB Data Mapper JSON Schema Specification Using Data Mapper Mediator in the ESB Profile DBLookup Mediator DB Report Mediator Drop Mediator EJB Mediator Enqueue Mediator Enrich Mediator Entitlement Mediator Advanced Callback Properties Event Mediator FastXSLT Mediator Fault Mediator Filter Mediator ForEach Mediator Header Mediator In and Out Mediators Iterate Mediator Log Mediator Loopback Mediator OAuth Mediator PayloadFactory Mediator POJOCommand Mediator Property Mediator Properties Reference Generic Properties HTTP Transport Properties SOAP Headers Axis2 Properties Synapse Message Context Properties Accessing Properties with XPath Property Group Mediator Publish Event Mediator Respond Mediator Rule Mediator Script Mediator Script Mediator with Nashorn Support Send Mediator Sequence Mediator Smooks Mediator Spring Mediator Store Mediator Switch Mediator Throttle Mediator Transaction Mediator Transaction Mediator Example URLRewrite Mediator Validate Mediator XQuery Mediator XSLT Mediator Creating Custom Mediators Writing Custom Mediator Implementations Writing Custom Configuration Implementations for Mediators Mediation Sequences Working with Sequences via Tooling Prioritizing Messages Adding a Priority Executor Editing a Priority Executor Deleting a Priority Executor Debugging Mediation","title":"EI650 (WSO2 Enterprise Integrator 6.5.0)"},{"location":"references/ndex/#ei650-wso2-enterprise-integrator-650","text":"","title":"EI650 (WSO2 Enterprise Integrator 6.5.0)"},{"location":"references/ndex/#available-pages","text":"Working with Mediators Working with Mediators via Tooling ESB Mediators Aggregate Mediator Bean Mediator Builder Mediator Cache Mediator Call Mediator Call Template Mediator Callout Mediator Class Mediator Clone Mediator Conditional Router Mediator Data Mapper Mediator Creating a JSON Schema Manually WSO2 ESB Data Mapper JSON Schema Specification Using Data Mapper Mediator in the ESB Profile DBLookup Mediator DB Report Mediator Drop Mediator EJB Mediator Enqueue Mediator Enrich Mediator Entitlement Mediator Advanced Callback Properties Event Mediator FastXSLT Mediator Fault Mediator Filter Mediator ForEach Mediator Header Mediator In and Out Mediators Iterate Mediator Log Mediator Loopback Mediator OAuth Mediator PayloadFactory Mediator POJOCommand Mediator Property Mediator Properties Reference Generic Properties HTTP Transport Properties SOAP Headers Axis2 Properties Synapse Message Context Properties Accessing Properties with XPath Property Group Mediator Publish Event Mediator Respond Mediator Rule Mediator Script Mediator Script Mediator with Nashorn Support Send Mediator Sequence Mediator Smooks Mediator Spring Mediator Store Mediator Switch Mediator Throttle Mediator Transaction Mediator Transaction Mediator Example URLRewrite Mediator Validate Mediator XQuery Mediator XSLT Mediator Creating Custom Mediators Writing Custom Mediator Implementations Writing Custom Configuration Implementations for Mediators Mediation Sequences Working with Sequences via Tooling Prioritizing Messages Adding a Priority Executor Editing a Priority Executor Deleting a Priority Executor Debugging Mediation","title":"Available Pages:"},{"location":"references/oAuth-Mediator/","text":"OAuth Mediator \u00b6 The OAuth Mediator supports 2 forms of OAuth. It bypasses the RESTful requests and authenticates users against WSO2 Identity Server. When a client tries to invoke a RESTful service, it may be required to verify the credentials of the client. This can be achieved by registering an OAuth application in the WSO2 Identity Server. When the client sends a REST call with the Authorization header to the the ESB profile , the OAuth mediator validates it with the Identity server and proceeds. See 2-legged OAuth for Securing a RESTful Service for detailed instructions to carry out this process. Info If you are using OAuth 1 a, you will get the org.apache.synapse.SynapseException: Unable to find SCOPE value in Synapse Message Context error when the SCOPE property is not set in the synapse message context. To avoid this error, add a property with the name scope and a value in the synapse message context as shown in the Example section. Syntax | Configuration | Example Syntax \u00b6 <oauthService remoteServiceUrl=\"\" username=\"\" password=\"\"/> Configuration \u00b6 The parameters available to configure the OAuth mediator are as follows. Parameter Name Description OAuth Server The server URL of the WSO2 Identity Server. Username The user name to be used to log into the WSO2 Identity Server. Password The password used to log into the WSO2 Identity Server. Example \u00b6 In the following OAuth mediator configuration accesses a remote service via the https://localhost:9443/service URL. The user accessing this service is authenticated via the OAuth application registered in the WSO2 Identity Server and accessed via the http://ws.apache.org/ns/synapse URL. The username used to log into the WSO2 Identity Server is foo and the password is bar . Both the user name and the password should be registered in the Identity Server. The Property mediator adds a property named scope to the synapse message context. The value of this property will be used by the OAuth mediator to send the OAuth request. Info The following example is applicable for OAuth 2.0 as well. <property name=\"scope\" scope=\"default\" type=\"STRING\" value=\"123\"/> <oauthService xmlns=\"http://ws.apache.org/ns/synapse\" remoteServiceUrl=\"https://localhost:9443/services\" username=\"foo\" password=\"bar\" />","title":"OAuth Mediator"},{"location":"references/oAuth-Mediator/#oauth-mediator","text":"The OAuth Mediator supports 2 forms of OAuth. It bypasses the RESTful requests and authenticates users against WSO2 Identity Server. When a client tries to invoke a RESTful service, it may be required to verify the credentials of the client. This can be achieved by registering an OAuth application in the WSO2 Identity Server. When the client sends a REST call with the Authorization header to the the ESB profile , the OAuth mediator validates it with the Identity server and proceeds. See 2-legged OAuth for Securing a RESTful Service for detailed instructions to carry out this process. Info If you are using OAuth 1 a, you will get the org.apache.synapse.SynapseException: Unable to find SCOPE value in Synapse Message Context error when the SCOPE property is not set in the synapse message context. To avoid this error, add a property with the name scope and a value in the synapse message context as shown in the Example section. Syntax | Configuration | Example","title":"OAuth Mediator"},{"location":"references/oAuth-Mediator/#syntax","text":"<oauthService remoteServiceUrl=\"\" username=\"\" password=\"\"/>","title":"Syntax"},{"location":"references/oAuth-Mediator/#configuration","text":"The parameters available to configure the OAuth mediator are as follows. Parameter Name Description OAuth Server The server URL of the WSO2 Identity Server. Username The user name to be used to log into the WSO2 Identity Server. Password The password used to log into the WSO2 Identity Server.","title":"Configuration"},{"location":"references/oAuth-Mediator/#example","text":"In the following OAuth mediator configuration accesses a remote service via the https://localhost:9443/service URL. The user accessing this service is authenticated via the OAuth application registered in the WSO2 Identity Server and accessed via the http://ws.apache.org/ns/synapse URL. The username used to log into the WSO2 Identity Server is foo and the password is bar . Both the user name and the password should be registered in the Identity Server. The Property mediator adds a property named scope to the synapse message context. The value of this property will be used by the OAuth mediator to send the OAuth request. Info The following example is applicable for OAuth 2.0 as well. <property name=\"scope\" scope=\"default\" type=\"STRING\" value=\"123\"/> <oauthService xmlns=\"http://ws.apache.org/ns/synapse\" remoteServiceUrl=\"https://localhost:9443/services\" username=\"foo\" password=\"bar\" />","title":"Example"},{"location":"references/pOJOCommand-Mediator/","text":"POJOCommand Mediator \u00b6 Info Note Please note that this feature is deprecated. Info Tip This mediator implements the popular command pattern. The POJOCommand (or Command) Mediator creates an instance of the specified command class, which may implement the org.apache.synapse.Command interface or should have a public void method public void execute() . If any properties are specified, the corresponding setter methods are invoked on the class before each message is executed. It should be noted that a new instance of the POJOCommand class is created to process each message. After execution of the POJOCommand Mediator, the new value returned by a call to the corresponding getter method is stored back in the message or in the context depending on the action attribute of the property. The action attribute may specify whether this behavior is expected or not via the Read , Update and ReadAndUpdate properties. Syntax | Configuration Syntax \u00b6 <pojoCommand name=\"class-name\"> ( <property name=\"string\" value=\"string\"/> | <property name=\"string\" context-name=\"literal\" [action=(ReadContext | UpdateContext | ReadAndUpdateContext)]> (either literal or XML child) </property> | <property name=\"string\" expression=\"xpath\" [action=(ReadMessage | UpdateMessage | ReadAndUpdateMessage)]/> ) * </pojoCommand> Configuration \u00b6 To load the POJOCommand class, enter the class name in the Class Name parameter and click Load Class . Parameters available to configure properties for the POJOCommand mediator are as follows. Parameter Description Property Name The name of the property. This will be automatically loaded from the class. Read Info The value to set for the property. You can select one of the following sources in the From field. Value : Select this if you want the property value to be a static value. This static value should be entered in the Value field. Message : Select this if you want to read the property value from an incoming message. The XPath expression to execute on the relevant message should be entered in the Value field. Context : Select this if you want to read a value from message context properties. The relevant property key should be entered in the Value field. Update Info This parameter specifies the action to be executed on the property value. You can select one for the following actions in the To field. None : Select this if no activity should be performed on the property value. Message : Select this if you want to update the message. The XPath expression of the element you want to update should be entered in the Value field. Context : Select this if you want to update properties (message context). The relevant property key should be entered in the Value field. Action Click Delete to delete a property.","title":"POJOCommand Mediator"},{"location":"references/pOJOCommand-Mediator/#pojocommand-mediator","text":"Info Note Please note that this feature is deprecated. Info Tip This mediator implements the popular command pattern. The POJOCommand (or Command) Mediator creates an instance of the specified command class, which may implement the org.apache.synapse.Command interface or should have a public void method public void execute() . If any properties are specified, the corresponding setter methods are invoked on the class before each message is executed. It should be noted that a new instance of the POJOCommand class is created to process each message. After execution of the POJOCommand Mediator, the new value returned by a call to the corresponding getter method is stored back in the message or in the context depending on the action attribute of the property. The action attribute may specify whether this behavior is expected or not via the Read , Update and ReadAndUpdate properties. Syntax | Configuration","title":"POJOCommand Mediator"},{"location":"references/pOJOCommand-Mediator/#syntax","text":"<pojoCommand name=\"class-name\"> ( <property name=\"string\" value=\"string\"/> | <property name=\"string\" context-name=\"literal\" [action=(ReadContext | UpdateContext | ReadAndUpdateContext)]> (either literal or XML child) </property> | <property name=\"string\" expression=\"xpath\" [action=(ReadMessage | UpdateMessage | ReadAndUpdateMessage)]/> ) * </pojoCommand>","title":"Syntax"},{"location":"references/pOJOCommand-Mediator/#configuration","text":"To load the POJOCommand class, enter the class name in the Class Name parameter and click Load Class . Parameters available to configure properties for the POJOCommand mediator are as follows. Parameter Description Property Name The name of the property. This will be automatically loaded from the class. Read Info The value to set for the property. You can select one of the following sources in the From field. Value : Select this if you want the property value to be a static value. This static value should be entered in the Value field. Message : Select this if you want to read the property value from an incoming message. The XPath expression to execute on the relevant message should be entered in the Value field. Context : Select this if you want to read a value from message context properties. The relevant property key should be entered in the Value field. Update Info This parameter specifies the action to be executed on the property value. You can select one for the following actions in the To field. None : Select this if no activity should be performed on the property value. Message : Select this if you want to update the message. The XPath expression of the element you want to update should be entered in the Value field. Context : Select this if you want to update properties (message context). The relevant property key should be entered in the Value field. Action Click Delete to delete a property.","title":"Configuration"},{"location":"references/payloadFactory-Mediator/","text":"PayloadFactory Mediator \u00b6 The PayloadFactory Mediator transforms or replaces the contents of a message. Each argument in the mediator configuration can be a static value, or you can specify an XPath or JSON expression to get the value at runtime by evaluating the provided expression against the existing SOAP message. You can configure the format of the request or response and map it to the arguments provided. Info The PayloadFactory mediator is a content aware mediator. Syntax | Configuration Syntax \u00b6 <payloadFactory media-type=\"xml | json\"> <format ../> <args> <arg (value=\"string\" | expression=\" {xpath} | {json} | {text} \")/>* </args> </payloadFactory> The media-type attribute specifies whether to format the message in XML, JSON, or text. If no media type is specified, the message is formatted in XML. If you want to change the payload type of the outgoing message, such as to change it to JSON, add the messageType property after the </payloadFactory> tag. For example: ``` html/xml ... ------------------------------------------------------------------------ ### Configuration Parameters available to configure the PayloadFactory mediator are as follows: <table> <thead> <tr class=\"header\"> <th>Parameter Name</th> <th>Description</th> </tr> </thead> <tbody> <tr class=\"odd\"> <td><strong>Payload Media-Type</strong></td> <td>This parameter is used to specify whether the message payload should be created in JSON, XML, or text.</td> </tr> <tr class=\"even\"> <td><strong>Payload Format</strong></td> <td><p><strong>Define Inline</strong> : If this is selected, the payload format can be defined within the PayloadFactory mediator configuration by entering it in the text field which appears. To add content to the payload, enter variables for each value you want to add using the format $ <em>n</em> (starting with 1 and incrementing with each additional variable, i.e., $1, $2, etc.). You will then create arguments in the same order as the variables to specify each variable's actual value.</p> <p><strong>Pick from Registry</strong> : If this is selected, an existing payload format which is saved in the Registry can be selected. Click either <strong>Governance Registry</strong> or <strong>Configuration Registry</strong> as relevant to select the payload format from the resource tree.</p></td> </tr> <tr class=\"odd\"> <td><strong>Arguments</strong></td> <td><div class=\"content-wrapper\"> <p>This section is used to add an argument that defines the actual value of each variable in the format definition. The arguments must be entered in the same order as the variables in the format, so that the first argument defines the value for variable $1, the second argument defines the value for variable $2, etc. An argument can specify a literal string (e.g., \"John\") or an XPath or JSON expression that extracts the value from the content in the incoming payload.</p> </div></td> </tr> </tbody> </table> Examples - [Example 1: XML](#PayloadFactoryMediator-Example1:XML) - [Example 2: JSON](#PayloadFactoryMediator-Example2:JSON) - [Example 3: Adding arguments](#PayloadFactoryMediator-Example3:Addingarguments) - [Example 4: Suppressing the namespace](#PayloadFactoryMediator-Example4:Suppressingthenamespace) - [Example 5: Including a complete SOAP envelope as the format](#PayloadFactoryMediator-Example5:IncludingacompleteSOAPenvelopeastheformat) - [Example 6: Uploading a file to an HTTP endpoint via a multipart request](#PayloadFactoryMediator-Example6:UploadingafiletoanHTTPendpointviaamultipartrequest) - [Example 7: Adding a literal argument](#PayloadFactoryMediator-Example7:Addingaliteralargument) - [Example 8: Adding a custom SOAP header](#PayloadFactoryMediator-Example8:AddingacustomSOAPheader) - [Samples](#PayloadFactoryMediator-Samples) This section provides examples of using PayloadFactory mediator to generate XML and JSON messages. #### Example 1: XML This example is based on [Sample 17: Transforming / Replacing Message Content with PayloadFactory Mediator](https://docs.wso2.com/pages/viewpage.action?pageId=50500282) . ``` html/xml <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"main\"> <in> <!-- using payloadFactory mediator to transform the request message --> <payloadFactory media-type=\"xml\"> <format> <m:getQuote xmlns:m=\"http://services.samples\"> <m:request> <m:symbol>$1</m:symbol> </m:request> </m:getQuote> </format> <args> <arg xmlns:m0=\"http://services.samples\" expression=\"//m0:Code\"/> </args> </payloadFactory> </in> <out> <!-- using payloadFactory mediator to transform the response message --> <payloadFactory media-type=\"xml\"> <format> <m:CheckPriceResponse xmlns:m=\"http://services.samples/xsd\"> <m:Code>$1</m:Code> <m:Price>$2</m:Price> </m:CheckPriceResponse> </format> <args> <arg xmlns:m0=\"http://services.samples/xsd\" expression=\"//m0:symbol\"/> <arg xmlns:m0=\"http://services.samples/xsd\" expression=\"//m0:last\"/> </args> </payloadFactory> </out> <send/> </sequence> </definitions> Example 2: JSON \u00b6 This example sends a JSON message to the back end. For more information on using JSON with the ESB profile , see Working with JSON Message Payloads . ``` html/xml { \"coordinates\": null, \"created_at\": \"Fri Jun 24 17:43:26 +0000 2011\", \"truncated\": false, \"favorited\": false, \"id_str\": \"$1\", \"entities\": { \"urls\": [ ], \"hashtags\": [ { \"text\": \"$2\", \"indices\": [ 35, 45 ] } ], \"user_mentions\": [ ] }, \"in_reply_to_user_id_str\": null, \"contributors\": null, \"text\": \"$3\", \"retweet_count\": 0, \"id\": \"##\", \"in_reply_to_status_id_str\": null, \"geo\": null, \"retweeted\": false, \"in_reply_to_user_id\": null, \"source\": \"<a href=\\\"http://sites.google.com/site/yorufukurou/\\\" rel=\\\"nofollow\\\">YoruFukurou</a>\", \"in_reply_to_screen_name\": null, \"user\": { \"id_str\": \"##\", \"id\": \"##\" }, \"place\": null, \"in_reply_to_status_id\": null } </format> <args> <arg expression=\"$.entities.hashtags[0].text\" evaluator=\"json\"/> <arg expression=\"//entities/hashtags/text\"/> <arg expression=\"//user/id\"/> <arg expression=\"//user/id_str\"/> <arg expression=\"$.user.id\" evaluator=\"json\"/> <arg expression=\"$.user.id_str\" evaluator=\"json\"/> </args> </payloadFactory> <property name=\"messageType\" value=\"application/json\" scope=\"axis2\"/> ``` Note \u00b6 By default, JSON messages are converted to XML when they are received by the PayloadFactor mediator. However, if you enable the JSON stream formatter and builder, incoming JSON messages are left in JSON format, which improves performance. To enable them, uncomment the following lines in <PRODUCT_HOME>/repository/conf/axis2/axis2.xml : html/xml <!--messageFormatter contentType=\"application/json\" class=\"org.apache.axis2.json.JSONStreamFormatter\"/--> <!--messageBuilder contentType=\"application/json\" class=\"org.apache.axis2.json.JSONStreamBuilder\"/--> When the JSON stream formatter and builder are enabled, if you specify a JSON expression in the PayloadFactory mediator, you must use the evaluator attribute to specify that it is JSON. You can also use the evaluator to specify that an XPath expression is XML, or if you omit the evaluator attribute, XML is assumed by default. For example: XML <arg xmlns:m0= \" http://sample \" expression=\"// m0:symbol \" evaluator=\u201dxml\u201d /> or <arg xmlns:m0= \" http://sample \" expression=\"// m0:symbol \" /> JSON <arg expression=\"$.user.id\" evaluator=\"json\" /> Example 3: Adding arguments \u00b6 In the following configuration, the values for format parameters code and price will be assigned with values that are evaluated from arguments given in the specified order. ``` html/xml 1</m:code> <m:price> 1</m:code> <m:price> 2 #### Example 4: Suppressing the namespace To prevent the ESB profile from adding the default Synapse namespace in an element in the payload format, use ` xmlns=\"\" ` as shown in the following example. ``` java <ser:getPersonByUmid xmlns:ser=\"http://service.directory.com> <umid xmlns=\"\">sagara</umid> </ser:getPersonByUmid> Example 5: Including a complete SOAP envelope as the format \u00b6 In the following configuration, an entire SOAP envelope is added as the format defined inline. This is useful when you want to generate the result of the PayloadFactory mediator as a complete SOAP message with SOAP headers. <payloadFactory media-type=\"xml\"> <format> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Body> <error> <mes>$1</mes> </error> </soapenv:Body> </soapenv:Envelope> </format> <args> <arg value=\" Your request did not return any results. Please enter a valid EIN and try again\"/> </args> </payloadFactory> Example 6: Uploading a file to an HTTP endpoint via a multipart request \u00b6 The below example configuration uses VFS to upload the file in the specified location to the given HTTP endpoint via a HTTP multipart request. <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"smooksample\" startOnLoad=\"true\" statistics=\"disable\" trace=\"disable\" transports=\"vfs\"> <target> <inSequence> <enrich> <source clone=\"true\" type=\"body\"/> <target property=\"originalBody\" type=\"property\"/> </enrich> <property name=\"messageType\" scope=\"axis2\" type=\"STRING\" value=\"multipart/form-data\"/> <payloadFactory media-type=\"xml\"> <format> <root xmlns=\"\"> <customFieldOne>$1</customFieldOne> <customFieldTwo>$2</customFieldTwo> <file xmlns=\"http://org.apache.axis2/xsd/form-data\" charset=\"US-ASCII\" content-type=\"text/plain\" filename=\"$3\" name=\"file1\">$4</file> </root> </format> <args> <arg value=\"Some value 1\"/> <arg value=\"Some value 2\"/> <arg evaluator=\"xml\" expression=\"$trp:FILE_NAME\"/> <arg evaluator=\"xml\" expression=\"$ctx:originalBody\"/> </args> </payloadFactory> <header name=\"Content-Type\" scope=\"transport\" value=\"multipart/form-data\"/> <property name=\"messageType\" scope=\"axis2\" type=\"STRING\" value=\"multipart/form-data\"/> <property name=\"OUT_ONLY\" scope=\"default\" type=\"STRING\" value=\"true\"/> <send> <endpoint> <address format=\"rest\" uri=\"http://localhost:3000/upload/\"/> </endpoint> </send> </inSequence> </target> <parameter name=\"transport.PollInterval\">5</parameter> <parameter name=\"transport.vfs.FileURI\">file:///<YOUR_FILE_LOCATION></parameter> <parameter name=\"transport.vfs.ContentType\">application/octet-stream</parameter> <parameter name=\"transport.vfs.ActionAfterProcess\">DELETE</parameter> <parameter name=\"transport.vfs.FileNamePattern\">.*\\..*</parameter> <description/> </proxy> In the above example, the following property mediator configuration sets the message type as multipart/form-data . <property name=\"messageType\" scope=\"axis2\" type=\"STRING\" value=\"multipart/form-data\"/> The below file parameter of the payload factory mediator defines the HTTP multipart request. Tip Do not change the http://org.apache.axis2/xsd/form-data namesapce. <file xmlns=\"http://org.apache.axis2/xsd/form-data\" charset=\"US-ASCII\" content-type=\"text/plain\" filename=\"$3\" name=\"file1\">$4</file> Also, the below property mediator configuration sets the content of the uploaded file. <header name=\"Content-Type\" scope=\"transport\" value=\"multipart/form-data\"/> <property name=\"messageType\" scope=\"axis2\" type=\"STRING\" value=\"multipart/form-data\"/> Example 7: Adding a literal argument \u00b6 The following example adds a literal argument to the Payload Factory mediator, and sets it to true. This allows you to consider the type of the argument value as String and to stop processing it. <api xmlns=\"http://ws.apache.org/ns/synapse\" name=\"payload\" context=\"/payload\"> <resource methods=\"POST\"> <inSequence> <property name=\"getvalue\" expression=\"json-eval($.hello)\"/> <payloadFactory media-type=\"json\"> <format>{\"newValue\" : \"$1\"}</format> <args> <arg evaluator=\"xml\" literal=\"true\" expression=\"get-property('getvalue')\"/> </args> </payloadFactory> <respond/> </inSequence> </resource> </api> Following is a sample payload (i.e., a.json file), which you can process using the above configuration. a.json {\"hello\" : \"<pqr>abc</pqr>\"} You can use the below sample cURL command to send the request to the above configuration. curl -d @a.json http://localhost:8280/payload -H \"Content-Type: application/json\" -v You view the below output: {\"newValue\" : \"{\"pqr\":\"abc\"}\"} Info If you do not add the literal=\"true\" within the argument in the Payload Factory mediator of the above configuration, you view the output as follows: {\"newValue\" : \"<pqr>abc</pqr>\"} Example 8: Adding a custom SOAP header \u00b6 You can add custom SOAP headers to a request by using the PayloadFactory Mediator in a proxy service as shown in the example below. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <proxy name=\"StockQuoteProxy\" transports=\"https http\" startOnLoad=\"true\" trace=\"disable\"> <description/> <target> <endpoint> <address uri=\"http://localhost:9001/services/SimpleStockQuoteService\"/> </endpoint> <inSequence> <log level=\"full\"/> <payloadFactory media-type=\"xml\"> <format> <soapenv:Envelope xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:xsd=\"http://services.samples/xsd\" xmlns:ser=\"http://services.samples\"> <soapenv:Header> <ser:authenticationRequest> <userName xmlns=\"\">$1</userName> <password xmlns=\"\">$2</password> </ser:authenticationRequest> </soapenv:Header> <soapenv:Body> <ser:getQuote> <ser:request> <xsd:symbol>$3</xsd:symbol> </ser:request> </ser:getQuote> </soapenv:Body> </soapenv:Envelope> </format> <args> <arg value=\"punnadi\"/> <arg value=\"password\"/> <arg value=\"hello\"/> </args> </payloadFactory> </inSequence> <outSequence> <send/> </outSequence> </target> <publishWSDL uri=\"file:repository/samples/resources/proxy/sample_proxy_1.wsdl\"/> </proxy> <sequence name=\"fault\"> <log level=\"full\"> <property name=\"MESSAGE\" value=\"Executing default \"fault\" sequence\"/> <property name=\"ERROR_CODE\" expression=\"get-property('ERROR_CODE')\"/> <property name=\"ERROR_MESSAGE\" expression=\"get-property('ERROR_MESSAGE')\"/> </log> <drop/> </sequence> <sequence name=\"main\"> <log/> <drop/> </sequence> </definitions> Samples \u00b6 The following samples demonstrate the use of the PayloadFactory mediator. Sample 17: Transforming / Replacing Message Content with PayloadFactory Mediator","title":"PayloadFactory Mediator"},{"location":"references/payloadFactory-Mediator/#payloadfactory-mediator","text":"The PayloadFactory Mediator transforms or replaces the contents of a message. Each argument in the mediator configuration can be a static value, or you can specify an XPath or JSON expression to get the value at runtime by evaluating the provided expression against the existing SOAP message. You can configure the format of the request or response and map it to the arguments provided. Info The PayloadFactory mediator is a content aware mediator. Syntax | Configuration","title":"PayloadFactory Mediator"},{"location":"references/payloadFactory-Mediator/#syntax","text":"<payloadFactory media-type=\"xml | json\"> <format ../> <args> <arg (value=\"string\" | expression=\" {xpath} | {json} | {text} \")/>* </args> </payloadFactory> The media-type attribute specifies whether to format the message in XML, JSON, or text. If no media type is specified, the message is formatted in XML. If you want to change the payload type of the outgoing message, such as to change it to JSON, add the messageType property after the </payloadFactory> tag. For example: ``` html/xml ... ------------------------------------------------------------------------ ### Configuration Parameters available to configure the PayloadFactory mediator are as follows: <table> <thead> <tr class=\"header\"> <th>Parameter Name</th> <th>Description</th> </tr> </thead> <tbody> <tr class=\"odd\"> <td><strong>Payload Media-Type</strong></td> <td>This parameter is used to specify whether the message payload should be created in JSON, XML, or text.</td> </tr> <tr class=\"even\"> <td><strong>Payload Format</strong></td> <td><p><strong>Define Inline</strong> : If this is selected, the payload format can be defined within the PayloadFactory mediator configuration by entering it in the text field which appears. To add content to the payload, enter variables for each value you want to add using the format $ <em>n</em> (starting with 1 and incrementing with each additional variable, i.e., $1, $2, etc.). You will then create arguments in the same order as the variables to specify each variable's actual value.</p> <p><strong>Pick from Registry</strong> : If this is selected, an existing payload format which is saved in the Registry can be selected. Click either <strong>Governance Registry</strong> or <strong>Configuration Registry</strong> as relevant to select the payload format from the resource tree.</p></td> </tr> <tr class=\"odd\"> <td><strong>Arguments</strong></td> <td><div class=\"content-wrapper\"> <p>This section is used to add an argument that defines the actual value of each variable in the format definition. The arguments must be entered in the same order as the variables in the format, so that the first argument defines the value for variable $1, the second argument defines the value for variable $2, etc. An argument can specify a literal string (e.g., \"John\") or an XPath or JSON expression that extracts the value from the content in the incoming payload.</p> </div></td> </tr> </tbody> </table> Examples - [Example 1: XML](#PayloadFactoryMediator-Example1:XML) - [Example 2: JSON](#PayloadFactoryMediator-Example2:JSON) - [Example 3: Adding arguments](#PayloadFactoryMediator-Example3:Addingarguments) - [Example 4: Suppressing the namespace](#PayloadFactoryMediator-Example4:Suppressingthenamespace) - [Example 5: Including a complete SOAP envelope as the format](#PayloadFactoryMediator-Example5:IncludingacompleteSOAPenvelopeastheformat) - [Example 6: Uploading a file to an HTTP endpoint via a multipart request](#PayloadFactoryMediator-Example6:UploadingafiletoanHTTPendpointviaamultipartrequest) - [Example 7: Adding a literal argument](#PayloadFactoryMediator-Example7:Addingaliteralargument) - [Example 8: Adding a custom SOAP header](#PayloadFactoryMediator-Example8:AddingacustomSOAPheader) - [Samples](#PayloadFactoryMediator-Samples) This section provides examples of using PayloadFactory mediator to generate XML and JSON messages. #### Example 1: XML This example is based on [Sample 17: Transforming / Replacing Message Content with PayloadFactory Mediator](https://docs.wso2.com/pages/viewpage.action?pageId=50500282) . ``` html/xml <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"main\"> <in> <!-- using payloadFactory mediator to transform the request message --> <payloadFactory media-type=\"xml\"> <format> <m:getQuote xmlns:m=\"http://services.samples\"> <m:request> <m:symbol>$1</m:symbol> </m:request> </m:getQuote> </format> <args> <arg xmlns:m0=\"http://services.samples\" expression=\"//m0:Code\"/> </args> </payloadFactory> </in> <out> <!-- using payloadFactory mediator to transform the response message --> <payloadFactory media-type=\"xml\"> <format> <m:CheckPriceResponse xmlns:m=\"http://services.samples/xsd\"> <m:Code>$1</m:Code> <m:Price>$2</m:Price> </m:CheckPriceResponse> </format> <args> <arg xmlns:m0=\"http://services.samples/xsd\" expression=\"//m0:symbol\"/> <arg xmlns:m0=\"http://services.samples/xsd\" expression=\"//m0:last\"/> </args> </payloadFactory> </out> <send/> </sequence> </definitions>","title":"Syntax"},{"location":"references/payloadFactory-Mediator/#example-2-json","text":"This example sends a JSON message to the back end. For more information on using JSON with the ESB profile , see Working with JSON Message Payloads . ``` html/xml { \"coordinates\": null, \"created_at\": \"Fri Jun 24 17:43:26 +0000 2011\", \"truncated\": false, \"favorited\": false, \"id_str\": \"$1\", \"entities\": { \"urls\": [ ], \"hashtags\": [ { \"text\": \"$2\", \"indices\": [ 35, 45 ] } ], \"user_mentions\": [ ] }, \"in_reply_to_user_id_str\": null, \"contributors\": null, \"text\": \"$3\", \"retweet_count\": 0, \"id\": \"##\", \"in_reply_to_status_id_str\": null, \"geo\": null, \"retweeted\": false, \"in_reply_to_user_id\": null, \"source\": \"<a href=\\\"http://sites.google.com/site/yorufukurou/\\\" rel=\\\"nofollow\\\">YoruFukurou</a>\", \"in_reply_to_screen_name\": null, \"user\": { \"id_str\": \"##\", \"id\": \"##\" }, \"place\": null, \"in_reply_to_status_id\": null } </format> <args> <arg expression=\"$.entities.hashtags[0].text\" evaluator=\"json\"/> <arg expression=\"//entities/hashtags/text\"/> <arg expression=\"//user/id\"/> <arg expression=\"//user/id_str\"/> <arg expression=\"$.user.id\" evaluator=\"json\"/> <arg expression=\"$.user.id_str\" evaluator=\"json\"/> </args> </payloadFactory> <property name=\"messageType\" value=\"application/json\" scope=\"axis2\"/> ```","title":"Example 2: JSON"},{"location":"references/payloadFactory-Mediator/#note","text":"By default, JSON messages are converted to XML when they are received by the PayloadFactor mediator. However, if you enable the JSON stream formatter and builder, incoming JSON messages are left in JSON format, which improves performance. To enable them, uncomment the following lines in <PRODUCT_HOME>/repository/conf/axis2/axis2.xml : html/xml <!--messageFormatter contentType=\"application/json\" class=\"org.apache.axis2.json.JSONStreamFormatter\"/--> <!--messageBuilder contentType=\"application/json\" class=\"org.apache.axis2.json.JSONStreamBuilder\"/--> When the JSON stream formatter and builder are enabled, if you specify a JSON expression in the PayloadFactory mediator, you must use the evaluator attribute to specify that it is JSON. You can also use the evaluator to specify that an XPath expression is XML, or if you omit the evaluator attribute, XML is assumed by default. For example: XML <arg xmlns:m0= \" http://sample \" expression=\"// m0:symbol \" evaluator=\u201dxml\u201d /> or <arg xmlns:m0= \" http://sample \" expression=\"// m0:symbol \" /> JSON <arg expression=\"$.user.id\" evaluator=\"json\" />","title":"Note"},{"location":"references/payloadFactory-Mediator/#example-3-adding-arguments","text":"In the following configuration, the values for format parameters code and price will be assigned with values that are evaluated from arguments given in the specified order. ``` html/xml 1</m:code> <m:price> 1</m:code> <m:price> 2 #### Example 4: Suppressing the namespace To prevent the ESB profile from adding the default Synapse namespace in an element in the payload format, use ` xmlns=\"\" ` as shown in the following example. ``` java <ser:getPersonByUmid xmlns:ser=\"http://service.directory.com> <umid xmlns=\"\">sagara</umid> </ser:getPersonByUmid>","title":"Example 3: Adding arguments"},{"location":"references/payloadFactory-Mediator/#example-5-including-a-complete-soap-envelope-as-the-format","text":"In the following configuration, an entire SOAP envelope is added as the format defined inline. This is useful when you want to generate the result of the PayloadFactory mediator as a complete SOAP message with SOAP headers. <payloadFactory media-type=\"xml\"> <format> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Body> <error> <mes>$1</mes> </error> </soapenv:Body> </soapenv:Envelope> </format> <args> <arg value=\" Your request did not return any results. Please enter a valid EIN and try again\"/> </args> </payloadFactory>","title":"Example 5: Including a complete SOAP envelope as the format"},{"location":"references/payloadFactory-Mediator/#example-6-uploading-a-file-to-an-http-endpoint-via-a-multipart-request","text":"The below example configuration uses VFS to upload the file in the specified location to the given HTTP endpoint via a HTTP multipart request. <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"smooksample\" startOnLoad=\"true\" statistics=\"disable\" trace=\"disable\" transports=\"vfs\"> <target> <inSequence> <enrich> <source clone=\"true\" type=\"body\"/> <target property=\"originalBody\" type=\"property\"/> </enrich> <property name=\"messageType\" scope=\"axis2\" type=\"STRING\" value=\"multipart/form-data\"/> <payloadFactory media-type=\"xml\"> <format> <root xmlns=\"\"> <customFieldOne>$1</customFieldOne> <customFieldTwo>$2</customFieldTwo> <file xmlns=\"http://org.apache.axis2/xsd/form-data\" charset=\"US-ASCII\" content-type=\"text/plain\" filename=\"$3\" name=\"file1\">$4</file> </root> </format> <args> <arg value=\"Some value 1\"/> <arg value=\"Some value 2\"/> <arg evaluator=\"xml\" expression=\"$trp:FILE_NAME\"/> <arg evaluator=\"xml\" expression=\"$ctx:originalBody\"/> </args> </payloadFactory> <header name=\"Content-Type\" scope=\"transport\" value=\"multipart/form-data\"/> <property name=\"messageType\" scope=\"axis2\" type=\"STRING\" value=\"multipart/form-data\"/> <property name=\"OUT_ONLY\" scope=\"default\" type=\"STRING\" value=\"true\"/> <send> <endpoint> <address format=\"rest\" uri=\"http://localhost:3000/upload/\"/> </endpoint> </send> </inSequence> </target> <parameter name=\"transport.PollInterval\">5</parameter> <parameter name=\"transport.vfs.FileURI\">file:///<YOUR_FILE_LOCATION></parameter> <parameter name=\"transport.vfs.ContentType\">application/octet-stream</parameter> <parameter name=\"transport.vfs.ActionAfterProcess\">DELETE</parameter> <parameter name=\"transport.vfs.FileNamePattern\">.*\\..*</parameter> <description/> </proxy> In the above example, the following property mediator configuration sets the message type as multipart/form-data . <property name=\"messageType\" scope=\"axis2\" type=\"STRING\" value=\"multipart/form-data\"/> The below file parameter of the payload factory mediator defines the HTTP multipart request. Tip Do not change the http://org.apache.axis2/xsd/form-data namesapce. <file xmlns=\"http://org.apache.axis2/xsd/form-data\" charset=\"US-ASCII\" content-type=\"text/plain\" filename=\"$3\" name=\"file1\">$4</file> Also, the below property mediator configuration sets the content of the uploaded file. <header name=\"Content-Type\" scope=\"transport\" value=\"multipart/form-data\"/> <property name=\"messageType\" scope=\"axis2\" type=\"STRING\" value=\"multipart/form-data\"/>","title":"Example 6: Uploading a file to an HTTP endpoint via a multipart request"},{"location":"references/payloadFactory-Mediator/#example-7-adding-a-literal-argument","text":"The following example adds a literal argument to the Payload Factory mediator, and sets it to true. This allows you to consider the type of the argument value as String and to stop processing it. <api xmlns=\"http://ws.apache.org/ns/synapse\" name=\"payload\" context=\"/payload\"> <resource methods=\"POST\"> <inSequence> <property name=\"getvalue\" expression=\"json-eval($.hello)\"/> <payloadFactory media-type=\"json\"> <format>{\"newValue\" : \"$1\"}</format> <args> <arg evaluator=\"xml\" literal=\"true\" expression=\"get-property('getvalue')\"/> </args> </payloadFactory> <respond/> </inSequence> </resource> </api> Following is a sample payload (i.e., a.json file), which you can process using the above configuration. a.json {\"hello\" : \"<pqr>abc</pqr>\"} You can use the below sample cURL command to send the request to the above configuration. curl -d @a.json http://localhost:8280/payload -H \"Content-Type: application/json\" -v You view the below output: {\"newValue\" : \"{\"pqr\":\"abc\"}\"} Info If you do not add the literal=\"true\" within the argument in the Payload Factory mediator of the above configuration, you view the output as follows: {\"newValue\" : \"<pqr>abc</pqr>\"}","title":"Example 7: Adding a literal argument"},{"location":"references/payloadFactory-Mediator/#example-8-adding-a-custom-soap-header","text":"You can add custom SOAP headers to a request by using the PayloadFactory Mediator in a proxy service as shown in the example below. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <proxy name=\"StockQuoteProxy\" transports=\"https http\" startOnLoad=\"true\" trace=\"disable\"> <description/> <target> <endpoint> <address uri=\"http://localhost:9001/services/SimpleStockQuoteService\"/> </endpoint> <inSequence> <log level=\"full\"/> <payloadFactory media-type=\"xml\"> <format> <soapenv:Envelope xmlns:soapenv=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:xsd=\"http://services.samples/xsd\" xmlns:ser=\"http://services.samples\"> <soapenv:Header> <ser:authenticationRequest> <userName xmlns=\"\">$1</userName> <password xmlns=\"\">$2</password> </ser:authenticationRequest> </soapenv:Header> <soapenv:Body> <ser:getQuote> <ser:request> <xsd:symbol>$3</xsd:symbol> </ser:request> </ser:getQuote> </soapenv:Body> </soapenv:Envelope> </format> <args> <arg value=\"punnadi\"/> <arg value=\"password\"/> <arg value=\"hello\"/> </args> </payloadFactory> </inSequence> <outSequence> <send/> </outSequence> </target> <publishWSDL uri=\"file:repository/samples/resources/proxy/sample_proxy_1.wsdl\"/> </proxy> <sequence name=\"fault\"> <log level=\"full\"> <property name=\"MESSAGE\" value=\"Executing default \"fault\" sequence\"/> <property name=\"ERROR_CODE\" expression=\"get-property('ERROR_CODE')\"/> <property name=\"ERROR_MESSAGE\" expression=\"get-property('ERROR_MESSAGE')\"/> </log> <drop/> </sequence> <sequence name=\"main\"> <log/> <drop/> </sequence> </definitions>","title":"Example 8: Adding a custom SOAP header"},{"location":"references/payloadFactory-Mediator/#samples","text":"The following samples demonstrate the use of the PayloadFactory mediator. Sample 17: Transforming / Replacing Message Content with PayloadFactory Mediator","title":"Samples"},{"location":"references/prioritizing-Messages/","text":"Prioritizing Messages \u00b6 Info Note Please note that this feature is deprecated. You can prioritize messages to ensure that high-priority messages are not dropped. Prioritization is implemented at two levels in WSO2 EI: HTTP transport level - If users would like to use the EI as a pure router. Message mediation level - If users use EI for heavy processing like XSLT and XQuery. From the users perspective, key to any priority mediation is to determine the priority of an incoming message. At the message mediation layer, this can be done using content filters. This means the full power of EI configuration language is available to the user for determining the priority of a given message. For example, a message may contain an element called \"priority\" and depending on its value the priority can be determined. At the HTTP layer, user has access to HTTP headers, HTTP parameters and URL values. By looking at these values, user can determine the priority of a given message. The priority mediation implementation is based on Queues and ThreadPoolExecutors . ThreadPoolExecutor accepts a BlockingQueue implementation. A custom blocking queue that can be used to order the jobs based on priority was implemented. ThreadPoolExecutor starts queuing only when the all the core threads are busy. Every message should get equal priority until all the core threads are used. Internally custom BlockingQueue uses multiple queues for accepting jobs with different priorities. Once jobs are put into the queue, it uses a pluggable algorithm for choosing the next job. The default algorithm chooses the jobs based on a priority-based, round-robin algorithm. For example, let's say we have two priorities, 10 and 1. This algorithm tries to fetch 10 items with priority 10 and then 1 item with the priority 1. Priority Executors \u00b6 Priority executors can be used with the Enqueue Mediator to execute sequences with a given priority. Priority executors are used in high-load scenarios when you wants to execute different sequences for messages with different priorities. This approach allows you to control the resources allocated to executing sequences and to prevent high-priority messages from getting delayed and dropped. A priority has a valid meaning comparing to other priorities specified. For example, if there are two priorities with value 10 and 1, a message with priority 10 will get 10 times more resources than messages with priority 1. Priority Executor Configuration \u00b6 <priority-executor name=\"priority-executor\"> <queues isFixed=\"true|false\" nextQueue=\"class implementing NextQueueAlgorithm\"> <queue [size=\"size of the queue\"] priority=\"priority of the messages put in to this queue\"/>* </queues> <threads core=\"core number of threads\" max=\"max number of threads' keep-alive=\"keep alive time\"/> </priority-executor> Core priority executors' attributes: queues - Defines separate queues for different priorities in a Thread Pool Executor. isFixed -Controls the queues to have a fixed depths or un-bounded capacities. Info Note An executor should have at least two or more queues. If only one queue is used, there is no point in using a priority executor. size - Defines a size of a queue. priority -Defines a priority of a queue. Info Note If the queues has unlimited length, no more than core threads will be used. core - Defines a core number of Priority Executor threads. When EI is started with the priority executor, this number of threads will be created. max - Defines the maximum number of threads this executor will have. keep-alive - Defines the keep-alive time of threads. If the number of threads in the executor exceeds the core threads, they will be in active for the keep-alive time only. After the keep-alive time, those threads will be be removed from the system. The following topics describe how to manage your priority executors: Adding a Priority Executor Editing a Priority Executor Deleting a Priority Executor To see a sample of priority-based mediation, see Sample 652: Priority Based Message Mediation .","title":"Prioritizing Messages"},{"location":"references/prioritizing-Messages/#prioritizing-messages","text":"Info Note Please note that this feature is deprecated. You can prioritize messages to ensure that high-priority messages are not dropped. Prioritization is implemented at two levels in WSO2 EI: HTTP transport level - If users would like to use the EI as a pure router. Message mediation level - If users use EI for heavy processing like XSLT and XQuery. From the users perspective, key to any priority mediation is to determine the priority of an incoming message. At the message mediation layer, this can be done using content filters. This means the full power of EI configuration language is available to the user for determining the priority of a given message. For example, a message may contain an element called \"priority\" and depending on its value the priority can be determined. At the HTTP layer, user has access to HTTP headers, HTTP parameters and URL values. By looking at these values, user can determine the priority of a given message. The priority mediation implementation is based on Queues and ThreadPoolExecutors . ThreadPoolExecutor accepts a BlockingQueue implementation. A custom blocking queue that can be used to order the jobs based on priority was implemented. ThreadPoolExecutor starts queuing only when the all the core threads are busy. Every message should get equal priority until all the core threads are used. Internally custom BlockingQueue uses multiple queues for accepting jobs with different priorities. Once jobs are put into the queue, it uses a pluggable algorithm for choosing the next job. The default algorithm chooses the jobs based on a priority-based, round-robin algorithm. For example, let's say we have two priorities, 10 and 1. This algorithm tries to fetch 10 items with priority 10 and then 1 item with the priority 1.","title":"Prioritizing Messages"},{"location":"references/prioritizing-Messages/#priority-executors","text":"Priority executors can be used with the Enqueue Mediator to execute sequences with a given priority. Priority executors are used in high-load scenarios when you wants to execute different sequences for messages with different priorities. This approach allows you to control the resources allocated to executing sequences and to prevent high-priority messages from getting delayed and dropped. A priority has a valid meaning comparing to other priorities specified. For example, if there are two priorities with value 10 and 1, a message with priority 10 will get 10 times more resources than messages with priority 1.","title":"Priority Executors"},{"location":"references/prioritizing-Messages/#priority-executor-configuration","text":"<priority-executor name=\"priority-executor\"> <queues isFixed=\"true|false\" nextQueue=\"class implementing NextQueueAlgorithm\"> <queue [size=\"size of the queue\"] priority=\"priority of the messages put in to this queue\"/>* </queues> <threads core=\"core number of threads\" max=\"max number of threads' keep-alive=\"keep alive time\"/> </priority-executor> Core priority executors' attributes: queues - Defines separate queues for different priorities in a Thread Pool Executor. isFixed -Controls the queues to have a fixed depths or un-bounded capacities. Info Note An executor should have at least two or more queues. If only one queue is used, there is no point in using a priority executor. size - Defines a size of a queue. priority -Defines a priority of a queue. Info Note If the queues has unlimited length, no more than core threads will be used. core - Defines a core number of Priority Executor threads. When EI is started with the priority executor, this number of threads will be created. max - Defines the maximum number of threads this executor will have. keep-alive - Defines the keep-alive time of threads. If the number of threads in the executor exceeds the core threads, they will be in active for the keep-alive time only. After the keep-alive time, those threads will be be removed from the system. The following topics describe how to manage your priority executors: Adding a Priority Executor Editing a Priority Executor Deleting a Priority Executor To see a sample of priority-based mediation, see Sample 652: Priority Based Message Mediation .","title":"Priority Executor Configuration"},{"location":"references/properties-Reference/","text":"Properties Reference \u00b6 Properties provide the means of accessing various types of information regarding a message that passes through the ESB profile. You can also use properties to control the behavior of the ESB profile on a given message. Following are the types of properties you can use: Generic Properties : Allow you to configure messages as they're processed by the ESB profile, such as marking a message as out-only (no response message will be expected), adding a custom error message or code to the message, and disabling WS-Addressing headers. HTTP Transport Properties : Allow you to configure how the HTTP transport processes messages, such as forcing a 202 HTTP response to the client so that it stops waiting for a response, setting the HTTP status code, and appending a context to the target URL in RESTful invocations. SOAP Headers : Provide information about the message, such as the To and From values. Axis2 Properties : Allow you to configure the web services engine in the ESB profile, such as specifying how to cache JMS objects, setting the minimum and maximum threads for consuming messages, and forcing outgoing HTTP/S messages to use HTTP 1.0. Synapse Message Context Properties : Allow you to get information about the message, such as the date/time it was sent, the message format, and the message operation. For many properties, you can use the Property mediator to retrieve and set the properties. Additionally, for information on the XPath extension functions and Synapse XPath variables that you can use, see Accessing Properties with XPath .","title":"Properties Reference"},{"location":"references/properties-Reference/#properties-reference","text":"Properties provide the means of accessing various types of information regarding a message that passes through the ESB profile. You can also use properties to control the behavior of the ESB profile on a given message. Following are the types of properties you can use: Generic Properties : Allow you to configure messages as they're processed by the ESB profile, such as marking a message as out-only (no response message will be expected), adding a custom error message or code to the message, and disabling WS-Addressing headers. HTTP Transport Properties : Allow you to configure how the HTTP transport processes messages, such as forcing a 202 HTTP response to the client so that it stops waiting for a response, setting the HTTP status code, and appending a context to the target URL in RESTful invocations. SOAP Headers : Provide information about the message, such as the To and From values. Axis2 Properties : Allow you to configure the web services engine in the ESB profile, such as specifying how to cache JMS objects, setting the minimum and maximum threads for consuming messages, and forcing outgoing HTTP/S messages to use HTTP 1.0. Synapse Message Context Properties : Allow you to get information about the message, such as the date/time it was sent, the message format, and the message operation. For many properties, you can use the Property mediator to retrieve and set the properties. Additionally, for information on the XPath extension functions and Synapse XPath variables that you can use, see Accessing Properties with XPath .","title":"Properties Reference"},{"location":"references/property-Group-Mediator/","text":"Property Group Mediator \u00b6 The Property Group Mediator is similar to the Property Mediator . It sets or removes properties on the message context flowing through synapse. However, unlike the Property mediator, the Property Group mediator handles multiple properties as a group. You can select the property action (i.e., whether the property must be added to or removed from the message context) for each individual property. Therefore, in a scenario where you need to set/remove multiple properties, you can add a single Property Group Mediator configuration instead of multiple Property Mediator configurations. Info The Property Group mediator is a conditionally content aware mediator. Syntax | Configuration | | Example Syntax \u00b6 <propertyGroup> <property name=\"name0\" value=\"value0\"/> <property name=\"name1\" value=\"value1\"/> <property name=\"name2\" value=\"value2\"/> ........ </propertyGroup> Configuration \u00b6 The Property Group Mediator configuration includes a description and a set of properties grouped together. In the source view, multiple Property Mediator configurations are enclosed within the <propertyGroup> element. You can also add a description in the opening element. (e.g., <propertyGroup description=\"A group of properties to include in the greeting message\"> ). In the design view, you can configure the Property Group Mediator as follows: Enter a meaningful description for the property group in the Description field. To add a new property, click the Add a new element icon. {width=\"40\" height=\"40\"} As a result, the Property Mediator dialog box opens. Here, you can select a predefined property from the list or configure a custom property. For detailed descriptions of the parameters you need to configure for each property, see Property Mediator - Configurations . To remove a property, click the Delete selected element(s) icon. {width=\"40\" height=\"38\"} To arrange the properties in the required order within the property group configuration, you can select any property and then click the following icons to move it up/down the list. {width=\"40\" height=\"40\"} {width=\"40\" height=\"38\"} Example \u00b6 The following Propert Group Mediator configuration adds the From , Message , and To properties to the message context. It also removes the MessageID property from the context. All four properties are handled together as a group. <propertyGroup description=\"A group of properties to include in the greeting.\"> <property action=\"remove\" name=\"MessageID\" scope=\"default\"/> <property name=\"From\" scope=\"default\" type=\"STRING\" value=\"\"/> <property name=\"Message\" scope=\"default\" type=\"STRING\" value=\"Welcome to XXX group!\"/> <property name=\"To\" scope=\"default\" type=\"STRING\" value=\"\"/> </propertyGroup>","title":"Property Group Mediator"},{"location":"references/property-Group-Mediator/#property-group-mediator","text":"The Property Group Mediator is similar to the Property Mediator . It sets or removes properties on the message context flowing through synapse. However, unlike the Property mediator, the Property Group mediator handles multiple properties as a group. You can select the property action (i.e., whether the property must be added to or removed from the message context) for each individual property. Therefore, in a scenario where you need to set/remove multiple properties, you can add a single Property Group Mediator configuration instead of multiple Property Mediator configurations. Info The Property Group mediator is a conditionally content aware mediator. Syntax | Configuration | | Example","title":"Property Group Mediator"},{"location":"references/property-Group-Mediator/#syntax","text":"<propertyGroup> <property name=\"name0\" value=\"value0\"/> <property name=\"name1\" value=\"value1\"/> <property name=\"name2\" value=\"value2\"/> ........ </propertyGroup>","title":"Syntax"},{"location":"references/property-Group-Mediator/#configuration","text":"The Property Group Mediator configuration includes a description and a set of properties grouped together. In the source view, multiple Property Mediator configurations are enclosed within the <propertyGroup> element. You can also add a description in the opening element. (e.g., <propertyGroup description=\"A group of properties to include in the greeting message\"> ). In the design view, you can configure the Property Group Mediator as follows: Enter a meaningful description for the property group in the Description field. To add a new property, click the Add a new element icon. {width=\"40\" height=\"40\"} As a result, the Property Mediator dialog box opens. Here, you can select a predefined property from the list or configure a custom property. For detailed descriptions of the parameters you need to configure for each property, see Property Mediator - Configurations . To remove a property, click the Delete selected element(s) icon. {width=\"40\" height=\"38\"} To arrange the properties in the required order within the property group configuration, you can select any property and then click the following icons to move it up/down the list. {width=\"40\" height=\"40\"} {width=\"40\" height=\"38\"}","title":"Configuration"},{"location":"references/property-Group-Mediator/#example","text":"The following Propert Group Mediator configuration adds the From , Message , and To properties to the message context. It also removes the MessageID property from the context. All four properties are handled together as a group. <propertyGroup description=\"A group of properties to include in the greeting.\"> <property action=\"remove\" name=\"MessageID\" scope=\"default\"/> <property name=\"From\" scope=\"default\" type=\"STRING\" value=\"\"/> <property name=\"Message\" scope=\"default\" type=\"STRING\" value=\"Welcome to XXX group!\"/> <property name=\"To\" scope=\"default\" type=\"STRING\" value=\"\"/> </propertyGroup>","title":"Example"},{"location":"references/property-Mediator/","text":"Property Mediator \u00b6 The Property Mediator has no direct impact on the message, but rather on the message context flowing through Synapse. You can retrieve the properties set on a message later through the Synapse XPath Variables or the get-property() extension function. A property can have a defined scope for which it is valid. If a property has no defined scope, it defaults to the Synapse message context scope. Using the property element with the action specified as remove, you can remove any existing message context properties. Info The Property mediator is a conditionally content aware mediator. Syntax | Configuration Syntax \u00b6 <property name=\"string\" [action=set|remove] [type=\"string\"] (value=\"literal\" | expression=\"xpath\") [scope=default|transport|axis2|axis2-client] [pattern=\"regex\" [group=\"integer\"]]> <xml-element/>? </property> Configuration \u00b6 The parameters available for configuring the Property mediator are as follows: Parameter Name Description Name A name for the property. !!! tip For names of the generic properties that come by default, see Generic Properties . You can select them from the drop-down list if you are adding the Property Mediator via Tooling as shown below. Action The action to be performed for the property. Set : If this is selected, the property will be set in the message context. Remove : If this is selected, the property will be removed from the message context. Set Action As The possible values for this parameter are as follows: Value : If this is selected, a static value would be considered as the property value and this value should be entered in the Value parameter. Expression : If this is selected, the property value will be determined during mediation by evaluating an expression. This expression should be entered in the Expression parameter. Type The data type for the property. Property mediator will handle the property as a property of selected type. Available values are as follows. STRING INTEGER BOOLEAN DOUBLE FLOAT LONG SHORT OM String is the default type. !!! info The OM type is used to set xml property values on the message context. This is useful when the expression associated with the property mediator evaluates to an XML node during mediation. When the OM type is used, the XML is converted to an AXIOM OMElement before it is assigned to a property. Value If the Value option is selected for the Set Action As parameter, the property value should be entered as a constant in this parameter. Expression If the Expression option is selected for the Set Action As parameter, the expression which determines the property value should be entered in this parameter. This expression can be an XPath expression or a JSONPath expression. When specifying a JSONPath, use the format json-eval(<JSON_PATH>) , such as json-eval(getQuote.request.symbol) . In both XPath and JSONPath expressions, you can return the value of another property by calling get-property(property-name) . For example, you might create a property called JSON_PATH of which the value is json-eval(pizza.toppings) , and then you could create another property called JSON_PRINT of which the value is get-property('JSON_PATH') , allowing you to use the value of the JSON_PATH property in the JSON_PRINT property. For more information on using JSON with the ESB profile , see Working with JSON Message Payloads . !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Pattern This parameter is used to enter a regular expression that will be evaluated against the value of the property or result of the XPath/JSON Path expression. Group The number (index) of the matching item evaluated using the regular expression entered in the Pattern parameter. Scope The scope at which the property will be set or removed from. Possible values are as follows. Synapse : This is the default scope. The properties set in this scope last as long as the transaction (request-response) exists. Transport : The properties set in this scope will be considered transport headers. For example, if it is required to send an HTTP header named 'CustomHeader' with an outgoing request, you can use the property mediator configuration with this scope. Axis2 : Properties set in this scope have a shorter life span than those set in the Synapse scope. They are mainly used for passing parameters to the underlying Axis2 engine axis2-client : This is similar to the Synapse scope. The difference between the two scopes is that the axis2-client scope can be accessed inside the mediate() method of a mediator via a custom mediator created using the Class mediator . F or further information, s ee axis2-client . Operation : This scope is used to retrieve a property in the operation context level. Registry : This scope is used to retrieve properties within the registry . System : This scope is used to retrieve Java system properties. For a detailed explanation of each scope, s ee Accessing Properties with XPath . Examples Example 1: Setting and logging and property Example 2: Sending a fault message based on the Accept http header Example 3: Reading a property stored in the Registry Example 4: Reading a file stored in the Registry Example 5: Reading SOAP headers Example 1: Setting and logging and property \u00b6 In this example, we are setting the property symbol and later we can log it using the Log Mediator . ``` html/xml <log level=\"custom\"> <property name=\"symbol\" expression=\"get-property('symbol')\"/> </log> Info Note There are predefined XPath variables (such as $ctx ) that you can directly use in the Synapse configuration, instead of using the synapse:get-property() function. These XPath variables get properties of various scopes and have better performance than the get-property() function, which can have much lower performance because it does a registry lookup. These XPath variables get properties of various scopes. For more information on these XPath variables, see Accessing Properties with XPath . xml <payloadFactory media-type=\"xml\"> <format> <m:getQuote xmlns:m=\"http://services.samples\"> <m:request> <m:symbol>Error</m:symbol> </m:request> </m:getQuote> </format> </payloadFactory> <property name=\"messageType\" expression=\"$ctx:accept\" scope=\"axis2\" /> <respond/> Example 3: Reading a property stored in the Registry \u00b6 You can read a property that is stored in the Registry by using the get-property() method in your Synapse configuration. For example, the following Synapse configuration retrieves the abc property of the collection gov:/data/xml/collectionx , and stores it in the regProperty property. <property name=\"regProperty\" expression=\"get-property('registry', 'gov:/data/xml/collectionx@abc')\"/> Info You can use the following syntaxes to read properties or resources stored in the gov or conf Registries. When specifying the path to the resource, do not give the absolute path. Instead, use the gov or conf prefixes. Reading a property stored under a collection \u00b6 get-property('registry','gov:<path to resource from governance>@<propertyname>') get-property('registry','conf:<path to resource from config>@<propertyname>') Reading a property stored under a resource \u00b6 get-property('registry','gov:<path to resource from governance>/@<propertyname>') get-property('registry','conf:<path to resource from config>/@<propertyname>') Reading an XML resource \u00b6 get-property('registry','gov:<path to resource from governance>') get-property('registry','conf:<path to resource from config>') Example 4: Reading a file stored in the Registry \u00b6 Following is an example, in which you read an XML file that is stored in the registry using XPath, to retrieve a value from it. Assume you have the following XML file stored in the Registry (i.e., gov:/test.xml ). test.xml <root> <book>A Song of Ice and Fire</book> <author>George R. R. Martin</author> </root> Your Synapse configuration should be as follows. This uses XPath to read XML. reg_xpath.xml <property name=\"xmlFile\" expression=\"get-property('registry','gov:/test.xml')\" scope=\"default\" type=\"OM\"></property> <log level=\"custom\"> <property name=\"Book_Name\" expression=\"$ctx:xmlFile//book\"></property> </log> Your output log will look like this. [2015-09-21 16:01:28,750] INFO - LogMediator Book_Name = A Song of Ice and Fire Example 5: Reading SOAP headers \u00b6 SOAP headers provide information about the message, such as the To and From values. You can use the get-property() function of the Property mediator to retrieve these headers. You can also add Custom SOAP Headers using the PayloadFactory mediator and the Script Mediator . To \u00b6 Header Name To Possible Values Any URI Description The To header of the message. Example get-property(\"To\") From \u00b6 Header Name From Possible Values Any URI Description The From header of the message. Example get-property(\"From\") Action \u00b6 Header Name Action Possible Values Any URI Description The SOAPAction header of the message. Example get-property(\"Action\") ReplyTo \u00b6 Header Name ReplyTo Possible Values Any URI Description The ReplyTo header of the message. Example \\<header name=\"ReplyTo\" action=\"remove\"/> MessageID \u00b6 Header Name MessageID Possible Values UUID Description The unique message ID of the message. It is not recommended to make alterations to this property of a message. Example get-property(\"MessageID\") RelatesTo \u00b6 Header Name RelatesTo Possible Values UUID Description The unique ID of the request to which the current message is related. It is not recommended to make changes. Example get-property(\"RelatesTo\") FaultTo \u00b6 Header Name FaultTo Possible Values Any URI Description The FaultTo header of the message. Example <header name= \"FaultTo\" value= \"http://localhost:9000\" />","title":"Property Mediator"},{"location":"references/property-Mediator/#property-mediator","text":"The Property Mediator has no direct impact on the message, but rather on the message context flowing through Synapse. You can retrieve the properties set on a message later through the Synapse XPath Variables or the get-property() extension function. A property can have a defined scope for which it is valid. If a property has no defined scope, it defaults to the Synapse message context scope. Using the property element with the action specified as remove, you can remove any existing message context properties. Info The Property mediator is a conditionally content aware mediator. Syntax | Configuration","title":"Property Mediator"},{"location":"references/property-Mediator/#syntax","text":"<property name=\"string\" [action=set|remove] [type=\"string\"] (value=\"literal\" | expression=\"xpath\") [scope=default|transport|axis2|axis2-client] [pattern=\"regex\" [group=\"integer\"]]> <xml-element/>? </property>","title":"Syntax"},{"location":"references/property-Mediator/#configuration","text":"The parameters available for configuring the Property mediator are as follows: Parameter Name Description Name A name for the property. !!! tip For names of the generic properties that come by default, see Generic Properties . You can select them from the drop-down list if you are adding the Property Mediator via Tooling as shown below. Action The action to be performed for the property. Set : If this is selected, the property will be set in the message context. Remove : If this is selected, the property will be removed from the message context. Set Action As The possible values for this parameter are as follows: Value : If this is selected, a static value would be considered as the property value and this value should be entered in the Value parameter. Expression : If this is selected, the property value will be determined during mediation by evaluating an expression. This expression should be entered in the Expression parameter. Type The data type for the property. Property mediator will handle the property as a property of selected type. Available values are as follows. STRING INTEGER BOOLEAN DOUBLE FLOAT LONG SHORT OM String is the default type. !!! info The OM type is used to set xml property values on the message context. This is useful when the expression associated with the property mediator evaluates to an XML node during mediation. When the OM type is used, the XML is converted to an AXIOM OMElement before it is assigned to a property. Value If the Value option is selected for the Set Action As parameter, the property value should be entered as a constant in this parameter. Expression If the Expression option is selected for the Set Action As parameter, the expression which determines the property value should be entered in this parameter. This expression can be an XPath expression or a JSONPath expression. When specifying a JSONPath, use the format json-eval(<JSON_PATH>) , such as json-eval(getQuote.request.symbol) . In both XPath and JSONPath expressions, you can return the value of another property by calling get-property(property-name) . For example, you might create a property called JSON_PATH of which the value is json-eval(pizza.toppings) , and then you could create another property called JSON_PRINT of which the value is get-property('JSON_PATH') , allowing you to use the value of the JSON_PATH property in the JSON_PRINT property. For more information on using JSON with the ESB profile , see Working with JSON Message Payloads . !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Pattern This parameter is used to enter a regular expression that will be evaluated against the value of the property or result of the XPath/JSON Path expression. Group The number (index) of the matching item evaluated using the regular expression entered in the Pattern parameter. Scope The scope at which the property will be set or removed from. Possible values are as follows. Synapse : This is the default scope. The properties set in this scope last as long as the transaction (request-response) exists. Transport : The properties set in this scope will be considered transport headers. For example, if it is required to send an HTTP header named 'CustomHeader' with an outgoing request, you can use the property mediator configuration with this scope. Axis2 : Properties set in this scope have a shorter life span than those set in the Synapse scope. They are mainly used for passing parameters to the underlying Axis2 engine axis2-client : This is similar to the Synapse scope. The difference between the two scopes is that the axis2-client scope can be accessed inside the mediate() method of a mediator via a custom mediator created using the Class mediator . F or further information, s ee axis2-client . Operation : This scope is used to retrieve a property in the operation context level. Registry : This scope is used to retrieve properties within the registry . System : This scope is used to retrieve Java system properties. For a detailed explanation of each scope, s ee Accessing Properties with XPath . Examples Example 1: Setting and logging and property Example 2: Sending a fault message based on the Accept http header Example 3: Reading a property stored in the Registry Example 4: Reading a file stored in the Registry Example 5: Reading SOAP headers","title":"Configuration"},{"location":"references/property-Mediator/#example-1-setting-and-logging-and-property","text":"In this example, we are setting the property symbol and later we can log it using the Log Mediator . ``` html/xml <log level=\"custom\"> <property name=\"symbol\" expression=\"get-property('symbol')\"/> </log> Info Note There are predefined XPath variables (such as $ctx ) that you can directly use in the Synapse configuration, instead of using the synapse:get-property() function. These XPath variables get properties of various scopes and have better performance than the get-property() function, which can have much lower performance because it does a registry lookup. These XPath variables get properties of various scopes. For more information on these XPath variables, see Accessing Properties with XPath . xml <payloadFactory media-type=\"xml\"> <format> <m:getQuote xmlns:m=\"http://services.samples\"> <m:request> <m:symbol>Error</m:symbol> </m:request> </m:getQuote> </format> </payloadFactory> <property name=\"messageType\" expression=\"$ctx:accept\" scope=\"axis2\" /> <respond/>","title":"Example 1: Setting and logging and property"},{"location":"references/property-Mediator/#example-3-reading-a-property-stored-in-the-registry","text":"You can read a property that is stored in the Registry by using the get-property() method in your Synapse configuration. For example, the following Synapse configuration retrieves the abc property of the collection gov:/data/xml/collectionx , and stores it in the regProperty property. <property name=\"regProperty\" expression=\"get-property('registry', 'gov:/data/xml/collectionx@abc')\"/> Info You can use the following syntaxes to read properties or resources stored in the gov or conf Registries. When specifying the path to the resource, do not give the absolute path. Instead, use the gov or conf prefixes.","title":"Example 3: Reading a property stored in the Registry"},{"location":"references/property-Mediator/#reading-a-property-stored-under-a-collection","text":"get-property('registry','gov:<path to resource from governance>@<propertyname>') get-property('registry','conf:<path to resource from config>@<propertyname>')","title":"Reading a property stored under a collection"},{"location":"references/property-Mediator/#reading-a-property-stored-under-a-resource","text":"get-property('registry','gov:<path to resource from governance>/@<propertyname>') get-property('registry','conf:<path to resource from config>/@<propertyname>')","title":"Reading a property stored under a resource"},{"location":"references/property-Mediator/#reading-an-xml-resource","text":"get-property('registry','gov:<path to resource from governance>') get-property('registry','conf:<path to resource from config>')","title":"Reading an XML resource"},{"location":"references/property-Mediator/#example-4-reading-a-file-stored-in-the-registry","text":"Following is an example, in which you read an XML file that is stored in the registry using XPath, to retrieve a value from it. Assume you have the following XML file stored in the Registry (i.e., gov:/test.xml ). test.xml <root> <book>A Song of Ice and Fire</book> <author>George R. R. Martin</author> </root> Your Synapse configuration should be as follows. This uses XPath to read XML. reg_xpath.xml <property name=\"xmlFile\" expression=\"get-property('registry','gov:/test.xml')\" scope=\"default\" type=\"OM\"></property> <log level=\"custom\"> <property name=\"Book_Name\" expression=\"$ctx:xmlFile//book\"></property> </log> Your output log will look like this. [2015-09-21 16:01:28,750] INFO - LogMediator Book_Name = A Song of Ice and Fire","title":"Example 4: Reading a file stored in the Registry"},{"location":"references/property-Mediator/#example-5-reading-soap-headers","text":"SOAP headers provide information about the message, such as the To and From values. You can use the get-property() function of the Property mediator to retrieve these headers. You can also add Custom SOAP Headers using the PayloadFactory mediator and the Script Mediator .","title":"Example 5: Reading SOAP headers"},{"location":"references/property-Mediator/#to","text":"Header Name To Possible Values Any URI Description The To header of the message. Example get-property(\"To\")","title":"To"},{"location":"references/property-Mediator/#from","text":"Header Name From Possible Values Any URI Description The From header of the message. Example get-property(\"From\")","title":"From"},{"location":"references/property-Mediator/#action","text":"Header Name Action Possible Values Any URI Description The SOAPAction header of the message. Example get-property(\"Action\")","title":"Action"},{"location":"references/property-Mediator/#replyto","text":"Header Name ReplyTo Possible Values Any URI Description The ReplyTo header of the message. Example \\<header name=\"ReplyTo\" action=\"remove\"/>","title":"ReplyTo"},{"location":"references/property-Mediator/#messageid","text":"Header Name MessageID Possible Values UUID Description The unique message ID of the message. It is not recommended to make alterations to this property of a message. Example get-property(\"MessageID\")","title":"MessageID"},{"location":"references/property-Mediator/#relatesto","text":"Header Name RelatesTo Possible Values UUID Description The unique ID of the request to which the current message is related. It is not recommended to make changes. Example get-property(\"RelatesTo\")","title":"RelatesTo"},{"location":"references/property-Mediator/#faultto","text":"Header Name FaultTo Possible Values Any URI Description The FaultTo header of the message. Example <header name= \"FaultTo\" value= \"http://localhost:9000\" />","title":"FaultTo"},{"location":"references/publish-Event-Mediator/","text":"Publish Event Mediator \u00b6 The Publish Event mediator constructs events and publishes them to different systems such as BAM and CEP. This is done via event sinks. Info The Publish Event mediator is a content-aware mediator. Syntax | Configuration | Example Syntax \u00b6 <publishEvent> <eventSink>String</eventSink> <streamName>String</streamName> <streamVersion>String</streamVersion> <attributes> <meta> <attribute name=\"string\" type=\"dataType\" default=\"\" (value=\"literal\" | expression=\"[XPath\") /> </meta> <correlation> <attribute name=\"string\" type=\"dataType\" default=\"\" (value=\"literal\" | expression=\"[XPath\") /> </correlation> <payload> <attribute name=\"string\" type=\"dataType\" default=\"\" (value=\"literal\" | expression=\"[XPath\") /> </payload> <arbitrary> <attribute name=\"string\" type=\"dataType\" default=\"\" value=\"literal\" /> </arbitrary> </attributes> </publishEvent> Configuration \u00b6 Parameters that can be configured for the Publish Event mediator are as follows. Parameter Name Description Stream Name The name of the stream to be used for sending data. Stream Version The version of the stream to be used for sending data. EventSink The name of the event sink to which the events should be published. You can add the following four types of attributes to a Publish Event mediator configuration. Meta Attributes : The list of attributes which are included in the Meta section of the event. Correlated Attributes : The list of attributes that are included in the Correlated section of the event. Payload Attributes : The list of attributes that are included in the Payload section of the event. Arbitrary Attributes : The list of attributes that are included in the Arbitrary section of the event. The parameters that are available to configure an individual attribute are as follows. Parameter Name Description Attribute Name The name of the attribute. Attribute Value This parameter specifies whether the value of the attribute should be a static value or an expression. Value : Select this if the attribute value should be a static value, and enter the relevant value in the Value/Expression parameter. Expression : Select this if the attribute value should be evaluated via an XPath expression, and enter the relevant expression in the Value/Expression parameter. Value/Expression The value of the attribute. You can enter a static value, or an expression to evaluate the value depending on the selection made in the Attribute Value parameter. Type The data type of the attribute. Action Click Delete in the relevant row to delete an attribute. Example \u00b6 In this configuration, the Publish Event mediator uses four attributes to extract information from the ESB profile . This information is published in an event sink in the ESB profile named sample_event_sink . <publishEvent> <eventSink>sample_event_sink</eventSink> <streamName>stream_88</streamName> <streamVersion>1.1.2</streamVersion> <attributes> <meta> <attribute name=\"http_method\" type=\"STRING\" defaultValue=\"\" expression=\"get-property('axis2', 'HTTP_METHOD')\"/> <attribute name=\"destination\" type=\"STRING\" defaultValue=\"\" expression=\"get-property('To')\"/> </meta> <correlation> <attribute name=\"date\" type=\"STRING\" defaultValue=\"\" expression=\"get-property('SYSTEM_DATE')\"/> </correlation> <payload> <attribute xmlns:m0=\"http://services.samples\" name=\"symbol\" type=\"STRING\" defaultValue=\"\" expression=\"$body/m0:getQuote/m0:request/m0:symbol\"/> </payload> </attributes> </publishEvent>","title":"Publish Event Mediator"},{"location":"references/publish-Event-Mediator/#publish-event-mediator","text":"The Publish Event mediator constructs events and publishes them to different systems such as BAM and CEP. This is done via event sinks. Info The Publish Event mediator is a content-aware mediator. Syntax | Configuration | Example","title":"Publish Event Mediator"},{"location":"references/publish-Event-Mediator/#syntax","text":"<publishEvent> <eventSink>String</eventSink> <streamName>String</streamName> <streamVersion>String</streamVersion> <attributes> <meta> <attribute name=\"string\" type=\"dataType\" default=\"\" (value=\"literal\" | expression=\"[XPath\") /> </meta> <correlation> <attribute name=\"string\" type=\"dataType\" default=\"\" (value=\"literal\" | expression=\"[XPath\") /> </correlation> <payload> <attribute name=\"string\" type=\"dataType\" default=\"\" (value=\"literal\" | expression=\"[XPath\") /> </payload> <arbitrary> <attribute name=\"string\" type=\"dataType\" default=\"\" value=\"literal\" /> </arbitrary> </attributes> </publishEvent>","title":"Syntax"},{"location":"references/publish-Event-Mediator/#configuration","text":"Parameters that can be configured for the Publish Event mediator are as follows. Parameter Name Description Stream Name The name of the stream to be used for sending data. Stream Version The version of the stream to be used for sending data. EventSink The name of the event sink to which the events should be published. You can add the following four types of attributes to a Publish Event mediator configuration. Meta Attributes : The list of attributes which are included in the Meta section of the event. Correlated Attributes : The list of attributes that are included in the Correlated section of the event. Payload Attributes : The list of attributes that are included in the Payload section of the event. Arbitrary Attributes : The list of attributes that are included in the Arbitrary section of the event. The parameters that are available to configure an individual attribute are as follows. Parameter Name Description Attribute Name The name of the attribute. Attribute Value This parameter specifies whether the value of the attribute should be a static value or an expression. Value : Select this if the attribute value should be a static value, and enter the relevant value in the Value/Expression parameter. Expression : Select this if the attribute value should be evaluated via an XPath expression, and enter the relevant expression in the Value/Expression parameter. Value/Expression The value of the attribute. You can enter a static value, or an expression to evaluate the value depending on the selection made in the Attribute Value parameter. Type The data type of the attribute. Action Click Delete in the relevant row to delete an attribute.","title":"Configuration"},{"location":"references/publish-Event-Mediator/#example","text":"In this configuration, the Publish Event mediator uses four attributes to extract information from the ESB profile . This information is published in an event sink in the ESB profile named sample_event_sink . <publishEvent> <eventSink>sample_event_sink</eventSink> <streamName>stream_88</streamName> <streamVersion>1.1.2</streamVersion> <attributes> <meta> <attribute name=\"http_method\" type=\"STRING\" defaultValue=\"\" expression=\"get-property('axis2', 'HTTP_METHOD')\"/> <attribute name=\"destination\" type=\"STRING\" defaultValue=\"\" expression=\"get-property('To')\"/> </meta> <correlation> <attribute name=\"date\" type=\"STRING\" defaultValue=\"\" expression=\"get-property('SYSTEM_DATE')\"/> </correlation> <payload> <attribute xmlns:m0=\"http://services.samples\" name=\"symbol\" type=\"STRING\" defaultValue=\"\" expression=\"$body/m0:getQuote/m0:request/m0:symbol\"/> </payload> </attributes> </publishEvent>","title":"Example"},{"location":"references/respond-Mediator/","text":"Respond Mediator \u00b6 The Respond Mediator stops the processing on the current message and sends the message back to the client as a response. Syntax | Configuration | Example Syntax \u00b6 The respond token refers to a \\< respond > element, which is used to stop further processing of a message and send the message back to the client. <respond/> Configuration \u00b6 As with other mediators, after adding the Respond mediator to a sequence, you can click its up and down arrows to move its location in the sequence. Example \u00b6 Assume that you have a configuration that sends the request to the Stock Quote service and changes the response value when the symbol is WSO2 or CRF. Also assume that you want to temporarily change the configuration so that if the symbol is CRF, the ESB profile just sends the message back to the client without sending it to the Stock Quote service or performing any additional processing. To achieve this, you can add the Respond mediator at the beginning of the CRF case as shown below. All the configuration after the Respond mediator is ignored. As a result, the rest of the CRF case configuration is left intact, allowing you to revert to the original behavior in the future by removing the Respond mediator if required. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"main\"> <in> <switch source=\"//m0:getQuote/m0:request/m0:symbol\" xmlns:m0=\"http://services.samples\"> <case regex=\"WSO2\"> <property name=\"symbol\" value=\"Great stock - WSO2\"/> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </case> <case regex=\"CRF\"> <respond/> <property name=\"symbol\" value=\"Are you sure? - CRF\"/> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </case> <default> <property name=\"symbol\" expression=\"fn:concat('Normal Stock - ', //m0:getQuote/m0:request/m0:symbol)\" xmlns:m0=\"http://services.samples\"/> </default> </switch> </in> <out> <send/> </out> </sequence> </definitions>","title":"Respond Mediator"},{"location":"references/respond-Mediator/#respond-mediator","text":"The Respond Mediator stops the processing on the current message and sends the message back to the client as a response. Syntax | Configuration | Example","title":"Respond Mediator"},{"location":"references/respond-Mediator/#syntax","text":"The respond token refers to a \\< respond > element, which is used to stop further processing of a message and send the message back to the client. <respond/>","title":"Syntax"},{"location":"references/respond-Mediator/#configuration","text":"As with other mediators, after adding the Respond mediator to a sequence, you can click its up and down arrows to move its location in the sequence.","title":"Configuration"},{"location":"references/respond-Mediator/#example","text":"Assume that you have a configuration that sends the request to the Stock Quote service and changes the response value when the symbol is WSO2 or CRF. Also assume that you want to temporarily change the configuration so that if the symbol is CRF, the ESB profile just sends the message back to the client without sending it to the Stock Quote service or performing any additional processing. To achieve this, you can add the Respond mediator at the beginning of the CRF case as shown below. All the configuration after the Respond mediator is ignored. As a result, the rest of the CRF case configuration is left intact, allowing you to revert to the original behavior in the future by removing the Respond mediator if required. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"main\"> <in> <switch source=\"//m0:getQuote/m0:request/m0:symbol\" xmlns:m0=\"http://services.samples\"> <case regex=\"WSO2\"> <property name=\"symbol\" value=\"Great stock - WSO2\"/> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </case> <case regex=\"CRF\"> <respond/> <property name=\"symbol\" value=\"Are you sure? - CRF\"/> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </case> <default> <property name=\"symbol\" expression=\"fn:concat('Normal Stock - ', //m0:getQuote/m0:request/m0:symbol)\" xmlns:m0=\"http://services.samples\"/> </default> </switch> </in> <out> <send/> </out> </sequence> </definitions>","title":"Example"},{"location":"references/rule-Mediator/","text":"Rule Mediator \u00b6 The Rule Mediator integrates the WSO2 Rules component to the the ESB profile in order to define dynamic integration decisions in terms of rules. The Rule mediator uses an XML message as an input and produces a processed XML message after applying a set of rules. The result xml message can be used as the new soap body message. Alternatively, the information in the processed XML message can be used route the message or do any further processing. or the information can be used to route the message. The use the information to route the message or do any other processing. Info The Rule mediator is a content aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <rule> <ruleset> <source [ key=\"xs:string\" ]> [ in-Lined ] </source> <creation> <property name=\"xs:string\" value=\"xs:string\"/>* </creation> </ruleset> <session type=\"[stateless|stateful]\"/>* <facts> <fact name=\"xs:string\" type=\"xs:string\" expression=\"xs:string\" value=\"xs:string\"/>+ </facts> <results> <result name=\"xs:string\" type=\"xs:string\" expression=\"xs:string\" value=\"xs:string\"/>* </results> [ <childMediators> <mediator/>* </childMediators> ] </rule> Configuration \u00b6 The parameters available to configure the Rule mediator are categorised into the following main elements. Source \u00b6 This section is used to enter the source from which the XML input for the mediator should be taken. The parameters available in this section are as follows. Parameter Name Description Value This parameter can be used to enter a static value as the source. XPath This parameter enables you to use an XPath expression to obtain the input facts of the message from a SOAP body, a SOAP header or a property. This XPath expression can be used to specify the message path for the relevant fact type even when creating fact objects. Target \u00b6 This section is used to enter details of the destination to which the result of the mediation should be added. The parameters available in this section are as follows. Parameter Name Description Value This parameter is used to enter a static value to specify the location go which the resulting message should be added. Result Xpath This parameter is used to derive the location to which the resulting message should be added via an Xpath expression. Xpath This parameter is used to enter a Xpath expression to specify a part of the generated result XML to be added to the target. Action This parameter is used to specify whether the result XML should replace the target, or whether it should be added as a child or a sibling. Rule Set \u00b6 The rule set contains the rules that apply to the input and output facts based on which WSO2 BRS can create the web service WSDL. input facts are the facts that are sent by the rule service client and the output facts are the facts which are received by the client. The parameters available in this section are as follows. Parameter Name Description Rule Script As This parameter is used to specify how you want to enter the rule script. Possible options are as follows. In-Lined : If this is selected, the rule script can be added within the mediator configuration. Key : If this is selected, the rule script can be saved in the Registry and accessed via a key. URL : If this is selected, you can refer to a rule script via a URL. !!! info If the rule set is non-XML, you may need to wrap it with a CDATA section inside a XML tag as shown in the example below. <X><![CDATA[ native code]]></X> !!! info Note The key or inline Rule script must be defined. Otherwise, the Rule Mediator configuration will be invalid. Rule Type This parameter is used to specify whether the rule type is Regular or Decision Table . Input Facts \u00b6 Input facts are facts that are sent by the rule service client. This section contains parameters that can be used to configure the input facts. Values should be defined for the following parameters before you add the individual input facts. Parameter Name Description Wrapper Name The name of the wrapping element for all the facts. NameSpace The namespace for the wrapping element. You can click Add a Fact to add an input fact. The parameters available to configure an input fact are as follows. Parameter Name Description Type The fact type. You can select any of the registered fact types . Element Name The element name of the fact in the XML configuration. NameSpace The namespace for the element. XPath This parameter can be used to enter an XPath expression to create an input fact using a part of the input XML. NS Editor Click this link to edit the namespaces. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This parameter can be used to delete and existing fact. Output Facts \u00b6 Output facts are the facts received by the rule service client after the rules in the rule set are applied. Parameter Name Description Wrapper Name The name of the wrapping element for all the output facts. NameSpace The namespace for the wrapping element. You can click Add a Fact to add an output fact. The parameters available to configure an output fact are as follows. Parameter Name Description Type The fact type. You can select any of the registered fact types . Element Name The fact type. You can select any of the registered fact types . NameSpace The namespace of the element. Action This parameter can be used to delete and existing fact. Examples \u00b6 In this example, the rule script is picked from the registry with key rule/sample.xml . There is one fact and it is a string variable. Its value is calculated from the current SOAP message using an expression. The Rule engine uses these facts to decide what rules should be applied. <rule> <ruleset> <source key=\"rule/sample.xml\"/> </ruleset> <facts> <fact name=\"symbol\" type=\"java.lang.String\" expression=\"//m0:getQuote/m0:request/m0:symbol/child::text()\" xmlns:m0=\"http://services.samples\"/> </facts> <childMediators> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> <drop/> </childMediators> </rule>","title":"Rule Mediator"},{"location":"references/rule-Mediator/#rule-mediator","text":"The Rule Mediator integrates the WSO2 Rules component to the the ESB profile in order to define dynamic integration decisions in terms of rules. The Rule mediator uses an XML message as an input and produces a processed XML message after applying a set of rules. The result xml message can be used as the new soap body message. Alternatively, the information in the processed XML message can be used route the message or do any further processing. or the information can be used to route the message. The use the information to route the message or do any other processing. Info The Rule mediator is a content aware mediator. Syntax | Configuration | Examples","title":"Rule Mediator"},{"location":"references/rule-Mediator/#syntax","text":"<rule> <ruleset> <source [ key=\"xs:string\" ]> [ in-Lined ] </source> <creation> <property name=\"xs:string\" value=\"xs:string\"/>* </creation> </ruleset> <session type=\"[stateless|stateful]\"/>* <facts> <fact name=\"xs:string\" type=\"xs:string\" expression=\"xs:string\" value=\"xs:string\"/>+ </facts> <results> <result name=\"xs:string\" type=\"xs:string\" expression=\"xs:string\" value=\"xs:string\"/>* </results> [ <childMediators> <mediator/>* </childMediators> ] </rule>","title":"Syntax"},{"location":"references/rule-Mediator/#configuration","text":"The parameters available to configure the Rule mediator are categorised into the following main elements.","title":"Configuration"},{"location":"references/rule-Mediator/#source","text":"This section is used to enter the source from which the XML input for the mediator should be taken. The parameters available in this section are as follows. Parameter Name Description Value This parameter can be used to enter a static value as the source. XPath This parameter enables you to use an XPath expression to obtain the input facts of the message from a SOAP body, a SOAP header or a property. This XPath expression can be used to specify the message path for the relevant fact type even when creating fact objects.","title":"Source"},{"location":"references/rule-Mediator/#target","text":"This section is used to enter details of the destination to which the result of the mediation should be added. The parameters available in this section are as follows. Parameter Name Description Value This parameter is used to enter a static value to specify the location go which the resulting message should be added. Result Xpath This parameter is used to derive the location to which the resulting message should be added via an Xpath expression. Xpath This parameter is used to enter a Xpath expression to specify a part of the generated result XML to be added to the target. Action This parameter is used to specify whether the result XML should replace the target, or whether it should be added as a child or a sibling.","title":"Target"},{"location":"references/rule-Mediator/#rule-set","text":"The rule set contains the rules that apply to the input and output facts based on which WSO2 BRS can create the web service WSDL. input facts are the facts that are sent by the rule service client and the output facts are the facts which are received by the client. The parameters available in this section are as follows. Parameter Name Description Rule Script As This parameter is used to specify how you want to enter the rule script. Possible options are as follows. In-Lined : If this is selected, the rule script can be added within the mediator configuration. Key : If this is selected, the rule script can be saved in the Registry and accessed via a key. URL : If this is selected, you can refer to a rule script via a URL. !!! info If the rule set is non-XML, you may need to wrap it with a CDATA section inside a XML tag as shown in the example below. <X><![CDATA[ native code]]></X> !!! info Note The key or inline Rule script must be defined. Otherwise, the Rule Mediator configuration will be invalid. Rule Type This parameter is used to specify whether the rule type is Regular or Decision Table .","title":"Rule Set"},{"location":"references/rule-Mediator/#input-facts","text":"Input facts are facts that are sent by the rule service client. This section contains parameters that can be used to configure the input facts. Values should be defined for the following parameters before you add the individual input facts. Parameter Name Description Wrapper Name The name of the wrapping element for all the facts. NameSpace The namespace for the wrapping element. You can click Add a Fact to add an input fact. The parameters available to configure an input fact are as follows. Parameter Name Description Type The fact type. You can select any of the registered fact types . Element Name The element name of the fact in the XML configuration. NameSpace The namespace for the element. XPath This parameter can be used to enter an XPath expression to create an input fact using a part of the input XML. NS Editor Click this link to edit the namespaces. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This parameter can be used to delete and existing fact.","title":"Input Facts"},{"location":"references/rule-Mediator/#output-facts","text":"Output facts are the facts received by the rule service client after the rules in the rule set are applied. Parameter Name Description Wrapper Name The name of the wrapping element for all the output facts. NameSpace The namespace for the wrapping element. You can click Add a Fact to add an output fact. The parameters available to configure an output fact are as follows. Parameter Name Description Type The fact type. You can select any of the registered fact types . Element Name The fact type. You can select any of the registered fact types . NameSpace The namespace of the element. Action This parameter can be used to delete and existing fact.","title":"Output Facts"},{"location":"references/rule-Mediator/#examples","text":"In this example, the rule script is picked from the registry with key rule/sample.xml . There is one fact and it is a string variable. Its value is calculated from the current SOAP message using an expression. The Rule engine uses these facts to decide what rules should be applied. <rule> <ruleset> <source key=\"rule/sample.xml\"/> </ruleset> <facts> <fact name=\"symbol\" type=\"java.lang.String\" expression=\"//m0:getQuote/m0:request/m0:symbol/child::text()\" xmlns:m0=\"http://services.samples\"/> </facts> <childMediators> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> <drop/> </childMediators> </rule>","title":"Examples"},{"location":"references/sOAP-Headers/","text":"SOAP Headers \u00b6 [ To ] [ From ] [ Action ] [ ReplyTo ] [ MessageID ] [ RelatesTo ] [ FaultTo ] SOAP headers provide information about the message, such as the To and From values. You can use the get-property() function in the Property mediator to retrieve these headers. You can also add Custom SOAP headers using either the PayloadFactory mediator or the Script mediator . To \u00b6 Header Name To Possible Values Any URI Description The To header of the message. Example get-property(\"To\") From \u00b6 Header Name From Possible Values Any URI Description The From header of the message. Example get-property(\"From\") Action \u00b6 Header Name Action Possible Values Any URI Description The SOAPAction header of the message. Example get-property(\"Action\") ReplyTo \u00b6 Header Name ReplyTo Possible Values Any URI Description The ReplyTo header of the message. Example \\<header name=\"ReplyTo\" action=\"remove\"/> MessageID \u00b6 Header Name MessageID Possible Values UUID Description The unique message ID of the message. It is not recommended to make alterations to this property of a message. Example get-property(\"MessageID\") RelatesTo \u00b6 Header Name RelatesTo Possible Values UUID Description The unique ID of the request to which the current message is related. It is not recommended to make changes. Example get-property(\"RelatesTo\") FaultTo \u00b6 Header Name FaultTo Possible Values Any URI Description The FaultTo header of the message. Example <header name= \"FaultTo\" value= \"http://localhost:9000\" />","title":"SOAP Headers"},{"location":"references/sOAP-Headers/#soap-headers","text":"[ To ] [ From ] [ Action ] [ ReplyTo ] [ MessageID ] [ RelatesTo ] [ FaultTo ] SOAP headers provide information about the message, such as the To and From values. You can use the get-property() function in the Property mediator to retrieve these headers. You can also add Custom SOAP headers using either the PayloadFactory mediator or the Script mediator .","title":"SOAP Headers"},{"location":"references/sOAP-Headers/#to","text":"Header Name To Possible Values Any URI Description The To header of the message. Example get-property(\"To\")","title":"To"},{"location":"references/sOAP-Headers/#from","text":"Header Name From Possible Values Any URI Description The From header of the message. Example get-property(\"From\")","title":"From"},{"location":"references/sOAP-Headers/#action","text":"Header Name Action Possible Values Any URI Description The SOAPAction header of the message. Example get-property(\"Action\")","title":"Action"},{"location":"references/sOAP-Headers/#replyto","text":"Header Name ReplyTo Possible Values Any URI Description The ReplyTo header of the message. Example \\<header name=\"ReplyTo\" action=\"remove\"/>","title":"ReplyTo"},{"location":"references/sOAP-Headers/#messageid","text":"Header Name MessageID Possible Values UUID Description The unique message ID of the message. It is not recommended to make alterations to this property of a message. Example get-property(\"MessageID\")","title":"MessageID"},{"location":"references/sOAP-Headers/#relatesto","text":"Header Name RelatesTo Possible Values UUID Description The unique ID of the request to which the current message is related. It is not recommended to make changes. Example get-property(\"RelatesTo\")","title":"RelatesTo"},{"location":"references/sOAP-Headers/#faultto","text":"Header Name FaultTo Possible Values Any URI Description The FaultTo header of the message. Example <header name= \"FaultTo\" value= \"http://localhost:9000\" />","title":"FaultTo"},{"location":"references/script-Mediator-with-Nashorn-Support/","text":"Script Mediator with Nashorn Support \u00b6 From WSO2 EI 6.2.0 onwards, the Script Mediator of the ESB profile uses Nashorn to execute JavaScripts in addition to its default Rhino engine. You can perform all Script Mediator functionalities that the Rhino engine provides, with the Nashorn engine as well. Info You need to install JDK 8 version update 112 or later to use Nashorn support, If not, you get the following exception when you call the setPayloadJSON() method: java.lang.ClassCastException: jdk.nashorn.internal.runtime.Undefined cannot be cast to java.lang.String For more information, see the bug that causes this , which is fixed in JDK 8 version update 112 or later. The Rhino engine uses E4X XML objects to handle XML payloads. However, E4X is deprecated in the Nashorn engine. Instead, Nashorn supports the AXIOM XML parser to parse XML streams. You can use the getParsedOMElement(InputStream stream) method to parse an XML stream into an OMElement, and the getXpathResult(String expression) method to retrieve the AXIOMXPath using an Xpath query expression to access and modify it. The XPath equivalents for a few common XML navigation operations are as follows. Operation XPath expression E4X equivalent Select all the children of an element element/* element.* Select all the attributes of element element/@* element.@* Select all the descendants (children, grandchildren, etc.) of an element element//descendent element..descendent Select the parent of an element .. or parent::element element.parent() Select a specific child (e.g. foo:bar ) of an element, where 'foo' is the prefix of a declared namespace xmlns:foo=\"...\" element/foo:bar var foo = new Namespace(...); element.foo::bar Return the full name (including prefixes if available) of an element name(element) element.name() Return the local name of an element local-name(element) element.localName() Return the namespace URI of an element (if available) namespace-uri(element) element.namespace() Return the collection of namespaces. For E4X: returns as an Array of Namespace objects For XPath: returns as a set of Namespaces nodes element/namespace::* element.inScopeNamespaces() Return the processing instructions of the specified children of an element (returns all if omitted) element/processing-instructions(name) element.processingInstructions(name) Return the concatenated text nodes and descendants of an element string(element) stringValue(element); You can create a Script mediator with Nashorn support using one of the following methods. Store the JavaScript program statements in a separate file, and refer it via a Local or Remote Registry entry . Embed the JavaScript program statements inline within the Synapse configuration. You can invoke a function within the corresponding script using the Script Mediator. Hence, you can predefine the Synapse configuration in a script variable named mc , and access it via these functions. The mc variable represents an implementation of the NashornScriptMessageContext.java MessageContext. It is an implementation of the ScriptMessageContext interface, which contains the following methods. You can access these methods within the script (e.g., mc.methodName ). Method name Description If a value is returned or not getParsedOMElement(stream) Converts the input stream of an XML String or document into an OMElement. This is useful to traverse the XML document. Yes getXpathResult(expression) Converts a String, which represents an Xpath expression into an AXIOM Xpath. This is useful to traverse the XML document. Yes addHeader(mustUnderstand, content) Adds a new SOAP header to the message. Content can be XML, String, parsed w3c.dom.Document, or an OMElement returned by the getParsedOMElement(stream) method. No getEnvelopeXML() Retrieves the XML representation of the complete SOAP envelope. Yes getPayloadJSON() Retrieves the JSON representation of a SOAP Body payload. Yes getPayloadXML() Retrieves the XML representation of a SOAP Body payload. This method Returns a OMElement so the user can directly access elements using Xpath. Yes getProperty(name) Retrieves a specified property from the current message context. Yes setPayloadJSON(payload) Sets the JSON representation of a payload obtained via the getPayloadJSON() method in the current message context. No setPayloadXML(payload) Sets the SOAP body payload from the XML. Payload can be XML, String, parsed w3c.dom.Document or an OMElement returned by the getParsedOMElement(stream) method. No setProperty(key, value) Replaces the existing property values a property in the current message context. No The Script Mediator has the flexibility of a class Mediator, with access to the Synapse Message Context and Synapse Environment APIs. For both types of script mediator definitions, the Message Context passed into the script has additional methods over the standard Synapse Message Context, to enable working with natural XML to JavaScript. Info The Script mediator is a content aware mediator. Syntax | Configuration | Examples Syntax \u00b6 Click on the relevant tab to view the syntax for a script mediator using an Inline script, or a script mediator using a script of the Registry. Using an Inline script Using a script in the Registry The following syntax applies when you create a Script mediator with the script program statements embedded inline within the Synapse configuration. <script language=\"nashornJs\"><![CDATA[......Nashorn JavaScript source code\u2026\u2026.. ]]></script> The following syntax applies when you create a Script mediator with the script program statements stored in a separate file, referenced via the Local or Remote Registry entry . This uses a key to refer to a script, which is already saved in the Registry. <script key=\"string\" language=\"nashornJs\" [function=\"script-function-name\"]> <include key=\"string\"/> </script> Configuration \u00b6 Click on the relevant tab to view the required UI configuration depending on the script type you have selected. The available script types are as follows: Inline : If this script type is selected, the script is specified inline. Registry Key : If this script type is selected, a script which is already saved in the registry will be referred using a key. Inline Using a script in the Registry The parameters available to configure a Script mediator using an inline script are as follows. Parameter Name Description Language The scripting language for the Script mediator. You can select from the following available languages. JavaScript - This is represented as js in the source view. Groovy - This is represented as groovy in the source view. Ruby - This is represented as rb in the source view. Source Enter the source in this parameter. The parameters available to configure a Script mediator using a script saved in the registry are as follows. Parameter Name Description Language The scripting language for the Script mediator. You can select from the following available languages. JavaScript - This is represented as js in the source view. Groovy - This is represented as groovy in the source view. Ruby - This is represented as rb in the source view. Function The function of the selected script language to be invoked. This is an optional parameter. If no value is specified, a default function named mediate will be applied. This function considers the Synapse MessageContext as a single parameter. The function may return a boolean. If it does not, then the value true is assumed and the Script mediator returns this value. Key Type You can select one of the following options. Static Key : If this is selected, an existing key can be selected from the registry for the Key parameter. Dynamic Key : If this is selected, the key can be entered dynamically in the Key parameter. Key The Registry location of the source. You can click either Configuration Registry or the Governance Registry to select the source from the resource tree. Include keys This parameter allows you to include functions defined in two or more scripts your Script mediator configuration. After pointing to one script in the Key parameter, you can click Add Include Key to add the function in another script. When you click Add Include Key , the following parameters will be displayed. Enter the script to be included in the Key parameter by clicking either Configuration Registry or the Governance Registry and then selecting the relevant script from the resource tree. Examples \u00b6 Example 1 - Using an inline script Example 2 - Using a script saved in the registry Examples for methods Assume the below SOAP envelope for the following examples: <?xml version='1.0' encoding='utf-8'?> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header> <AuthenticationInfo xmlns=\"http://www.x\" soapenv:mustUnderstand=\"0\"> <userName>\u201dwso2\u201d</userName> <password>\u201dwso2\u201d</password> </AuthenticationInfo> </soapenv:Header> <soapenv:Body> <company> <staff id=\"1001\"> <firstname>\u201dyong\u201d</firstname> <lastname>\u201dmook kim\u201d</lastname> <nickname>\u201dmkyong\u201d</nickname> <salary>100000</salary> </staff> <staff id=\"2001\"> <firstname> \u201clow\u201d</firstname> <lastname>\u201dyin fong\u201d</lastname> <nickname>\u201dfong fong\u201d</nickname> <salary>200000</salary> </staff> </company> </soapenv:Body> </soapenv:Envelope> Example 1 - Using an inline script \u00b6 The following configuration is an example of using a Nashorn JavaScript script inline in the Script Mediator. In this script, you get the message payload in XML and you use an Xpath query to access the elements of it. <script language=\"nashornJs\"><![CDATA[var symbol = mc.getPayloadXML(); var expression = \"//firstname\"; var xpath = mc.getXpathResult(expression); var c1l = xpath.selectNodes(symbol); var first_name = c1l.get(0).getText(); ]]></script> Example 2 - Using a script saved in the registry \u00b6 In the following example, the script is loaded from the Registry using the key repository/conf/sample/resources/script/test.js . <script language=\"nashornJs\" key=\"repository/conf/sample/resources/script/test.js\" function=\"testFunction\"/> The script language=\"nashornJs\" property indicates that the invoked function should be in the Nashorn JavaScript language. You need to save the function named testFunction , which is invoked in the above example as a resource in the Registry. Following is an example for the script of the function. function testFunction(mc) { var symbol = mc.getPayloadXML(); var expression = \"//firstname\"; var xpath = mc.getXpathResult(expression); var c1l = xpath.selectNodes(symbol); var first_name = c1l.get(0).getText(); } Examples for methods \u00b6 The following Script Mediator configuration examples show how you can include some of the commonly used methods in the invoked script. Methods SOAP envelope Sample Script Mediator configuration setPayloadXML(payload) getPayloadXML(mustUnderstand,content) addHeader() getEnvelopeXML() getXpathResult(expression) getParsedOMElement(stream) <?xml version='1.0' encoding='utf-8' ?> <soapenv:Envelope xmlns:soapenv= \"http://schemas.xmlsoap.org/soap/envelope/\" > <soapenv:Header> <AuthenticationInfo xmlns= \"http://www.x\" soapenv:mustUnderstand= \"0\" > <userName> \u201dwso2\u201d </userName> <password> \u201dwso2\u201d </password> </AuthenticationInfo> </soapenv:Header> <soapenv:Body> <company> <staff id= \"1001\" > <firstname> \u201dyong\u201d </firstname> <lastname> \u201dmook kim\u201d </lastname> <nickname> \u201dmkyong\u201d </nickname> <salary> 100000 </salary> </staff> <staff id= \"2001\" > <firstname> \u201clow\u201d </firstname> <lastname> \u201dyin fong\u201d </lastname> <nickname> \u201dfong fong\u201d </nickname> <salary> 200000 </salary> </staff> </company> </soapenv:Body> </soapenv:Envelope> <script language= \"nashornJs\" > <![CDATA[ var symbol = mc.getPayloadXML(); var expression = \"//firstname\"; var xpath = mc.getXpathResult(expression); var nameList = xpath.selectNodes(symbol); var name = nameList.get(0).getText(); mc.setPayloadXML(symbol); //here we are setting payload by a parsed xml document or this coud be done by passing xml string as well var password = \"wso2\"; var username = \"wso2\"; var headerXml = \"<AuthenticationInfo xmlns=\\\"http://www\" + \".w3.org/1999/xhtml\\\"><userName>\" + username + \"</userName><password>\"+ password + \"</password\" + \"></AuthenticationInfo>\"; mc.addHeader(false, headerXml ); var envelope = mc.getEnvelopeXML(); expression = \"//nickname\"; stream = new byteArrayStream(envelope.getBytes()); var docEnvelope = mc.getParsedOMElement(stream); xpath = mc.getXpathResult(expression); var nickNameList = xpath.selectNodes(docEnvelope); var nickname = nickNameList.get(0).getText(); ]]> </script> setProperty(\"key\", value); getPayloadJSON() setPayloadJSON(payload) <?xml version='1.0' encoding='utf-8' ?> <soapenv:Envelope xmlns:soapenv= \"http://schemas.xmlsoap.org/soap/envelope/\" > <soapenv:Body> <jsonObject> <appointmentNo> 55 </appointmentNo> <doctorName> thomascollins </doctorName> <patient> JohnDoe </patient> <actualFee> 7000.0 </actualFee> <discount> 0 </discount> <discounted> 7000.0 </discounted> <paymentID> c3b7cab3-8e22-4319-a43f-b1bef3de2693 </paymentID> <status> Settled </status> </jsonObject> </soapenv:Body> </soapenv:Envelope> <script language= \"nashornJs\" ><![CDATA[ var doctor = { name : \"x\" , fee : 200 }; mc. setProperty ( \"scriptObject\" , doctor); var payload = mc. getPayloadJSON (); var results = payload. actualFee ; doctor. fee = results; mc. setPayloadJSON (doctor); ]]></script>","title":"Script Mediator with Nashorn Support"},{"location":"references/script-Mediator-with-Nashorn-Support/#script-mediator-with-nashorn-support","text":"From WSO2 EI 6.2.0 onwards, the Script Mediator of the ESB profile uses Nashorn to execute JavaScripts in addition to its default Rhino engine. You can perform all Script Mediator functionalities that the Rhino engine provides, with the Nashorn engine as well. Info You need to install JDK 8 version update 112 or later to use Nashorn support, If not, you get the following exception when you call the setPayloadJSON() method: java.lang.ClassCastException: jdk.nashorn.internal.runtime.Undefined cannot be cast to java.lang.String For more information, see the bug that causes this , which is fixed in JDK 8 version update 112 or later. The Rhino engine uses E4X XML objects to handle XML payloads. However, E4X is deprecated in the Nashorn engine. Instead, Nashorn supports the AXIOM XML parser to parse XML streams. You can use the getParsedOMElement(InputStream stream) method to parse an XML stream into an OMElement, and the getXpathResult(String expression) method to retrieve the AXIOMXPath using an Xpath query expression to access and modify it. The XPath equivalents for a few common XML navigation operations are as follows. Operation XPath expression E4X equivalent Select all the children of an element element/* element.* Select all the attributes of element element/@* element.@* Select all the descendants (children, grandchildren, etc.) of an element element//descendent element..descendent Select the parent of an element .. or parent::element element.parent() Select a specific child (e.g. foo:bar ) of an element, where 'foo' is the prefix of a declared namespace xmlns:foo=\"...\" element/foo:bar var foo = new Namespace(...); element.foo::bar Return the full name (including prefixes if available) of an element name(element) element.name() Return the local name of an element local-name(element) element.localName() Return the namespace URI of an element (if available) namespace-uri(element) element.namespace() Return the collection of namespaces. For E4X: returns as an Array of Namespace objects For XPath: returns as a set of Namespaces nodes element/namespace::* element.inScopeNamespaces() Return the processing instructions of the specified children of an element (returns all if omitted) element/processing-instructions(name) element.processingInstructions(name) Return the concatenated text nodes and descendants of an element string(element) stringValue(element); You can create a Script mediator with Nashorn support using one of the following methods. Store the JavaScript program statements in a separate file, and refer it via a Local or Remote Registry entry . Embed the JavaScript program statements inline within the Synapse configuration. You can invoke a function within the corresponding script using the Script Mediator. Hence, you can predefine the Synapse configuration in a script variable named mc , and access it via these functions. The mc variable represents an implementation of the NashornScriptMessageContext.java MessageContext. It is an implementation of the ScriptMessageContext interface, which contains the following methods. You can access these methods within the script (e.g., mc.methodName ). Method name Description If a value is returned or not getParsedOMElement(stream) Converts the input stream of an XML String or document into an OMElement. This is useful to traverse the XML document. Yes getXpathResult(expression) Converts a String, which represents an Xpath expression into an AXIOM Xpath. This is useful to traverse the XML document. Yes addHeader(mustUnderstand, content) Adds a new SOAP header to the message. Content can be XML, String, parsed w3c.dom.Document, or an OMElement returned by the getParsedOMElement(stream) method. No getEnvelopeXML() Retrieves the XML representation of the complete SOAP envelope. Yes getPayloadJSON() Retrieves the JSON representation of a SOAP Body payload. Yes getPayloadXML() Retrieves the XML representation of a SOAP Body payload. This method Returns a OMElement so the user can directly access elements using Xpath. Yes getProperty(name) Retrieves a specified property from the current message context. Yes setPayloadJSON(payload) Sets the JSON representation of a payload obtained via the getPayloadJSON() method in the current message context. No setPayloadXML(payload) Sets the SOAP body payload from the XML. Payload can be XML, String, parsed w3c.dom.Document or an OMElement returned by the getParsedOMElement(stream) method. No setProperty(key, value) Replaces the existing property values a property in the current message context. No The Script Mediator has the flexibility of a class Mediator, with access to the Synapse Message Context and Synapse Environment APIs. For both types of script mediator definitions, the Message Context passed into the script has additional methods over the standard Synapse Message Context, to enable working with natural XML to JavaScript. Info The Script mediator is a content aware mediator. Syntax | Configuration | Examples","title":"Script Mediator with Nashorn Support"},{"location":"references/script-Mediator-with-Nashorn-Support/#syntax","text":"Click on the relevant tab to view the syntax for a script mediator using an Inline script, or a script mediator using a script of the Registry. Using an Inline script Using a script in the Registry The following syntax applies when you create a Script mediator with the script program statements embedded inline within the Synapse configuration. <script language=\"nashornJs\"><![CDATA[......Nashorn JavaScript source code\u2026\u2026.. ]]></script> The following syntax applies when you create a Script mediator with the script program statements stored in a separate file, referenced via the Local or Remote Registry entry . This uses a key to refer to a script, which is already saved in the Registry. <script key=\"string\" language=\"nashornJs\" [function=\"script-function-name\"]> <include key=\"string\"/> </script>","title":"Syntax"},{"location":"references/script-Mediator-with-Nashorn-Support/#configuration","text":"Click on the relevant tab to view the required UI configuration depending on the script type you have selected. The available script types are as follows: Inline : If this script type is selected, the script is specified inline. Registry Key : If this script type is selected, a script which is already saved in the registry will be referred using a key. Inline Using a script in the Registry The parameters available to configure a Script mediator using an inline script are as follows. Parameter Name Description Language The scripting language for the Script mediator. You can select from the following available languages. JavaScript - This is represented as js in the source view. Groovy - This is represented as groovy in the source view. Ruby - This is represented as rb in the source view. Source Enter the source in this parameter. The parameters available to configure a Script mediator using a script saved in the registry are as follows. Parameter Name Description Language The scripting language for the Script mediator. You can select from the following available languages. JavaScript - This is represented as js in the source view. Groovy - This is represented as groovy in the source view. Ruby - This is represented as rb in the source view. Function The function of the selected script language to be invoked. This is an optional parameter. If no value is specified, a default function named mediate will be applied. This function considers the Synapse MessageContext as a single parameter. The function may return a boolean. If it does not, then the value true is assumed and the Script mediator returns this value. Key Type You can select one of the following options. Static Key : If this is selected, an existing key can be selected from the registry for the Key parameter. Dynamic Key : If this is selected, the key can be entered dynamically in the Key parameter. Key The Registry location of the source. You can click either Configuration Registry or the Governance Registry to select the source from the resource tree. Include keys This parameter allows you to include functions defined in two or more scripts your Script mediator configuration. After pointing to one script in the Key parameter, you can click Add Include Key to add the function in another script. When you click Add Include Key , the following parameters will be displayed. Enter the script to be included in the Key parameter by clicking either Configuration Registry or the Governance Registry and then selecting the relevant script from the resource tree.","title":"Configuration"},{"location":"references/script-Mediator-with-Nashorn-Support/#examples","text":"Example 1 - Using an inline script Example 2 - Using a script saved in the registry Examples for methods Assume the below SOAP envelope for the following examples: <?xml version='1.0' encoding='utf-8'?> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header> <AuthenticationInfo xmlns=\"http://www.x\" soapenv:mustUnderstand=\"0\"> <userName>\u201dwso2\u201d</userName> <password>\u201dwso2\u201d</password> </AuthenticationInfo> </soapenv:Header> <soapenv:Body> <company> <staff id=\"1001\"> <firstname>\u201dyong\u201d</firstname> <lastname>\u201dmook kim\u201d</lastname> <nickname>\u201dmkyong\u201d</nickname> <salary>100000</salary> </staff> <staff id=\"2001\"> <firstname> \u201clow\u201d</firstname> <lastname>\u201dyin fong\u201d</lastname> <nickname>\u201dfong fong\u201d</nickname> <salary>200000</salary> </staff> </company> </soapenv:Body> </soapenv:Envelope>","title":"Examples"},{"location":"references/script-Mediator-with-Nashorn-Support/#example-1-using-an-inline-script","text":"The following configuration is an example of using a Nashorn JavaScript script inline in the Script Mediator. In this script, you get the message payload in XML and you use an Xpath query to access the elements of it. <script language=\"nashornJs\"><![CDATA[var symbol = mc.getPayloadXML(); var expression = \"//firstname\"; var xpath = mc.getXpathResult(expression); var c1l = xpath.selectNodes(symbol); var first_name = c1l.get(0).getText(); ]]></script>","title":"Example 1 - Using an inline script"},{"location":"references/script-Mediator-with-Nashorn-Support/#example-2-using-a-script-saved-in-the-registry","text":"In the following example, the script is loaded from the Registry using the key repository/conf/sample/resources/script/test.js . <script language=\"nashornJs\" key=\"repository/conf/sample/resources/script/test.js\" function=\"testFunction\"/> The script language=\"nashornJs\" property indicates that the invoked function should be in the Nashorn JavaScript language. You need to save the function named testFunction , which is invoked in the above example as a resource in the Registry. Following is an example for the script of the function. function testFunction(mc) { var symbol = mc.getPayloadXML(); var expression = \"//firstname\"; var xpath = mc.getXpathResult(expression); var c1l = xpath.selectNodes(symbol); var first_name = c1l.get(0).getText(); }","title":"Example 2 - Using a script saved in the registry"},{"location":"references/script-Mediator-with-Nashorn-Support/#examples-for-methods","text":"The following Script Mediator configuration examples show how you can include some of the commonly used methods in the invoked script. Methods SOAP envelope Sample Script Mediator configuration setPayloadXML(payload) getPayloadXML(mustUnderstand,content) addHeader() getEnvelopeXML() getXpathResult(expression) getParsedOMElement(stream) <?xml version='1.0' encoding='utf-8' ?> <soapenv:Envelope xmlns:soapenv= \"http://schemas.xmlsoap.org/soap/envelope/\" > <soapenv:Header> <AuthenticationInfo xmlns= \"http://www.x\" soapenv:mustUnderstand= \"0\" > <userName> \u201dwso2\u201d </userName> <password> \u201dwso2\u201d </password> </AuthenticationInfo> </soapenv:Header> <soapenv:Body> <company> <staff id= \"1001\" > <firstname> \u201dyong\u201d </firstname> <lastname> \u201dmook kim\u201d </lastname> <nickname> \u201dmkyong\u201d </nickname> <salary> 100000 </salary> </staff> <staff id= \"2001\" > <firstname> \u201clow\u201d </firstname> <lastname> \u201dyin fong\u201d </lastname> <nickname> \u201dfong fong\u201d </nickname> <salary> 200000 </salary> </staff> </company> </soapenv:Body> </soapenv:Envelope> <script language= \"nashornJs\" > <![CDATA[ var symbol = mc.getPayloadXML(); var expression = \"//firstname\"; var xpath = mc.getXpathResult(expression); var nameList = xpath.selectNodes(symbol); var name = nameList.get(0).getText(); mc.setPayloadXML(symbol); //here we are setting payload by a parsed xml document or this coud be done by passing xml string as well var password = \"wso2\"; var username = \"wso2\"; var headerXml = \"<AuthenticationInfo xmlns=\\\"http://www\" + \".w3.org/1999/xhtml\\\"><userName>\" + username + \"</userName><password>\"+ password + \"</password\" + \"></AuthenticationInfo>\"; mc.addHeader(false, headerXml ); var envelope = mc.getEnvelopeXML(); expression = \"//nickname\"; stream = new byteArrayStream(envelope.getBytes()); var docEnvelope = mc.getParsedOMElement(stream); xpath = mc.getXpathResult(expression); var nickNameList = xpath.selectNodes(docEnvelope); var nickname = nickNameList.get(0).getText(); ]]> </script> setProperty(\"key\", value); getPayloadJSON() setPayloadJSON(payload) <?xml version='1.0' encoding='utf-8' ?> <soapenv:Envelope xmlns:soapenv= \"http://schemas.xmlsoap.org/soap/envelope/\" > <soapenv:Body> <jsonObject> <appointmentNo> 55 </appointmentNo> <doctorName> thomascollins </doctorName> <patient> JohnDoe </patient> <actualFee> 7000.0 </actualFee> <discount> 0 </discount> <discounted> 7000.0 </discounted> <paymentID> c3b7cab3-8e22-4319-a43f-b1bef3de2693 </paymentID> <status> Settled </status> </jsonObject> </soapenv:Body> </soapenv:Envelope> <script language= \"nashornJs\" ><![CDATA[ var doctor = { name : \"x\" , fee : 200 }; mc. setProperty ( \"scriptObject\" , doctor); var payload = mc. getPayloadJSON (); var results = payload. actualFee ; doctor. fee = results; mc. setPayloadJSON (doctor); ]]></script>","title":"Examples for methods"},{"location":"references/script-Mediator/","text":"Script Mediator \u00b6 The Script Mediator is used to invoke the functions of a variety of scripting languages such as JavaScript, Groovy, or Ruby. Note The ESB profile of WSO2 EI uses Rhino engine to execute JavaScripts. Rhino engine converts the script to a method inside a Java class. Therefore, when processing large JSON data volumes, the code length must be less than 65536 characters, since the Script mediator converts the payload into a Java object. However, you can use the following alternative options to process large JSON data volumes. Achieve the same functionality via a Class mediator . If the original message consists of repetitive sections, you can use the Iterate mediator to generate a relatively small payload using those repetitive sections. This will then allow you to use the Script mediator. From WSO2 EI 6.2.0 onwards, the Script Mediator supports using Nashorn to execute JavaScripts, in addition to its default Rhino engine. For more information, see Script Mediator with Nashorn Support . A Script mediator can be created in one of the following methods. With the script program statements stored in a separate file, referenced via the Local or Remote Registry entry . With the script program statements embedded inline within the Synapse configuration. Synapse uses the Apache Bean Scripting Framework for scripting language support. Any script language supported by BSF may be used to implement the Synapse Mediator. With the Script Mediator, you can invoke a function in the corresponding script. With these functions, it is possible to access the Synapse predefined in a script variable named mc . The mc variable represents an implementation of the MessageContext , named ScriptMessageContext.java , which contains the following methods that can be accessed within the script as mc.methodName . Return? Method Name Description Yes getPayloadXML() This gets an XML representation of SOAP Body payload. No setPayloadXML(payload) This sets the SOAP body payload from XML. No addHeader(mustUnderstand, content) This adds a new SOAP header to the message. Yes getEnvelopeXML() This gets the XML representation of the complete SOAP envelope. No setTo(reference) This is used to set the value which specifies the receiver of the message. Yes setFaultTo(reference) This is used to set the value which specifies the receiver of the faults relating to the message. No setFrom(reference) This is used to set the value which specifies the sender of the message. No setReplyTo(reference) This is used to set the value which specifies the receiver of the replies to the message. Yes getPayloadJSON() This gets the JSON representation of a SOAP Body payload. No setPayloadJSON( payload ) This sets the JSON representation of a payload obtained via the getPayloadJSON() method and sets it in the current message context. Yes getProperty(name) This gets a property from the current message context. No setProperty(key, value) This is used to set a property in the current message context. The previously set property values are replaced by this method. Implementing a Mediator with a script language has advantages over using the built-in Synapse Mediator types or implementing a custom Java class Mediator. The Script Mediators have the flexibility of a class Mediator with access to the Synapse MessageContext and SynapseEnvironment APIs. Also, the ease of use and dynamic nature of scripting languages allow the rapid development and prototyping of custom mediators. An additional benefit of some scripting languages is that they have very simple and elegant XML manipulation capabilities, which make them very usable in a Synapse mediation environment. e.g., JavaScript E4X or Ruby REXML. For both types of script mediator definitions, the MessageContext passed into the script has additional methods over the standard Synapse MessageContext to enable working with XML natural to the scripting language. Example are when using JavaScript getPayloadXML and setPayloadXML , E4X XML objects and when using Ruby, REXML documents. Info The Script mediator is a content-aware mediator. Prerequisites | Syntax | Configuration | Examples Prerequisites \u00b6 Listed below are the prerequisites for writing a Script mediator using JavaScript, Groovy, or Ruby. Scripting Language Prerequisite Groovy Download the groovy-all -2.4.4.jar file and copy it to the <EI_HOME>/ dropins directory. Note that when you define the script , you need to start by importing Groovy. Ruby Install the JRuby engine for mediation. This is available in the WSO2 P2 repository as a feature ( WSO2 Carbon - JRuby Engine for Mediation ). See the instructions on how to install features in WSO2 EI . Alternatively, you can download and install the JRuby engine manually: Download the jruby-complete-1.3.0.wso2v1.jar file from the WSO2 P2 repository and copy it to the <EI_HOME>/ dropins directory. JavaScript The JavaScript/E4X support is enabled by default in the WSO2 Enterprise Integrator distribution and ready for use. Syntax \u00b6 Click on the relevant tab to view the syntax for a script mediator using an Inline script, or a script mediator using a script of a registry Using an Inline script Using a script of the registry The following syntax applies when you create a Script mediator with the script program statements embedded inline within the Synapse configuration. <script language=\"js\"><![CDATA[...script source code...]]></script> The following syntax applies when you create a Script mediator with the script program statements stored in a separate file, referenced via the Local or Remote Registry entry . Info If you are creating the Registry Resource via Tooling, y ou need not specify the content/media type, because it gets automatically applied when you select the JavaScript File Template as shown below. <script key=\"string\" language=\"js\" [function=\"script-function-name\"]> <include key=\"string\"/> </script> Configuration \u00b6 Click on the relevant tab to view the required UI configuration depending on the script type you have selected. The available script types are as follows: Inline : If this script type is selected, the script is specified inline. Registry : If this script type is selected, a script which is already saved in the registry will be referred using a key. Inline Using a script of the registry The parameters available to configure a Script mediator using an inline script are as follows. Parameter Name Description Language The scripting language for the Script mediator. You can select from the following available languages. JavaScript - This is represented as js in the source view. Groovy - This is represented as groovy in the source view. Ruby - This is represented as rb in the source view. Source Enter the source in this parameter. Note: If you are using Groovy as the scripting language, you need to first import Groovy in your script by adding the following: import groovy.json.*; The parameters available to configure a Script mediator using a script saved in the registry are as follows. Parameter Name Description Language The scripting language for the Script mediator. You can select from the following available languages. JavaScript - This is represented as js in the source view. Groovy - This is represented as groovy in the source view. Note: Be sure that your script starts with the following, which indicates that Groovy is imported: import groovy.json.*; Ruby - This is represented as rb in the source view. Function The function of the selected script language to be invoked. This is an optional parameter. If no value is specified, a default function named mediate will be applied. This function considers the Synapse MessageContext as a single parameter. The function may return a boolean. If it does not, then the value true is assumed and the Script mediator returns this value. Key Type You can select one of the following options. Static Key : If this is selected, an existing key can be selected from the registry for the Key parameter. Dynamic Key : If this is selected, the key can be entered dynamically in the Key parameter. Key The Registry location of the source. You can click either Configuration Registry or the Governance Registry to select the source from the resource tree. Include keys This parameter allows you to include functions defined in two or more scripts your Script mediator configuration. After pointing to one script in the Key parameter, you can click Add Include Key to add the function in another script. When you click Add Include Key , the following parameters will be displayed. Enter the script to be included in the Key parameter by clicking either Configuration Registry or the Governance Registry and then selecting the relevant script from the resource tree. Examples \u00b6 Example 1 - Using an inline script Example 2 - Using a script saved in the registry Example 3 - Adding an Include key Example 4 - Adding a custom SOAP header Example per method Samples Example 1 - Using an inline script \u00b6 The following configuration is an example of an inline mediator using JavaScript/E4X which returns false if the SOAP message body contains an element named symbol , which has a value of IBM . <script language=\"js\"><![CDATA[mc.getPayloadXML()..symbol != \"IBM\";]]></script> Example 2 - Using a script saved in the registry \u00b6 In the following example, script is loaded from the registry by using the key repository/conf/sample/resources/script/test.js . <script language=\"js\" key=\"repository/conf/sample/resources/script/test.js\" function=\"testFunction\"/> script language=\"js\" indicates that the function invoked should be in the JavaScript language. The function named testFunction which is invoked should be saved as a resource in the Registry . The script can be as shown in the example below. function testFunction(mc) { var symbol = mc.getPayloadXML()..*::Code.toString(); mc.setPayloadXML( <m:getQuote xmlns:m=\"http://services.samples/xsd\"> <m:request> <m:symbol>{symbol}</m:symbol> </m:request> </m:getQuote>); } Example 3 - Adding an Include key \u00b6 The following configuration has an include key . <script language=\"js\" key=\"stockquoteScript\" function=\"transformRequest\"> <include key=\"sampleScript\"/> </script> The script is written in JavaScript. The function to be executed is transformRequest . This function may be as follows in a script saved in the Registry . // stockquoteTransform.js function transformRequest(mc) { transformRequestFunction(mc); } function transformResponse(mc) { transformResponseFunction(mc); } In addition, the function in the script named sampleScript which is included in the mediation configuration via the include key sub element is also executed in the mediation. Note that in order to do this, sampleScript script should also be saved as a resource in the Registry . This script can be as follows. // sample.js function transformRequestFunction(mc) { var symbol = mc.getPayloadXML()..*::Code.toString(); mc.setPayloadXML( <m:getquote m=\"http://services.samples\"> <m:request> <m:symbol>{symbol}</m:symbol> </m:request> </m:getquote>); } function transformResponse(mc) { var symbol = mc.getPayloadXML()..*::symbol.toString(); var price = mc.getPayloadXML()..*::last.toString(); mc.setPayloadXML( <m:checkpriceresponse m=\"http://services.samples/xsd\"> <m:code>{symbol}</m:code> <m:price>{price}</m:price> </m:checkpriceresponse>); } Example 4 - Adding a custom SOAP header \u00b6 You can add custom SOAP headers to a request by using the addHeader(mustUnderstand, content) of the Script Mediator in a proxy service as shown in the example below. <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"CustomSOAPHeaderProxy\" startOnLoad=\"true\" statistics=\"disable\" trace=\"disable\" transports=\"http,https\"> <target> <inSequence> <log level=\"full\"> <property name=\"Message\" value=\"IncomingRequest\"/> </log> <script language=\"js\">mc.addHeader(false, &lt;ns:sampleCustomHeader xmlns:ns=\"gsb:http://wso2.org/sample\"&gt;&lt;ns:customInfo&gt;CustomHeader&lt;/ns:customInfo&gt;&lt;/ns:sampleCustomHeader&gt;);</script> <log level=\"full\"> <property name=\"Message\" value=\"UpdatedMessage\"/> </log> <drop/> </inSequence> </target> <description/> </proxy> Example per method \u00b6 The following table contains examples of how some of the commonly used methods can be included in the script invoked by the following sample Script mediator configuration. <script language=\"js\" key=\"conf:/repository/EI/transform.js\" function=\"transform\"/> Return? Method Name Example Yes getPayloadXML() The script invoked can be as follows. // sample.js02.function transformRequestFunction(mc) { var symbol = mc . getPayloadXML ().. *:: Code . toString () ; mc . setPayloadXML ( < m : getquote m = \"http://services.samples\" > < m : request > < m : symbol >{ symbol } </m : symbol > </m : request > </m : getquote > ) ; } mc.getPayloadXML() returns the response received in XML form. No setPayloadXML(payload) See the example above for the getPayloadXML() method. mc.setPayloadXML( <m:getquote m=\"http://services.samples\"> <m:request> <m:symbol>{symbol}</m:symbol </m:request> </m:getquote> ) is used in that script to set the XML representation of the SOAP body (obtained using the getPayloadXML() method) to the current message context. No addHeader(mustUnderstand, Object content) The script invoked can be as follows. < script language = \"js\" > <! [CDATA[ var wsse = new Namespace ( 'http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd' ) ; var envelope = mc . getEnvelopeXML () ; var username = envelope .. wsse :: Username . toString () ; var password = envelope .. wsse :: Password . toString () ; mc . addHeader ( false , < urn : AuthenticationInfo >< urn : userName >{ username } </urn : userName >< urn : password >{ password } </urn : password > </urn : AuthenticationInfo > ) ; ]] > </script > The addHeader method configured as mc.addHeader(false, <urn:AuthenticationInfo><urn:userName>{username}</urn:userName><urn:password>{password}</urn:password></urn:AuthenticationInfo>) in the above script is used to extract user name and password values included in the request and add them to the header structure required for the backend service. No getEnvelopeXML() The script invoked can be as follows. < script language = \"js\" > <! [CDATA[ var wsse = new Namespace ( 'http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd' ) ; var envelope = mc . getEnvelopeXML () ; var username = envelope .. wsse :: Username . toString () ; var password = envelope .. wsse :: Password . toString () ; mc . addHeader ( false , < urn : AuthenticationInfo >< urn : userName >{ username } </urn : userName >< urn : password >{ password } </urn : password > </urn : AuthenticationInfo > ) ; ]] > </script > See more at: http://sajithblogs.blogspot.com/2013/08/wso2-EI-adding-complex-soap-headers-to.html#sthash.jqpiEmf0.dpuf Yes getPayloadJSON() The script invoked can be as follows. function transform (mc) { payload = mc . getPayloadJSON () ; results = payload . results ; var response = new Array () ; for (i = 0 ; i < results . length ; ++ i) { location_object = results[i] ; l = new Object () ; l . name = location_object . name ; l . tags = location_object . types ; l . id = \"ID:\" + ( location_object . id ) ; response[i] = l ; } mc . setPayloadJSON (response) ; } mc.getPayloadJSON() returns the JSON payload (received as the response) as a JavaScript object. This object can be manipulated as a normal JavaScript variable within a script as shown in the above JavaScript code. See JSON Support for further information about how this script is used. No setPayloadJSON(payload) See the example script for the getPayloadJSON() method. The mc.setPayloadJSON() method can be used to replace the existing payload with a new payload. In the above script, we build a new array object by using the fields of the incoming JSON payload and set that array object as the new payload. See JSON Support for further information about how this script is used Yes getProperty (name) The script invoked can be as follows. <script language= \"js\" > <![CDATA[ var time1 = mc. getProperty ( \"TIME_1\" ); var time2 = mc. getProperty ( \"TIME_2\" ); var timeTaken = time2 - time1; print ( \"Time Duration : \" + timeTaken + \" ms \" ); mc. setProperty ( \"RESPONSE_TIME\" , timeTaken); ]]> </script> In this example, the getProperty method is used to get two time durations. The difference between the two time durations is calculated and the setProperty method is used to set this difference in the message context. No setProperty(property) See the example for the getProperty method. The setProperty method is used to set the response time calculated from the time durations obtained (using the getProperty method) in the message context. !!! note In the ESB profile due to a Rhino engine upgrade, when strings are concatenated and set as a property in the message context, you need to use the toString() method to convert the result to a string. In the following example, var result = \"a\" and then result = result + \"b\" . When concatenating these strings, the script invoked needs to be as follows: <script language= \"js\" > <![CDATA[ {var result = \"a\"; result = result + \"b\"; mc.setProperty('result_str', result.toString()); } ]]> </script> Samples \u00b6 The following samples demonstrate how to use the Script mediator. Sample 350: Introduction to the Script Mediator Using JavaScript Sample 351: Inline script mediation with JavaScript Sample 352: Accessing Synapse message context API using a scripting language Sample 353: Using Ruby Scripts for Mediation Sample 354: Using Inline Ruby Scripts for Mediation See also Sample 441: Converting JSON to XML Using JavaScript","title":"Script Mediator"},{"location":"references/script-Mediator/#script-mediator","text":"The Script Mediator is used to invoke the functions of a variety of scripting languages such as JavaScript, Groovy, or Ruby. Note The ESB profile of WSO2 EI uses Rhino engine to execute JavaScripts. Rhino engine converts the script to a method inside a Java class. Therefore, when processing large JSON data volumes, the code length must be less than 65536 characters, since the Script mediator converts the payload into a Java object. However, you can use the following alternative options to process large JSON data volumes. Achieve the same functionality via a Class mediator . If the original message consists of repetitive sections, you can use the Iterate mediator to generate a relatively small payload using those repetitive sections. This will then allow you to use the Script mediator. From WSO2 EI 6.2.0 onwards, the Script Mediator supports using Nashorn to execute JavaScripts, in addition to its default Rhino engine. For more information, see Script Mediator with Nashorn Support . A Script mediator can be created in one of the following methods. With the script program statements stored in a separate file, referenced via the Local or Remote Registry entry . With the script program statements embedded inline within the Synapse configuration. Synapse uses the Apache Bean Scripting Framework for scripting language support. Any script language supported by BSF may be used to implement the Synapse Mediator. With the Script Mediator, you can invoke a function in the corresponding script. With these functions, it is possible to access the Synapse predefined in a script variable named mc . The mc variable represents an implementation of the MessageContext , named ScriptMessageContext.java , which contains the following methods that can be accessed within the script as mc.methodName . Return? Method Name Description Yes getPayloadXML() This gets an XML representation of SOAP Body payload. No setPayloadXML(payload) This sets the SOAP body payload from XML. No addHeader(mustUnderstand, content) This adds a new SOAP header to the message. Yes getEnvelopeXML() This gets the XML representation of the complete SOAP envelope. No setTo(reference) This is used to set the value which specifies the receiver of the message. Yes setFaultTo(reference) This is used to set the value which specifies the receiver of the faults relating to the message. No setFrom(reference) This is used to set the value which specifies the sender of the message. No setReplyTo(reference) This is used to set the value which specifies the receiver of the replies to the message. Yes getPayloadJSON() This gets the JSON representation of a SOAP Body payload. No setPayloadJSON( payload ) This sets the JSON representation of a payload obtained via the getPayloadJSON() method and sets it in the current message context. Yes getProperty(name) This gets a property from the current message context. No setProperty(key, value) This is used to set a property in the current message context. The previously set property values are replaced by this method. Implementing a Mediator with a script language has advantages over using the built-in Synapse Mediator types or implementing a custom Java class Mediator. The Script Mediators have the flexibility of a class Mediator with access to the Synapse MessageContext and SynapseEnvironment APIs. Also, the ease of use and dynamic nature of scripting languages allow the rapid development and prototyping of custom mediators. An additional benefit of some scripting languages is that they have very simple and elegant XML manipulation capabilities, which make them very usable in a Synapse mediation environment. e.g., JavaScript E4X or Ruby REXML. For both types of script mediator definitions, the MessageContext passed into the script has additional methods over the standard Synapse MessageContext to enable working with XML natural to the scripting language. Example are when using JavaScript getPayloadXML and setPayloadXML , E4X XML objects and when using Ruby, REXML documents. Info The Script mediator is a content-aware mediator. Prerequisites | Syntax | Configuration | Examples","title":"Script Mediator"},{"location":"references/script-Mediator/#prerequisites","text":"Listed below are the prerequisites for writing a Script mediator using JavaScript, Groovy, or Ruby. Scripting Language Prerequisite Groovy Download the groovy-all -2.4.4.jar file and copy it to the <EI_HOME>/ dropins directory. Note that when you define the script , you need to start by importing Groovy. Ruby Install the JRuby engine for mediation. This is available in the WSO2 P2 repository as a feature ( WSO2 Carbon - JRuby Engine for Mediation ). See the instructions on how to install features in WSO2 EI . Alternatively, you can download and install the JRuby engine manually: Download the jruby-complete-1.3.0.wso2v1.jar file from the WSO2 P2 repository and copy it to the <EI_HOME>/ dropins directory. JavaScript The JavaScript/E4X support is enabled by default in the WSO2 Enterprise Integrator distribution and ready for use.","title":"Prerequisites"},{"location":"references/script-Mediator/#syntax","text":"Click on the relevant tab to view the syntax for a script mediator using an Inline script, or a script mediator using a script of a registry Using an Inline script Using a script of the registry The following syntax applies when you create a Script mediator with the script program statements embedded inline within the Synapse configuration. <script language=\"js\"><![CDATA[...script source code...]]></script> The following syntax applies when you create a Script mediator with the script program statements stored in a separate file, referenced via the Local or Remote Registry entry . Info If you are creating the Registry Resource via Tooling, y ou need not specify the content/media type, because it gets automatically applied when you select the JavaScript File Template as shown below. <script key=\"string\" language=\"js\" [function=\"script-function-name\"]> <include key=\"string\"/> </script>","title":"Syntax"},{"location":"references/script-Mediator/#configuration","text":"Click on the relevant tab to view the required UI configuration depending on the script type you have selected. The available script types are as follows: Inline : If this script type is selected, the script is specified inline. Registry : If this script type is selected, a script which is already saved in the registry will be referred using a key. Inline Using a script of the registry The parameters available to configure a Script mediator using an inline script are as follows. Parameter Name Description Language The scripting language for the Script mediator. You can select from the following available languages. JavaScript - This is represented as js in the source view. Groovy - This is represented as groovy in the source view. Ruby - This is represented as rb in the source view. Source Enter the source in this parameter. Note: If you are using Groovy as the scripting language, you need to first import Groovy in your script by adding the following: import groovy.json.*; The parameters available to configure a Script mediator using a script saved in the registry are as follows. Parameter Name Description Language The scripting language for the Script mediator. You can select from the following available languages. JavaScript - This is represented as js in the source view. Groovy - This is represented as groovy in the source view. Note: Be sure that your script starts with the following, which indicates that Groovy is imported: import groovy.json.*; Ruby - This is represented as rb in the source view. Function The function of the selected script language to be invoked. This is an optional parameter. If no value is specified, a default function named mediate will be applied. This function considers the Synapse MessageContext as a single parameter. The function may return a boolean. If it does not, then the value true is assumed and the Script mediator returns this value. Key Type You can select one of the following options. Static Key : If this is selected, an existing key can be selected from the registry for the Key parameter. Dynamic Key : If this is selected, the key can be entered dynamically in the Key parameter. Key The Registry location of the source. You can click either Configuration Registry or the Governance Registry to select the source from the resource tree. Include keys This parameter allows you to include functions defined in two or more scripts your Script mediator configuration. After pointing to one script in the Key parameter, you can click Add Include Key to add the function in another script. When you click Add Include Key , the following parameters will be displayed. Enter the script to be included in the Key parameter by clicking either Configuration Registry or the Governance Registry and then selecting the relevant script from the resource tree.","title":"Configuration"},{"location":"references/script-Mediator/#examples","text":"Example 1 - Using an inline script Example 2 - Using a script saved in the registry Example 3 - Adding an Include key Example 4 - Adding a custom SOAP header Example per method Samples","title":"Examples"},{"location":"references/script-Mediator/#example-1-using-an-inline-script","text":"The following configuration is an example of an inline mediator using JavaScript/E4X which returns false if the SOAP message body contains an element named symbol , which has a value of IBM . <script language=\"js\"><![CDATA[mc.getPayloadXML()..symbol != \"IBM\";]]></script>","title":"Example 1 - Using an inline script"},{"location":"references/script-Mediator/#example-2-using-a-script-saved-in-the-registry","text":"In the following example, script is loaded from the registry by using the key repository/conf/sample/resources/script/test.js . <script language=\"js\" key=\"repository/conf/sample/resources/script/test.js\" function=\"testFunction\"/> script language=\"js\" indicates that the function invoked should be in the JavaScript language. The function named testFunction which is invoked should be saved as a resource in the Registry . The script can be as shown in the example below. function testFunction(mc) { var symbol = mc.getPayloadXML()..*::Code.toString(); mc.setPayloadXML( <m:getQuote xmlns:m=\"http://services.samples/xsd\"> <m:request> <m:symbol>{symbol}</m:symbol> </m:request> </m:getQuote>); }","title":"Example 2 - Using a script saved in the registry"},{"location":"references/script-Mediator/#example-3-adding-an-include-key","text":"The following configuration has an include key . <script language=\"js\" key=\"stockquoteScript\" function=\"transformRequest\"> <include key=\"sampleScript\"/> </script> The script is written in JavaScript. The function to be executed is transformRequest . This function may be as follows in a script saved in the Registry . // stockquoteTransform.js function transformRequest(mc) { transformRequestFunction(mc); } function transformResponse(mc) { transformResponseFunction(mc); } In addition, the function in the script named sampleScript which is included in the mediation configuration via the include key sub element is also executed in the mediation. Note that in order to do this, sampleScript script should also be saved as a resource in the Registry . This script can be as follows. // sample.js function transformRequestFunction(mc) { var symbol = mc.getPayloadXML()..*::Code.toString(); mc.setPayloadXML( <m:getquote m=\"http://services.samples\"> <m:request> <m:symbol>{symbol}</m:symbol> </m:request> </m:getquote>); } function transformResponse(mc) { var symbol = mc.getPayloadXML()..*::symbol.toString(); var price = mc.getPayloadXML()..*::last.toString(); mc.setPayloadXML( <m:checkpriceresponse m=\"http://services.samples/xsd\"> <m:code>{symbol}</m:code> <m:price>{price}</m:price> </m:checkpriceresponse>); }","title":"Example 3 - Adding an Include key"},{"location":"references/script-Mediator/#example-4-adding-a-custom-soap-header","text":"You can add custom SOAP headers to a request by using the addHeader(mustUnderstand, content) of the Script Mediator in a proxy service as shown in the example below. <proxy xmlns=\"http://ws.apache.org/ns/synapse\" name=\"CustomSOAPHeaderProxy\" startOnLoad=\"true\" statistics=\"disable\" trace=\"disable\" transports=\"http,https\"> <target> <inSequence> <log level=\"full\"> <property name=\"Message\" value=\"IncomingRequest\"/> </log> <script language=\"js\">mc.addHeader(false, &lt;ns:sampleCustomHeader xmlns:ns=\"gsb:http://wso2.org/sample\"&gt;&lt;ns:customInfo&gt;CustomHeader&lt;/ns:customInfo&gt;&lt;/ns:sampleCustomHeader&gt;);</script> <log level=\"full\"> <property name=\"Message\" value=\"UpdatedMessage\"/> </log> <drop/> </inSequence> </target> <description/> </proxy>","title":"Example 4 - Adding a custom SOAP header"},{"location":"references/script-Mediator/#example-per-method","text":"The following table contains examples of how some of the commonly used methods can be included in the script invoked by the following sample Script mediator configuration. <script language=\"js\" key=\"conf:/repository/EI/transform.js\" function=\"transform\"/> Return? Method Name Example Yes getPayloadXML() The script invoked can be as follows. // sample.js02.function transformRequestFunction(mc) { var symbol = mc . getPayloadXML ().. *:: Code . toString () ; mc . setPayloadXML ( < m : getquote m = \"http://services.samples\" > < m : request > < m : symbol >{ symbol } </m : symbol > </m : request > </m : getquote > ) ; } mc.getPayloadXML() returns the response received in XML form. No setPayloadXML(payload) See the example above for the getPayloadXML() method. mc.setPayloadXML( <m:getquote m=\"http://services.samples\"> <m:request> <m:symbol>{symbol}</m:symbol </m:request> </m:getquote> ) is used in that script to set the XML representation of the SOAP body (obtained using the getPayloadXML() method) to the current message context. No addHeader(mustUnderstand, Object content) The script invoked can be as follows. < script language = \"js\" > <! [CDATA[ var wsse = new Namespace ( 'http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd' ) ; var envelope = mc . getEnvelopeXML () ; var username = envelope .. wsse :: Username . toString () ; var password = envelope .. wsse :: Password . toString () ; mc . addHeader ( false , < urn : AuthenticationInfo >< urn : userName >{ username } </urn : userName >< urn : password >{ password } </urn : password > </urn : AuthenticationInfo > ) ; ]] > </script > The addHeader method configured as mc.addHeader(false, <urn:AuthenticationInfo><urn:userName>{username}</urn:userName><urn:password>{password}</urn:password></urn:AuthenticationInfo>) in the above script is used to extract user name and password values included in the request and add them to the header structure required for the backend service. No getEnvelopeXML() The script invoked can be as follows. < script language = \"js\" > <! [CDATA[ var wsse = new Namespace ( 'http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd' ) ; var envelope = mc . getEnvelopeXML () ; var username = envelope .. wsse :: Username . toString () ; var password = envelope .. wsse :: Password . toString () ; mc . addHeader ( false , < urn : AuthenticationInfo >< urn : userName >{ username } </urn : userName >< urn : password >{ password } </urn : password > </urn : AuthenticationInfo > ) ; ]] > </script > See more at: http://sajithblogs.blogspot.com/2013/08/wso2-EI-adding-complex-soap-headers-to.html#sthash.jqpiEmf0.dpuf Yes getPayloadJSON() The script invoked can be as follows. function transform (mc) { payload = mc . getPayloadJSON () ; results = payload . results ; var response = new Array () ; for (i = 0 ; i < results . length ; ++ i) { location_object = results[i] ; l = new Object () ; l . name = location_object . name ; l . tags = location_object . types ; l . id = \"ID:\" + ( location_object . id ) ; response[i] = l ; } mc . setPayloadJSON (response) ; } mc.getPayloadJSON() returns the JSON payload (received as the response) as a JavaScript object. This object can be manipulated as a normal JavaScript variable within a script as shown in the above JavaScript code. See JSON Support for further information about how this script is used. No setPayloadJSON(payload) See the example script for the getPayloadJSON() method. The mc.setPayloadJSON() method can be used to replace the existing payload with a new payload. In the above script, we build a new array object by using the fields of the incoming JSON payload and set that array object as the new payload. See JSON Support for further information about how this script is used Yes getProperty (name) The script invoked can be as follows. <script language= \"js\" > <![CDATA[ var time1 = mc. getProperty ( \"TIME_1\" ); var time2 = mc. getProperty ( \"TIME_2\" ); var timeTaken = time2 - time1; print ( \"Time Duration : \" + timeTaken + \" ms \" ); mc. setProperty ( \"RESPONSE_TIME\" , timeTaken); ]]> </script> In this example, the getProperty method is used to get two time durations. The difference between the two time durations is calculated and the setProperty method is used to set this difference in the message context. No setProperty(property) See the example for the getProperty method. The setProperty method is used to set the response time calculated from the time durations obtained (using the getProperty method) in the message context. !!! note In the ESB profile due to a Rhino engine upgrade, when strings are concatenated and set as a property in the message context, you need to use the toString() method to convert the result to a string. In the following example, var result = \"a\" and then result = result + \"b\" . When concatenating these strings, the script invoked needs to be as follows: <script language= \"js\" > <![CDATA[ {var result = \"a\"; result = result + \"b\"; mc.setProperty('result_str', result.toString()); } ]]> </script>","title":"Example per method"},{"location":"references/script-Mediator/#samples","text":"The following samples demonstrate how to use the Script mediator. Sample 350: Introduction to the Script Mediator Using JavaScript Sample 351: Inline script mediation with JavaScript Sample 352: Accessing Synapse message context API using a scripting language Sample 353: Using Ruby Scripts for Mediation Sample 354: Using Inline Ruby Scripts for Mediation See also Sample 441: Converting JSON to XML Using JavaScript","title":"Samples"},{"location":"references/send-Mediator/","text":"Send Mediator \u00b6 The Send Mediator is used to send messages out of Synapse to an endpoint. The Send Mediator also copies any message context properties from the current message context to the reply message received on the execution of the send operation, so that the response could be correlated back to the request. Messages may be correlated by WS-A MessageID, or even simple custom text labels. Info A send operation can be blocking or non-blocking depending on the actual transport implementation used. The default NIO-based http/s implementation does not block on a send. Therefore, if a message should be sent and further processed (e.g. transformed) afterwards, it is required to clone the message into two copies and then perform the processing to avoid conflicts. Info The Send mediator is a content-unaware mediator. Note Do not add any mediator configurations after Send mediator in the same sequence, because the ESB profile does not process them. Any mediator configuration after the Send mediator should go to the outSequence or receive sequence. Syntax | Configuration | Examples Syntax \u00b6 <send/> If the message is to be sent to one or more endpoints, use the following syntax. <send> (endpointref | endpoint)+ </send> The endpointref token refers to the following: <endpoint key=\"name\"/> The endpoint token refers to an anonymous endpoint definition. Configuration \u00b6 Parameter Name Description Select Endpoint Type This parameter is used to specify the endpoint type to which the message should be sent. The available options are as follows. None : If this is selected for a Send mediator included in the Out sequence, the message is not sent to any endpoint, but it will be sent back to the client. If this option is selected for a Send mediator included in the In sequence, the message will be sent to the URL specified in its To header. Define Inline : If this is selected, the endpoint to which the message should be sent can be included within the Send mediator configuration. Click Add to add the required endpoint. See Adding an Endpoint for further details. Pick from Registry : If this is selected, the message can be sent to a pre-defined endpoint which is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. XPath : If this is selected, the endpoint to which the message should be sent will be derived via an XPath expression. You are required to enter the relevant XPath expression in the text field that appears when this option is selected. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Receiving Sequence Type The sequence to use for handling the response from the endpoint. Possible options are as follows. Default : If this is selected, the mediation sequence in the Out sequence will be used. Static : If this is selected, the sequence will be static. You can select a pre-defined sequence that is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the resource tree. Dynamic : If this is selected, the sequence will be derived via an XPath expression. The XPath expression should be entered in the Receiving Sequence parameter which appears when this option is selected. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Build Message Before Sending This parameter is used to specify whether the message should be built before sending or not. The possible values are as follows. Yes : If this is selected, the full message XML is built in the memory before the message is sent. Yes should be selected if your configuration includes a logic that is performed after the Send has initiated. No : If this is selected, the full message XML is not built in the memory before the message is sent. This improves performance. Examples \u00b6 Example 1 - Send mediator used in the In sequence and Out sequence \u00b6 In this example, the first send operation is included in the In mediator. Both the request and response will go through the main sequence, but only request messages will go through the In mediator. Similarly, only response messages will go through the Out mediator. The request will be forwarded to the endpoint with the given address. The response will go through the second send operation, which in this example just sends it back to the client because there is no Out endpoint specified. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <in> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> <drop/> </in> <out> <send/> </out> </definitions> Example 2 - Specifying a response handling sequence (service chaining) \u00b6 ``` html/xml In this example, requests are sent to the ` PersonInfoEpr ` endpoint, and responses from the service at that endpoint are handled by a sequence named personInfoSeq. This approach is particularly useful for service chaining. For example, if you want to take the responses from the ` PersonInfoEpr ` service and send them to the ` CreditEpr ` service for additional processing before sending the final response back to the client. In this case, you can configure the ` personInfoSeq ` sequence to send the response to the ` CreditEpr ` service and also specify another receive sequence named ` creditSeq ` that sends the response from the ` CreditEpr ` service back to the client. Following is the configuration of these sequences. ``` html/xml <sequence name=\"personInfoSeq\"> <xslt key=\"xslt\"> <property name=\"amount\" expression=\"get-property('ORG_AMOUNT')\"/> </xslt> <send receive=\"creditSeq\"> <endpoint key=\"CreditEpr\"/> </send> </sequence> <sequence name=\"creditSeq\"> <log level=\"full\"/> <send/> </sequence> Example 3 - Configuring a blocking/non-blocking send operation \u00b6 In this example, the Send mediator in a proxy service using the VFS transport is transferring a file to a VFS endpoint. VFS is a non-blocking transport by default, which means a new thread is spawned for each outgoing message. The Property mediator added before the Send mediator removes the ClientAPINonBlocking property from the message to perform the mediation in a single thread. This is required when the file being transferred is large and you want to avoid out-of-memory failures. <inSequence> <property name=\"ClientApiNonBlocking\" value=\"true\" scope=\"axis2\" action=\"remove\"/> <send> <endpoint name=\"FileEpr\"> <address uri=\"vfs:file:////home/shammi/file-out\"/> </endpoint> </send> </inSequence>","title":"Send Mediator"},{"location":"references/send-Mediator/#send-mediator","text":"The Send Mediator is used to send messages out of Synapse to an endpoint. The Send Mediator also copies any message context properties from the current message context to the reply message received on the execution of the send operation, so that the response could be correlated back to the request. Messages may be correlated by WS-A MessageID, or even simple custom text labels. Info A send operation can be blocking or non-blocking depending on the actual transport implementation used. The default NIO-based http/s implementation does not block on a send. Therefore, if a message should be sent and further processed (e.g. transformed) afterwards, it is required to clone the message into two copies and then perform the processing to avoid conflicts. Info The Send mediator is a content-unaware mediator. Note Do not add any mediator configurations after Send mediator in the same sequence, because the ESB profile does not process them. Any mediator configuration after the Send mediator should go to the outSequence or receive sequence. Syntax | Configuration | Examples","title":"Send Mediator"},{"location":"references/send-Mediator/#syntax","text":"<send/> If the message is to be sent to one or more endpoints, use the following syntax. <send> (endpointref | endpoint)+ </send> The endpointref token refers to the following: <endpoint key=\"name\"/> The endpoint token refers to an anonymous endpoint definition.","title":"Syntax"},{"location":"references/send-Mediator/#configuration","text":"Parameter Name Description Select Endpoint Type This parameter is used to specify the endpoint type to which the message should be sent. The available options are as follows. None : If this is selected for a Send mediator included in the Out sequence, the message is not sent to any endpoint, but it will be sent back to the client. If this option is selected for a Send mediator included in the In sequence, the message will be sent to the URL specified in its To header. Define Inline : If this is selected, the endpoint to which the message should be sent can be included within the Send mediator configuration. Click Add to add the required endpoint. See Adding an Endpoint for further details. Pick from Registry : If this is selected, the message can be sent to a pre-defined endpoint which is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required endpoint from the resource tree. XPath : If this is selected, the endpoint to which the message should be sent will be derived via an XPath expression. You are required to enter the relevant XPath expression in the text field that appears when this option is selected. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Receiving Sequence Type The sequence to use for handling the response from the endpoint. Possible options are as follows. Default : If this is selected, the mediation sequence in the Out sequence will be used. Static : If this is selected, the sequence will be static. You can select a pre-defined sequence that is currently saved as a resource in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the resource tree. Dynamic : If this is selected, the sequence will be derived via an XPath expression. The XPath expression should be entered in the Receiving Sequence parameter which appears when this option is selected. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Build Message Before Sending This parameter is used to specify whether the message should be built before sending or not. The possible values are as follows. Yes : If this is selected, the full message XML is built in the memory before the message is sent. Yes should be selected if your configuration includes a logic that is performed after the Send has initiated. No : If this is selected, the full message XML is not built in the memory before the message is sent. This improves performance.","title":"Configuration"},{"location":"references/send-Mediator/#examples","text":"","title":"Examples"},{"location":"references/send-Mediator/#example-1-send-mediator-used-in-the-in-sequence-and-out-sequence","text":"In this example, the first send operation is included in the In mediator. Both the request and response will go through the main sequence, but only request messages will go through the In mediator. Similarly, only response messages will go through the Out mediator. The request will be forwarded to the endpoint with the given address. The response will go through the second send operation, which in this example just sends it back to the client because there is no Out endpoint specified. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <in> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> <drop/> </in> <out> <send/> </out> </definitions>","title":"Example 1 - Send mediator used in the In sequence and Out sequence"},{"location":"references/send-Mediator/#example-2-specifying-a-response-handling-sequence-service-chaining","text":"``` html/xml In this example, requests are sent to the ` PersonInfoEpr ` endpoint, and responses from the service at that endpoint are handled by a sequence named personInfoSeq. This approach is particularly useful for service chaining. For example, if you want to take the responses from the ` PersonInfoEpr ` service and send them to the ` CreditEpr ` service for additional processing before sending the final response back to the client. In this case, you can configure the ` personInfoSeq ` sequence to send the response to the ` CreditEpr ` service and also specify another receive sequence named ` creditSeq ` that sends the response from the ` CreditEpr ` service back to the client. Following is the configuration of these sequences. ``` html/xml <sequence name=\"personInfoSeq\"> <xslt key=\"xslt\"> <property name=\"amount\" expression=\"get-property('ORG_AMOUNT')\"/> </xslt> <send receive=\"creditSeq\"> <endpoint key=\"CreditEpr\"/> </send> </sequence> <sequence name=\"creditSeq\"> <log level=\"full\"/> <send/> </sequence>","title":"Example 2 - Specifying a response handling sequence (service chaining)"},{"location":"references/send-Mediator/#example-3-configuring-a-blockingnon-blocking-send-operation","text":"In this example, the Send mediator in a proxy service using the VFS transport is transferring a file to a VFS endpoint. VFS is a non-blocking transport by default, which means a new thread is spawned for each outgoing message. The Property mediator added before the Send mediator removes the ClientAPINonBlocking property from the message to perform the mediation in a single thread. This is required when the file being transferred is large and you want to avoid out-of-memory failures. <inSequence> <property name=\"ClientApiNonBlocking\" value=\"true\" scope=\"axis2\" action=\"remove\"/> <send> <endpoint name=\"FileEpr\"> <address uri=\"vfs:file:////home/shammi/file-out\"/> </endpoint> </send> </inSequence>","title":"Example 3 - Configuring a blocking/non-blocking send operation"},{"location":"references/sequence-Mediator/","text":"Sequence Mediator \u00b6 The Sequence Mediator refers to an already defined sequence element, which is used to invoke a named sequence of mediators. This is useful when you need to use a particular set on mediators in a given order repeatedly. You can alternatively select a pre-defined sequence from the Registry as the in/out/fault sequence for a proxy service or a REST service without adding any mediator configurations inline. The difference between these two options are described in the table below. Attribute Picking a pre-defined sequence as in/out/fault sequence Referring to a pre-defined sequence via the Sequence mediator Adding other mediators Other mediator configurations that are not already included in the pre-defined sequence cannot be added to the in/out/fault sequence. Other mediator configurations that are not already included in the pre-defined sequence can be added to the in/out/fault sequence Applying changes done to the pre-defined sequence Any changes done to the sequence saved in the Registry after it was selected as the in/out/fault sequence will not be considered when carrying out mediation. Any changes done to the sequence saved in the Registry after it was selected as the in/out/fault sequence will be considered when carrying out mediation. Info The Sequence mediator is a content-unaware mediator. Syntax | Configuration | Examples Syntax \u00b6 A sequence ref token refers to a \\< sequence > element, which is used to invoke a named sequence of mediators. <sequence key=\"name\"/> Configuration \u00b6 The parameters available to configure the Sequence mediator are as follows. Parameter Name Description Key Type This parameter defines whether the key to access the required sequence is a static key or a dynamic key. Possible values are as follows. Static Key : If this is selected, the key to access the sequence is a static value. You can click either Configuration Registry or Governance Registry as relevant to select the require key from the resource tree for the Referring Sequence parameter. Dynamic Key : If this is selected, you can define the key to access the sequence as a dynamic value by entering an XPath expression in the Referring Sequence parameter. Referring sequence The key to access the sequence saved in the registry. You can enter a static value selected from the resource tree, or an XPath expression based on the option you selected for the Key Type parameter. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Examples \u00b6 In this example, the following sequence named StoreSend is saved in the Configuration registry. It includes a Store Mediator to store the request in a message store named JMSMS and a Send Mediator to send it to an endpoint afterwards. <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"conf:/StoreSend\"> <axis2ns4:store xmlns:axis2ns4=\"http://ws.apache.org/ns/synapse\" messageStore=\"JMSMS\" sequence=\"conf:/repository/components/org.wso2.carbon.throttle/templates\"></axis2ns4:store> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"></address> </endpoint> </send> </sequence> The Sequence mediator configuration can be as follows to invoke the StoreSend sequence after using a PayloadFactory mediator to transform the contents of the request. <inSequence xmlns=\"http://ws.apache.org/ns/synapse\"> <payloadFactory media-type=\"xml\"> <format> <m:checkpriceresponse xmlns:m=\"http://services.samples/xsd\"> <m:code>$1</m:code> <m:price>$2</m:price> </m:checkpriceresponse> </format> <args> <arg expression=\"//m0:symbol\" xmlns:m0=\"http://services.samples/xsd\"> <arg expression=\"//m0:last\" xmlns:m0=\"http://services.samples/xsd\"> </arg></arg></args> </payloadFactory> <sequence key=\"conf:/StoreSend\"></sequence> </inSequence>","title":"Sequence Mediator"},{"location":"references/sequence-Mediator/#sequence-mediator","text":"The Sequence Mediator refers to an already defined sequence element, which is used to invoke a named sequence of mediators. This is useful when you need to use a particular set on mediators in a given order repeatedly. You can alternatively select a pre-defined sequence from the Registry as the in/out/fault sequence for a proxy service or a REST service without adding any mediator configurations inline. The difference between these two options are described in the table below. Attribute Picking a pre-defined sequence as in/out/fault sequence Referring to a pre-defined sequence via the Sequence mediator Adding other mediators Other mediator configurations that are not already included in the pre-defined sequence cannot be added to the in/out/fault sequence. Other mediator configurations that are not already included in the pre-defined sequence can be added to the in/out/fault sequence Applying changes done to the pre-defined sequence Any changes done to the sequence saved in the Registry after it was selected as the in/out/fault sequence will not be considered when carrying out mediation. Any changes done to the sequence saved in the Registry after it was selected as the in/out/fault sequence will be considered when carrying out mediation. Info The Sequence mediator is a content-unaware mediator. Syntax | Configuration | Examples","title":"Sequence Mediator"},{"location":"references/sequence-Mediator/#syntax","text":"A sequence ref token refers to a \\< sequence > element, which is used to invoke a named sequence of mediators. <sequence key=\"name\"/>","title":"Syntax"},{"location":"references/sequence-Mediator/#configuration","text":"The parameters available to configure the Sequence mediator are as follows. Parameter Name Description Key Type This parameter defines whether the key to access the required sequence is a static key or a dynamic key. Possible values are as follows. Static Key : If this is selected, the key to access the sequence is a static value. You can click either Configuration Registry or Governance Registry as relevant to select the require key from the resource tree for the Referring Sequence parameter. Dynamic Key : If this is selected, you can define the key to access the sequence as a dynamic value by entering an XPath expression in the Referring Sequence parameter. Referring sequence The key to access the sequence saved in the registry. You can enter a static value selected from the resource tree, or an XPath expression based on the option you selected for the Key Type parameter. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression.","title":"Configuration"},{"location":"references/sequence-Mediator/#examples","text":"In this example, the following sequence named StoreSend is saved in the Configuration registry. It includes a Store Mediator to store the request in a message store named JMSMS and a Send Mediator to send it to an endpoint afterwards. <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"conf:/StoreSend\"> <axis2ns4:store xmlns:axis2ns4=\"http://ws.apache.org/ns/synapse\" messageStore=\"JMSMS\" sequence=\"conf:/repository/components/org.wso2.carbon.throttle/templates\"></axis2ns4:store> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"></address> </endpoint> </send> </sequence> The Sequence mediator configuration can be as follows to invoke the StoreSend sequence after using a PayloadFactory mediator to transform the contents of the request. <inSequence xmlns=\"http://ws.apache.org/ns/synapse\"> <payloadFactory media-type=\"xml\"> <format> <m:checkpriceresponse xmlns:m=\"http://services.samples/xsd\"> <m:code>$1</m:code> <m:price>$2</m:price> </m:checkpriceresponse> </format> <args> <arg expression=\"//m0:symbol\" xmlns:m0=\"http://services.samples/xsd\"> <arg expression=\"//m0:last\" xmlns:m0=\"http://services.samples/xsd\"> </arg></arg></args> </payloadFactory> <sequence key=\"conf:/StoreSend\"></sequence> </inSequence>","title":"Examples"},{"location":"references/smooks-Mediator/","text":"Smooks Mediator \u00b6 The Smooks Mediator can be used to apply lightweight transformations on messages in an efficient manner. Smooks is a powerful framework for processing, manipulating and transforming XML. More information about Smooks can be obtained from the official Smooks website . Syntax | Configuration | Examples Syntax \u00b6 <smooks [config-key=\"string\"]> <input [type=\"|text|xml\"]/> <output [type=\"|text|xml|java\"] [property=\"string\"] [action=\"string\"]/> </smooks> Configuration \u00b6 The parameters available for configuring the Smooks mediator are as follows: Parameter Name Description Config-Key The key to access the Smooks configuration. The Smooks configuration should be saved in the Registry as a local entry before it can be used here. Click either Configuration Registry or Governance Registry to select the Smooks configuration from the resource tree. Input You can select either XML or Text as the input. Expression This parameter is used to enter an XPath expression to select the exact message block to which the transformation should be applied. If no expression is entered, the transformation would apply to the entire message body by default. Output The format of the output. The output type can be XML, Text or Java, and the output can be one of the following. Property : If this is selected, the output defined as a property will be saved in the message context for future use. Expression : If this is selected, the output is defined as an expression and the following additional actions can be performed. Add :The selected node will be added as a child to the message. Replace: Selected node will be replaced in the message. Sibling : Selected node will be added as a sibling. Examples \u00b6 Example 1: Performance tuning \u00b6 Smooks can be used to split a file and send split results to a JMS endpoint. In this case, having a value other than -1 for jms:highWaterMark in the Smooks configuration file can result in a low throughput for message publishing, since Smooks will spend resources on message counting while the messages are being published. Therefore, it is recommended to use -1 as the highWaterMark value for high throughput values. The following is a sample Smooks configuration file with this setting. For more information on creating the Smooks configuration file, see the documentation in the official Smooks website . Sample Smooks configuration <?xml version=\"1.0\" encoding=\"UTF-8\"?> <smooks-resource-list xmlns=\"http://www.milyn.org/xsd/smooks-1.1.xsd\" xmlns:core=\"http://www.milyn.org/xsd/smooks/smooks-core-1.3.xsd\" xmlns:ftl=\"http://www.milyn.org/xsd/smooks/freemarker-1.1.xsd\" xmlns:jms=\"http://www.milyn.org/xsd/smooks/jms-routing-1.2.xsd\"> <!-- Filter the message using the SAX Filter (i.e. not DOM, so no intermediate DOM, so we can process huge messages... --> <core:filterSettings type=\"SAX\" /> <!-- Capture the message data 2 seperate DOM model, for \"order\" and \"order-item\" fragments... --> <resource-config selector=\"order,order-item\"> <resource>org.milyn.delivery.DomModelCreator</resource> </resource-config> <!-- At each \"order-iteam\", apply a template to the \"order\" and \"order-item\" DOM model... --> <ftl:freemarker applyOnElement=\"order-item\"> <!-- Note in the template that we need to use the special FreeMarker variable \".vars\" because of the hyphenated variable names (\"order-item\"). See http://freemarker.org/docs/ref_specvar.html. --> <ftl:template>/repository/resources/orderitem-split.ftl.txt</ftl:template> <ftl:use> <!-- Bind the templating result into the bean context, from where it can be accessed by the JMSRouter (configured above). --> <ftl:bindTo id=\"orderItem_xml\" /> </ftl:use> </ftl:freemarker> <!-- At each \"order-item\", route the \"orderItem_xml\" to the ActiveMQ JMS Queue... --> <jms:router routeOnElement=\"order-item\" beanId=\"orderItem_xml\" destination=\"smooks.exampleQueue\"> <jms:message> <!-- Need to use special FreeMarker variable \".vars\" --> <jms:correlationIdPattern>${order.@id}-${.vars[\"order-item\"].@id}</jms:correlationIdPattern> </jms:message> <jms:jndi properties=\"/repository/conf/jndi.properties\" /> <jms:highWaterMark mark=\"-1\" /> </jms:router> </smooks-resource-list> Example 2: Referring files from the Smooks configuration \u00b6 The following Smooks configuration refers a bindings file named mapping.xml . This file should be saved in via the class path. These bindings will be applied during mediation when the Smooks configuration is included in a Smooks mediator configuration via the Registry. <smooks-resource-list xmlns=\"http://www.milyn.org/xsd/smooks-1.0.xsd\"> <resource-config selector=\"org.xml.sax.driver\"> <resource>org.milyn.smooks.edi.EDIReader</resource> </resource-config> </smooks-resource-list> #### Samples [Sample 654: Smooks Mediator](https://docs.wso2.com/display/EI6xx/Sample+654%3A+Smooks+Mediator)","title":"Smooks Mediator"},{"location":"references/smooks-Mediator/#smooks-mediator","text":"The Smooks Mediator can be used to apply lightweight transformations on messages in an efficient manner. Smooks is a powerful framework for processing, manipulating and transforming XML. More information about Smooks can be obtained from the official Smooks website . Syntax | Configuration | Examples","title":"Smooks Mediator"},{"location":"references/smooks-Mediator/#syntax","text":"<smooks [config-key=\"string\"]> <input [type=\"|text|xml\"]/> <output [type=\"|text|xml|java\"] [property=\"string\"] [action=\"string\"]/> </smooks>","title":"Syntax"},{"location":"references/smooks-Mediator/#configuration","text":"The parameters available for configuring the Smooks mediator are as follows: Parameter Name Description Config-Key The key to access the Smooks configuration. The Smooks configuration should be saved in the Registry as a local entry before it can be used here. Click either Configuration Registry or Governance Registry to select the Smooks configuration from the resource tree. Input You can select either XML or Text as the input. Expression This parameter is used to enter an XPath expression to select the exact message block to which the transformation should be applied. If no expression is entered, the transformation would apply to the entire message body by default. Output The format of the output. The output type can be XML, Text or Java, and the output can be one of the following. Property : If this is selected, the output defined as a property will be saved in the message context for future use. Expression : If this is selected, the output is defined as an expression and the following additional actions can be performed. Add :The selected node will be added as a child to the message. Replace: Selected node will be replaced in the message. Sibling : Selected node will be added as a sibling.","title":"Configuration"},{"location":"references/smooks-Mediator/#examples","text":"","title":"Examples"},{"location":"references/smooks-Mediator/#example-1-performance-tuning","text":"Smooks can be used to split a file and send split results to a JMS endpoint. In this case, having a value other than -1 for jms:highWaterMark in the Smooks configuration file can result in a low throughput for message publishing, since Smooks will spend resources on message counting while the messages are being published. Therefore, it is recommended to use -1 as the highWaterMark value for high throughput values. The following is a sample Smooks configuration file with this setting. For more information on creating the Smooks configuration file, see the documentation in the official Smooks website . Sample Smooks configuration <?xml version=\"1.0\" encoding=\"UTF-8\"?> <smooks-resource-list xmlns=\"http://www.milyn.org/xsd/smooks-1.1.xsd\" xmlns:core=\"http://www.milyn.org/xsd/smooks/smooks-core-1.3.xsd\" xmlns:ftl=\"http://www.milyn.org/xsd/smooks/freemarker-1.1.xsd\" xmlns:jms=\"http://www.milyn.org/xsd/smooks/jms-routing-1.2.xsd\"> <!-- Filter the message using the SAX Filter (i.e. not DOM, so no intermediate DOM, so we can process huge messages... --> <core:filterSettings type=\"SAX\" /> <!-- Capture the message data 2 seperate DOM model, for \"order\" and \"order-item\" fragments... --> <resource-config selector=\"order,order-item\"> <resource>org.milyn.delivery.DomModelCreator</resource> </resource-config> <!-- At each \"order-iteam\", apply a template to the \"order\" and \"order-item\" DOM model... --> <ftl:freemarker applyOnElement=\"order-item\"> <!-- Note in the template that we need to use the special FreeMarker variable \".vars\" because of the hyphenated variable names (\"order-item\"). See http://freemarker.org/docs/ref_specvar.html. --> <ftl:template>/repository/resources/orderitem-split.ftl.txt</ftl:template> <ftl:use> <!-- Bind the templating result into the bean context, from where it can be accessed by the JMSRouter (configured above). --> <ftl:bindTo id=\"orderItem_xml\" /> </ftl:use> </ftl:freemarker> <!-- At each \"order-item\", route the \"orderItem_xml\" to the ActiveMQ JMS Queue... --> <jms:router routeOnElement=\"order-item\" beanId=\"orderItem_xml\" destination=\"smooks.exampleQueue\"> <jms:message> <!-- Need to use special FreeMarker variable \".vars\" --> <jms:correlationIdPattern>${order.@id}-${.vars[\"order-item\"].@id}</jms:correlationIdPattern> </jms:message> <jms:jndi properties=\"/repository/conf/jndi.properties\" /> <jms:highWaterMark mark=\"-1\" /> </jms:router> </smooks-resource-list>","title":"Example 1: Performance tuning"},{"location":"references/smooks-Mediator/#example-2-referring-files-from-the-smooks-configuration","text":"The following Smooks configuration refers a bindings file named mapping.xml . This file should be saved in via the class path. These bindings will be applied during mediation when the Smooks configuration is included in a Smooks mediator configuration via the Registry. <smooks-resource-list xmlns=\"http://www.milyn.org/xsd/smooks-1.0.xsd\"> <resource-config selector=\"org.xml.sax.driver\"> <resource>org.milyn.smooks.edi.EDIReader</resource> </resource-config> </smooks-resource-list> #### Samples [Sample 654: Smooks Mediator](https://docs.wso2.com/display/EI6xx/Sample+654%3A+Smooks+Mediator)","title":"Example 2: Referring files from the Smooks configuration"},{"location":"references/spring-Mediator/","text":"Spring Mediator \u00b6 Info Note Please note that this feature is deprecated. The Spring Mediator exposes a spring bean as a mediator. The Spring Mediator creates an instance of a mediator, which is managed by Spring. This Spring bean must implement the Mediator interface for it to act as a Mediator. Info Note the following: Spring support in the ESB profile is based on Spring version 2.5. The Spring mediator is a content aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <spring:spring bean=\"exampleBean\" key=\"string\"/> The attributes of the \\< spring > element: key - References the Spring ApplicationContext/Configuration (i.e. spring configuration XML) used for the bean. This key can be a registry key or local entry key . bean - Is used for looking up a Spring bean from the spring Application Context. Therefore, a bean with the same name must be in the given spring configuration. In addition, bean must implement the Mediator interface. Configuration \u00b6 These are the options for the Spring Mediator: Bean - Is used for looking up a Spring bean from the spring Application Context. Key - The Registry reference to the spring Application-Context/Configuration used for the bean. You can select it by clicking the \"Configuration Registry\" or \"Governance Registry\" links. Examples \u00b6 <spring:spring bean=\"springtest\" key=\"conf/sample/resources/spring/springsample.xml\"/> In the above configuration, the spring XML is in the registry and it can be looked up using the registry key conf/sample/resources/spring/springsample.xml . This spring XML (i.e springsample.xml ) must contain a bean with the name springtest . The following figure shows an example that can be used as the registry resource - springsample.xml . <!DOCTYPE beans PUBLIC \"-//SPRING//DTD BEAN//EN\" \"http://www.springframework.org/dtd/spring-beans.dtd\"> <beans> <bean id=\"springtest\" class=\"org.apache.synapse.mediators.spring.SpringTestBean\" singleton=\"false\"> <property name=\"testProperty\" value=\"100\"/> </bean> </beans> Also, you need to b uild the JAR file of the following Spring Bean class and place it in the <EI_HOME>/repository/components/lib/ directory. package org.apache.synapse.mediators.spring; import org.apache.synapse.MessageContext; import org.apache.synapse.mediators.AbstractMediator; public class SpringTestBean extends AbstractMediator{ private String testProperty; public void setTestProperty(String testProperty){ this.testProperty = testProperty; } public boolean mediate(MessageContext mc) { // Do somthing useful.. // Note the access to the Synapse Message context return true; } } For more examples, see Mediating with Spring .","title":"Spring Mediator"},{"location":"references/spring-Mediator/#spring-mediator","text":"Info Note Please note that this feature is deprecated. The Spring Mediator exposes a spring bean as a mediator. The Spring Mediator creates an instance of a mediator, which is managed by Spring. This Spring bean must implement the Mediator interface for it to act as a Mediator. Info Note the following: Spring support in the ESB profile is based on Spring version 2.5. The Spring mediator is a content aware mediator. Syntax | Configuration | Examples","title":"Spring Mediator"},{"location":"references/spring-Mediator/#syntax","text":"<spring:spring bean=\"exampleBean\" key=\"string\"/> The attributes of the \\< spring > element: key - References the Spring ApplicationContext/Configuration (i.e. spring configuration XML) used for the bean. This key can be a registry key or local entry key . bean - Is used for looking up a Spring bean from the spring Application Context. Therefore, a bean with the same name must be in the given spring configuration. In addition, bean must implement the Mediator interface.","title":"Syntax"},{"location":"references/spring-Mediator/#configuration","text":"These are the options for the Spring Mediator: Bean - Is used for looking up a Spring bean from the spring Application Context. Key - The Registry reference to the spring Application-Context/Configuration used for the bean. You can select it by clicking the \"Configuration Registry\" or \"Governance Registry\" links.","title":"Configuration"},{"location":"references/spring-Mediator/#examples","text":"<spring:spring bean=\"springtest\" key=\"conf/sample/resources/spring/springsample.xml\"/> In the above configuration, the spring XML is in the registry and it can be looked up using the registry key conf/sample/resources/spring/springsample.xml . This spring XML (i.e springsample.xml ) must contain a bean with the name springtest . The following figure shows an example that can be used as the registry resource - springsample.xml . <!DOCTYPE beans PUBLIC \"-//SPRING//DTD BEAN//EN\" \"http://www.springframework.org/dtd/spring-beans.dtd\"> <beans> <bean id=\"springtest\" class=\"org.apache.synapse.mediators.spring.SpringTestBean\" singleton=\"false\"> <property name=\"testProperty\" value=\"100\"/> </bean> </beans> Also, you need to b uild the JAR file of the following Spring Bean class and place it in the <EI_HOME>/repository/components/lib/ directory. package org.apache.synapse.mediators.spring; import org.apache.synapse.MessageContext; import org.apache.synapse.mediators.AbstractMediator; public class SpringTestBean extends AbstractMediator{ private String testProperty; public void setTestProperty(String testProperty){ this.testProperty = testProperty; } public boolean mediate(MessageContext mc) { // Do somthing useful.. // Note the access to the Synapse Message context return true; } } For more examples, see Mediating with Spring .","title":"Examples"},{"location":"references/store-Mediator/","text":"Store Mediator \u00b6 The Store mediator enqueues messages passing through its mediation sequence in a given message store . It can serve as a dead letter channel if it is included in a fault sequence and if its message store is connected to a message processor that forwards all the messages in the store to an endpoint. Syntax \u00b6 Info The Store mediator is a content aware mediator. <axis2ns1:store xmlns:axis2ns1=\"http://ws.apache.org/ns/synapse\" messageStore=\"JMSMS\" sequence=\"storeSequence\"></axis2ns1:store> You can dynamically route messages to a Message Store via the Store mediator by resolving the name of the Message Store from the message context. To enable this, give a path expression (followed by its namespace definitions) for the value of the store name attribute as shown below. <axis2ns1:store xmlns:axis2ns1=\"http://ws.apache.org/ns/synapse\" messagestore=\"{//m:msgstr/m:arg/m:value}\" xmlns:m=\"http://services.samples/xsd\" sequence=\"storeSequence\"></axis2ns1:store> Configuration \u00b6 The parameters available to configure the Store mediator is as follows. Parameter Name Description Message Store The Message Store, in which messages will be stored. You can give the name of the Message Store either as a value or as an expression . !!! tip You should add the message store to the ESB profile before you can select it here. To give the Message Store name as a value, select Value for Specify As , and selct the name of the Message Store from the drop down of Value . To give the Message Store name as an expression, select Expression for Specify As , and e nter the XPath to derive the Message Store from the message context. In the namespace editor, add the namespaces that are used in the XPath. On Store Sequence The sequence that will be called before the message gets stored. This sequence should be pre-defined in the registry before it can be entered here. Click either Configuration Registry or Governance Registry to select the required sequence from the resource tree. For more information on configuring the Registry, go to Working with the Registry in the Common Admin Guide. Examples \u00b6 Following are examples demonstrating the usage of the Store mediator. Example 1 - Defining the Message Store as a value \u00b6 A proxy service can be configured with the Store mediator as follows to save messages in a message store named JMSMS . <proxy name=\"SimpleProxy\" transports=\"http https\" startonload=\"true\" trace=\"disable\" xmlns=\"http://ws.apache.org/ns/synapse\"> <target> <inSequence> <property name=\"FORCE_SC_ACCEPTED\" value=\"true\" scope=\"axis2\" type=\"STRING\"></property> <property name=\"OUT_ONLY\" value=\"true\" scope=\"default\" type=\"STRING\"></property> <store messageStore=\"JMSMS\"></store> </inSequence> </target> </proxy> Example 2 - Defining the Message Store as an XPath expression \u00b6 A proxy service can be configured with the Store mediator as follows to save messages in a Message Store, which is dynamically set via the message context specified using an XPath expression. Tip You can use the SimpleStock Quote service as the backend service and SOAP UI as the Client to try out the below example. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <registry provider=\"org.wso2.carbon.mediation.registry.WSO2Registry\"> <parameter name=\"cachableDuration\">15000</parameter> </registry> <taskManager provider=\"org.wso2.carbon.mediation.ntask.NTaskTaskManager\"/> <proxy name=\"StoreMediatorProxy\" startOnLoad=\"true\" transports=\"http https\"> <description/> <target> <inSequence> <store messageStore=\"{//ser:getQuote/ser:request/ser:symbol}\" xmlns:ser=\"http://services.samples\"/> </inSequence> </target> </proxy> <endpoint name=\"StoreMediatorEndpoint\"> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> <sequence name=\"fault\"> <!-- Log the message at the full log level with the ERROR_MESSAGE and the ERROR_CODE--> <log level=\"full\"> <property name=\"MESSAGE\" value=\"Executing default 'fault' sequence\"/> <property expression=\"get-property('ERROR_CODE')\" name=\"ERROR_CODE\"/> <property expression=\"get-property('ERROR_MESSAGE')\" name=\"ERROR_MESSAGE\"/> </log> <!-- Drops the messages by default if there is a fault --> <drop/> </sequence> <sequence name=\"main\"> <in> <!-- Log all messages passing through --> <log level=\"full\"/> <!-- ensure that the default configuration only sends if it is one of samples --> <!-- Otherwise Synapse would be an open proxy by default (BAD!) --> <filter regex=\"http://localhost:9000.*\" source=\"get-property('To')\"> <!-- Send the messages where they have been sent (i.e. implicit \"To\" EPR) --> <send/> </filter> </in> <out> <send/> </out> <description>The main sequence for the message mediation</description> </sequence> <messageStore name=\"StoreMediatorStore\"/> <messageProcessor class=\"org.apache.synapse.message.processor.impl.forwarder.ScheduledMessageForwardingProcessor\" messageStore=\"StoreMediatorStore\" name=\"StoreMediatorProcessor\" targetEndpoint=\"StoreMediatorEndpoint\"> <parameter name=\"client.retry.interval\">1000</parameter> <parameter name=\"throttle\">false</parameter> <parameter name=\"max.delivery.attempts\">4</parameter> <parameter name=\"member.count\">1</parameter> <parameter name=\"max.delivery.drop\">Disabled</parameter> <parameter name=\"interval\">1000</parameter> <parameter name=\"is.active\">true</parameter> <parameter name=\"target.endpoint\">StoreMediatorEndpoint</parameter> </messageProcessor> <!-- You can add any flat sequences, endpoints, etc.. to this synapse.xml file if you do *not* want to keep the artifacts in several files --> </definitions>","title":"Store Mediator"},{"location":"references/store-Mediator/#store-mediator","text":"The Store mediator enqueues messages passing through its mediation sequence in a given message store . It can serve as a dead letter channel if it is included in a fault sequence and if its message store is connected to a message processor that forwards all the messages in the store to an endpoint.","title":"Store Mediator"},{"location":"references/store-Mediator/#syntax","text":"Info The Store mediator is a content aware mediator. <axis2ns1:store xmlns:axis2ns1=\"http://ws.apache.org/ns/synapse\" messageStore=\"JMSMS\" sequence=\"storeSequence\"></axis2ns1:store> You can dynamically route messages to a Message Store via the Store mediator by resolving the name of the Message Store from the message context. To enable this, give a path expression (followed by its namespace definitions) for the value of the store name attribute as shown below. <axis2ns1:store xmlns:axis2ns1=\"http://ws.apache.org/ns/synapse\" messagestore=\"{//m:msgstr/m:arg/m:value}\" xmlns:m=\"http://services.samples/xsd\" sequence=\"storeSequence\"></axis2ns1:store>","title":"Syntax"},{"location":"references/store-Mediator/#configuration","text":"The parameters available to configure the Store mediator is as follows. Parameter Name Description Message Store The Message Store, in which messages will be stored. You can give the name of the Message Store either as a value or as an expression . !!! tip You should add the message store to the ESB profile before you can select it here. To give the Message Store name as a value, select Value for Specify As , and selct the name of the Message Store from the drop down of Value . To give the Message Store name as an expression, select Expression for Specify As , and e nter the XPath to derive the Message Store from the message context. In the namespace editor, add the namespaces that are used in the XPath. On Store Sequence The sequence that will be called before the message gets stored. This sequence should be pre-defined in the registry before it can be entered here. Click either Configuration Registry or Governance Registry to select the required sequence from the resource tree. For more information on configuring the Registry, go to Working with the Registry in the Common Admin Guide.","title":"Configuration"},{"location":"references/store-Mediator/#examples","text":"Following are examples demonstrating the usage of the Store mediator.","title":"Examples"},{"location":"references/store-Mediator/#example-1-defining-the-message-store-as-a-value","text":"A proxy service can be configured with the Store mediator as follows to save messages in a message store named JMSMS . <proxy name=\"SimpleProxy\" transports=\"http https\" startonload=\"true\" trace=\"disable\" xmlns=\"http://ws.apache.org/ns/synapse\"> <target> <inSequence> <property name=\"FORCE_SC_ACCEPTED\" value=\"true\" scope=\"axis2\" type=\"STRING\"></property> <property name=\"OUT_ONLY\" value=\"true\" scope=\"default\" type=\"STRING\"></property> <store messageStore=\"JMSMS\"></store> </inSequence> </target> </proxy>","title":"Example 1 - Defining the Message Store as a value"},{"location":"references/store-Mediator/#example-2-defining-the-message-store-as-an-xpath-expression","text":"A proxy service can be configured with the Store mediator as follows to save messages in a Message Store, which is dynamically set via the message context specified using an XPath expression. Tip You can use the SimpleStock Quote service as the backend service and SOAP UI as the Client to try out the below example. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <registry provider=\"org.wso2.carbon.mediation.registry.WSO2Registry\"> <parameter name=\"cachableDuration\">15000</parameter> </registry> <taskManager provider=\"org.wso2.carbon.mediation.ntask.NTaskTaskManager\"/> <proxy name=\"StoreMediatorProxy\" startOnLoad=\"true\" transports=\"http https\"> <description/> <target> <inSequence> <store messageStore=\"{//ser:getQuote/ser:request/ser:symbol}\" xmlns:ser=\"http://services.samples\"/> </inSequence> </target> </proxy> <endpoint name=\"StoreMediatorEndpoint\"> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> <sequence name=\"fault\"> <!-- Log the message at the full log level with the ERROR_MESSAGE and the ERROR_CODE--> <log level=\"full\"> <property name=\"MESSAGE\" value=\"Executing default 'fault' sequence\"/> <property expression=\"get-property('ERROR_CODE')\" name=\"ERROR_CODE\"/> <property expression=\"get-property('ERROR_MESSAGE')\" name=\"ERROR_MESSAGE\"/> </log> <!-- Drops the messages by default if there is a fault --> <drop/> </sequence> <sequence name=\"main\"> <in> <!-- Log all messages passing through --> <log level=\"full\"/> <!-- ensure that the default configuration only sends if it is one of samples --> <!-- Otherwise Synapse would be an open proxy by default (BAD!) --> <filter regex=\"http://localhost:9000.*\" source=\"get-property('To')\"> <!-- Send the messages where they have been sent (i.e. implicit \"To\" EPR) --> <send/> </filter> </in> <out> <send/> </out> <description>The main sequence for the message mediation</description> </sequence> <messageStore name=\"StoreMediatorStore\"/> <messageProcessor class=\"org.apache.synapse.message.processor.impl.forwarder.ScheduledMessageForwardingProcessor\" messageStore=\"StoreMediatorStore\" name=\"StoreMediatorProcessor\" targetEndpoint=\"StoreMediatorEndpoint\"> <parameter name=\"client.retry.interval\">1000</parameter> <parameter name=\"throttle\">false</parameter> <parameter name=\"max.delivery.attempts\">4</parameter> <parameter name=\"member.count\">1</parameter> <parameter name=\"max.delivery.drop\">Disabled</parameter> <parameter name=\"interval\">1000</parameter> <parameter name=\"is.active\">true</parameter> <parameter name=\"target.endpoint\">StoreMediatorEndpoint</parameter> </messageProcessor> <!-- You can add any flat sequences, endpoints, etc.. to this synapse.xml file if you do *not* want to keep the artifacts in several files --> </definitions>","title":"Example 2 - Defining the Message Store as an XPath expression"},{"location":"references/switch-Mediator/","text":"Switch Mediator \u00b6 The Switch Mediator is an XPath or JSONPath filter. The XPath or JSONPath is evaluated and returns a string. This string is matched against the regular expression in each switch case mediator, in the specified order. If a matching case is found, it will be executed, and the remaining switch case mediators are not processed. If none of the case statements are matching, and a default case is specified, the default will be executed. Info The Switch mediator is a conditionally content aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <switch source=\"[XPath|json-eval(JSON Path)]\"> <case regex=\"string\"> mediator+ </case>+ <default> mediator+ </default>? </switch> Configuration \u00b6 The parameters available to configure the Switch mediator are as follows. Parameter Name Description Source XPath The source XPath or JSONPath to be evaluated. When specifying a JSONPath, use the format json-eval(<JSON_PATH>) , such as json-eval(getQuote.request.symbol) . For more information on using JSON with the the ESB profile , see Working with JSON Message Payloads . If you use namespaces in the expression, click Namespaces and map the namespace prefix to the correct URI. Number of cases This parameter displays the number of cases currently added to the Switch mediator configuration. See Switch-case-mediator for instructions to add a case. Specify default case Click this link to add a default switch-case mediator. Adding a default switch case mediator is optional. If it is specified, it will be executed if no matching switch-case is identified. Switch-case mediator \u00b6 To add a case, click Add case , which adds an empty switch-case mediator under the Switch mediator. A switch-case mediator would appear as a child of the Switch mediator. Click Case to configure the switch-case mediator. The page will expand to display the section shown below where a regular expression can be added in the Case Value (Regular Expression) parameter. Click Case again and click Add Child , and add the mediator(s) you want to execute when this case matches the switching value. Examples \u00b6 In this example the Property mediator sets the local property named symbol on the current message depending on the evaluation of the string. It will get the text of symbol element and match it against the values MSFT and IBM . If the text does not match either of these symbols, the default case will be executed. <switch source=\"//m0:getQuote/m0:request/m0:symbol\" xmlns:m0=\"http://services.samples/xsd\"> <case regex=\"IBM\"> <!-- the property mediator sets a local property on the *current* message --> <property name=\"symbol\" value=\"Great stock - IBM\"/> </case> <case regex=\"MSFT\"> <property name=\"symbol\" value=\"Are you sure? - MSFT\"/> </case> <default> <!-- it is possible to assign the result of an XPath or JSON Path expression as well --> <property name=\"symbol\" expression=\"fn:concat('Normal Stock - ', //m0:getQuote/m0:request/m0:symbol)\" xmlns:m0=\"http://services.samples/xsd\"/> </default> </switch>","title":"Switch Mediator"},{"location":"references/switch-Mediator/#switch-mediator","text":"The Switch Mediator is an XPath or JSONPath filter. The XPath or JSONPath is evaluated and returns a string. This string is matched against the regular expression in each switch case mediator, in the specified order. If a matching case is found, it will be executed, and the remaining switch case mediators are not processed. If none of the case statements are matching, and a default case is specified, the default will be executed. Info The Switch mediator is a conditionally content aware mediator. Syntax | Configuration | Examples","title":"Switch Mediator"},{"location":"references/switch-Mediator/#syntax","text":"<switch source=\"[XPath|json-eval(JSON Path)]\"> <case regex=\"string\"> mediator+ </case>+ <default> mediator+ </default>? </switch>","title":"Syntax"},{"location":"references/switch-Mediator/#configuration","text":"The parameters available to configure the Switch mediator are as follows. Parameter Name Description Source XPath The source XPath or JSONPath to be evaluated. When specifying a JSONPath, use the format json-eval(<JSON_PATH>) , such as json-eval(getQuote.request.symbol) . For more information on using JSON with the the ESB profile , see Working with JSON Message Payloads . If you use namespaces in the expression, click Namespaces and map the namespace prefix to the correct URI. Number of cases This parameter displays the number of cases currently added to the Switch mediator configuration. See Switch-case-mediator for instructions to add a case. Specify default case Click this link to add a default switch-case mediator. Adding a default switch case mediator is optional. If it is specified, it will be executed if no matching switch-case is identified.","title":"Configuration"},{"location":"references/switch-Mediator/#switch-case-mediator","text":"To add a case, click Add case , which adds an empty switch-case mediator under the Switch mediator. A switch-case mediator would appear as a child of the Switch mediator. Click Case to configure the switch-case mediator. The page will expand to display the section shown below where a regular expression can be added in the Case Value (Regular Expression) parameter. Click Case again and click Add Child , and add the mediator(s) you want to execute when this case matches the switching value.","title":"Switch-case mediator"},{"location":"references/switch-Mediator/#examples","text":"In this example the Property mediator sets the local property named symbol on the current message depending on the evaluation of the string. It will get the text of symbol element and match it against the values MSFT and IBM . If the text does not match either of these symbols, the default case will be executed. <switch source=\"//m0:getQuote/m0:request/m0:symbol\" xmlns:m0=\"http://services.samples/xsd\"> <case regex=\"IBM\"> <!-- the property mediator sets a local property on the *current* message --> <property name=\"symbol\" value=\"Great stock - IBM\"/> </case> <case regex=\"MSFT\"> <property name=\"symbol\" value=\"Are you sure? - MSFT\"/> </case> <default> <!-- it is possible to assign the result of an XPath or JSON Path expression as well --> <property name=\"symbol\" expression=\"fn:concat('Normal Stock - ', //m0:getQuote/m0:request/m0:symbol)\" xmlns:m0=\"http://services.samples/xsd\"/> </default> </switch>","title":"Examples"},{"location":"references/synapse-Message-Context-Properties/","text":"Synapse Message Context Properties \u00b6 [ SYSTEM_DATE ] [ SYSTEM_TIME ] [ To, From, Action, FaultTo, ReplyTo, MessageID ] [ MESSAGE_FORMAT ] [ OperationName ] The Synapse message context properties allow you to get information about the message, such as the date/time it was sent, the message format, and the message operation. You can use the get-property() function in the Property mediator with the scope set to Synapse to retrieve these properties. SYSTEM_DATE \u00b6 Name SYSTEM_DATE Possible Values - Default Behavior - Scope - Description Returns the current date as a String. Optionally, a date format as per the standard date format may be supplied as follows: Management Console synapse:'get-property(\"SYSTEM_DATE\", \"yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSSXXX\")' EI Tooling Enter the following in the Namespaced Property Editor: get-property(\"SYSTEM_DATE\", \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\") SYSTEM_TIME \u00b6 Name SYSTEM_TIME Possible Values - Default Behavior - Scope - Description Returns the current time in milliseconds, i.e. the difference, measured in milliseconds, between the current time and midnight, January 1, 1970 UTC. To, From, Action, FaultTo, ReplyTo, MessageID \u00b6 Names To, From, Action, FaultTo, ReplyTo, MessageID Possible Values - Default Behavior - Scope - Description The message To, Action and WS-Addressing properties. MESSAGE_FORMAT \u00b6 Names MESSAGE_FORMAT Possible Values - Default Behavior - Scope - Description Returns the message format, i.e. returns pox, get, soap11 or soap12. OperationName \u00b6 Names OperationName Possible Values - Default Behavior - Scope - Description Returns the operation name for the message.","title":"Synapse Message Context Properties"},{"location":"references/synapse-Message-Context-Properties/#synapse-message-context-properties","text":"[ SYSTEM_DATE ] [ SYSTEM_TIME ] [ To, From, Action, FaultTo, ReplyTo, MessageID ] [ MESSAGE_FORMAT ] [ OperationName ] The Synapse message context properties allow you to get information about the message, such as the date/time it was sent, the message format, and the message operation. You can use the get-property() function in the Property mediator with the scope set to Synapse to retrieve these properties.","title":"Synapse Message Context Properties"},{"location":"references/synapse-Message-Context-Properties/#system95date","text":"Name SYSTEM_DATE Possible Values - Default Behavior - Scope - Description Returns the current date as a String. Optionally, a date format as per the standard date format may be supplied as follows: Management Console synapse:'get-property(\"SYSTEM_DATE\", \"yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSSXXX\")' EI Tooling Enter the following in the Namespaced Property Editor: get-property(\"SYSTEM_DATE\", \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\")","title":"SYSTEM_DATE"},{"location":"references/synapse-Message-Context-Properties/#system95time","text":"Name SYSTEM_TIME Possible Values - Default Behavior - Scope - Description Returns the current time in milliseconds, i.e. the difference, measured in milliseconds, between the current time and midnight, January 1, 1970 UTC.","title":"SYSTEM_TIME"},{"location":"references/synapse-Message-Context-Properties/#to-from-action-faultto-replyto-messageid","text":"Names To, From, Action, FaultTo, ReplyTo, MessageID Possible Values - Default Behavior - Scope - Description The message To, Action and WS-Addressing properties.","title":"To, From, Action, FaultTo, ReplyTo, MessageID"},{"location":"references/synapse-Message-Context-Properties/#message95format","text":"Names MESSAGE_FORMAT Possible Values - Default Behavior - Scope - Description Returns the message format, i.e. returns pox, get, soap11 or soap12.","title":"MESSAGE_FORMAT"},{"location":"references/synapse-Message-Context-Properties/#operationname","text":"Names OperationName Possible Values - Default Behavior - Scope - Description Returns the operation name for the message.","title":"OperationName"},{"location":"references/throttle-Mediator/","text":"Throttle Mediator \u00b6 The Throttle Mediator can be used to restrict access to services. This is useful when services used at the enterprise level and it is required to avoid heavy loads that can cause performance issues in the system. It can also be used when you want to avoid certain user groups (i.e. IP addresses and domains) accessing your system. The Throttle mediator defines a throttle group which includes the following. A throttle policy which defines the extent to which, individuals and groups of IP addresses/domains should be allowed to access the service. A mediation sequence to handle requests that were accepted based on the throttle policy. A mediation sequence to handle requests that were rejected based on the throttle policy. Info The Throttle mediator is a content unaware mediator. Syntax | Configuration | Examples Syntax \u00b6 <throttle [onReject=\"string\"] [onAccept=\"string\"] id=\"string\"> (<policy key=\"string\"/> | <policy>..</policy>) <onReject>..</onReject>? <onAccept>..</onAccept>? </throttle> Configuration \u00b6 The configuration of the Throttle mediator are divided into following sections. Before you edit these sections, enter an ID for the Throttle group in the Throttle Group ID parameter. Throttle Policy On Acceptance On Rejection Throttle Policy \u00b6 This section is used to specify the throttle policy that should apply to the requests passing through the Throttle mediator. A throttle policy has a number of entries defining the extent to which, an individual or a group of IP addresses/domains should be allowed to access the service. The parameters available to be configured in this section are as follows. Parameter Name Description Throttle Policy This section is used to specify the policy for throttling. The following options are available. In-Lined Policy : If this is selected, the Throttle policy can be defined within the Throttle mediator configuration. Click Throttle Policy Editor to open the Mediator Throttling Configuration dialog box where the details relating to the Throttle policy can be entered. The parameters in this dialog box are described in the table below. Referring Policy : If this is selected, you can refer to a pre-defined Throttle policy which is saved in the Registry. You can enter the key to access the policy in the Referring Policy parameter. Click on either Configuration Registry or Governance Registry to select the relevant policy from the Resource Tree. The parameters available in the Mediator Throttling Configuration dialog box to configure the Throttling policy are as follows. Parameter Name Description Maximum Concurrent Accesses The maximum number of messages that are served at a given time. The number of messages between the inflow throttle handler and the outflow throttle handler cannot exceed the value entered for this parameter at any given time. This parameter value is applied to the entire system. It is not restricted to one or more specific IP addresses/domains. !!! info When this parameter is used, the same Throttle mediator ID should be included in the response flow so that the completed responses are deducted from the available limit. Range This parameter is used to specify the IP addresses/domains to which the entry in the current row should be applied If you want to apply the entry to a range of IP addresses, enter the range in this parameter, e.g., 8.100.1.30 \u2013 8.100.1.45 . Alternatively, you can enter a single IP address to apply the entry to only one IP address. If you want to apply the entry to a domain, enter the required domain ID in this parameter. If you want to apply the entry to all the IP addresses/domains that are not configured in the other configurations, enter other in this parameter. Type This parameter is used to specify whether the value(s) entered in the Range parameter refers to IP addresses or domains. Max Request Count This parameter specifies the maximum number of requests that should be handled within the time interval specified in the Unit Time parameter. !!! tip This parameter is applicable only when the value selected for the Access parameter is Control . Unit Time (ms) The time interval for which the maximum number of requests specified for the Throttle ID in the Max Request Count parameter apply. !!! tip This parameter is applicable only when the value selected for the Access parameter is Control . Prohibit Time Period (ms) If the number of requests entered in the Max Request Count parameter is achieved before the time interval entered in the Unit Time (ms) parameter has elapsed, no more requests are taken by the inflow throttle handler for the time period entered in this parameter. Entering a value in this parameter alters the unit time. For example: Max Request Count = 50 Unit Time = 50000 ms Prohibit Time Period = 5000 ms If 50 requests are received within 50000 milliseconds , no requests will be taken for the next 5000 milliseconds. Thus, the time slot considered as the unit time is changed to 40000 milliseconds. If no value is entered in the Prohibit Time Period (ms) parameter, no requests will be taken until 15000 more milliseconds (i.e. the remainder of the unit time) have elapsed. !!! tip This parameter is applicable only when the value selected for the Access parameter is Control . Access This parameter is used to specify the extent to which the IP addresses/domains specified in the Range parameter are allowed access to the service to which the throttle policy is applied. Possible values are as follows. Allow : If this is selected, the specified IP addresses/domains are allowed to access the services to which the throttle ID is applied without any restrictions. Deny : If this is selected, specified IP addresses/domains are not allowed to access the services to which the throttle ID is applied . Control : If this is selected, the specified IP addresses/domains a re allowed to access the services to which the throttle ID is applied. However, the number of times they can access the services is controlled by the Max Request Count , Unit Time (ms) and the Prohibit Time Period (ms) parameters. Action This parameter can be used to delete the entry. On Acceptance \u00b6 This section is used to specify the mediation sequence that should be applied when a request is accepted based on the throttle policy defined for the Throttle mediator. The parameters available to be configured in this section are as follows. Parameter Name Description Specify As This parameter is used to specify how the On Acceptance sequence is defined. The following options are available. In-Lined Policy : If this is selected, the mediation sequence to be applied to accepted requests can be defined within the Throttle mediator configuration. Click on the OnAccept node in the mediation tree to define the sequence in-line. Referring Policy : If this is selected, you can refer to a pre-defined mediation sequence in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the Resource Tree. On Rejection \u00b6 This section is used to specify the mediation sequence that should be applied when a request is rejected based on the throttle policy defined for the Throttle mediator. The parameters available to be configured in this section are as follows. Parameter Name Description Specify As This parameter is used to specify how the On Acceptance sequence is defined. The following options are available. In-Lined Policy : If this is selected, the mediation sequence to be applied to rejected requests can be defined within the Throttle mediator configuration. Click on the OnReject node in the mediation tree to define the sequence in-line. Referring Policy : If this is selected, you can refer to a pre-defined mediation sequence in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the Resource Tree. Examples \u00b6 In this example, the Throttle Mediator inside the In Mediator . Therefore, all request messages directed to the main sequence will be subjected to throttling. The Throttle Mediator has policy , onAccept and onReject tags at top level. The policy tag specifies the throttling policy for throttling messages. The onAccept sequence includes a Log mediator with a custom log to log the accepted requests.Then the Send mediator sends these requests to http://localhost:9000/services/SimpleStockQuoteService . The OnReject sequence too includes a Log mediator with a custom log to log the rejected requests. Then a Fault mediator is used to convert the message into a fault message. The fault message is then returned to the client as a response using the Respond mediator , and then dropped using the Drop mediator . Example for a concurrency-based policy \u00b6 This sample policy only contains a component called MaximumConcurrentAccess . This indicates the maximum number of concurrent requests that can pass through Synapse on a single unit of time, and this value applies to all the IP addresses and domains. <in> <throttle id=\"A\"> <policy> <!-- define throttle policy --> <wsp:Policy xmlns:wsp=\"http://schemas.xmlsoap.org/ws/2004/09/policy\" xmlns:throttle=\"http://www.wso2.org/products/wso2commons/throttle\"> <throttle:ThrottleAssertion> <throttle:MaximumConcurrentAccess>10</throttle:MaximumConcurrentAccess> </throttle:ThrottleAssertion> </wsp:Policy> </policy> <onAccept> <log level=\"custom\"> <property name=\"text\" value=\"**Access Accept**\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </onAccept> <onReject> <log level=\"custom\"> <property name=\"text\" value=\"**Access Denied**\"/> </log> <makefault> <code value=\"tns:Receiver\" xmlns:tns=\"http://www.w3.org/2003/05/soap-envelope\"/> <reason value=\"**Access Denied**\"/> </makefault> <respond/> <drop/> </onReject> </throttle> </in> Example for a rates-based policy \u00b6 This sample policy only contains a rates-based policy. This indicates the maximum number of concurrent requests that can pass through Synapse on a single unit of time, and this value applies to all the IP addresses and domains. <in> <throttle id=\"A\"> <policy> <!-- define throttle policy --> <wsp:Policy xmlns:wsp=\"http://schemas.xmlsoap.org/ws/2004/09/policy\" xmlns:throttle=\"http://www.wso2.org/products/wso2commons/throttle\"> <throttle:MaximumCount>4</throttle:MaximumCount> <throttle:UnitTime>800000</throttle:UnitTime> <throttle:ProhibitTimePeriod wsp:Optional=\"true\">1000</throttle:ProhibitTimePeriod> </wsp:Policy> </policy> <onAccept> <log level=\"custom\"> <property name=\"text\" value=\"**Access Accept**\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </onAccept> <onReject> <log level=\"custom\"> <property name=\"text\" value=\"**Access Denied**\"/> </log> <makefault> <code value=\"tns:Receiver\" xmlns:tns=\"http://www.w3.org/2003/05/soap-envelope\"/> <reason value=\"**Access Denied**\"/> </makefault> <respond/> <drop/> </onReject> </throttle> </in>","title":"Throttle Mediator"},{"location":"references/throttle-Mediator/#throttle-mediator","text":"The Throttle Mediator can be used to restrict access to services. This is useful when services used at the enterprise level and it is required to avoid heavy loads that can cause performance issues in the system. It can also be used when you want to avoid certain user groups (i.e. IP addresses and domains) accessing your system. The Throttle mediator defines a throttle group which includes the following. A throttle policy which defines the extent to which, individuals and groups of IP addresses/domains should be allowed to access the service. A mediation sequence to handle requests that were accepted based on the throttle policy. A mediation sequence to handle requests that were rejected based on the throttle policy. Info The Throttle mediator is a content unaware mediator. Syntax | Configuration | Examples","title":"Throttle Mediator"},{"location":"references/throttle-Mediator/#syntax","text":"<throttle [onReject=\"string\"] [onAccept=\"string\"] id=\"string\"> (<policy key=\"string\"/> | <policy>..</policy>) <onReject>..</onReject>? <onAccept>..</onAccept>? </throttle>","title":"Syntax"},{"location":"references/throttle-Mediator/#configuration","text":"The configuration of the Throttle mediator are divided into following sections. Before you edit these sections, enter an ID for the Throttle group in the Throttle Group ID parameter. Throttle Policy On Acceptance On Rejection","title":"Configuration"},{"location":"references/throttle-Mediator/#throttle-policy","text":"This section is used to specify the throttle policy that should apply to the requests passing through the Throttle mediator. A throttle policy has a number of entries defining the extent to which, an individual or a group of IP addresses/domains should be allowed to access the service. The parameters available to be configured in this section are as follows. Parameter Name Description Throttle Policy This section is used to specify the policy for throttling. The following options are available. In-Lined Policy : If this is selected, the Throttle policy can be defined within the Throttle mediator configuration. Click Throttle Policy Editor to open the Mediator Throttling Configuration dialog box where the details relating to the Throttle policy can be entered. The parameters in this dialog box are described in the table below. Referring Policy : If this is selected, you can refer to a pre-defined Throttle policy which is saved in the Registry. You can enter the key to access the policy in the Referring Policy parameter. Click on either Configuration Registry or Governance Registry to select the relevant policy from the Resource Tree. The parameters available in the Mediator Throttling Configuration dialog box to configure the Throttling policy are as follows. Parameter Name Description Maximum Concurrent Accesses The maximum number of messages that are served at a given time. The number of messages between the inflow throttle handler and the outflow throttle handler cannot exceed the value entered for this parameter at any given time. This parameter value is applied to the entire system. It is not restricted to one or more specific IP addresses/domains. !!! info When this parameter is used, the same Throttle mediator ID should be included in the response flow so that the completed responses are deducted from the available limit. Range This parameter is used to specify the IP addresses/domains to which the entry in the current row should be applied If you want to apply the entry to a range of IP addresses, enter the range in this parameter, e.g., 8.100.1.30 \u2013 8.100.1.45 . Alternatively, you can enter a single IP address to apply the entry to only one IP address. If you want to apply the entry to a domain, enter the required domain ID in this parameter. If you want to apply the entry to all the IP addresses/domains that are not configured in the other configurations, enter other in this parameter. Type This parameter is used to specify whether the value(s) entered in the Range parameter refers to IP addresses or domains. Max Request Count This parameter specifies the maximum number of requests that should be handled within the time interval specified in the Unit Time parameter. !!! tip This parameter is applicable only when the value selected for the Access parameter is Control . Unit Time (ms) The time interval for which the maximum number of requests specified for the Throttle ID in the Max Request Count parameter apply. !!! tip This parameter is applicable only when the value selected for the Access parameter is Control . Prohibit Time Period (ms) If the number of requests entered in the Max Request Count parameter is achieved before the time interval entered in the Unit Time (ms) parameter has elapsed, no more requests are taken by the inflow throttle handler for the time period entered in this parameter. Entering a value in this parameter alters the unit time. For example: Max Request Count = 50 Unit Time = 50000 ms Prohibit Time Period = 5000 ms If 50 requests are received within 50000 milliseconds , no requests will be taken for the next 5000 milliseconds. Thus, the time slot considered as the unit time is changed to 40000 milliseconds. If no value is entered in the Prohibit Time Period (ms) parameter, no requests will be taken until 15000 more milliseconds (i.e. the remainder of the unit time) have elapsed. !!! tip This parameter is applicable only when the value selected for the Access parameter is Control . Access This parameter is used to specify the extent to which the IP addresses/domains specified in the Range parameter are allowed access to the service to which the throttle policy is applied. Possible values are as follows. Allow : If this is selected, the specified IP addresses/domains are allowed to access the services to which the throttle ID is applied without any restrictions. Deny : If this is selected, specified IP addresses/domains are not allowed to access the services to which the throttle ID is applied . Control : If this is selected, the specified IP addresses/domains a re allowed to access the services to which the throttle ID is applied. However, the number of times they can access the services is controlled by the Max Request Count , Unit Time (ms) and the Prohibit Time Period (ms) parameters. Action This parameter can be used to delete the entry.","title":"Throttle Policy"},{"location":"references/throttle-Mediator/#on-acceptance","text":"This section is used to specify the mediation sequence that should be applied when a request is accepted based on the throttle policy defined for the Throttle mediator. The parameters available to be configured in this section are as follows. Parameter Name Description Specify As This parameter is used to specify how the On Acceptance sequence is defined. The following options are available. In-Lined Policy : If this is selected, the mediation sequence to be applied to accepted requests can be defined within the Throttle mediator configuration. Click on the OnAccept node in the mediation tree to define the sequence in-line. Referring Policy : If this is selected, you can refer to a pre-defined mediation sequence in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the Resource Tree.","title":"On Acceptance"},{"location":"references/throttle-Mediator/#on-rejection","text":"This section is used to specify the mediation sequence that should be applied when a request is rejected based on the throttle policy defined for the Throttle mediator. The parameters available to be configured in this section are as follows. Parameter Name Description Specify As This parameter is used to specify how the On Acceptance sequence is defined. The following options are available. In-Lined Policy : If this is selected, the mediation sequence to be applied to rejected requests can be defined within the Throttle mediator configuration. Click on the OnReject node in the mediation tree to define the sequence in-line. Referring Policy : If this is selected, you can refer to a pre-defined mediation sequence in the registry. Click either Configuration Registry or Governance Registry as relevant to select the required sequence from the Resource Tree.","title":"On Rejection"},{"location":"references/throttle-Mediator/#examples","text":"In this example, the Throttle Mediator inside the In Mediator . Therefore, all request messages directed to the main sequence will be subjected to throttling. The Throttle Mediator has policy , onAccept and onReject tags at top level. The policy tag specifies the throttling policy for throttling messages. The onAccept sequence includes a Log mediator with a custom log to log the accepted requests.Then the Send mediator sends these requests to http://localhost:9000/services/SimpleStockQuoteService . The OnReject sequence too includes a Log mediator with a custom log to log the rejected requests. Then a Fault mediator is used to convert the message into a fault message. The fault message is then returned to the client as a response using the Respond mediator , and then dropped using the Drop mediator .","title":"Examples"},{"location":"references/throttle-Mediator/#example-for-a-concurrency-based-policy","text":"This sample policy only contains a component called MaximumConcurrentAccess . This indicates the maximum number of concurrent requests that can pass through Synapse on a single unit of time, and this value applies to all the IP addresses and domains. <in> <throttle id=\"A\"> <policy> <!-- define throttle policy --> <wsp:Policy xmlns:wsp=\"http://schemas.xmlsoap.org/ws/2004/09/policy\" xmlns:throttle=\"http://www.wso2.org/products/wso2commons/throttle\"> <throttle:ThrottleAssertion> <throttle:MaximumConcurrentAccess>10</throttle:MaximumConcurrentAccess> </throttle:ThrottleAssertion> </wsp:Policy> </policy> <onAccept> <log level=\"custom\"> <property name=\"text\" value=\"**Access Accept**\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </onAccept> <onReject> <log level=\"custom\"> <property name=\"text\" value=\"**Access Denied**\"/> </log> <makefault> <code value=\"tns:Receiver\" xmlns:tns=\"http://www.w3.org/2003/05/soap-envelope\"/> <reason value=\"**Access Denied**\"/> </makefault> <respond/> <drop/> </onReject> </throttle> </in>","title":"Example for a concurrency-based policy"},{"location":"references/throttle-Mediator/#example-for-a-rates-based-policy","text":"This sample policy only contains a rates-based policy. This indicates the maximum number of concurrent requests that can pass through Synapse on a single unit of time, and this value applies to all the IP addresses and domains. <in> <throttle id=\"A\"> <policy> <!-- define throttle policy --> <wsp:Policy xmlns:wsp=\"http://schemas.xmlsoap.org/ws/2004/09/policy\" xmlns:throttle=\"http://www.wso2.org/products/wso2commons/throttle\"> <throttle:MaximumCount>4</throttle:MaximumCount> <throttle:UnitTime>800000</throttle:UnitTime> <throttle:ProhibitTimePeriod wsp:Optional=\"true\">1000</throttle:ProhibitTimePeriod> </wsp:Policy> </policy> <onAccept> <log level=\"custom\"> <property name=\"text\" value=\"**Access Accept**\"/> </log> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </onAccept> <onReject> <log level=\"custom\"> <property name=\"text\" value=\"**Access Denied**\"/> </log> <makefault> <code value=\"tns:Receiver\" xmlns:tns=\"http://www.w3.org/2003/05/soap-envelope\"/> <reason value=\"**Access Denied**\"/> </makefault> <respond/> <drop/> </onReject> </throttle> </in>","title":"Example for a rates-based policy"},{"location":"references/transaction-Mediator-Example/","text":"Transaction Mediator Example \u00b6 The Transaction mediator supports distributed transactions using the Java transaction API (JTA). When it comes to distributed transactions, it involves accessing and updating data on two or more networked computers, often using multiple databases. For example, two databases or a database and a message queue such as JMS. You can use the Synapse configuration language to define when to start, commit, or roll back the transaction. For example, you can mark the start of a transaction at the start of a database commit, mark the end of the transaction at the end of database commit and roll back the transaction if an error occurs. Let's explore a basic transaction mediator scenario that demonstrates how the transaction mediator can be used to manage complex distributed transactions. Transaction mediator scenario | Prerequisites | Configuring the example scenario | Testing the example scenario Transaction mediator scenario \u00b6 You have a record in one database and you want to delete that record from the first database and add it to a second database. Assume that these two databases can either be run on the same server or can be in two remote servers. The database tables are defined in a way that the same entry cannot be added twice. Therefore, in the successful scenario, the record will be deleted from the first database and will be added to the second database. In the failure scenario, the record is already in the second database, hence the record will not be deleted from the first table nor will it be added into the second database. Prerequisites \u00b6 Windows, Linux or Solaris operating systems with WSO2 EI installed. Apache Derby database server. If you do not have the Apache Derby database set up, d ownload the Apache Derby distribution from http://db.apache.org . Configuring the example scenario \u00b6 Copy and paste the following configuration into \\< EI_HOME>/repository/deployment/server/synapse-configs/<node>/synapse.xml . The sample configuration used here is similar to sample 361 . Here via the In sequence a message is sent to the service, and via the Out sequence an entry from the first database is deleted and the second database is updated with that entry. If an entry that already exists is added once again to the second database, the entire transaction will roll back. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"myFaultHandler\"> <log level=\"custom\"> <property name=\"text\" value=\"** Rollback Transaction**\"/> </log> <transaction action=\"rollback\"/> <send/> </sequence> <sequence name=\"main\" onError=\"myFaultHandler\"> <in> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </in> <out> <transaction action=\"new\"/> <log level=\"custom\"> <property name=\"text\" value=\"** Reporting to the Database EIdb**\"/> </log> <dbreport useTransaction=\"true\" xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <dsName>java:jdbc/XADerbyDS</dsName> <icClass>org.jnp.interfaces.NamingContextFactory</icClass> <url>localhost:1099</url> <user>EI</user> <password>EI</password> </pool> </connection> <statement> <sql>delete from company where name =?</sql> <parameter expression=\"//m0:return/m1:symbol/child::text()\" xmlns:m0=\"http://services.samples\" xmlns:m1=\"http://services.samples/xsd\" type=\"VARCHAR\"/> </statement> </dbreport> <log level=\"custom\"> <property name=\"text\" value=\"** Reporting to the Database EIdb1**\"/> </log> <dbreport useTransaction=\"true\" xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <dsName>java:jdbc/XADerbyDS1</dsName> <icClass>org.jnp.interfaces.NamingContextFactory</icClass> <url>localhost:1099</url> <user>EI</user> <password>EI</password> </pool> </connection> <statement> <sql>INSERT into company values (?,'c4',?)</sql> <parameter expression=\"//m0:return/m1:symbol/child::text()\" xmlns:m1=\"http://services.samples/xsd\" xmlns:m0=\"http://services.samples\" type=\"VARCHAR\"/> <parameter expression=\"//m0:return/m1:last/child::text()\" xmlns:m1=\"http://services.samples/xsd\" xmlns:m0=\"http://services.samples\" type=\"DOUBLE\"/> </statement> </dbreport> <transaction action=\"commit\"/> <send/> </out> </sequence> </definitions> Setup two distributed Derby databases EIdb and EIdb1. For instructions on setting up the Derby databases, see Setting up the Derby database server . Create a table in EIdb by executing the following statement. CREATE table company(name varchar(10) primary key, id varchar(10), price double); Create a table in EIdb1 by executing the following statement. CREATE table company(name varchar(10) primary key, id varchar(10), price double); Insert records into the two tables that you created by executing the following statements. To insert records into the table in EIdb INSERT into company values ('IBM','c1',0.0); INSERT into company values ('SUN','c2',0.0); To insert records into the table in EIdb1 INSERT into company values ('SUN','c2',0.0); INSERT into company values ('MSFT','c3',0.0); !!! info Note When inserting records into the tables, the order of the record matters. Create the following data source declarations for the distributed databases in the <EI_HOME>/conf/datasources/ master-datasources.xml file. <datasources> <xa-datasource> <jndi-name>jdbc/XADerbyDS</jndi-name> <isSameRM-override-value>false</isSameRM-override-value> <xa-datasource-class>org.apache.derby.jdbc.ClientXADataSource</xa-datasource-class> <xa-datasource-property name=\"portNumber\">1527</xa-datasource-property> <xa-datasource-property name=\"DatabaseName\">EIdb</xa-datasource-property> <xa-datasource-property name=\"User\">EI</xa-datasource-property> <xa-datasource-property name=\"Password\">EI</xa-datasource-property> <metadata> <type-mapping>Derby</type-mapping> </metadata> </xa-datasource> </datasources> <datasources> <xa-datasource> <jndi-name>jdbc/XADerbyDS1</jndi-name> <isSameRM-override-value>false</isSameRM-override-value> <xa-datasource-class>org.apache.derby.jdbc.ClientXADataSource</xa-datasource-class> <xa-datasource-property name=\"portNumber\">1527</xa-datasource-property> <xa-datasource-property name=\"DatabaseName\">EIdb1</xa-datasource-property> <xa-datasource-property name=\"User\">EI</xa-datasource-property> <xa-datasource-property name=\"Password\">EI</xa-datasource-property> <metadata> <type-mapping>Derby</type-mapping> </metadata> </xa-datasource> </datasources> Deploy the back-end service SimpleStockQuoteService . For instructions on deploying sample back-end services, see Deploying sample back-end services . Start the Axis2 server. For instructions on starting the Axis2 server, see Starting the Axis2 server . Testing the example scenario \u00b6 To test the successful scenario Execute the following command from the <EI_HOME>/samples/axis2Client directory. ant stockquote -Daddurl=http://localhost:9000/services/SimpleStockQuoteService -Dtrpurl=http://localhost:8280/ -Dsymbol=IBM Executing this command removes the IBM record from the first database and adds it to the second database. When you check both databases you will see that the IBM record is deleted from the first database and added to the second database. To test the f ailure scenario Execute the following command from the <EI_HOME>/samples/axis2Client directory. ant stockquote -Daddurl=http://localhost:9000/services/SimpleStockQuoteService -Dtrpurl=http://localhost:8280/ -Dsymbol=SUN Executing this command attempts to add an already existing record again to the second database, which results in the fault sequence being executed. You will see an exception raised for duplicate entries and the entire transaction will roll back. When you check both databases you will see that a record is neither deleted from the first database nor added into the second database.","title":"Transaction Mediator Example"},{"location":"references/transaction-Mediator-Example/#transaction-mediator-example","text":"The Transaction mediator supports distributed transactions using the Java transaction API (JTA). When it comes to distributed transactions, it involves accessing and updating data on two or more networked computers, often using multiple databases. For example, two databases or a database and a message queue such as JMS. You can use the Synapse configuration language to define when to start, commit, or roll back the transaction. For example, you can mark the start of a transaction at the start of a database commit, mark the end of the transaction at the end of database commit and roll back the transaction if an error occurs. Let's explore a basic transaction mediator scenario that demonstrates how the transaction mediator can be used to manage complex distributed transactions. Transaction mediator scenario | Prerequisites | Configuring the example scenario | Testing the example scenario","title":"Transaction Mediator Example"},{"location":"references/transaction-Mediator-Example/#transaction-mediator-scenario","text":"You have a record in one database and you want to delete that record from the first database and add it to a second database. Assume that these two databases can either be run on the same server or can be in two remote servers. The database tables are defined in a way that the same entry cannot be added twice. Therefore, in the successful scenario, the record will be deleted from the first database and will be added to the second database. In the failure scenario, the record is already in the second database, hence the record will not be deleted from the first table nor will it be added into the second database.","title":"Transaction mediator scenario"},{"location":"references/transaction-Mediator-Example/#prerequisites","text":"Windows, Linux or Solaris operating systems with WSO2 EI installed. Apache Derby database server. If you do not have the Apache Derby database set up, d ownload the Apache Derby distribution from http://db.apache.org .","title":"Prerequisites"},{"location":"references/transaction-Mediator-Example/#configuring-the-example-scenario","text":"Copy and paste the following configuration into \\< EI_HOME>/repository/deployment/server/synapse-configs/<node>/synapse.xml . The sample configuration used here is similar to sample 361 . Here via the In sequence a message is sent to the service, and via the Out sequence an entry from the first database is deleted and the second database is updated with that entry. If an entry that already exists is added once again to the second database, the entire transaction will roll back. <definitions xmlns=\"http://ws.apache.org/ns/synapse\"> <sequence name=\"myFaultHandler\"> <log level=\"custom\"> <property name=\"text\" value=\"** Rollback Transaction**\"/> </log> <transaction action=\"rollback\"/> <send/> </sequence> <sequence name=\"main\" onError=\"myFaultHandler\"> <in> <send> <endpoint> <address uri=\"http://localhost:9000/services/SimpleStockQuoteService\"/> </endpoint> </send> </in> <out> <transaction action=\"new\"/> <log level=\"custom\"> <property name=\"text\" value=\"** Reporting to the Database EIdb**\"/> </log> <dbreport useTransaction=\"true\" xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <dsName>java:jdbc/XADerbyDS</dsName> <icClass>org.jnp.interfaces.NamingContextFactory</icClass> <url>localhost:1099</url> <user>EI</user> <password>EI</password> </pool> </connection> <statement> <sql>delete from company where name =?</sql> <parameter expression=\"//m0:return/m1:symbol/child::text()\" xmlns:m0=\"http://services.samples\" xmlns:m1=\"http://services.samples/xsd\" type=\"VARCHAR\"/> </statement> </dbreport> <log level=\"custom\"> <property name=\"text\" value=\"** Reporting to the Database EIdb1**\"/> </log> <dbreport useTransaction=\"true\" xmlns=\"http://ws.apache.org/ns/synapse\"> <connection> <pool> <dsName>java:jdbc/XADerbyDS1</dsName> <icClass>org.jnp.interfaces.NamingContextFactory</icClass> <url>localhost:1099</url> <user>EI</user> <password>EI</password> </pool> </connection> <statement> <sql>INSERT into company values (?,'c4',?)</sql> <parameter expression=\"//m0:return/m1:symbol/child::text()\" xmlns:m1=\"http://services.samples/xsd\" xmlns:m0=\"http://services.samples\" type=\"VARCHAR\"/> <parameter expression=\"//m0:return/m1:last/child::text()\" xmlns:m1=\"http://services.samples/xsd\" xmlns:m0=\"http://services.samples\" type=\"DOUBLE\"/> </statement> </dbreport> <transaction action=\"commit\"/> <send/> </out> </sequence> </definitions> Setup two distributed Derby databases EIdb and EIdb1. For instructions on setting up the Derby databases, see Setting up the Derby database server . Create a table in EIdb by executing the following statement. CREATE table company(name varchar(10) primary key, id varchar(10), price double); Create a table in EIdb1 by executing the following statement. CREATE table company(name varchar(10) primary key, id varchar(10), price double); Insert records into the two tables that you created by executing the following statements. To insert records into the table in EIdb INSERT into company values ('IBM','c1',0.0); INSERT into company values ('SUN','c2',0.0); To insert records into the table in EIdb1 INSERT into company values ('SUN','c2',0.0); INSERT into company values ('MSFT','c3',0.0); !!! info Note When inserting records into the tables, the order of the record matters. Create the following data source declarations for the distributed databases in the <EI_HOME>/conf/datasources/ master-datasources.xml file. <datasources> <xa-datasource> <jndi-name>jdbc/XADerbyDS</jndi-name> <isSameRM-override-value>false</isSameRM-override-value> <xa-datasource-class>org.apache.derby.jdbc.ClientXADataSource</xa-datasource-class> <xa-datasource-property name=\"portNumber\">1527</xa-datasource-property> <xa-datasource-property name=\"DatabaseName\">EIdb</xa-datasource-property> <xa-datasource-property name=\"User\">EI</xa-datasource-property> <xa-datasource-property name=\"Password\">EI</xa-datasource-property> <metadata> <type-mapping>Derby</type-mapping> </metadata> </xa-datasource> </datasources> <datasources> <xa-datasource> <jndi-name>jdbc/XADerbyDS1</jndi-name> <isSameRM-override-value>false</isSameRM-override-value> <xa-datasource-class>org.apache.derby.jdbc.ClientXADataSource</xa-datasource-class> <xa-datasource-property name=\"portNumber\">1527</xa-datasource-property> <xa-datasource-property name=\"DatabaseName\">EIdb1</xa-datasource-property> <xa-datasource-property name=\"User\">EI</xa-datasource-property> <xa-datasource-property name=\"Password\">EI</xa-datasource-property> <metadata> <type-mapping>Derby</type-mapping> </metadata> </xa-datasource> </datasources> Deploy the back-end service SimpleStockQuoteService . For instructions on deploying sample back-end services, see Deploying sample back-end services . Start the Axis2 server. For instructions on starting the Axis2 server, see Starting the Axis2 server .","title":"Configuring the example scenario"},{"location":"references/transaction-Mediator-Example/#testing-the-example-scenario","text":"To test the successful scenario Execute the following command from the <EI_HOME>/samples/axis2Client directory. ant stockquote -Daddurl=http://localhost:9000/services/SimpleStockQuoteService -Dtrpurl=http://localhost:8280/ -Dsymbol=IBM Executing this command removes the IBM record from the first database and adds it to the second database. When you check both databases you will see that the IBM record is deleted from the first database and added to the second database. To test the f ailure scenario Execute the following command from the <EI_HOME>/samples/axis2Client directory. ant stockquote -Daddurl=http://localhost:9000/services/SimpleStockQuoteService -Dtrpurl=http://localhost:8280/ -Dsymbol=SUN Executing this command attempts to add an already existing record again to the second database, which results in the fault sequence being executed. You will see an exception raised for duplicate entries and the entire transaction will roll back. When you check both databases you will see that a record is neither deleted from the first database nor added into the second database.","title":"Testing the example scenario"},{"location":"references/transaction-Mediator/","text":"Transaction Mediator \u00b6 A transaction is a set of operations executed as a single unit. It also can be defined as an agreement, which is carried out between separate entities or objects. The Transaction Mediator is used to manage distributed transactions in the ESB profile of WSO2 EI by providing transaction functionality for its child mediators. Info In addition to distributed transactions, the ESB profile also supports Java Message Service (JMS) transactions. For more information on transactions, see Working with Transactions . Syntax | Configuration | Examples Syntax \u00b6 <transaction action=\"commit|fault-if-no-tx|new|resume|suspend|rollback|use-existing-or-new\"/> Configuration \u00b6 The Action parameter is used to select a transaction action to be performed. Available values are as follows. Action Description Commit Transaction (commit) This marks the transaction as completed and ends the transaction. Fault if no Transaction (fault-if-no-tx) This goes to the error handler if there is no transaction. Initiate new Transaction (new) This provides the entry point for a new transaction. Resume Transaction (resume) This resumes a paused transaction. Suspend Transaction (suspend) This pauses a transaction. Rollback Transaction (rollback This rolls back a transaction. Use existing or Initiate Transaction (use-existing-or new) If a transaction already exists, this value continues it. If no transaction already exists, a new transaction will be created. Examples \u00b6 For an example of using the Transaction mediator, see Transaction Mediator Example .","title":"Transaction Mediator"},{"location":"references/transaction-Mediator/#transaction-mediator","text":"A transaction is a set of operations executed as a single unit. It also can be defined as an agreement, which is carried out between separate entities or objects. The Transaction Mediator is used to manage distributed transactions in the ESB profile of WSO2 EI by providing transaction functionality for its child mediators. Info In addition to distributed transactions, the ESB profile also supports Java Message Service (JMS) transactions. For more information on transactions, see Working with Transactions . Syntax | Configuration | Examples","title":"Transaction Mediator"},{"location":"references/transaction-Mediator/#syntax","text":"<transaction action=\"commit|fault-if-no-tx|new|resume|suspend|rollback|use-existing-or-new\"/>","title":"Syntax"},{"location":"references/transaction-Mediator/#configuration","text":"The Action parameter is used to select a transaction action to be performed. Available values are as follows. Action Description Commit Transaction (commit) This marks the transaction as completed and ends the transaction. Fault if no Transaction (fault-if-no-tx) This goes to the error handler if there is no transaction. Initiate new Transaction (new) This provides the entry point for a new transaction. Resume Transaction (resume) This resumes a paused transaction. Suspend Transaction (suspend) This pauses a transaction. Rollback Transaction (rollback This rolls back a transaction. Use existing or Initiate Transaction (use-existing-or new) If a transaction already exists, this value continues it. If no transaction already exists, a new transaction will be created.","title":"Configuration"},{"location":"references/transaction-Mediator/#examples","text":"For an example of using the Transaction mediator, see Transaction Mediator Example .","title":"Examples"},{"location":"references/uRLRewrite-Mediator/","text":"URLRewrite Mediator \u00b6 The URLRewrite Mediator is used to modify and transform the URL values available in messages. This can be done by defining a rewrite action for each fragment of a selected property value. Alternatively, you can rewrite the entire URL string at once. Info The URLRewrite mediator is a content aware mediator. Syntax | Configuration | Example Syntax \u00b6 <rewrite [inProperty=\"string\"] [outProperty=\"string\"]> <rewriterule> <condition> ... </condition>? <action [type=\"append|prepend|replace|remove|set\"] [value=\"string\"] [xpath=\"xpath\"] [fragment=\"protocol|host|port|path|query|ref|user|full\"] [regex=\"regex\"]>+ </rewriterule>+ </rewrite> Configuration \u00b6 The parameters available to configure the URL Rewrite mediator are as follows. Parameter Name Description In Property This parameter is used to enter property of which the value should be considered the input URL. The rewrite rules are applied to the value of the property entered in this parameter to generate the result URL. If no property is entered, the rewrite rules will be applied to the To header of the message. Out Property This parameter is used to enter the property to which the transformations done via the rewrite rules should be applied. If no property is entered, the transformations will be applied to the To header of the message. The Rewrite mediator applies the URL transformations by evaluating a set of rules on the message. To add a rule to the mediator, click Add Rule . The rewrite rule will be added to the mediator tree as a child of the URLRewrite mediator. Click URLRewriteRule in the mediator to configure the rewrite rule you added. A rule can consist of one or more rewrite actions and an optional condition. The Condition parameter is used to enter the optional condition. If a condition is specified, it will be evaluated before the rewrite actions, and the rewrite actions will be executed only if the condition evaluates to true . A rewrite action is added by clicking Add Action which would display a row. The parameters available to configure a rewrite action are as follows. Parameter Name Description Action This parameter is used to specify the action to be performed by the rewrite action. Each rewrite action is performed on a fragment entered in the Fragment parameter. Possible values are as follows. Replace : If this is selected, the existing in property value fragment will be replaced by the result value. Remove : If this is selected, the result value will be removed from the in property value fragment. Append : If this is selected, the result value will be added to the end of the in property value fragment. Prepend : If this is selected, the result value will be added to the beginning of the in property value fragment. Set : If this is selected, the result value will be set as the in property value fragment. Fragment The fragment of the in property (i.e. input URL) for which the rewrite action should be performed. The available fragments are as follows. Protocol : Host Port Path Query Ref User Full !!! info Note that this breakdown is inline with the URI specification (RFC2396). URL rewrite mediator enables rewriting each of the above segments separately and finally combining them to get the final URL value. It also supports rewriting the entire URL string at once. Option This parameter is used to define the result value of the rewrite action. Select one of the following. Value : If this is selected, the result value would be a static value. Expression : If this is selected, the result value will be evaluated using an expression. Value/Expression This parameter is used to enter the result value of the URLRewrite mediator as a static value or an expression, depending on what you selected for the Option parameter. Namespace Editor You can click this link to add namespaces when you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Regex This parameter is used to specify which part of the in property value fragment should be replaced by the result value if you selected Replace for the Action parameter. Delete Click Delete in the relevant row to remove a rewrite action. Example \u00b6 In this example, the URLRewrite mediator has a rewrite action which replaces the value soap with value services in the fragment path of the input URL. Since no in property or an out property is specified, the To header of the request is both the input to which the rewrite rule is applied and the target where the result URL is set. This configuration is typically used when the address URL of a request contains the context soap which needs to be converted since all the services are deployed under a context named services in the EI server. Thus, the URL http://localhost:8280/soap/StockQuoteProxy1 is rewritten as http://localhost:8280/ services /StockQuoteProxy1 to ensure that the requests are successfully delivered to the server. <rewrite> <rewriterule> <action type=\"replace\" regex=\"soap\" value=\"services\" fragment=\"path\" /> </rewriterule> </rewrite> Samples \u00b6 For more examples, see: Sample 450: Introduction to the URLRewrite Mediator Sample 451: Conditional URL Rewriting Sample 452; Conditional URL Rewriting with Multiple Rules","title":"URLRewrite Mediator"},{"location":"references/uRLRewrite-Mediator/#urlrewrite-mediator","text":"The URLRewrite Mediator is used to modify and transform the URL values available in messages. This can be done by defining a rewrite action for each fragment of a selected property value. Alternatively, you can rewrite the entire URL string at once. Info The URLRewrite mediator is a content aware mediator. Syntax | Configuration | Example","title":"URLRewrite Mediator"},{"location":"references/uRLRewrite-Mediator/#syntax","text":"<rewrite [inProperty=\"string\"] [outProperty=\"string\"]> <rewriterule> <condition> ... </condition>? <action [type=\"append|prepend|replace|remove|set\"] [value=\"string\"] [xpath=\"xpath\"] [fragment=\"protocol|host|port|path|query|ref|user|full\"] [regex=\"regex\"]>+ </rewriterule>+ </rewrite>","title":"Syntax"},{"location":"references/uRLRewrite-Mediator/#configuration","text":"The parameters available to configure the URL Rewrite mediator are as follows. Parameter Name Description In Property This parameter is used to enter property of which the value should be considered the input URL. The rewrite rules are applied to the value of the property entered in this parameter to generate the result URL. If no property is entered, the rewrite rules will be applied to the To header of the message. Out Property This parameter is used to enter the property to which the transformations done via the rewrite rules should be applied. If no property is entered, the transformations will be applied to the To header of the message. The Rewrite mediator applies the URL transformations by evaluating a set of rules on the message. To add a rule to the mediator, click Add Rule . The rewrite rule will be added to the mediator tree as a child of the URLRewrite mediator. Click URLRewriteRule in the mediator to configure the rewrite rule you added. A rule can consist of one or more rewrite actions and an optional condition. The Condition parameter is used to enter the optional condition. If a condition is specified, it will be evaluated before the rewrite actions, and the rewrite actions will be executed only if the condition evaluates to true . A rewrite action is added by clicking Add Action which would display a row. The parameters available to configure a rewrite action are as follows. Parameter Name Description Action This parameter is used to specify the action to be performed by the rewrite action. Each rewrite action is performed on a fragment entered in the Fragment parameter. Possible values are as follows. Replace : If this is selected, the existing in property value fragment will be replaced by the result value. Remove : If this is selected, the result value will be removed from the in property value fragment. Append : If this is selected, the result value will be added to the end of the in property value fragment. Prepend : If this is selected, the result value will be added to the beginning of the in property value fragment. Set : If this is selected, the result value will be set as the in property value fragment. Fragment The fragment of the in property (i.e. input URL) for which the rewrite action should be performed. The available fragments are as follows. Protocol : Host Port Path Query Ref User Full !!! info Note that this breakdown is inline with the URI specification (RFC2396). URL rewrite mediator enables rewriting each of the above segments separately and finally combining them to get the final URL value. It also supports rewriting the entire URL string at once. Option This parameter is used to define the result value of the rewrite action. Select one of the following. Value : If this is selected, the result value would be a static value. Expression : If this is selected, the result value will be evaluated using an expression. Value/Expression This parameter is used to enter the result value of the URLRewrite mediator as a static value or an expression, depending on what you selected for the Option parameter. Namespace Editor You can click this link to add namespaces when you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Regex This parameter is used to specify which part of the in property value fragment should be replaced by the result value if you selected Replace for the Action parameter. Delete Click Delete in the relevant row to remove a rewrite action.","title":"Configuration"},{"location":"references/uRLRewrite-Mediator/#example","text":"In this example, the URLRewrite mediator has a rewrite action which replaces the value soap with value services in the fragment path of the input URL. Since no in property or an out property is specified, the To header of the request is both the input to which the rewrite rule is applied and the target where the result URL is set. This configuration is typically used when the address URL of a request contains the context soap which needs to be converted since all the services are deployed under a context named services in the EI server. Thus, the URL http://localhost:8280/soap/StockQuoteProxy1 is rewritten as http://localhost:8280/ services /StockQuoteProxy1 to ensure that the requests are successfully delivered to the server. <rewrite> <rewriterule> <action type=\"replace\" regex=\"soap\" value=\"services\" fragment=\"path\" /> </rewriterule> </rewrite>","title":"Example"},{"location":"references/uRLRewrite-Mediator/#samples","text":"For more examples, see: Sample 450: Introduction to the URLRewrite Mediator Sample 451: Conditional URL Rewriting Sample 452; Conditional URL Rewriting with Multiple Rules","title":"Samples"},{"location":"references/using-Data-Mapper-Mediator-in-the-ESB-Profile/","text":"Using Data Mapper Mediator in the ESB Profile \u00b6 Prerequisites Introduction Creating the ESB configuration project Deploying the configurations Invoking the created REST API Prerequisites \u00b6 Set up the following prerequisites before you begin. Download and run WSO2 EI . For instructions, see Running the Product . Install the WSO2 EI Tooling to use the Data Mapper mediator, which supports the data mapping editor. For instructions, see Installing WSO2 Integration Studio . Download and launch a REST client into your web browser. For example, this guide uses the Postman REST client t o send the requests to the ESB profile and receive the responses. Introduction \u00b6 This sample demonstrates how to create a mapping configuration for different data formats using the Data Mapper mediator. It uses a simple the ESB profile configuration with only a Data Mapper mediator, and a Respond mediator to check the converted message. T he input employee message in XML format, and the output engineer message in JSON format, which is sent to the client as the response. Creating the ESB configuration project \u00b6 Follow the steps below to create an ESB configuration project to contain the Data Mapping configurations using WSO2 Ei Tooling . Open WSO2 EI Tooling. Right click on the Project Explorer area, click New , and then click EI Solution Project as shown below. !!! info WSO2 EI Tooling now provides this new option to create an **EI Solution Project** for you to define all different configurations you need for the project using a wizard. {width=\"600\"} Enter a name for the project, and untick Create Connector Exporter Project ( since you do not need Connectors in your configuration) in the following wizard page . {width=\"500\"} Click Finish .You view the following project files created in the Project Explorer tab. Right click ESBDataMappingProject workspace file, click New , and then click REST API as shown below, to create a new REST API project in the ESB profile . {width=\"500\"} Select Create A New API Artifact , and then click Next as shown below. {width=\"400\"} 6. Enter a name for the Synapse API Artifact, enter /convertMenu for Context to configure the REST API project to listen for POST requests on the /convertMenu URL, and then click Finish as shown below. 7. Drag and drop a Data Mapper mediator and a Respond mediator as shown below. 8. Click on the API Resource, and then click on its Properties tab, and select True as the value for the Post method as shown below, to create the API resource listening to POST requests . 9. Double click on the D ata Mapping mediator, t o configure it. Y ou view a dialog box to create a registry resource project. 10. Enter a name for the configuration, and point the Registry Resource project to save it as shown below. !!! tip This configuration name is the prefix used for the configuration files that you deploy to the EI server related to the Data Mapper. Since you created an ESB Solution project, it directly points you to that project to save in it. Otherwise, you need to click the **Create new project** link, to create a new Registry Resource project and then point to it . Click OK . You view the f ollowing Data Mapper diagram editor in the new WSO2 Data Mapper Graphical perspective. !!! info You can switch to another perspective by either selecting another in top toolbar tags or by c licking **Window-\\>Perspective-\\>Open Perspective-\\>Other** in the top menu bar. Create an XML file (e.g., input.xml ) by copying the following sample content of a food menu, and save it in your local file system. !!! tip Use this sample XML message to load the input format to the Data Mapper editor. <breakfast_menu> <food> <name>Belgian Waffles</name> <price>$5.95</price> <description>Two of our famous Belgian Waffles with plenty of real maple syrup</description> <calories>650</calories> <orgin>Belgian</orgin> <veg>true</veg> </food> <food> <name>Strawberry Belgian Waffles</name> <price>$7.95</price> <description>Light Belgian waffles covered with strawberries and whipped cream</description> <calories>900</calories> <orgin>Belgian</orgin> <veg>true</veg> </food> <food> <name>Berry-Berry Belgian Waffles</name> <price>$8.95</price> <description>Light Belgian waffles covered with an assortment of fresh berries and whipped cream</description> <calories>900</calories> <orgin>Belgian</orgin> <veg>true</veg> </food> <food> <name>French Toast</name> <price>$4.50</price> <description>Thick slices made from our homemade sourdough bread</description> <calories>600</calories> <orgin>French</orgin> <veg>true</veg> </food> <food> <name>Homestyle Breakfast</name> <price>$6.95</price> <description>Two eggs, bacon or sausage, toast, and our ever-popular hash browns</description> <calories>950</calories> <orgin>French</orgin> <veg>false</veg> </food> </breakfast_menu> Right-click on the top title bar of the Input box and, click Load Input as shown below . The operation palettes that appear on the left-hand side allows you to provide the input message format to begin the mapping. Select XML as the Resource Type as shown below. !!! info You can select one out of the following resource types , to load the input and output message formats to Data Mapper. - **XML:** to load a sample XML message and WSO2 Data Mapper Editor will generate the JSON schema to represent the XML according to the WSO2 Data Mapper Schema specification. - **JSON:** to load a sample JSON message. - **CSV:** to load a sample JSON/CSV message. ****For CSV you need to provide the column names as the first record** .** - **XSD:** to load an XSD schema file, which defines your XML message format. - **JSONSCHEMA:** to load a JSON schema for your message according to the WSO2 Data Mapper schema specification. - **CONNECTOR:** to map a message, which is an output of a Connector. Select the **Connector** **Type** in the **Input** box, and it will list down all available connectors. Then, select the operation from the menu that appears in front of Data Mapper mediator. Click the file system link in Select resource from , select the XML file you saved in your local file system in step 12 , and click Open . You view the input format loaded in the Input box in the editor as shown below. Create another XML file (e.g., output.xml ) by copying the following sample content of a food menu, and save it in your local file system. !!! tip Use this sample XML message to load the output format to the Data Mapper editor. <menu> <item> <name>Belgian Waffles</name> <price>$5.95</price> <calories>650</calories> <orgin>Belgian</orgin> <veg>true</veg> <description>Two of our famous Belgian Waffles with plenty of real maple syrup</description> </item> <item> <name>Strawberry Belgian Waffles</name> <price>$7.95</price> <calories>900</calories> <orgin>Belgian</orgin> <veg>true</veg> <description>Light Belgian waffles covered with strawberries and whipped cream</description> </item> <item> <name>Berry-Berry Belgian Waffles</name> <price>$8.95</price> <calories>900</calories> <orgin>Belgian</orgin> <veg>true</veg> <description>Light Belgian waffles covered with an assortment of fresh berries and whipped cream</description> </item> <item> <name>French Toast</name> <price>$4.50</price> <calories>600</calories> <orgin>French</orgin> <veg>true</veg> <description>Thick slices made from our homemade sourdough bread</description> </item> <item> <name>Homestyle Breakfast</name> <price>$6.95</price> <calories>950</calories> <orgin>French</orgin> <veg>false</veg> <description>Two eggs, bacon or sausage, toast, and our ever-popular hash browns</description> </item> </menu> Right-click on the top title bar of the Output box and, click Load Output as shown below . The operation palettes that appear on the left-hand side allows you to provide the output message format. Click the file system link in Select resource from , select the XML file you saved in your local file system in step 16 , and click Open . You view the input format loaded in the Output box in the editor as shown below. Check the Input and Output boxes with the sample messages, to see if the element types (i.e. (Arrays, Objects and Primitive values) are correctly identified or not. Following signs will help you to identify them correctly. {} - represents object elements [] - represents array elements \\<> - represents primitive field values A - represents XML attribute values Click on the Data Mapper mediator. You view the following in the Properties tab of the Data Mapper mediator configuration as shown below. Configuration: Script file that is used to execute the mapping. Input Schema: JSON schema, which represents the input message format. Output Schema: JSON schema, which represents the output message format. Input Type: Expected input message type (xml/json/csv). Output Type: Target output message type (xml/json/csv). Check if you set the input and type and output type correctly. !!! note If you do not set the input type and output type correctly in the mediator configuration, your mapping will fail during runtime. Follow the steps below to do the mapping using operators as shown below. !!! note - The mapping done in the below example is that: name is mapped via uppercase operator and calories undergo a mathematical calculation to get the output as follows: ` output calories =Round( (calories*1.13) + 6.75) ` - You can only connect primitive data values such as Strings, numbers, boolean and etc. You cannot map Array and object values. Drag and drop an Upper Case operator and connect the name in both Input and Output boxes to it. Connect price in the Input box to the same in the Output box. Connect description in the Input box to the same in the Output box. Connect origin in the Input box to the same in the Output box. Connect veg in the Input box to the same in the Output box. Dran and drop the following operators: Multiply , Add , Round ** ** Drag and drop a Constant operator, and enter 1.13 as its Value in the Properties section. !!! tip To update the titile of the **Constant** box with the value, save the diagram, close the **FoodMapping.datamapper\\_diagram** and re-open it by double-clicking on the **Data Mapper** icon in the **FoodMenuConversion.xml** file. Drag and drop another Constant operator, and enter 6.75 as its Value in the Properties section. Connect the calories variable in both the Input and Output boxes via the Operators as shown below. From To Input Box \u2192 calories Multiply Operator Constant 1.13 Multiply Operator Multiply Operator Add Operator Constant 6.75 Add Operator Add Operator Round Operator Round Operator Output Box \u2192 calories Press Ctrl+S keys in each tab, to save all the configurations. Deploying the configurations \u00b6 After creating the Data Mapper configurations, follow the steps below to deploy the created REST API and the configurations in the ESB profile by including them in a C-App. Open WSO2 EI Tooling. Expand the C-APP project that was created when you created the ESB Solution project (i.e. EIDataMappingProjectCompositeApplication ), and double-click on the POM file. You view the following screen to select project files into the C-APP. !!! info You need to refresh the screen to view the registry resource files . Once you refresh the screen, you view all the artifacts in the workspace. {width=\"800\"} Click the refresh button in the top right-hand corner to load newly added registry files, as shown below. Select the REST API file and the three registry resource files containing the mapping configuration, input schema, and output schema as shown below. Configuration: Script file that is used to execute the mapping. Input schema: JSON schema which represents the input message format. Output schema: JSON schema which represents the output message format. {width=\"700\"} Start WSO2 ESB server. For instructions, see Running the Product . Click the Servers Tab in WSO2 EI Tooling, and click the No servers are available. Click this link to create a new server... link as shown below. {width=\"800\"} Click WSO2 , click WSO2 Carbon remote server , and then click Next as shown below. Enter the URL of the WSO2 EI for Server URL , and click Finish as shown below. You view the WSO2 EI server added in the Servers tab as shown below. Right-click on WSO2 Carbon remote server at localhost , and then click Add & Remove . Select the C-App in the Available: box, click Add to move it to the Configured: box, and then click Finish as shown below. You view the C-App added to the WSO2 EI server as shown below. Log in to the WSO2 EI Management Console using the following URL and admin/admin credentials: https://\\<ESB_HOST>:\\<ESB_PORT>carbon/ Click Main , and then click APIs in the Service Bus menu. You view the deployed REST API invocation URL as shown below. {width=\"900\"} Invoking the created REST API \u00b6 Follow the steps below to test invoking the created REST API. Open Postman REST client. Enter the following details to create the client message, enter the content of the XML file you created in step 12 as the payload in the text area provided, and click Send as shown below. URL: http://:8280/convertMenu Method: POST Body: raw xml /application Message: Enter the inpu {width=\"900\"} You view the expected JSON message received as shown below. {width=\"900\"} Similarly, you can use the above instructions to check the following message conversions: T he input employee message in XML format, and the output engineer message in XML/JSON/CSV formats, which is sent to the client as the response. (i.e. XML->XML/JSON/CSV) T he input employee message in JSON format, and the output engineer message in XML/JSON/CSV formats, which is sent to the client as the response. (i.e. JSON->XML/JSON/CSV) T he input employee message in CSV format, and the output engineer message in XML/JSON/CSV formats, which is sent to the client as the response. (i.e. CSV->XML/JSON/CSV) Note In the above sample, the output message format is fully compatible to represent as JSON and CSV. However, t his is not guaranteed in every occasion. For example, if you have defined a complex XML output message with namespaces and attributes, JSON message or CSV will not be built as expected.","title":"Using Data Mapper Mediator in the ESB Profile"},{"location":"references/using-Data-Mapper-Mediator-in-the-ESB-Profile/#using-data-mapper-mediator-in-the-esb-profile","text":"Prerequisites Introduction Creating the ESB configuration project Deploying the configurations Invoking the created REST API","title":"Using Data Mapper Mediator in the ESB Profile"},{"location":"references/using-Data-Mapper-Mediator-in-the-ESB-Profile/#prerequisites","text":"Set up the following prerequisites before you begin. Download and run WSO2 EI . For instructions, see Running the Product . Install the WSO2 EI Tooling to use the Data Mapper mediator, which supports the data mapping editor. For instructions, see Installing WSO2 Integration Studio . Download and launch a REST client into your web browser. For example, this guide uses the Postman REST client t o send the requests to the ESB profile and receive the responses.","title":"Prerequisites"},{"location":"references/using-Data-Mapper-Mediator-in-the-ESB-Profile/#introduction","text":"This sample demonstrates how to create a mapping configuration for different data formats using the Data Mapper mediator. It uses a simple the ESB profile configuration with only a Data Mapper mediator, and a Respond mediator to check the converted message. T he input employee message in XML format, and the output engineer message in JSON format, which is sent to the client as the response.","title":"Introduction"},{"location":"references/using-Data-Mapper-Mediator-in-the-ESB-Profile/#creating-the-esb-configuration-project","text":"Follow the steps below to create an ESB configuration project to contain the Data Mapping configurations using WSO2 Ei Tooling . Open WSO2 EI Tooling. Right click on the Project Explorer area, click New , and then click EI Solution Project as shown below. !!! info WSO2 EI Tooling now provides this new option to create an **EI Solution Project** for you to define all different configurations you need for the project using a wizard. {width=\"600\"} Enter a name for the project, and untick Create Connector Exporter Project ( since you do not need Connectors in your configuration) in the following wizard page . {width=\"500\"} Click Finish .You view the following project files created in the Project Explorer tab. Right click ESBDataMappingProject workspace file, click New , and then click REST API as shown below, to create a new REST API project in the ESB profile . {width=\"500\"} Select Create A New API Artifact , and then click Next as shown below. {width=\"400\"} 6. Enter a name for the Synapse API Artifact, enter /convertMenu for Context to configure the REST API project to listen for POST requests on the /convertMenu URL, and then click Finish as shown below. 7. Drag and drop a Data Mapper mediator and a Respond mediator as shown below. 8. Click on the API Resource, and then click on its Properties tab, and select True as the value for the Post method as shown below, to create the API resource listening to POST requests . 9. Double click on the D ata Mapping mediator, t o configure it. Y ou view a dialog box to create a registry resource project. 10. Enter a name for the configuration, and point the Registry Resource project to save it as shown below. !!! tip This configuration name is the prefix used for the configuration files that you deploy to the EI server related to the Data Mapper. Since you created an ESB Solution project, it directly points you to that project to save in it. Otherwise, you need to click the **Create new project** link, to create a new Registry Resource project and then point to it . Click OK . You view the f ollowing Data Mapper diagram editor in the new WSO2 Data Mapper Graphical perspective. !!! info You can switch to another perspective by either selecting another in top toolbar tags or by c licking **Window-\\>Perspective-\\>Open Perspective-\\>Other** in the top menu bar. Create an XML file (e.g., input.xml ) by copying the following sample content of a food menu, and save it in your local file system. !!! tip Use this sample XML message to load the input format to the Data Mapper editor. <breakfast_menu> <food> <name>Belgian Waffles</name> <price>$5.95</price> <description>Two of our famous Belgian Waffles with plenty of real maple syrup</description> <calories>650</calories> <orgin>Belgian</orgin> <veg>true</veg> </food> <food> <name>Strawberry Belgian Waffles</name> <price>$7.95</price> <description>Light Belgian waffles covered with strawberries and whipped cream</description> <calories>900</calories> <orgin>Belgian</orgin> <veg>true</veg> </food> <food> <name>Berry-Berry Belgian Waffles</name> <price>$8.95</price> <description>Light Belgian waffles covered with an assortment of fresh berries and whipped cream</description> <calories>900</calories> <orgin>Belgian</orgin> <veg>true</veg> </food> <food> <name>French Toast</name> <price>$4.50</price> <description>Thick slices made from our homemade sourdough bread</description> <calories>600</calories> <orgin>French</orgin> <veg>true</veg> </food> <food> <name>Homestyle Breakfast</name> <price>$6.95</price> <description>Two eggs, bacon or sausage, toast, and our ever-popular hash browns</description> <calories>950</calories> <orgin>French</orgin> <veg>false</veg> </food> </breakfast_menu> Right-click on the top title bar of the Input box and, click Load Input as shown below . The operation palettes that appear on the left-hand side allows you to provide the input message format to begin the mapping. Select XML as the Resource Type as shown below. !!! info You can select one out of the following resource types , to load the input and output message formats to Data Mapper. - **XML:** to load a sample XML message and WSO2 Data Mapper Editor will generate the JSON schema to represent the XML according to the WSO2 Data Mapper Schema specification. - **JSON:** to load a sample JSON message. - **CSV:** to load a sample JSON/CSV message. ****For CSV you need to provide the column names as the first record** .** - **XSD:** to load an XSD schema file, which defines your XML message format. - **JSONSCHEMA:** to load a JSON schema for your message according to the WSO2 Data Mapper schema specification. - **CONNECTOR:** to map a message, which is an output of a Connector. Select the **Connector** **Type** in the **Input** box, and it will list down all available connectors. Then, select the operation from the menu that appears in front of Data Mapper mediator. Click the file system link in Select resource from , select the XML file you saved in your local file system in step 12 , and click Open . You view the input format loaded in the Input box in the editor as shown below. Create another XML file (e.g., output.xml ) by copying the following sample content of a food menu, and save it in your local file system. !!! tip Use this sample XML message to load the output format to the Data Mapper editor. <menu> <item> <name>Belgian Waffles</name> <price>$5.95</price> <calories>650</calories> <orgin>Belgian</orgin> <veg>true</veg> <description>Two of our famous Belgian Waffles with plenty of real maple syrup</description> </item> <item> <name>Strawberry Belgian Waffles</name> <price>$7.95</price> <calories>900</calories> <orgin>Belgian</orgin> <veg>true</veg> <description>Light Belgian waffles covered with strawberries and whipped cream</description> </item> <item> <name>Berry-Berry Belgian Waffles</name> <price>$8.95</price> <calories>900</calories> <orgin>Belgian</orgin> <veg>true</veg> <description>Light Belgian waffles covered with an assortment of fresh berries and whipped cream</description> </item> <item> <name>French Toast</name> <price>$4.50</price> <calories>600</calories> <orgin>French</orgin> <veg>true</veg> <description>Thick slices made from our homemade sourdough bread</description> </item> <item> <name>Homestyle Breakfast</name> <price>$6.95</price> <calories>950</calories> <orgin>French</orgin> <veg>false</veg> <description>Two eggs, bacon or sausage, toast, and our ever-popular hash browns</description> </item> </menu> Right-click on the top title bar of the Output box and, click Load Output as shown below . The operation palettes that appear on the left-hand side allows you to provide the output message format. Click the file system link in Select resource from , select the XML file you saved in your local file system in step 16 , and click Open . You view the input format loaded in the Output box in the editor as shown below. Check the Input and Output boxes with the sample messages, to see if the element types (i.e. (Arrays, Objects and Primitive values) are correctly identified or not. Following signs will help you to identify them correctly. {} - represents object elements [] - represents array elements \\<> - represents primitive field values A - represents XML attribute values Click on the Data Mapper mediator. You view the following in the Properties tab of the Data Mapper mediator configuration as shown below. Configuration: Script file that is used to execute the mapping. Input Schema: JSON schema, which represents the input message format. Output Schema: JSON schema, which represents the output message format. Input Type: Expected input message type (xml/json/csv). Output Type: Target output message type (xml/json/csv). Check if you set the input and type and output type correctly. !!! note If you do not set the input type and output type correctly in the mediator configuration, your mapping will fail during runtime. Follow the steps below to do the mapping using operators as shown below. !!! note - The mapping done in the below example is that: name is mapped via uppercase operator and calories undergo a mathematical calculation to get the output as follows: ` output calories =Round( (calories*1.13) + 6.75) ` - You can only connect primitive data values such as Strings, numbers, boolean and etc. You cannot map Array and object values. Drag and drop an Upper Case operator and connect the name in both Input and Output boxes to it. Connect price in the Input box to the same in the Output box. Connect description in the Input box to the same in the Output box. Connect origin in the Input box to the same in the Output box. Connect veg in the Input box to the same in the Output box. Dran and drop the following operators: Multiply , Add , Round ** ** Drag and drop a Constant operator, and enter 1.13 as its Value in the Properties section. !!! tip To update the titile of the **Constant** box with the value, save the diagram, close the **FoodMapping.datamapper\\_diagram** and re-open it by double-clicking on the **Data Mapper** icon in the **FoodMenuConversion.xml** file. Drag and drop another Constant operator, and enter 6.75 as its Value in the Properties section. Connect the calories variable in both the Input and Output boxes via the Operators as shown below. From To Input Box \u2192 calories Multiply Operator Constant 1.13 Multiply Operator Multiply Operator Add Operator Constant 6.75 Add Operator Add Operator Round Operator Round Operator Output Box \u2192 calories Press Ctrl+S keys in each tab, to save all the configurations.","title":"Creating the ESB configuration project"},{"location":"references/using-Data-Mapper-Mediator-in-the-ESB-Profile/#deploying-the-configurations","text":"After creating the Data Mapper configurations, follow the steps below to deploy the created REST API and the configurations in the ESB profile by including them in a C-App. Open WSO2 EI Tooling. Expand the C-APP project that was created when you created the ESB Solution project (i.e. EIDataMappingProjectCompositeApplication ), and double-click on the POM file. You view the following screen to select project files into the C-APP. !!! info You need to refresh the screen to view the registry resource files . Once you refresh the screen, you view all the artifacts in the workspace. {width=\"800\"} Click the refresh button in the top right-hand corner to load newly added registry files, as shown below. Select the REST API file and the three registry resource files containing the mapping configuration, input schema, and output schema as shown below. Configuration: Script file that is used to execute the mapping. Input schema: JSON schema which represents the input message format. Output schema: JSON schema which represents the output message format. {width=\"700\"} Start WSO2 ESB server. For instructions, see Running the Product . Click the Servers Tab in WSO2 EI Tooling, and click the No servers are available. Click this link to create a new server... link as shown below. {width=\"800\"} Click WSO2 , click WSO2 Carbon remote server , and then click Next as shown below. Enter the URL of the WSO2 EI for Server URL , and click Finish as shown below. You view the WSO2 EI server added in the Servers tab as shown below. Right-click on WSO2 Carbon remote server at localhost , and then click Add & Remove . Select the C-App in the Available: box, click Add to move it to the Configured: box, and then click Finish as shown below. You view the C-App added to the WSO2 EI server as shown below. Log in to the WSO2 EI Management Console using the following URL and admin/admin credentials: https://\\<ESB_HOST>:\\<ESB_PORT>carbon/ Click Main , and then click APIs in the Service Bus menu. You view the deployed REST API invocation URL as shown below. {width=\"900\"}","title":"Deploying the configurations"},{"location":"references/using-Data-Mapper-Mediator-in-the-ESB-Profile/#invoking-the-created-rest-api","text":"Follow the steps below to test invoking the created REST API. Open Postman REST client. Enter the following details to create the client message, enter the content of the XML file you created in step 12 as the payload in the text area provided, and click Send as shown below. URL: http://:8280/convertMenu Method: POST Body: raw xml /application Message: Enter the inpu {width=\"900\"} You view the expected JSON message received as shown below. {width=\"900\"} Similarly, you can use the above instructions to check the following message conversions: T he input employee message in XML format, and the output engineer message in XML/JSON/CSV formats, which is sent to the client as the response. (i.e. XML->XML/JSON/CSV) T he input employee message in JSON format, and the output engineer message in XML/JSON/CSV formats, which is sent to the client as the response. (i.e. JSON->XML/JSON/CSV) T he input employee message in CSV format, and the output engineer message in XML/JSON/CSV formats, which is sent to the client as the response. (i.e. CSV->XML/JSON/CSV) Note In the above sample, the output message format is fully compatible to represent as JSON and CSV. However, t his is not guaranteed in every occasion. For example, if you have defined a complex XML output message with namespaces and attributes, JSON message or CSV will not be built as expected.","title":"Invoking the created REST API"},{"location":"references/validate-Mediator/","text":"Validate Mediator \u00b6 You can use the Validate mediator to validate XML and JSON messages. Validating XML messages Validating JSON messages The Validate mediator validates XML messages against a specified schema. You can specify an XPath to extract and validate a specific part of the message. Otherwise, the mediator validates the first child of the SOAP body of the current message. Tip A Fault mediator should be added as a child to the Validate mediator in order specify the fault sequence to be followed if the validation fails. Info The Validate mediator is a content aware mediator. Syntax \u00b6 <validate [source=\"xpath\"]> <property name=\"validation-feature-id\" value=\"true|false\"/>* <schema key=\"string\"/>+ <on-fail> mediator+ </on-fail> </validate> Configuration \u00b6 The mediator configuration can be divided into the following sections. Schema keys defined for Validate Mediator \u00b6 This section is used to specify the key to access the main schema based on which validation is carried out, as well as to specify the XML which needs to be validated. The parameters available in this section are as follows. Parameter Name Description Source The XPath expression to extract the XML that needs to be validated. The Validate mediator validates the evaluation of this expression against the schema specified in the Schema keys defined for Validate Mediator table. If this is not specified, the validation is performed against the first child of the SOAP body of the current message. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Enable Cache Schema This check box is enabled by default to ensure that schemas retrieved from the registry for one service/REST API are cached for future use. !!! tip Using Templates? Be sure to disable this check box if you are using the Validate mediator inside a Template . Since multiple proxy services/REST APIs will be accessing one template, schemas that are cached for one service can interrupt another service that uses the same template. Schema keys defined for Validate Mediator table The key for the schema location. It can be specified using one of the following methods. If the key is a static value, select Static Key from the list and enter a static key in the data field. This value should be pre-defined and saved as a resource in the Registry. Click either Configuration Registry or Governance Registry as relevant to select the required key from the resource tree. If the key is a dynamic value, Select Dynamic Key from the list and enter an expression to calculate the value in the data field. Click Add Key to add a new schema key. Click Delete in the relevant row to delete a schema key. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Features Defined for Validator Mediator \u00b6 This section is used to specify which features of the Validate mediator should be enabled and which should be disabled. The parameters available in this section are as follows. Info Only the FEATURE_SECURE_PROCESSING feature is currently supported by the validator. Parameter Name Description Feature Name The name of the feature. Value Click True to enable the feature, or click False to disable the feature. Action Click Delete in the relevant row to delete a feature. Resources of the Validate Mediator \u00b6 A resource in the Validate mediator configuration enables you to import a schema referenced within another schema. In order to access such a schema via a resource, the parent schema should be saved as a resource in the Registry. The parameters available in this section are as follows. Parameter Name Description Location The location of the schema to be imported. The value entered here should be equal to the value of the schema location attribute within the relevant < xsd:import > element in the parent schema. Key The key to access the parent schema saved in the Registry. Click either Configuration Registry or Governance Registry as relevant to select the key from the resource tree. Examples \u00b6 Example 1 - Basic configuration \u00b6 In this example, the required schema for validating messages going through the validate mediator is given as a registry key, schema\\sample.xsd . No source attribute is specified, and therefore the schema will be used to validate the first child of the SOAP body. The mediation logic to follow if the validation fails is defined within the on-fail element. In this example, the Fault Mediator creates a SOAP fault to be sent back to the party which sent the message. <validate cache-schema=\"true\"> <schema key=\"schema\\sample.xsd\"/> <on-fail> <makefault> <code value=\"tns:Receiver\" xmlns:tns=\"http://www.w3.org/2003/05/soap-envelope\"/> <reason value=\"Invalid Request!!!\"/> </makefault> <property name=\"RESPONSE\" value=\"true\"/> <header name=\"To\" expression=\"get-property('ReplyTo')\"/> </on-fail> </validate> Example 2 - Validate mediator with resources \u00b6 In this example, the following schema named 08MockServiceSchema is saved in the Registry. This schema is located in MockDataTypes.xsd . A reference is made within this schema to another schema named 08SOAPFaults which is located in SOAPFaults.xsd . <xsd:import namespace= \"http://samples.synapse.com/08MockServiceSchema\" schemalocation= \"MockDataTypes.xsd\"> <xsd:import namespace= \"http://samples.synapse.com/08SOAPFaults\" schemalocation= \"../Common/SOAPFaults.xsd\"> </xsd:import> The Validate mediator can be configured as follows. <validate cache-schema=\"true\"> <schema key=\"MockDataTypes.xsd\"/> <resource location=\"../Common/SOAPFaults.xsd\" key=\"conf:custom/schema/SOAPFaults.xsd\"/> <on-fail> <log level=\"custom\"> <property name=\"validation failed\" value=\"Validation failed ###\"/> <property name=\"error_msg\" expression=\"$ctx:ERROR_MESSAGE\"/> </log> </on-fail> </validate> The schema used by the validate mediator is MockDataTypes.xsd . In addition, a resource is used to import the 08 SOAPFaults schema which is referred in the 08MockServiceSchema schema. Note that the value ../Common/SOAPFaults.xsd which is specified as the location for the schema to be imported is the same as the location specified for 08 SOAPFaults schema in the 08MockServiceSchema configuration. The on-fail sequence of this Validate mediator includes a Log mediator which is added as a child to the Validate mediator. This log mediator uses two properties to generate the error message Validation failed ### when the validation of a message against the schemas specified fails. The Validate mediator validates JSON messages against a specified JSON schema. You can specify a JSONPath to extract and validate a specific part of the message. Otherwise, the mediator validates the complete content of the current message. Tip A Fault mediator or PayloadFactory mediator should be added as a child to the Validate mediator in order specify the fault sequence to be followed if the validation fails. Info The Validate mediator is a content aware mediator. Syntax \u00b6 <validate [source=\"xpath\"]> <schema key=\"string\"/>+ <on-fail> mediator+ </on-fail> </validate> Configuration \u00b6 The mediator configuration can be divided into the following sections. Schema keys defined for Validate Mediator \u00b6 This section is used to specify the key to access the main schema based on which validation is carried out, as well as to specify the JSON message which needs to be validated. The parameters available in this section are as follows. Parameter Name Description Source The JSONPath expression to extract the JSON element that needs to be validated. The Validate mediator validates the evaluation of this expression against the schema specified in the Schema keys defined for Validate Mediator table. If this is not specified, the validation is performed against the whole body of the current message. E.g: json-eval($.msg)\" Enable Cache Schema This check box is enabled by default to ensure that schemas retrieved from the registry for one service/REST API are cached for future use. !!! tip Using Templates? Be sure to disable this check box if you are using the Validate mediator inside a Template . Since multiple proxy services/REST APIs will be accessing one template, schemas that are cached for one service can interrupt another service that uses the same template. Schema keys defined for Validate Mediator The key for the schema location. It can be specified using one of the following methods. If the key is a static value, select Static Key from the list and enter a static key in the data field. This value should be pre-defined and saved as a resource in the Registry . Click either Configuration Registry or Governance Registry as relevant to select the required key from the resource tree. If the key is a dynamic value, Select Dynamic Key from the list and enter an expression to calculate the value in the data field. Click Add Key to add a new schema key. Click Delete in the relevant row to delete a schema key. Examples \u00b6 Following examples use the below sample schema StockQuoteSchema.json file. Add this sample schema file (i.e. StockQuoteSchema.json ) to the following Registry path: conf:/schema/StockQuoteSchema. json. For instructions on adding the schema file to the Registry path, see Adding a Resource . Tip When adding this sample schema file to the Registry, specify the Media Type as application/json. { \"$schema\": \"http://json-schema.org/draft-04/schema#\", \"type\": \"object\", \"properties\": { \"getQuote\": { \"type\": \"object\", \"properties\": { \"request\": { \"type\": \"object\", \"properties\": { \"symbol\": { \"type\": \"string\" } }, \"required\": [ \"symbol\" ] } }, \"required\": [ \"request\" ] } }, \"required\": [ \"getQuote\" ] } Example 1 - Basic configuration \u00b6 In this example, the required schema for validating messages going through the Validate mediator is given as a registry key (i.e. schema\\StockQuoteSchema.json ). You do not have any source attributes specified. Therefore, the schema will be used to validate the complete JSON body. The mediation logic to follow if the validation fails is defined within the on-fail element. In this example, the PayloadFactory mediator creates a fault to be sent back to the party, which sends the message. <validate cache-schema=\"true\"> <schema key=\"conf:/schema/StockQuoteSchema.json\"/> <on-fail> <payloadFactory media-type=\"json\"> <format>{\"Error\":\"$1\"}</format> <args> <arg evaluator=\"xml\" expression=\"$ctx:ERROR_MESSAGE\"/> </args> </payloadFactory> <property name=\"HTTP_SC\" value=\"500\" scope=\"axis2\"/> <respond/> </on-fail> </validate> An example for a valid JSON payload request is given below. { \"getQuote\": { \"request\": { \"symbol\": \"WSO2\" } } } Example 2 - Validate mediator with source (JSONPath) \u00b6 In this example, it extracts the message element from the JSON request body and validates only that part of the message against the given schema. <validate cache-schema=\"true\" source=\"json-eval($.msg)\"> <schema key=\"conf:/schema/StockQuoteSchema.json\"/> <on-fail> <payloadFactory media-type=\"json\"> <format>{\"Error\":$1\"}</format> <args> <arg evaluator=\"xml\" expression=\"$ctx:ERROR_MESSAGE\"/> </args> </payloadFactory> <property name=\"HTTP_SC\" value=\"500\" scope=\"axis2\"/> <respond/> </on-fail> </validate> An example for a valid JSON request payload is given below. { \"msg\": { \"getQuote\": { \"request\": { \"symbol\": \"WSO2\" } } } }","title":"Validate Mediator"},{"location":"references/validate-Mediator/#validate-mediator","text":"You can use the Validate mediator to validate XML and JSON messages. Validating XML messages Validating JSON messages The Validate mediator validates XML messages against a specified schema. You can specify an XPath to extract and validate a specific part of the message. Otherwise, the mediator validates the first child of the SOAP body of the current message. Tip A Fault mediator should be added as a child to the Validate mediator in order specify the fault sequence to be followed if the validation fails. Info The Validate mediator is a content aware mediator.","title":"Validate Mediator"},{"location":"references/validate-Mediator/#syntax","text":"<validate [source=\"xpath\"]> <property name=\"validation-feature-id\" value=\"true|false\"/>* <schema key=\"string\"/>+ <on-fail> mediator+ </on-fail> </validate>","title":"Syntax"},{"location":"references/validate-Mediator/#configuration","text":"The mediator configuration can be divided into the following sections.","title":"Configuration"},{"location":"references/validate-Mediator/#schema-keys-defined-for-validate-mediator","text":"This section is used to specify the key to access the main schema based on which validation is carried out, as well as to specify the XML which needs to be validated. The parameters available in this section are as follows. Parameter Name Description Source The XPath expression to extract the XML that needs to be validated. The Validate mediator validates the evaluation of this expression against the schema specified in the Schema keys defined for Validate Mediator table. If this is not specified, the validation is performed against the first child of the SOAP body of the current message. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Enable Cache Schema This check box is enabled by default to ensure that schemas retrieved from the registry for one service/REST API are cached for future use. !!! tip Using Templates? Be sure to disable this check box if you are using the Validate mediator inside a Template . Since multiple proxy services/REST APIs will be accessing one template, schemas that are cached for one service can interrupt another service that uses the same template. Schema keys defined for Validate Mediator table The key for the schema location. It can be specified using one of the following methods. If the key is a static value, select Static Key from the list and enter a static key in the data field. This value should be pre-defined and saved as a resource in the Registry. Click either Configuration Registry or Governance Registry as relevant to select the required key from the resource tree. If the key is a dynamic value, Select Dynamic Key from the list and enter an expression to calculate the value in the data field. Click Add Key to add a new schema key. Click Delete in the relevant row to delete a schema key. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression.","title":"Schema keys defined for Validate Mediator"},{"location":"references/validate-Mediator/#features-defined-for-validator-mediator","text":"This section is used to specify which features of the Validate mediator should be enabled and which should be disabled. The parameters available in this section are as follows. Info Only the FEATURE_SECURE_PROCESSING feature is currently supported by the validator. Parameter Name Description Feature Name The name of the feature. Value Click True to enable the feature, or click False to disable the feature. Action Click Delete in the relevant row to delete a feature.","title":"Features Defined for Validator Mediator"},{"location":"references/validate-Mediator/#resources-of-the-validate-mediator","text":"A resource in the Validate mediator configuration enables you to import a schema referenced within another schema. In order to access such a schema via a resource, the parent schema should be saved as a resource in the Registry. The parameters available in this section are as follows. Parameter Name Description Location The location of the schema to be imported. The value entered here should be equal to the value of the schema location attribute within the relevant < xsd:import > element in the parent schema. Key The key to access the parent schema saved in the Registry. Click either Configuration Registry or Governance Registry as relevant to select the key from the resource tree.","title":"Resources of the Validate Mediator"},{"location":"references/validate-Mediator/#examples","text":"","title":"Examples"},{"location":"references/validate-Mediator/#example-1-basic-configuration","text":"In this example, the required schema for validating messages going through the validate mediator is given as a registry key, schema\\sample.xsd . No source attribute is specified, and therefore the schema will be used to validate the first child of the SOAP body. The mediation logic to follow if the validation fails is defined within the on-fail element. In this example, the Fault Mediator creates a SOAP fault to be sent back to the party which sent the message. <validate cache-schema=\"true\"> <schema key=\"schema\\sample.xsd\"/> <on-fail> <makefault> <code value=\"tns:Receiver\" xmlns:tns=\"http://www.w3.org/2003/05/soap-envelope\"/> <reason value=\"Invalid Request!!!\"/> </makefault> <property name=\"RESPONSE\" value=\"true\"/> <header name=\"To\" expression=\"get-property('ReplyTo')\"/> </on-fail> </validate>","title":"Example 1 - Basic configuration"},{"location":"references/validate-Mediator/#example-2-validate-mediator-with-resources","text":"In this example, the following schema named 08MockServiceSchema is saved in the Registry. This schema is located in MockDataTypes.xsd . A reference is made within this schema to another schema named 08SOAPFaults which is located in SOAPFaults.xsd . <xsd:import namespace= \"http://samples.synapse.com/08MockServiceSchema\" schemalocation= \"MockDataTypes.xsd\"> <xsd:import namespace= \"http://samples.synapse.com/08SOAPFaults\" schemalocation= \"../Common/SOAPFaults.xsd\"> </xsd:import> The Validate mediator can be configured as follows. <validate cache-schema=\"true\"> <schema key=\"MockDataTypes.xsd\"/> <resource location=\"../Common/SOAPFaults.xsd\" key=\"conf:custom/schema/SOAPFaults.xsd\"/> <on-fail> <log level=\"custom\"> <property name=\"validation failed\" value=\"Validation failed ###\"/> <property name=\"error_msg\" expression=\"$ctx:ERROR_MESSAGE\"/> </log> </on-fail> </validate> The schema used by the validate mediator is MockDataTypes.xsd . In addition, a resource is used to import the 08 SOAPFaults schema which is referred in the 08MockServiceSchema schema. Note that the value ../Common/SOAPFaults.xsd which is specified as the location for the schema to be imported is the same as the location specified for 08 SOAPFaults schema in the 08MockServiceSchema configuration. The on-fail sequence of this Validate mediator includes a Log mediator which is added as a child to the Validate mediator. This log mediator uses two properties to generate the error message Validation failed ### when the validation of a message against the schemas specified fails. The Validate mediator validates JSON messages against a specified JSON schema. You can specify a JSONPath to extract and validate a specific part of the message. Otherwise, the mediator validates the complete content of the current message. Tip A Fault mediator or PayloadFactory mediator should be added as a child to the Validate mediator in order specify the fault sequence to be followed if the validation fails. Info The Validate mediator is a content aware mediator.","title":"Example 2 - Validate mediator with resources"},{"location":"references/validate-Mediator/#syntax_1","text":"<validate [source=\"xpath\"]> <schema key=\"string\"/>+ <on-fail> mediator+ </on-fail> </validate>","title":"Syntax"},{"location":"references/validate-Mediator/#configuration_1","text":"The mediator configuration can be divided into the following sections.","title":"Configuration"},{"location":"references/validate-Mediator/#schema-keys-defined-for-validate-mediator_1","text":"This section is used to specify the key to access the main schema based on which validation is carried out, as well as to specify the JSON message which needs to be validated. The parameters available in this section are as follows. Parameter Name Description Source The JSONPath expression to extract the JSON element that needs to be validated. The Validate mediator validates the evaluation of this expression against the schema specified in the Schema keys defined for Validate Mediator table. If this is not specified, the validation is performed against the whole body of the current message. E.g: json-eval($.msg)\" Enable Cache Schema This check box is enabled by default to ensure that schemas retrieved from the registry for one service/REST API are cached for future use. !!! tip Using Templates? Be sure to disable this check box if you are using the Validate mediator inside a Template . Since multiple proxy services/REST APIs will be accessing one template, schemas that are cached for one service can interrupt another service that uses the same template. Schema keys defined for Validate Mediator The key for the schema location. It can be specified using one of the following methods. If the key is a static value, select Static Key from the list and enter a static key in the data field. This value should be pre-defined and saved as a resource in the Registry . Click either Configuration Registry or Governance Registry as relevant to select the required key from the resource tree. If the key is a dynamic value, Select Dynamic Key from the list and enter an expression to calculate the value in the data field. Click Add Key to add a new schema key. Click Delete in the relevant row to delete a schema key.","title":"Schema keys defined for Validate Mediator"},{"location":"references/validate-Mediator/#examples_1","text":"Following examples use the below sample schema StockQuoteSchema.json file. Add this sample schema file (i.e. StockQuoteSchema.json ) to the following Registry path: conf:/schema/StockQuoteSchema. json. For instructions on adding the schema file to the Registry path, see Adding a Resource . Tip When adding this sample schema file to the Registry, specify the Media Type as application/json. { \"$schema\": \"http://json-schema.org/draft-04/schema#\", \"type\": \"object\", \"properties\": { \"getQuote\": { \"type\": \"object\", \"properties\": { \"request\": { \"type\": \"object\", \"properties\": { \"symbol\": { \"type\": \"string\" } }, \"required\": [ \"symbol\" ] } }, \"required\": [ \"request\" ] } }, \"required\": [ \"getQuote\" ] }","title":"Examples"},{"location":"references/validate-Mediator/#example-1-basic-configuration_1","text":"In this example, the required schema for validating messages going through the Validate mediator is given as a registry key (i.e. schema\\StockQuoteSchema.json ). You do not have any source attributes specified. Therefore, the schema will be used to validate the complete JSON body. The mediation logic to follow if the validation fails is defined within the on-fail element. In this example, the PayloadFactory mediator creates a fault to be sent back to the party, which sends the message. <validate cache-schema=\"true\"> <schema key=\"conf:/schema/StockQuoteSchema.json\"/> <on-fail> <payloadFactory media-type=\"json\"> <format>{\"Error\":\"$1\"}</format> <args> <arg evaluator=\"xml\" expression=\"$ctx:ERROR_MESSAGE\"/> </args> </payloadFactory> <property name=\"HTTP_SC\" value=\"500\" scope=\"axis2\"/> <respond/> </on-fail> </validate> An example for a valid JSON payload request is given below. { \"getQuote\": { \"request\": { \"symbol\": \"WSO2\" } } }","title":"Example 1 - Basic configuration"},{"location":"references/validate-Mediator/#example-2-validate-mediator-with-source-jsonpath","text":"In this example, it extracts the message element from the JSON request body and validates only that part of the message against the given schema. <validate cache-schema=\"true\" source=\"json-eval($.msg)\"> <schema key=\"conf:/schema/StockQuoteSchema.json\"/> <on-fail> <payloadFactory media-type=\"json\"> <format>{\"Error\":$1\"}</format> <args> <arg evaluator=\"xml\" expression=\"$ctx:ERROR_MESSAGE\"/> </args> </payloadFactory> <property name=\"HTTP_SC\" value=\"500\" scope=\"axis2\"/> <respond/> </on-fail> </validate> An example for a valid JSON request payload is given below. { \"msg\": { \"getQuote\": { \"request\": { \"symbol\": \"WSO2\" } } } }","title":"Example 2 - Validate mediator with source (JSONPath)"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/","text":"WSO2 ESB Data Mapper JSON Schema Specification \u00b6 The following specification defines the Data Mapper JSON schema of the ESB profile . It is intended to be the authoritative specification. Implementations of schemas for the Data Mapper mediator must adhere to this. Schema declaration Primitive types Complex types Defining WSO2 schemas to represent an XML payload Schema declaration \u00b6 A schema is represented in JSON by one of: A JSON string, naming a defined type. A JSON object, of the form: {\"type\": \"typeName\" ...attributes...} , where typeName is either a primitive or a derived type name, as defined below. A JSON array, representing a union of embedded types. A Data Mapper schema should start with the $schema attribute with the Data Mapper schema version. For example: { \u201c$schema\u201d:\u201d http://wso2-data-mapper-json-schema/1.0v \u201d} Also, it can contain following optional attributes that will define more information about the schema. \u201cid\u201d : a JSON string declaring a unique identifier for the schema. \u201ctitle\u201d : a JSON string defining the root element name. \u201cdescription\u201d : a JSON string providing a detailed description about the schema. \u201ctype\u201d : a JSON string providing the element type. \u201cnamespaces\u201d : a JSON array of JSON objects defining namespaces and prefix values used in the schema as shown in the following example. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-o1\u201d, \u201ctitle\u201d:\u201dRootElement\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent any form of object without any restriction\u201d , \"namespaces\":[ { \"prefix\":\"ns1\", \"url\":\"http://ns1.com\"}, {\"prefix\":\"ns2\", \"url\":\"http://ns2.com\"}] } Primitive types \u00b6 Primitive types have no specified attributes. The set of primitive type names are as follows. null : no value boolean : a binary value integer : integer value number : rational numbers string : unicode character sequence Primitive type names are also defined type names. Thus, for example, the schema \"string\" is equivalent to: {\"type\": \"string\"} Complex types \u00b6 The Data Mapper schema supports the following complex types: object and array. Object \u00b6 Object uses the type name \u201cobject\u201d , and supports the following attributes. \u201cid\u201d : a JSON string declaring a unique identifier for the object (required). \u201ctype\u201d : a JSON string providing the element type. \u201cdescription\u201d : a JSON string providing documentation to the user of this schema. \u201cproperties\u201d : a JSON object listing fields (required). Each field is a JSON object. \u201cattributes\u201d : a JSON object listing XML attribute fields. Each field is a JSON object. Arrays \u00b6 Arrays use the type name \"array\" , and support a single attribute out of the following. \u201citems\u201d : the schema representing the items of the of the array. \u201cid\u201d : a JSON string declaring a unique identifier for the object (required). \u201cattributes\u201d : a JSON object listing XML attribute fields. Each field is a JSON object. \u201cdescription\u201d : a JSON string providing documentation to the user of this schema For example, an array of an object containing a field named firstname is declared as shown below. { \"type\": \"array\", \"items\": [ { \"id\":\"http://jsonschema.net/employee/0\", \"type\":\"object\", \"properties\":{ \u201cfirstname\":{ \"id\":\"http://jsonschema.net/employee/0/firstname\", \"type\":\"string\" } } }] } Defining WSO2 schemas to represent an XML payload \u00b6 There are differences between XML and JSON message specifications. Therefore, to represent XML message formats in JSON schemas, you need to introduce a few more configurations as explained below. Representing XML attributes and namespaces in WSO2 JSON schemas \u00b6 For example, you can build a JSON schema, which follows the WSO2 specification using the following XML code as described below. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <ns:employees xmlns:ns=\"http://wso2.employee.info\" xmlns:sn=\"http://wso2.employee.address\"> <ns:employee> <ns:firstname>Mark</ns:firstname> <ns:lastname>Taylor</ns:lastname> <sn:addresses> <sn:address location=\"home\"> <sn:city postalcode=\"30000\">LA</sn:city> <sn:road>baker street</sn:road> </sn:address> <sn:address location=\"office\"> <sn:city postalcode=\"10003\">Colombo 03</sn:city> <sn:road>duplication road</sn:road> </sn:address> </sn:addresses> </ns:employee> <ns:employee> <ns:firstname>Mathew</ns:firstname> <ns:lastname>Hayden</ns:lastname> <sn:addresses> <sn:address location=\"home\"> <sn:city postalcode=\"60000\">Sydney</sn:city> <sn:road>101 street</sn:road> </sn:address> <sn:address location=\"office\"> <sn:city postalcode=\"10003\">Colombo 03</sn:city> <sn:road>duplication road</sn:road> </sn:address> </sn:addresses> </ns:employee> </ns:employees Info WSO2 Data Mapper supports o nly single rooted XML messages. In the above example, employees is the root element of the payload, and it should be the value of the title element. Also, there are two namespace values used. Those should be listed under the namespaces field with any prefix value. Info Prefix value can be any valid string that contains only [a-z,A-Z,0-1] characters. You need not match them with the prefix values of the sample. When you include above information, the schema will be as follows. Info The \"required\" field specifies the fields that are mandatory to contain in that level of schema. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-o1\u201d, \u201ctitle\u201d:\u201demployees\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent wso2 employee xml message format\u201d , \"required\":[ \"employees\" ], \"namespaces\":[ { \"prefix\":\"ns1\", \"url\":\"http://wso2.employee.info\"}, {\"prefix\":\"ns2\", \"url\":\"http://wso2.employee.address\"}] } Including the child elements and attribute values \u00b6 Define child elements under the \u201dproperties\u201d field as a JSON object with fields to describe the child element. In the above employee example, the employees element contains an array of similar employee elements. Hence, this can be represented as the following schema. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-employees\u201d, \u201ctitle\u201d:\u201demployees\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent wso2 employee xml message format\u201d , \u201cproperties\u201d: { \u201cemployee\u201d:{ \"id\":\"http://wso2-data-mapper-json-schema-sample-employees/employee\", \"type\":\"array\", \u201cItems\u201d:[ ], \"required\":[ \"arrayRequired\" ] } }, \"required\":[ \"employees\" ], \"namespaces\":[ { \"prefix\":\"ns1\", \"url\":\"http://wso2.employee.info\"}, {\"prefix\":\"ns2\", \"url\":\"http://wso2.employee.address\"}] } Since the employee element is an array type element, it contains a field named \u201citems\u201d , which defines the element format of the array of employee elements. It contains three child fields as firstname , lastname , and address with string, string, and object types accordingly. Hence, when you include these elements into the schema, it will look as the following schema. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-employees\u201d, \u201ctitle\u201d:\u201demployees\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent wso2 employee xml message format\u201d , \u201cproperties\u201d: { \u201cemployee\u201d:{ \"id\":\"http:/\u2026.employees/employee\", \"type\":\"array\", \u201cItems\u201d:[{ \"id\":\"http://jsonschema.net/employee/0\", \"type\":\"object\", \"properties\":{ \"firstname\":{ \"id\":\"http://.../employee/firstname\", \"type\":\"string\" }, \"lastname\":{ \"id\":\"http://.../employee/lastname\", \"type\":\"string\" }, \"addresses\":{ \"id\":\"http://.../employee//addresses\", \"type\":\"object\", \"properties\":{ \"address\":{ \"id\":\"http://.../employee/ addresses/address\", \"type\":\"array\", \"Items\":[ \u2026 ] } } } }, \"required\":[ \"firstname\", \"lastname\", \"address\" ] } ], \"required\":[\"arrayRequired\" ] } }, \"required\":[\"employees\" ], \"namespaces\":[ { \"prefix\":\"ns1\", \"url\":\"http://wso2.employee.info\"}, {\"prefix\":\"ns2\", \"url\":\"http://wso2.employee.address\"}] } Define the XML attributes under the \u201cattributes\u201d field similar to the \"properties in the element definition. In the above employees example, address array element and city element contain attributes, and those can be represented as follows. \"addresses\":{ \"id\":\"http://.../addresses\", \"type\":\"object\", \"properties\":{ \"address\":{ \"id\":\"http://.../addresses/address\", \"type\":\"array\", \"items\":[ { \"id\":\"http://.../addresses/address/element\", \"type\":\"object\", \"properties\":{ \"city\":{ \"id\":\"http://.../addresses/address/element/city\", \"type\":\"string\", \"attributes\":{ \"postalcode\":{ \"id\":\".../element/city/postalcode\", \"type\":\"string\" } } }, \"road\":{ \"id\":\".../addresses/address/element/road\", \"type\":\"string\" } } }], \u201cattributes\u201d:{ \"location\":{ \"id\":\".../addresses/address/element/location\", \"type\":\"string\" } } } } Now, the format of the XML payload is complete. However, you need to define namespaces. You have defined the namespaces used in the payload before with prefix values in the root element under the \u201cnamespaces\u201d tag. To assign the namespace to each element, you should only add the prefix before the element name with a colon as \u201cns1:employees\u201d , \u201cns1:employee\u201d etc. The complete schema to represent the employee payload is as follows. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-employees\u201d, \u201ctitle\u201d:\u201dns2:employees\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent wso2 employee xml message format\u201d , \u201cproperties\u201d: { \"ns2:employee\":{ \"id\":\"http://.../employee\", \"type\":\"array\", \"items\":[ { \"id\":\"http://.../employee/element\", \"type\":\"object\", \"properties\":{ \"ns2:firstname\":{ \"id\":\"http://.../employee/element/firstname\", \"type\":\"string\" }, \"ns2:lastname\":{ \"id\":\"http://.../employee/element/lastname\", \"type\":\"string\" }, \"ns1:addresses\":{ \"id\":\"http://.../employees/employee/element/addresses\", \"type\":\"object\", \"properties\":{ \"ns1:address\":{ \"id\":\"http://.../addresses/address\", \"type\":\"array\", \"items\":[ { \"id\":\"http://.../addresses/address/0\", \"type\":\"object\", \"properties\":{ \u201cns1:city\":{ \"id\":\"http://.../addresses/address/element/city\", \"type\":\"string\", \"attributes\":{ \"postalcode\":{ \"id\":\"http://.../city/-postalcode\", \"type\":\"string\" } } }, \"ns1:road\":{ \"id\":\"http://.../addresses/address/element/road\", \"type\":\"string\" } } \u201cattributes\u201d: { \"location\":{ \"id\":\"http://jsonschema.net/employees/employee/0/addresses/address/0/-location\", \"type\":\"string\" }, } } ] } } } }, \"required\":[ \"firstname\", \"lastname\", \"address\" ] } ], \"required\":[ \"arrayRequired\" ] } }, \"required\":[ \"employees\" ], \"namespaces\":[{ \"prefix\":\"ns1\", \"url\":\"http://wso2.employee.address\"},{\"prefix\":\"ns2\", \"url\":\"http://wso2.employee.info\"}] }","title":"WSO2 ESB Data Mapper JSON Schema Specification"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#wso2-esb-data-mapper-json-schema-specification","text":"The following specification defines the Data Mapper JSON schema of the ESB profile . It is intended to be the authoritative specification. Implementations of schemas for the Data Mapper mediator must adhere to this. Schema declaration Primitive types Complex types Defining WSO2 schemas to represent an XML payload","title":"WSO2 ESB Data Mapper JSON Schema Specification"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#schema-declaration","text":"A schema is represented in JSON by one of: A JSON string, naming a defined type. A JSON object, of the form: {\"type\": \"typeName\" ...attributes...} , where typeName is either a primitive or a derived type name, as defined below. A JSON array, representing a union of embedded types. A Data Mapper schema should start with the $schema attribute with the Data Mapper schema version. For example: { \u201c$schema\u201d:\u201d http://wso2-data-mapper-json-schema/1.0v \u201d} Also, it can contain following optional attributes that will define more information about the schema. \u201cid\u201d : a JSON string declaring a unique identifier for the schema. \u201ctitle\u201d : a JSON string defining the root element name. \u201cdescription\u201d : a JSON string providing a detailed description about the schema. \u201ctype\u201d : a JSON string providing the element type. \u201cnamespaces\u201d : a JSON array of JSON objects defining namespaces and prefix values used in the schema as shown in the following example. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-o1\u201d, \u201ctitle\u201d:\u201dRootElement\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent any form of object without any restriction\u201d , \"namespaces\":[ { \"prefix\":\"ns1\", \"url\":\"http://ns1.com\"}, {\"prefix\":\"ns2\", \"url\":\"http://ns2.com\"}] }","title":"Schema declaration"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#primitive-types","text":"Primitive types have no specified attributes. The set of primitive type names are as follows. null : no value boolean : a binary value integer : integer value number : rational numbers string : unicode character sequence Primitive type names are also defined type names. Thus, for example, the schema \"string\" is equivalent to: {\"type\": \"string\"}","title":"Primitive types"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#complex-types","text":"The Data Mapper schema supports the following complex types: object and array.","title":"Complex types"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#object","text":"Object uses the type name \u201cobject\u201d , and supports the following attributes. \u201cid\u201d : a JSON string declaring a unique identifier for the object (required). \u201ctype\u201d : a JSON string providing the element type. \u201cdescription\u201d : a JSON string providing documentation to the user of this schema. \u201cproperties\u201d : a JSON object listing fields (required). Each field is a JSON object. \u201cattributes\u201d : a JSON object listing XML attribute fields. Each field is a JSON object.","title":"Object"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#arrays","text":"Arrays use the type name \"array\" , and support a single attribute out of the following. \u201citems\u201d : the schema representing the items of the of the array. \u201cid\u201d : a JSON string declaring a unique identifier for the object (required). \u201cattributes\u201d : a JSON object listing XML attribute fields. Each field is a JSON object. \u201cdescription\u201d : a JSON string providing documentation to the user of this schema For example, an array of an object containing a field named firstname is declared as shown below. { \"type\": \"array\", \"items\": [ { \"id\":\"http://jsonschema.net/employee/0\", \"type\":\"object\", \"properties\":{ \u201cfirstname\":{ \"id\":\"http://jsonschema.net/employee/0/firstname\", \"type\":\"string\" } } }] }","title":"Arrays"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#defining-wso2-schemas-to-represent-an-xml-payload","text":"There are differences between XML and JSON message specifications. Therefore, to represent XML message formats in JSON schemas, you need to introduce a few more configurations as explained below.","title":"Defining WSO2 schemas to represent an XML payload"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#representing-xml-attributes-and-namespaces-in-wso2-json-schemas","text":"For example, you can build a JSON schema, which follows the WSO2 specification using the following XML code as described below. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <ns:employees xmlns:ns=\"http://wso2.employee.info\" xmlns:sn=\"http://wso2.employee.address\"> <ns:employee> <ns:firstname>Mark</ns:firstname> <ns:lastname>Taylor</ns:lastname> <sn:addresses> <sn:address location=\"home\"> <sn:city postalcode=\"30000\">LA</sn:city> <sn:road>baker street</sn:road> </sn:address> <sn:address location=\"office\"> <sn:city postalcode=\"10003\">Colombo 03</sn:city> <sn:road>duplication road</sn:road> </sn:address> </sn:addresses> </ns:employee> <ns:employee> <ns:firstname>Mathew</ns:firstname> <ns:lastname>Hayden</ns:lastname> <sn:addresses> <sn:address location=\"home\"> <sn:city postalcode=\"60000\">Sydney</sn:city> <sn:road>101 street</sn:road> </sn:address> <sn:address location=\"office\"> <sn:city postalcode=\"10003\">Colombo 03</sn:city> <sn:road>duplication road</sn:road> </sn:address> </sn:addresses> </ns:employee> </ns:employees Info WSO2 Data Mapper supports o nly single rooted XML messages. In the above example, employees is the root element of the payload, and it should be the value of the title element. Also, there are two namespace values used. Those should be listed under the namespaces field with any prefix value. Info Prefix value can be any valid string that contains only [a-z,A-Z,0-1] characters. You need not match them with the prefix values of the sample. When you include above information, the schema will be as follows. Info The \"required\" field specifies the fields that are mandatory to contain in that level of schema. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-o1\u201d, \u201ctitle\u201d:\u201demployees\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent wso2 employee xml message format\u201d , \"required\":[ \"employees\" ], \"namespaces\":[ { \"prefix\":\"ns1\", \"url\":\"http://wso2.employee.info\"}, {\"prefix\":\"ns2\", \"url\":\"http://wso2.employee.address\"}] }","title":"Representing XML attributes and namespaces in WSO2 JSON schemas"},{"location":"references/wSO2-ESB-Data-Mapper-JSON-Schema-Specification/#including-the-child-elements-and-attribute-values","text":"Define child elements under the \u201dproperties\u201d field as a JSON object with fields to describe the child element. In the above employee example, the employees element contains an array of similar employee elements. Hence, this can be represented as the following schema. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-employees\u201d, \u201ctitle\u201d:\u201demployees\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent wso2 employee xml message format\u201d , \u201cproperties\u201d: { \u201cemployee\u201d:{ \"id\":\"http://wso2-data-mapper-json-schema-sample-employees/employee\", \"type\":\"array\", \u201cItems\u201d:[ ], \"required\":[ \"arrayRequired\" ] } }, \"required\":[ \"employees\" ], \"namespaces\":[ { \"prefix\":\"ns1\", \"url\":\"http://wso2.employee.info\"}, {\"prefix\":\"ns2\", \"url\":\"http://wso2.employee.address\"}] } Since the employee element is an array type element, it contains a field named \u201citems\u201d , which defines the element format of the array of employee elements. It contains three child fields as firstname , lastname , and address with string, string, and object types accordingly. Hence, when you include these elements into the schema, it will look as the following schema. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-employees\u201d, \u201ctitle\u201d:\u201demployees\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent wso2 employee xml message format\u201d , \u201cproperties\u201d: { \u201cemployee\u201d:{ \"id\":\"http:/\u2026.employees/employee\", \"type\":\"array\", \u201cItems\u201d:[{ \"id\":\"http://jsonschema.net/employee/0\", \"type\":\"object\", \"properties\":{ \"firstname\":{ \"id\":\"http://.../employee/firstname\", \"type\":\"string\" }, \"lastname\":{ \"id\":\"http://.../employee/lastname\", \"type\":\"string\" }, \"addresses\":{ \"id\":\"http://.../employee//addresses\", \"type\":\"object\", \"properties\":{ \"address\":{ \"id\":\"http://.../employee/ addresses/address\", \"type\":\"array\", \"Items\":[ \u2026 ] } } } }, \"required\":[ \"firstname\", \"lastname\", \"address\" ] } ], \"required\":[\"arrayRequired\" ] } }, \"required\":[\"employees\" ], \"namespaces\":[ { \"prefix\":\"ns1\", \"url\":\"http://wso2.employee.info\"}, {\"prefix\":\"ns2\", \"url\":\"http://wso2.employee.address\"}] } Define the XML attributes under the \u201cattributes\u201d field similar to the \"properties in the element definition. In the above employees example, address array element and city element contain attributes, and those can be represented as follows. \"addresses\":{ \"id\":\"http://.../addresses\", \"type\":\"object\", \"properties\":{ \"address\":{ \"id\":\"http://.../addresses/address\", \"type\":\"array\", \"items\":[ { \"id\":\"http://.../addresses/address/element\", \"type\":\"object\", \"properties\":{ \"city\":{ \"id\":\"http://.../addresses/address/element/city\", \"type\":\"string\", \"attributes\":{ \"postalcode\":{ \"id\":\".../element/city/postalcode\", \"type\":\"string\" } } }, \"road\":{ \"id\":\".../addresses/address/element/road\", \"type\":\"string\" } } }], \u201cattributes\u201d:{ \"location\":{ \"id\":\".../addresses/address/element/location\", \"type\":\"string\" } } } } Now, the format of the XML payload is complete. However, you need to define namespaces. You have defined the namespaces used in the payload before with prefix values in the root element under the \u201cnamespaces\u201d tag. To assign the namespace to each element, you should only add the prefix before the element name with a colon as \u201cns1:employees\u201d , \u201cns1:employee\u201d etc. The complete schema to represent the employee payload is as follows. { \u201c$schema\u201d : \u201dhttp://wso2-data-mapper-json-schema/1.0v\u201d, \u201cid\u201d:\u201dhttp://wso2-data-mapper-json-schema-sample-employees\u201d, \u201ctitle\u201d:\u201dns2:employees\u201d, \"type\":\"object\", \u201cdescription\u201d:\u201dThis schema represent wso2 employee xml message format\u201d , \u201cproperties\u201d: { \"ns2:employee\":{ \"id\":\"http://.../employee\", \"type\":\"array\", \"items\":[ { \"id\":\"http://.../employee/element\", \"type\":\"object\", \"properties\":{ \"ns2:firstname\":{ \"id\":\"http://.../employee/element/firstname\", \"type\":\"string\" }, \"ns2:lastname\":{ \"id\":\"http://.../employee/element/lastname\", \"type\":\"string\" }, \"ns1:addresses\":{ \"id\":\"http://.../employees/employee/element/addresses\", \"type\":\"object\", \"properties\":{ \"ns1:address\":{ \"id\":\"http://.../addresses/address\", \"type\":\"array\", \"items\":[ { \"id\":\"http://.../addresses/address/0\", \"type\":\"object\", \"properties\":{ \u201cns1:city\":{ \"id\":\"http://.../addresses/address/element/city\", \"type\":\"string\", \"attributes\":{ \"postalcode\":{ \"id\":\"http://.../city/-postalcode\", \"type\":\"string\" } } }, \"ns1:road\":{ \"id\":\"http://.../addresses/address/element/road\", \"type\":\"string\" } } \u201cattributes\u201d: { \"location\":{ \"id\":\"http://jsonschema.net/employees/employee/0/addresses/address/0/-location\", \"type\":\"string\" }, } } ] } } } }, \"required\":[ \"firstname\", \"lastname\", \"address\" ] } ], \"required\":[ \"arrayRequired\" ] } }, \"required\":[ \"employees\" ], \"namespaces\":[{ \"prefix\":\"ns1\", \"url\":\"http://wso2.employee.address\"},{\"prefix\":\"ns2\", \"url\":\"http://wso2.employee.info\"}] }","title":"Including the child elements and attribute values"},{"location":"references/working-with-Mediators-via-Tooling/","text":"Working with Mediators via Tooling \u00b6 If you need to create a custom mediator that performs some logic on a message, you can either create a new mediator project , or import an existing mediator project using WSO2 Integration Studio. Tip You need to have WSO2 Integration Studio installed to create a new message store or to import an existing message store. For instructions, see Installing WSO2 Integration Studio . Once a mediator project is finalised, you can export it as a deployable artifact by right-clicking on the project and selecting Export Project as Deployable Archive . This creates a JAR file that you can deploy to the EI. Alternatively, you can group the mediator project as a Composite Application Project, create a Composite Application Archive (CAR), and deploy it to the EI. Info A URL classloader is used to load classes in the mediator (class mediators are not deployed as OSGi bundles). Therefore, it is only possible to refer to the class mediator from artifacts packed in the same CAR file in which the class mediator is packed. Accessing the class mediator from an artifact packed in another CAR file is not possible. However, it is possible to refer to the class mediator from a sequence packed in the same CAR file and call that sequence from any other artifact packed in other CAR files. Creating a mediator project \u00b6 Follow these steps to create a new mediator. Tip Alternatively, you can import a mediator project. To do this, o pen WSO2 Integration Studio, click File , and then click Import . Next, select Existing WSO2 Projects into workspace under the WSO2 category, click Next and upload the pre-packaged project. Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Mediator Project **** in the ****Getting Started**** tab as shown below. Leave the first option selected and click Next . The New Mediator Creation Wizard appears. {width=\"550\"} Do the following: Type a unique name for the project. Specify the package and class names you are creating. Optionally specify the location where you want to save the project (or leave the default location specified). Optionally specify the working set, if any, that you want to include in this project. A Maven POM file will be generated automatically for this project. If you want to include parent POM information in the file from another project in this workspace, click Next , click the Specify Parent from Workspace check box, and then select the parent project. Click Finish . The mediator project is created in the workspace location you specified with a new mediator class that extends org.apache.synapse.mediators.AbstractMediator . Importing a Java Mediator Project \u00b6 Follow the steps below to import a Java mediator project (that includes a Java class, which extends the org.apache.synapse.mediators.AbstractMediator class) to WSO2 Integration Studio. Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Mediator Project **** in the ****Getting Started**** tab as shown below. Select Import From Workspace and click Next . Specify the mediator project in this workspace that you want to import. Only projects with source files that extend org.apache.synapse.mediators.AbstractMediator are listed. Optionally, you can change the location where the mediator project will be created and add it to working sets. Click Finish . The mediator project you selected is created in the location you specified. Info The mediator projects you create using WOS2 Integration Studio are of the org.wso2.developerstudio.eclipse.artifact.mediator.project.nature nature by default. Follow the steps below to view this nature added to the <PROJECT_NAME>/target/.project file of the Java mediator project you imported. Click the View Menu icon, and click Filters and Customization . {width=\"613\" height=\"294\"} Deselect .*resources , and click OK .","title":"Working with Mediators via Tooling"},{"location":"references/working-with-Mediators-via-Tooling/#working-with-mediators-via-tooling","text":"If you need to create a custom mediator that performs some logic on a message, you can either create a new mediator project , or import an existing mediator project using WSO2 Integration Studio. Tip You need to have WSO2 Integration Studio installed to create a new message store or to import an existing message store. For instructions, see Installing WSO2 Integration Studio . Once a mediator project is finalised, you can export it as a deployable artifact by right-clicking on the project and selecting Export Project as Deployable Archive . This creates a JAR file that you can deploy to the EI. Alternatively, you can group the mediator project as a Composite Application Project, create a Composite Application Archive (CAR), and deploy it to the EI. Info A URL classloader is used to load classes in the mediator (class mediators are not deployed as OSGi bundles). Therefore, it is only possible to refer to the class mediator from artifacts packed in the same CAR file in which the class mediator is packed. Accessing the class mediator from an artifact packed in another CAR file is not possible. However, it is possible to refer to the class mediator from a sequence packed in the same CAR file and call that sequence from any other artifact packed in other CAR files.","title":"Working with Mediators via Tooling"},{"location":"references/working-with-Mediators-via-Tooling/#creating-a-mediator-project","text":"Follow these steps to create a new mediator. Tip Alternatively, you can import a mediator project. To do this, o pen WSO2 Integration Studio, click File , and then click Import . Next, select Existing WSO2 Projects into workspace under the WSO2 category, click Next and upload the pre-packaged project. Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Mediator Project **** in the ****Getting Started**** tab as shown below. Leave the first option selected and click Next . The New Mediator Creation Wizard appears. {width=\"550\"} Do the following: Type a unique name for the project. Specify the package and class names you are creating. Optionally specify the location where you want to save the project (or leave the default location specified). Optionally specify the working set, if any, that you want to include in this project. A Maven POM file will be generated automatically for this project. If you want to include parent POM information in the file from another project in this workspace, click Next , click the Specify Parent from Workspace check box, and then select the parent project. Click Finish . The mediator project is created in the workspace location you specified with a new mediator class that extends org.apache.synapse.mediators.AbstractMediator .","title":"Creating a mediator project"},{"location":"references/working-with-Mediators-via-Tooling/#importing-a-java-mediator-project","text":"Follow the steps below to import a Java mediator project (that includes a Java class, which extends the org.apache.synapse.mediators.AbstractMediator class) to WSO2 Integration Studio. Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Mediator Project **** in the ****Getting Started**** tab as shown below. Select Import From Workspace and click Next . Specify the mediator project in this workspace that you want to import. Only projects with source files that extend org.apache.synapse.mediators.AbstractMediator are listed. Optionally, you can change the location where the mediator project will be created and add it to working sets. Click Finish . The mediator project you selected is created in the location you specified. Info The mediator projects you create using WOS2 Integration Studio are of the org.wso2.developerstudio.eclipse.artifact.mediator.project.nature nature by default. Follow the steps below to view this nature added to the <PROJECT_NAME>/target/.project file of the Java mediator project you imported. Click the View Menu icon, and click Filters and Customization . {width=\"613\" height=\"294\"} Deselect .*resources , and click OK .","title":"Importing a Java Mediator Project"},{"location":"references/working-with-Mediators/","text":"Working with Mediators \u00b6 A mediator is the basic message processing unit and a fundamental part the ESB profile . A mediator can take a message, carry out some predefined actions on it, and output the modified message. For example, the Clone mediator splits a message into several clones, the Send mediator sends the messages, and the Aggregate mediator collects and merges the responses before sending them back to the client. The ESB profile ships with a range of mediators capable of carrying out various tasks on input messages, including functionality to match incompatible protocols, data formats and interaction patterns across different resources. Data can be split, cloned, aggregated, and enriched, allowing the ESB profile to match the different capabilities of services. XQuery and XSLT allow rich transformations on the messages. Rule-based message mediation allows users to cope with the uncertainty of business logic. Content-based routing using XPath filtering is supported in different flavors, allowing users to get the most convenient configuration experience. Built-in capability to handle Transactions allows message mediation to be done transactionally inside the ESB profile . With the eventing capabilities of the ESB profile , EDA based components can be easily interconnected, allowing the ESB profile to be used in the front-end of an organisation's SOA infrastructure. Mediators A mediator is a full-powered processing unit in the ESB profile . In run-time it has access to all parts of the ESB profile along with the current message. Usually, a mediator is configured using XML. Different mediators have their own XML configurations. At the run-time, a message is injected in to the mediator with the ESB profile information. Then this mediator can do virtually anything with the message. A user can write a mediator and put it into the ESB profile . This custom mediator and any other built-in mediator will be exactly the same as the API and the privileges (Refer to more information in Creating Custom Mediators ). Refer to ESB Mediators . Mediation Sequence A mediation sequence, commonly called a \"sequence\", is a list of mediators. That means, it can hold other mediators and execute them. It is part of the ESB profile 's core and message mediation cannot live without this mediator. When a message is delivered to a sequence, it sends the message through all its child mediators. Read more in Mediation Sequences . The Process of Message Mediation \u00b6 In case an error occurs in the main sequence while processing, the message goes to the fault sequence. For detailed information about mediators, refer to the following pages: Working with Mediators via Tooling ESB Mediators Creating Custom Mediators Mediation Sequences Prioritizing Messages Debugging Mediation","title":"Working with Mediators"},{"location":"references/working-with-Mediators/#working-with-mediators","text":"A mediator is the basic message processing unit and a fundamental part the ESB profile . A mediator can take a message, carry out some predefined actions on it, and output the modified message. For example, the Clone mediator splits a message into several clones, the Send mediator sends the messages, and the Aggregate mediator collects and merges the responses before sending them back to the client. The ESB profile ships with a range of mediators capable of carrying out various tasks on input messages, including functionality to match incompatible protocols, data formats and interaction patterns across different resources. Data can be split, cloned, aggregated, and enriched, allowing the ESB profile to match the different capabilities of services. XQuery and XSLT allow rich transformations on the messages. Rule-based message mediation allows users to cope with the uncertainty of business logic. Content-based routing using XPath filtering is supported in different flavors, allowing users to get the most convenient configuration experience. Built-in capability to handle Transactions allows message mediation to be done transactionally inside the ESB profile . With the eventing capabilities of the ESB profile , EDA based components can be easily interconnected, allowing the ESB profile to be used in the front-end of an organisation's SOA infrastructure. Mediators A mediator is a full-powered processing unit in the ESB profile . In run-time it has access to all parts of the ESB profile along with the current message. Usually, a mediator is configured using XML. Different mediators have their own XML configurations. At the run-time, a message is injected in to the mediator with the ESB profile information. Then this mediator can do virtually anything with the message. A user can write a mediator and put it into the ESB profile . This custom mediator and any other built-in mediator will be exactly the same as the API and the privileges (Refer to more information in Creating Custom Mediators ). Refer to ESB Mediators . Mediation Sequence A mediation sequence, commonly called a \"sequence\", is a list of mediators. That means, it can hold other mediators and execute them. It is part of the ESB profile 's core and message mediation cannot live without this mediator. When a message is delivered to a sequence, it sends the message through all its child mediators. Read more in Mediation Sequences .","title":"Working with Mediators"},{"location":"references/working-with-Mediators/#the-process-of-message-mediation","text":"In case an error occurs in the main sequence while processing, the message goes to the fault sequence. For detailed information about mediators, refer to the following pages: Working with Mediators via Tooling ESB Mediators Creating Custom Mediators Mediation Sequences Prioritizing Messages Debugging Mediation","title":"The Process of Message Mediation"},{"location":"references/working-with-Sequences-via-Tooling/","text":"Working with Sequences via Tooling \u00b6 You can create a sequence in your ESB Config project or in the registry and then add it right to that project's mediation workflow, or you can refer to it from a sequence mediator in the same ESB Config project or another project in this Eclipse workspace. This section describes how to create a new sequence or import an existing sequence from an XML file (such as a Synapse Configuration file), and how to use the sequence in your mediation flow. About dynamic sequences \u00b6 WSO2 Integration Studio allows you to create a Registry Resource project, which can be used to store Resources and Collections you want to deploy to the registry of a Carbon Server through a Composite Application (C-App) project. When you create a sequence, you can save it as a dynamic sequence in the Registry Resource project and refer to that sequence from the mediation flow. At runtime, when you deploy the CAR file with both the Registry Resource project and mediation flow, the ESB profile looks up and uses the sequence from the registry. Creating a new sequence \u00b6 Follow these steps to create a new, reusable sequence that you can add to your mediation workflow or refer to from a sequence mediator, or to create a sequence mediator and its underlying sequence all at once. To create a reusable sequence: Create an ESB Config project: Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Config Project **** in the ****Getting Started**** tab, and follow the instructions on the wizard. {width=\"800\" height=\"397\"} Right-click the ESB Config project on the project explorer, click Sequence . Select Create New Sequence and click Next . Specify a unique name for the sequence. !!! info Creating a Main Sequence If you want to create the default main sequence that just sends messages without mediation, be sure to name it ` main ` , which automatically populates the sequence with the default in and out sequences. Do one of the following: To save the sequence in an existing ESB Config project in your workspace, click Browse and select that project. To save the sequence in a new ESB Config project, click Create new Project and create the new project. To save the sequence as a dynamic sequence in a Registry Resource project, click Make this as Dynamic Sequence , specify the registry space (Governance or Configuration), click the Browse button at the top of the dialog box next to Save Sequence in and select the registry resource project, and then type the sequence name in the Registry Path box. Optionally, in the Advanced Configuration section, specify another sequence to run when there is an error and the endpoint where messages should be sent. Click Finish . The sequence is created in the sequences folder under the ESB Config or Registry Resource project you specified, and the sequence is open in the editor. Add the endpoints and other sequences you want in this sequence and then click File > Save . The sequence is now available in the Defined Sequences section of the tool palette and ready for use. To create a sequence when creating a sequence mediator: With your proxy service open in the editor, click Sequence Mediator in the tool palette and then click the location in the mediation workflow where you want to add this sequence. The sequence mediator is added to the workflow with a default name, which is highlighted and ready for you to change. Type the name you want for this sequence mediator and press Enter . Double-click the sequence mediator you just added. A sequence is created and opened in the editor using the same name you entered for the sequence mediator. Add the endpoints and other sequences you want in this sequence, and then click Save . The mediation workflow is updated with the endpoints you added to the sequence. The sequence is also now available in the Defined Sequences section of the tool palette and ready for use in other meditation workflows. Importing a sequence \u00b6 Follow these steps to import an existing sequence from an XML file (such as a Synapse configuration file) into an ESB Config project. Create an ESB Config project: Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Config Project **** in the ****Getting Started**** tab, and follow the instructions on the wizard. Right-click the ESB Config project on the project explorer, click Sequence . Select Import Sequence and click Next . Specify the sequence file by typing its full path name or clicking Browse and navigating to the file. In the Save Sequence In field, specify an existing ESB Config project in your workspace where you want to save the sequence, or click Create new Project to create a new ESB Config project and save the sequence there. In the Advanced Configuration section, select the sequences you want to import. Click Finish . The sequences you selected are created in the sequences folder under the ESB Config project you specified, and the first sequence is open in the editor. Using a sequence \u00b6 After you create a sequence, it appears in the Defined Sequences section of the tool palette. To use this sequence in a mediation flow, click the sequence in the tool palette and then click the spot on the canvas where you want the sequence to appear in the flow. The editor automatically adds any endpoints you used in your sequence. {width=\"900\"} If you want to use a sequence from a different project or from the registry, you need to create a sequence mediator and then refer to the sequence as follows: Click Sequence Mediator on the tool palette, and then click the spot on the canvas where you want the sequence to appear in the mediation workflow. Press Enter to accept the default name for now. In the Properties pane at the bottom of the window, click Static Reference Key , and then click the browse [...] button on the right. {width=\"900\"} In the Resource Key Editor, click Registry if the sequence is stored in the registry or Workspace if it's in another ESB Config project in this Eclipse workspace. If you are trying to select a sequence from the registry and no entries appear in the dialog box, click the add registry connection button (the first button in the upper right corner) and connect to the registry where the sequence resides. Navigate to the sequence you want, select it and click OK , and then click OK again. The sequence mediator name and static reference key are updated to point to the sequence you selected.","title":"Working with Sequences via Tooling"},{"location":"references/working-with-Sequences-via-Tooling/#working-with-sequences-via-tooling","text":"You can create a sequence in your ESB Config project or in the registry and then add it right to that project's mediation workflow, or you can refer to it from a sequence mediator in the same ESB Config project or another project in this Eclipse workspace. This section describes how to create a new sequence or import an existing sequence from an XML file (such as a Synapse Configuration file), and how to use the sequence in your mediation flow.","title":"Working with Sequences via Tooling"},{"location":"references/working-with-Sequences-via-Tooling/#about-dynamic-sequences","text":"WSO2 Integration Studio allows you to create a Registry Resource project, which can be used to store Resources and Collections you want to deploy to the registry of a Carbon Server through a Composite Application (C-App) project. When you create a sequence, you can save it as a dynamic sequence in the Registry Resource project and refer to that sequence from the mediation flow. At runtime, when you deploy the CAR file with both the Registry Resource project and mediation flow, the ESB profile looks up and uses the sequence from the registry.","title":"About dynamic sequences"},{"location":"references/working-with-Sequences-via-Tooling/#creating-a-new-sequence","text":"Follow these steps to create a new, reusable sequence that you can add to your mediation workflow or refer to from a sequence mediator, or to create a sequence mediator and its underlying sequence all at once. To create a reusable sequence: Create an ESB Config project: Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Config Project **** in the ****Getting Started**** tab, and follow the instructions on the wizard. {width=\"800\" height=\"397\"} Right-click the ESB Config project on the project explorer, click Sequence . Select Create New Sequence and click Next . Specify a unique name for the sequence. !!! info Creating a Main Sequence If you want to create the default main sequence that just sends messages without mediation, be sure to name it ` main ` , which automatically populates the sequence with the default in and out sequences. Do one of the following: To save the sequence in an existing ESB Config project in your workspace, click Browse and select that project. To save the sequence in a new ESB Config project, click Create new Project and create the new project. To save the sequence as a dynamic sequence in a Registry Resource project, click Make this as Dynamic Sequence , specify the registry space (Governance or Configuration), click the Browse button at the top of the dialog box next to Save Sequence in and select the registry resource project, and then type the sequence name in the Registry Path box. Optionally, in the Advanced Configuration section, specify another sequence to run when there is an error and the endpoint where messages should be sent. Click Finish . The sequence is created in the sequences folder under the ESB Config or Registry Resource project you specified, and the sequence is open in the editor. Add the endpoints and other sequences you want in this sequence and then click File > Save . The sequence is now available in the Defined Sequences section of the tool palette and ready for use. To create a sequence when creating a sequence mediator: With your proxy service open in the editor, click Sequence Mediator in the tool palette and then click the location in the mediation workflow where you want to add this sequence. The sequence mediator is added to the workflow with a default name, which is highlighted and ready for you to change. Type the name you want for this sequence mediator and press Enter . Double-click the sequence mediator you just added. A sequence is created and opened in the editor using the same name you entered for the sequence mediator. Add the endpoints and other sequences you want in this sequence, and then click Save . The mediation workflow is updated with the endpoints you added to the sequence. The sequence is also now available in the Defined Sequences section of the tool palette and ready for use in other meditation workflows.","title":"Creating a new sequence"},{"location":"references/working-with-Sequences-via-Tooling/#importing-a-sequence","text":"Follow these steps to import an existing sequence from an XML file (such as a Synapse configuration file) into an ESB Config project. Create an ESB Config project: Open WSO2 Integration Studio , click ****Miscellaneous \u2192 Create New Config Project **** in the ****Getting Started**** tab, and follow the instructions on the wizard. Right-click the ESB Config project on the project explorer, click Sequence . Select Import Sequence and click Next . Specify the sequence file by typing its full path name or clicking Browse and navigating to the file. In the Save Sequence In field, specify an existing ESB Config project in your workspace where you want to save the sequence, or click Create new Project to create a new ESB Config project and save the sequence there. In the Advanced Configuration section, select the sequences you want to import. Click Finish . The sequences you selected are created in the sequences folder under the ESB Config project you specified, and the first sequence is open in the editor.","title":"Importing a sequence"},{"location":"references/working-with-Sequences-via-Tooling/#using-a-sequence","text":"After you create a sequence, it appears in the Defined Sequences section of the tool palette. To use this sequence in a mediation flow, click the sequence in the tool palette and then click the spot on the canvas where you want the sequence to appear in the flow. The editor automatically adds any endpoints you used in your sequence. {width=\"900\"} If you want to use a sequence from a different project or from the registry, you need to create a sequence mediator and then refer to the sequence as follows: Click Sequence Mediator on the tool palette, and then click the spot on the canvas where you want the sequence to appear in the mediation workflow. Press Enter to accept the default name for now. In the Properties pane at the bottom of the window, click Static Reference Key , and then click the browse [...] button on the right. {width=\"900\"} In the Resource Key Editor, click Registry if the sequence is stored in the registry or Workspace if it's in another ESB Config project in this Eclipse workspace. If you are trying to select a sequence from the registry and no entries appear in the dialog box, click the add registry connection button (the first button in the upper right corner) and connect to the registry where the sequence resides. Navigate to the sequence you want, select it and click OK , and then click OK again. The sequence mediator name and static reference key are updated to point to the sequence you selected.","title":"Using a sequence"},{"location":"references/writing-Custom-Configuration-Implementations-for-Mediators/","text":"Writing Custom Configuration Implementations for Mediators \u00b6 You can write your own custom configurator for the Mediator implementation you write without relying on the Class Mediator or Spring extension for its initialization. The MediatorFactory implementation - Defines how to digest a custom XML configuration element to be used to create and configure the custom mediator instance. T he MediatorSerializer implementation - Defines how a configuration should be serialized back into an XML configuration. The custom MediatorFactory , MediatorSerializer implementations and the mediator class/es must be bundled as an OSGi bundle exporting these classes and placed into the \\< EI_HOME>/dropins folder, so that the Synapse runtime could find and load the definition. A custom JAR file must bundle your classes implementing the Mediator interface, MediatorSerializer and the MediatorFactory interfaces. It should also contain two text files named org.apache.synapse.config.xml.MediatorFactory and org.apache.synapse.config.xml.MediatorSerializer which will contain the fully qualified name(s) of your MediatorFactory and MediatorSerializer implementation classes. Any dependencies should be made available through OSGi bundles in the same plugins directory. The MediatorFactory interface listing, which you should implement, is given below and its getTagQName() method must define the fully qualified element of interest for custom configuration. The Synapse initialization will call back to this MediatorFactory instance through the createMediator(OMElement elem) method passing in this XML element, so that an instance of the mediator could be created utilizing the custom XML specification and returned. The MediatorFactory Interface \u00b6 package org.apache.synapse.config.xml; import ... /** * A mediator factory capable of creating an instance of a mediator through a given * XML should implement this interface */ public interface MediatorFactory { /** * Creates an instance of the mediator using the OMElement * @param elem * @return the created mediator */ public Mediator createMediator(OMElement elem); /** * The QName of this mediator element in the XML config * @return QName of the mediator element */ public QName getTagQName(); } The MediatorSerializer Interface \u00b6 package org.apache.synapse.config.xml; import ... /** * Interface which should be implemented by mediator serializers. Does the * reverse of the MediatorFactory */ public interface MediatorSerializer { /** * Return the XML representation of this mediator * @param m mediator to be serialized * @param parent the OMElement to which the serialization should be attached * @return the serialized mediator XML */ public OMElement serializeMediator(OMElement parent, Mediator m); /** * Return the class name of the mediator which can be serialized * @return the class name */ public String getMediatorClassName(); }","title":"Writing Custom Configuration Implementations for Mediators"},{"location":"references/writing-Custom-Configuration-Implementations-for-Mediators/#writing-custom-configuration-implementations-for-mediators","text":"You can write your own custom configurator for the Mediator implementation you write without relying on the Class Mediator or Spring extension for its initialization. The MediatorFactory implementation - Defines how to digest a custom XML configuration element to be used to create and configure the custom mediator instance. T he MediatorSerializer implementation - Defines how a configuration should be serialized back into an XML configuration. The custom MediatorFactory , MediatorSerializer implementations and the mediator class/es must be bundled as an OSGi bundle exporting these classes and placed into the \\< EI_HOME>/dropins folder, so that the Synapse runtime could find and load the definition. A custom JAR file must bundle your classes implementing the Mediator interface, MediatorSerializer and the MediatorFactory interfaces. It should also contain two text files named org.apache.synapse.config.xml.MediatorFactory and org.apache.synapse.config.xml.MediatorSerializer which will contain the fully qualified name(s) of your MediatorFactory and MediatorSerializer implementation classes. Any dependencies should be made available through OSGi bundles in the same plugins directory. The MediatorFactory interface listing, which you should implement, is given below and its getTagQName() method must define the fully qualified element of interest for custom configuration. The Synapse initialization will call back to this MediatorFactory instance through the createMediator(OMElement elem) method passing in this XML element, so that an instance of the mediator could be created utilizing the custom XML specification and returned.","title":"Writing Custom Configuration Implementations for Mediators"},{"location":"references/writing-Custom-Configuration-Implementations-for-Mediators/#the-mediatorfactory-interface","text":"package org.apache.synapse.config.xml; import ... /** * A mediator factory capable of creating an instance of a mediator through a given * XML should implement this interface */ public interface MediatorFactory { /** * Creates an instance of the mediator using the OMElement * @param elem * @return the created mediator */ public Mediator createMediator(OMElement elem); /** * The QName of this mediator element in the XML config * @return QName of the mediator element */ public QName getTagQName(); }","title":"The MediatorFactory Interface"},{"location":"references/writing-Custom-Configuration-Implementations-for-Mediators/#the-mediatorserializer-interface","text":"package org.apache.synapse.config.xml; import ... /** * Interface which should be implemented by mediator serializers. Does the * reverse of the MediatorFactory */ public interface MediatorSerializer { /** * Return the XML representation of this mediator * @param m mediator to be serialized * @param parent the OMElement to which the serialization should be attached * @return the serialized mediator XML */ public OMElement serializeMediator(OMElement parent, Mediator m); /** * Return the class name of the mediator which can be serialized * @return the class name */ public String getMediatorClassName(); }","title":"The MediatorSerializer Interface"},{"location":"references/writing-Custom-Mediator-Implementations/","text":"Writing Custom Mediator Implementations \u00b6 The following information concerning writing custom mediators implementations is available: MessageContext Interface Mediator Interface Leaf and Node Mediators, List Mediators and Filter Mediators MessageContext Interface \u00b6 The MessageContext interface is the primary interface of the Synapse API. This essentially defines the per-message context passed through the chain of mediators, for each and every message received and processed by Synapse. Each message instance is wrapped within a MessageContext instance, and the message context is set with the references to the SynapseConfiguration and SynapseEnvironment . The SynapseConfiguration holds the global configuration model that defines mediation rules, local registry entries and other configuration. The SynapseEnvironment gives access to the underlying SOAP implementation used - Apache Axis2. A typical mediator would need to manipulate the MessageContext by referring to the SynapseConfiguration . Info Note It is strongly recommended that the SynapseConfiguration is not updated by mediator instances as it is shared by all messages and may be updated by Synapse administration or configuration modules. Mediator instances may store local message properties into the MessageContext for later retrieval by successive mediators. Info Tip Extending the AbstractMediator class is the easier way to write a new mediator rather than implementing the Mediator interface. MessageContext Interface \u00b6 package org.apache.synapse; import ... public interface MessageContext { /** * Get a reference to the current SynapseConfiguration * * @return the current synapse configuration */ public SynapseConfiguration getConfiguration(); /** * Set or replace the Synapse Configuration instance to be used. May be used to * programmatically change the configuration at runtime etc. * * @param cfg The new synapse configuration instance */ public void setConfiguration(SynapseConfiguration cfg); /** * Returns a reference to the host Synapse Environment * @return the Synapse Environment */ public SynapseEnvironment getEnvironment(); /** * Sets the SynapseEnvironment reference to this context * @param se the reference to the Synapse Environment */ public void setEnvironment(SynapseEnvironment se); /** * Get the value of a custom (local) property set on the message instance * @param key key to look up property * @return value for the given key */ public Object getProperty(String key); /** * Set a custom (local) property with the given name on the message instance * @param key key to be used * @param value value to be saved */ public void setProperty(String key, Object value); /** * Returns the Set of keys over the properties on this message context * @return a Set of keys over message properties */ public Set getPropertyKeySet(); /** * Get the SOAP envelope of this message * @return the SOAP envelope of the message */ public SOAPEnvelope getEnvelope(); /** * Sets the given envelope as the current SOAPEnvelope for this message * @param envelope the envelope to be set * @throws org.apache.axis2.AxisFault on exception */ public void setEnvelope(SOAPEnvelope envelope) throws AxisFault; /** * SOAP message related getters and setters */ public ....get/set()... } The MessageContext interface is based on the Axis2 MessageContext interface and uses the EndpointReference and SOAPEnvelope classes/interfaces. The purpose of this interface is to capture a message as it flows through the system. As you see the message payload is represented using the SOAP infoset. Binary messages can be embedded in the Envelope using MTOM (SOAP Message Transmission Optimization Mechanism) or SwA (SOAP with Attachments) using the AXIOM (AXis Object Model) object model. Mediator Interface \u00b6 The second key interface for mediator writers is the Mediator interface. package org.apache.synapse; import org.apache.synapse.MessageContext; /** * All Synapse mediators must implement this Mediator interface. As a message passes * through the synapse system, each mediator's mediate() method is invoked in the * sequence/order defined in the SynapseConfiguration. */ public interface Mediator { /** * Invokes the mediator passing the current message for mediation. Each * mediator performs its mediation action, and returns true if mediation * should continue, or false if further mediation should be aborted. * * @param synCtx the current message for mediation * @return true if further mediation should continue */ public boolean mediate(MessageContext synCtx); /** * This is used for debugging purposes and exposes the type of the current * mediator for logging and debugging purposes * @return a String representation of the mediator type */ public String getType(); /** * This is used to check whether the tracing should be enabled on the current mediator or not * @return value that indicate whether tracing is on, off or unset */ public int getTraceState(); /** * This is used to set the value of tracing enable variable * @param traceState Set whether the tracing is enabled or not */ public void setTraceState(int traceState); } A mediator can read and/or modify the message in any suitable manner - adjusting the routing headers or changing the message body. If the mediate() method returns \"false\", it signals to the Synapse processing model to stop further processing of the message. For example, if the mediator is a security agent, it may decide that this message is dangerous and should not be processed further. This is generally the exception as mediators are usually designed to co-operate to process the message onwards. Leaf and Node Mediators, List Mediators and Filter Mediators \u00b6 Mediators may be Node mediators (they can contain child mediators) or Leaf mediators (mediators that does not hold any other child mediators). A Node mediator must implement the org.apache.synapse.mediators.ListMediator interface listed below or extend from org.apache.synapse.mediators.AbstractListMediator . The ListMediator Interface \u00b6 package org.apache.synapse.mediators; import java.util.List; /** * The List mediator executes a given sequence/list of child mediators */ public interface ListMediator extends Mediator { /** * Appends the specified mediator to the end of this mediator's (children) list * @param m the mediator to be added * @return true (as per the general contract of the Collection.add method) */ public boolean addChild(Mediator m); /** * Appends all of the mediators in the specified collection to the end of this mediator's (children) * list, in the order that they are returned by the specified collection's iterator * @param c the list of mediators to be added * @return true if this list changed as a result of the call */ public boolean addAll(List c); /** * Returns the mediator at the specified position * @param pos index of mediator to return * @return the mediator at the specified position in this list */ public Mediator getChild(int pos); /** * Removes the first occurrence in this list of the specified mediator * @param m mediator to be removed from this list, if present * @return true if this list contained the specified mediator */ public boolean removeChild(Mediator m); /** * Removes the mediator at the specified position in this list * @param pos the index of the mediator to remove * @return the mediator previously at the specified position */ public Mediator removeChild(int pos); /** * Return the list of mediators of this List mediator instance * @return the child/sub mediator list */ public List getList(); } A ListMediator implementation should call super.mediate(synCtx) to process its sub mediator sequence. A FilterMediator is a ListMediator that executes its sequence of sub mediators on successful outcome of a test condition. The Mediator instance that performs filtering should implement the FilterMediator interface. FilterMediator Interface \u00b6 package org.apache.synapse.mediators; import org.apache.synapse.MessageContext; /** * The filter mediator is a list mediator, which executes the given (sub) list of mediators * if the specified condition is satisfied * * @see FilterMediator#test(org.apache.synapse.MessageContext) */ public interface FilterMediator extends ListMediator { /** * Should return true if the sub/child mediators should execute. i.e. if the filter * condition is satisfied * @param synCtx * @return true if the configured filter condition evaluates to true */ public boolean test(MessageContext synCtx); }","title":"Writing Custom Mediator Implementations"},{"location":"references/writing-Custom-Mediator-Implementations/#writing-custom-mediator-implementations","text":"The following information concerning writing custom mediators implementations is available: MessageContext Interface Mediator Interface Leaf and Node Mediators, List Mediators and Filter Mediators","title":"Writing Custom Mediator Implementations"},{"location":"references/writing-Custom-Mediator-Implementations/#messagecontext-interface","text":"The MessageContext interface is the primary interface of the Synapse API. This essentially defines the per-message context passed through the chain of mediators, for each and every message received and processed by Synapse. Each message instance is wrapped within a MessageContext instance, and the message context is set with the references to the SynapseConfiguration and SynapseEnvironment . The SynapseConfiguration holds the global configuration model that defines mediation rules, local registry entries and other configuration. The SynapseEnvironment gives access to the underlying SOAP implementation used - Apache Axis2. A typical mediator would need to manipulate the MessageContext by referring to the SynapseConfiguration . Info Note It is strongly recommended that the SynapseConfiguration is not updated by mediator instances as it is shared by all messages and may be updated by Synapse administration or configuration modules. Mediator instances may store local message properties into the MessageContext for later retrieval by successive mediators. Info Tip Extending the AbstractMediator class is the easier way to write a new mediator rather than implementing the Mediator interface.","title":"MessageContext Interface"},{"location":"references/writing-Custom-Mediator-Implementations/#messagecontext-interface_1","text":"package org.apache.synapse; import ... public interface MessageContext { /** * Get a reference to the current SynapseConfiguration * * @return the current synapse configuration */ public SynapseConfiguration getConfiguration(); /** * Set or replace the Synapse Configuration instance to be used. May be used to * programmatically change the configuration at runtime etc. * * @param cfg The new synapse configuration instance */ public void setConfiguration(SynapseConfiguration cfg); /** * Returns a reference to the host Synapse Environment * @return the Synapse Environment */ public SynapseEnvironment getEnvironment(); /** * Sets the SynapseEnvironment reference to this context * @param se the reference to the Synapse Environment */ public void setEnvironment(SynapseEnvironment se); /** * Get the value of a custom (local) property set on the message instance * @param key key to look up property * @return value for the given key */ public Object getProperty(String key); /** * Set a custom (local) property with the given name on the message instance * @param key key to be used * @param value value to be saved */ public void setProperty(String key, Object value); /** * Returns the Set of keys over the properties on this message context * @return a Set of keys over message properties */ public Set getPropertyKeySet(); /** * Get the SOAP envelope of this message * @return the SOAP envelope of the message */ public SOAPEnvelope getEnvelope(); /** * Sets the given envelope as the current SOAPEnvelope for this message * @param envelope the envelope to be set * @throws org.apache.axis2.AxisFault on exception */ public void setEnvelope(SOAPEnvelope envelope) throws AxisFault; /** * SOAP message related getters and setters */ public ....get/set()... } The MessageContext interface is based on the Axis2 MessageContext interface and uses the EndpointReference and SOAPEnvelope classes/interfaces. The purpose of this interface is to capture a message as it flows through the system. As you see the message payload is represented using the SOAP infoset. Binary messages can be embedded in the Envelope using MTOM (SOAP Message Transmission Optimization Mechanism) or SwA (SOAP with Attachments) using the AXIOM (AXis Object Model) object model.","title":"MessageContext Interface"},{"location":"references/writing-Custom-Mediator-Implementations/#mediator-interface","text":"The second key interface for mediator writers is the Mediator interface. package org.apache.synapse; import org.apache.synapse.MessageContext; /** * All Synapse mediators must implement this Mediator interface. As a message passes * through the synapse system, each mediator's mediate() method is invoked in the * sequence/order defined in the SynapseConfiguration. */ public interface Mediator { /** * Invokes the mediator passing the current message for mediation. Each * mediator performs its mediation action, and returns true if mediation * should continue, or false if further mediation should be aborted. * * @param synCtx the current message for mediation * @return true if further mediation should continue */ public boolean mediate(MessageContext synCtx); /** * This is used for debugging purposes and exposes the type of the current * mediator for logging and debugging purposes * @return a String representation of the mediator type */ public String getType(); /** * This is used to check whether the tracing should be enabled on the current mediator or not * @return value that indicate whether tracing is on, off or unset */ public int getTraceState(); /** * This is used to set the value of tracing enable variable * @param traceState Set whether the tracing is enabled or not */ public void setTraceState(int traceState); } A mediator can read and/or modify the message in any suitable manner - adjusting the routing headers or changing the message body. If the mediate() method returns \"false\", it signals to the Synapse processing model to stop further processing of the message. For example, if the mediator is a security agent, it may decide that this message is dangerous and should not be processed further. This is generally the exception as mediators are usually designed to co-operate to process the message onwards.","title":"Mediator Interface"},{"location":"references/writing-Custom-Mediator-Implementations/#leaf-and-node-mediators-list-mediators-and-filter-mediators","text":"Mediators may be Node mediators (they can contain child mediators) or Leaf mediators (mediators that does not hold any other child mediators). A Node mediator must implement the org.apache.synapse.mediators.ListMediator interface listed below or extend from org.apache.synapse.mediators.AbstractListMediator .","title":"Leaf and Node Mediators, List Mediators and Filter Mediators"},{"location":"references/writing-Custom-Mediator-Implementations/#the-listmediator-interface","text":"package org.apache.synapse.mediators; import java.util.List; /** * The List mediator executes a given sequence/list of child mediators */ public interface ListMediator extends Mediator { /** * Appends the specified mediator to the end of this mediator's (children) list * @param m the mediator to be added * @return true (as per the general contract of the Collection.add method) */ public boolean addChild(Mediator m); /** * Appends all of the mediators in the specified collection to the end of this mediator's (children) * list, in the order that they are returned by the specified collection's iterator * @param c the list of mediators to be added * @return true if this list changed as a result of the call */ public boolean addAll(List c); /** * Returns the mediator at the specified position * @param pos index of mediator to return * @return the mediator at the specified position in this list */ public Mediator getChild(int pos); /** * Removes the first occurrence in this list of the specified mediator * @param m mediator to be removed from this list, if present * @return true if this list contained the specified mediator */ public boolean removeChild(Mediator m); /** * Removes the mediator at the specified position in this list * @param pos the index of the mediator to remove * @return the mediator previously at the specified position */ public Mediator removeChild(int pos); /** * Return the list of mediators of this List mediator instance * @return the child/sub mediator list */ public List getList(); } A ListMediator implementation should call super.mediate(synCtx) to process its sub mediator sequence. A FilterMediator is a ListMediator that executes its sequence of sub mediators on successful outcome of a test condition. The Mediator instance that performs filtering should implement the FilterMediator interface.","title":"The ListMediator Interface"},{"location":"references/writing-Custom-Mediator-Implementations/#filtermediator-interface","text":"package org.apache.synapse.mediators; import org.apache.synapse.MessageContext; /** * The filter mediator is a list mediator, which executes the given (sub) list of mediators * if the specified condition is satisfied * * @see FilterMediator#test(org.apache.synapse.MessageContext) */ public interface FilterMediator extends ListMediator { /** * Should return true if the sub/child mediators should execute. i.e. if the filter * condition is satisfied * @param synCtx * @return true if the configured filter condition evaluates to true */ public boolean test(MessageContext synCtx); }","title":"FilterMediator Interface"},{"location":"references/xQuery-Mediator/","text":"XQuery Mediator \u00b6 The XQuery Mediator performs an XQuery transformation on messages. Info The XQuery mediator is a content aware mediator. Syntax | Configuration | Examples Syntax \u00b6 <xquery key=\"string\" [target=\"xpath\"]> <variable name=\"string\" type=\"string\" [key=\"string\"] [expression=\"xpath\"] [value=\"string\"]/>? </xquery> Configuration \u00b6 The parameters available to configure the XQuery mediator are as follows. Parameter Name Description Key Type This parameter specifies whether the key which represents the XQuery transformation should be a static key or a dynamic key. Static : If this is selected, the key would be a static value. This value should be selected from the Registry for the Key parameter. Dynamic : If this is selected, the key would be a dynamic value which has to be evaluated via an XPath expression. The relevant XPath expression can be entered in the Key parameter. Key The key that represents the XQuery transformation. The value you enter depends on the value you selected for the Key Type parameter. If you selected Static for the Key Type parameter, click Configuration Registry or Governance Registry as relevant to select the key from the resource tree. If you selected Dynamic for the Key Type parameter, enter the XPatch expression which calculates the dynamic key. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Target This parameter specifies the node of the message to which the XQuery transformation should be applied. The node is evaluated via an XPath expression. If no value is specified for this parameter, the XQuery transformation is applied to the first child of the SOAP body. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Add Variable This link allows you to define one or more variables that could be bound to the dynamic context of the XQuery engine in order to be accessed during the XQuery script invocation. Click Add Variable to add a variable to the XQuery mediator configuration. The page will expand to display parameters relating to variables. The parameters displayed would differ depending on whether you select Value or Expression as the variable value type. Click on the relevant tab below to view the relevant UI configuration for variables. Value Expression Parameter Name Description Variable Type The data type of the variable. Supported values are as follows. INT INTEGER BOOLEAN BYTE DOUBLE SHORT LONG FLOAT STRING DOCUMENT ELEMENT Variable Name The name of the variable. It should correspond to the name of the variable declaration in the XQuery script. Value Type This parameter specifies whether the variable value should be a static value or a dynamic value Value : If this is selected, the variable value is a static value. The static value should be entered in the Value/Experession parameter. Expression : If this is selected the variable value is a dynamic value. The XPath expression to calculate it should be entered in the Value/Experession parameter. Value/Expression This parameter is used to enter the variable value. This can be a static value or an expression based on the value you selected for the Value Type parameter. Action This parameter allows the variable to be deleted. Parameter Name Description Variable Type The data type of the variable. This should be a valid type defined by the JSR-000225 (XQJ API). Supported values are as follows. INT INTEGER BOOLEAN BYTE DOUBLE SHORT LONG FLOAT STRING DOCUMENT DOCUMENT_ELEMENT ELEMENT Variable Name The name of the variable. It should correspond to the name of the variable declaration in the XQuery script. Value Type This parameter specifies whether the variable value should be a static value or a dynamic value Value : If this is selected, the variable value is a static value. The static value should be entered in the Value/Experession parameter. Expression : If this is selected the variable value is a dynamic value. The XPath expression to calculate it should be entered in the Value/Experession parameter. Value/Expression This parameter is used to enter the variable value. This can be a static value or an expression based on the value you selected for the Value Type parameter. Registry Key The key to acces the variable value if it is saved in the Registry. Click either Configuration Registry or Governance Registry in the Registry Browser parameter as relevant to select the required key from the resource tree. Registry Browser If the variable value is saved in the Registry, click either Configuration Registry or Governance Registry in the Registry Browser parameter as relevant to select the required key from the resource tree. NS Editor You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This parameter allows the variable to be deleted. Examples \u00b6 In this configuration, the XQuery script is saved in the registry, and it can be accessed via the xquery\\example.xq key. The XQuery configuration has one variable named payload of which the variable type is ELEMENT . As there is no expression in the variable definitions, the default value of the first child of the SOAP Body is used as the value of the variable p ayload . Within the XQuery script, you can access this variable by defining declare variable $payload as document-node() external; . <xquery key=\"xquery\\example.xq\"> <variable name=\"payload\" type=\"ELEMENT\"/> </xquery> Variables \u00b6 The following variable picks an XML resource from the registry using key misc/commission.xml and binds into XQuery Runtime so that it can be accessed within the XQuery script. <variable name=\"commission\" type=\"ELEMENT\" key=\"misc/commission.xml\"></variable> The value of the following variable is calculated from the current message SOAP Payload using an expression. The value type of the variable is DOUBLE . <variable name=\"price\" type=\"DOUBLE\" expression=\"self::node()//m0:return/m0:last/child::text()\" xmlns:m0=\"http://services.samples/xsd\"/>","title":"XQuery Mediator"},{"location":"references/xQuery-Mediator/#xquery-mediator","text":"The XQuery Mediator performs an XQuery transformation on messages. Info The XQuery mediator is a content aware mediator. Syntax | Configuration | Examples","title":"XQuery Mediator"},{"location":"references/xQuery-Mediator/#syntax","text":"<xquery key=\"string\" [target=\"xpath\"]> <variable name=\"string\" type=\"string\" [key=\"string\"] [expression=\"xpath\"] [value=\"string\"]/>? </xquery>","title":"Syntax"},{"location":"references/xQuery-Mediator/#configuration","text":"The parameters available to configure the XQuery mediator are as follows. Parameter Name Description Key Type This parameter specifies whether the key which represents the XQuery transformation should be a static key or a dynamic key. Static : If this is selected, the key would be a static value. This value should be selected from the Registry for the Key parameter. Dynamic : If this is selected, the key would be a dynamic value which has to be evaluated via an XPath expression. The relevant XPath expression can be entered in the Key parameter. Key The key that represents the XQuery transformation. The value you enter depends on the value you selected for the Key Type parameter. If you selected Static for the Key Type parameter, click Configuration Registry or Governance Registry as relevant to select the key from the resource tree. If you selected Dynamic for the Key Type parameter, enter the XPatch expression which calculates the dynamic key. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Target This parameter specifies the node of the message to which the XQuery transformation should be applied. The node is evaluated via an XPath expression. If no value is specified for this parameter, the XQuery transformation is applied to the first child of the SOAP body. !!! tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Add Variable This link allows you to define one or more variables that could be bound to the dynamic context of the XQuery engine in order to be accessed during the XQuery script invocation. Click Add Variable to add a variable to the XQuery mediator configuration. The page will expand to display parameters relating to variables. The parameters displayed would differ depending on whether you select Value or Expression as the variable value type. Click on the relevant tab below to view the relevant UI configuration for variables. Value Expression Parameter Name Description Variable Type The data type of the variable. Supported values are as follows. INT INTEGER BOOLEAN BYTE DOUBLE SHORT LONG FLOAT STRING DOCUMENT ELEMENT Variable Name The name of the variable. It should correspond to the name of the variable declaration in the XQuery script. Value Type This parameter specifies whether the variable value should be a static value or a dynamic value Value : If this is selected, the variable value is a static value. The static value should be entered in the Value/Experession parameter. Expression : If this is selected the variable value is a dynamic value. The XPath expression to calculate it should be entered in the Value/Experession parameter. Value/Expression This parameter is used to enter the variable value. This can be a static value or an expression based on the value you selected for the Value Type parameter. Action This parameter allows the variable to be deleted. Parameter Name Description Variable Type The data type of the variable. This should be a valid type defined by the JSR-000225 (XQJ API). Supported values are as follows. INT INTEGER BOOLEAN BYTE DOUBLE SHORT LONG FLOAT STRING DOCUMENT DOCUMENT_ELEMENT ELEMENT Variable Name The name of the variable. It should correspond to the name of the variable declaration in the XQuery script. Value Type This parameter specifies whether the variable value should be a static value or a dynamic value Value : If this is selected, the variable value is a static value. The static value should be entered in the Value/Experession parameter. Expression : If this is selected the variable value is a dynamic value. The XPath expression to calculate it should be entered in the Value/Experession parameter. Value/Expression This parameter is used to enter the variable value. This can be a static value or an expression based on the value you selected for the Value Type parameter. Registry Key The key to acces the variable value if it is saved in the Registry. Click either Configuration Registry or Governance Registry in the Registry Browser parameter as relevant to select the required key from the resource tree. Registry Browser If the variable value is saved in the Registry, click either Configuration Registry or Governance Registry in the Registry Browser parameter as relevant to select the required key from the resource tree. NS Editor You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Action This parameter allows the variable to be deleted.","title":"Configuration"},{"location":"references/xQuery-Mediator/#examples","text":"In this configuration, the XQuery script is saved in the registry, and it can be accessed via the xquery\\example.xq key. The XQuery configuration has one variable named payload of which the variable type is ELEMENT . As there is no expression in the variable definitions, the default value of the first child of the SOAP Body is used as the value of the variable p ayload . Within the XQuery script, you can access this variable by defining declare variable $payload as document-node() external; . <xquery key=\"xquery\\example.xq\"> <variable name=\"payload\" type=\"ELEMENT\"/> </xquery>","title":"Examples"},{"location":"references/xQuery-Mediator/#variables","text":"The following variable picks an XML resource from the registry using key misc/commission.xml and binds into XQuery Runtime so that it can be accessed within the XQuery script. <variable name=\"commission\" type=\"ELEMENT\" key=\"misc/commission.xml\"></variable> The value of the following variable is calculated from the current message SOAP Payload using an expression. The value type of the variable is DOUBLE . <variable name=\"price\" type=\"DOUBLE\" expression=\"self::node()//m0:return/m0:last/child::text()\" xmlns:m0=\"http://services.samples/xsd\"/>","title":"Variables"},{"location":"references/xSLT-Mediator/","text":"XSLT Mediator \u00b6 The XSLT Mediator applies a specified XSLT transformation to a selected element of the current message payload. In addition, you can: Specify properties already included in the mediation flow to be added to the XSLT script as XSLT parameters. Specify features to be enabled/disabled in the XSLT transformation. Import external XSLT scripts to the main XSLT script of the XSLT mediator by adding them as resources. Info The XSLT mediator is a content aware mediator. Syntax | Configuration | Examples | Samples Syntax \u00b6 <xslt key=\"string\" [source=\"xpath\"]> <property name=\"string\" (value=\"literal\" | expression=\"xpath\")/>* <feature name=\"string\" value=\"true| false\" />* <resource location=\"string\" key=\"string\"/>* </xslt> Configuration \u00b6 The parameters available for configuring the XSLT mediator are as follows. Parameter Name Description Key Type You can select one of the following options. Static Key : If this is selected, an existing key can be selected from the registry for the Key parameter. Dynamic Key : If this is selected, the key can be entered dynamically in the Key parameter. Key This specifies the registry key to refer the XSLT to. This supports static and dynamic keys. Source This determines the element to which the given XSLT transformation should be applied via an XPath expression. If the source element is not specified, the XSLT transformation is applied to the first child of the SOAP body. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Properties of the XSLT mediator This section is used to inject properties set in the mediation flow to the XSLT script as XSLT parameters. These are referred from the XSLT in transformation using the get-property(prop-name) XPath extension function. Parameters relating to the properties are as follows. Property Name : The name of the property to be passed into the transformations. Property Type : This specifies whether the property is given as a static value or an XPath expression. Value/Expression - This defines the static value or the XPath expression. Action - This parameter allows the property to be removed from the XSLT script if required. !!! info For example, define the transform.xslt.result.disableBuild property as shown below, to escape building the message at the XSLT transformation. It avoids replacing encoded values with real characters. E.g., If you do not add this property, \" &#10; \" in your XML content will be replaced by a new line, when the XML content is built at the XSLT mediator. Features of the XSLT mediator This section is used to specify features to be enabled/disabled in the XSLT transformation. For example, adding the http://ws.apache.org/ns/synapse/transform/feature/dom feature turns on DOM-based transformations instead of serializing elements into byte streams and/or temporary files. This approach can improve performance but might not work for all transformations. Parameters relating to the features are as follows. Feature Name : The name of the feature to be enabled/disabled in the XSLT transformation. Feature Value : This specified whether the feature is enabled or not. Select True to enable the feature or False to disable it. Action : This allows you to remove the feature from the XSLT transformation if required. Resources of the XSLT mediator This section is used to import external XSLT scripts to the main XSLT scripts defined in the XSLT mediator. The XSLT scripts to be imported are first added as resources in the registry. Parameters relating to the resources are as follows. Location : The location where the XSLT script to be imported is saved as a resource. Key : The registry key to which the XSLT should be referred. Browse for the relevant key in the Configuration registry or the Governance registry. Action : This allows you to remove the imported XSLT script added as a resource if required. Examples \u00b6 Example 1 - Applying a XSLT transformation to a element selected based on an XPath expression Example 2 - Adding properties as XSLT parameters Example 3 - Adding XSLT imports as resources Example 4 - Adding CDATA to be displayed in the output Example 1 - Applying a XSLT transformation to a element selected based on an XPath expression \u00b6 In this example, the XSLT can be picked by the key transform/example.xslt and the XSLT would be applied to a part of the message that is specified as an XPath expression. In this case, it is applied to s11:Body/child the message. <xslt xmlns=\"http://ws.apache.org/ns/synapse\" key=\"transform/example.xslt\" source=\"s11:Body/child\" /> Example 2 - Adding properties as XSLT parameters \u00b6 In this example, a property named PARAM_NAME is added to the XSLT script as an XSLT parameter. A XPath expression is used to assign this property the value of the ORDER_ID property in the default scope. <xslt key=\"keyToXSLTFile\"> <property expression=\"$ctx:ORDER_ID\" name=\"PARAM_NAME\"> </property></xslt> The XSLT script with the PARAM_NAME property added would look as follows. <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:param name=\"PARAM_NAME\"></xsl:param> <xsl:template match=\"/\"> <orders xmlns=\"http://services.samples\"> <xsl:attribute name=\"id\"> <xsl:value-of select=\"$PARAM_NAME\"> </xsl:value-of></xsl:attribute> </orders> </xsl:template> </xsl:stylesheet> Example 3 - Adding XSLT imports as resources \u00b6 In this example, two XSLT files saved in the registry under conf:/ as resources are imported to the main XSLT script of the XSLT mediator. xslt1.xslt <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:template match=\"//people/person\" name=\"FILL_PPL\"> <client> <firstname> <xsl:value-of select=\"firstname\"> </xsl:value-of></firstname> <lastname> <xsl:value-of select=\"lastname\"> </xsl:value-of></lastname> <age> <xsl:value-of select=\"age\"> </xsl:value-of></age> <country> <xsl:value-of select=\"country\"> </xsl:value-of></country> </client> </xsl:template> </xsl:stylesheet> xslt2.xslt <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:import href=\"xslt1.xslt\" /> <xsl:template match=\"/\"> <clients> <xsl:for-each select=\"//people/person\"> <xsl:call-template name=\"FILL_PPL\"></xsl:call-template> </xsl:for-each> </clients> </xsl:template> </xsl:stylesheet> \\<xsl:include href=\"xslt1.xslt\"> element indicates that the xslt1.xslt is included in xslt2.xslt . These two files can be imported to the script of the XSLT mediator as follows. <xslt key=\"conf:/xslt2.xslt\"> <resource key=\"conf:/xslt1.xslt\" location=\"xslt1.xslt\"> </resource></xslt> The following SOAP request can be used to test the above configuration of the XSLT mediator included in a proxy configuration. <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header> <soapenv:Body> <people> <person> <firstname>Isuru</firstname> <lastname>Udana</lastname> <gender>Male</gender> <age>26</age> <country>SriLanka</country> </person> <person> <firstname>Ishan</firstname> <lastname>Jayawardena</lastname> <gender>Male</gender> <age>26</age> <country>SriLanka</country> </person> </people> </soapenv:Body> </soapenv:Header></soapenv:Envelope> Example 4 - Adding CDATA to be displayed in the output \u00b6 Follow the steps below to add CDATA to display them in the output without processing them via the XSLT transformation. Create a file named XMLInputFactory.properties inside the <EI_HOME> directory, and include the following property in it: javax.xml.stream.isCoalescing= false Add the <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" indent=\"yes\"/> attribute to the XSL stylesheet. In the XSL stylesheet, wrap the encoded CDATA within the <xsl:text> elements with the disable-output-escaping=\"yes\" parameter as shown below. <xsl:text disable-output-escaping=\"yes\">&lt;![CDATA[</xsl:text> <xsl:copy-of select=\"*\"/> <xsl:text disable-output-escaping=\"yes\">]]&gt;</xsl:text> The following is an example of a XSL stylesheet, which includes CDATA to be displayed in the output. <xsl:stylesheet version=\"2.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" indent=\"yes\"/> <xsl:template match=\"/\"> <root> <xsl:copy> <xsl:text disable-output-escaping=\"yes\">&lt;![CDATA[</xsl:text> <xsl:copy-of select=\"*\"/> <xsl:text disable-output-escaping=\"yes\">]]&gt;</xsl:text> </xsl:copy> </root> </xsl:template> </xsl:stylesheet> You can use the following Synapse configuration to process the above XSL stylesheet via a XSLT mediator. In the above configuration, the XSL stylesheet is defined as a local entry named XSLTTest , and it is referred in the XSLT mediator configuration via the key attribute within the proxy service named XSLTProxy . <proxy name=\"XSLTProxy\" startOnLoad=\"true\" transports=\"http https\"> <description/> <target> <inSequence> <xslt key=\"XSLTTest\"/> <log level=\"full\"/> <respond/> </inSequence> </target> </proxy> <localEntry key=\"XSLTTest\"> <xsl:stylesheet version=\"2.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" indent=\"yes\"/> <xsl:template match=\"/\"> <root> <xsl:copy> <xsl:text disable-output-escaping=\"yes\">&lt;![CDATA[</xsl:text> <xsl:copy-of select=\"*\"/> <xsl:text disable-output-escaping=\"yes\">]]&gt;</xsl:text> </xsl:copy> </root> </xsl:template> </xsl:stylesheet> <description/> </localEntry> For example, pass the following payload to the XSLTProxy proxy service of the above configuration. <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header/> <soapenv:Body> <abc>testabc</abc> </soapenv:Body> </soapenv:Envelope> You view the output with the CDATA displayed as follows in the Console logs of the WSO2 EI server. INFO - LogMediator To: /services/XSLTProxy.XSLTProxyHttpSoap11Endpoint, WSAction: urn:mediate, SOAPAction: urn:mediate, MessageID: urn:uuid:266d380f-800f-479b-bee9-c30897efe562, Direction: request, Envelope: <?xml version='1.0' encoding='utf-8'?> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Body> <root xmlns=\"http://ws.apache.org/ns/synapse\"><![CDATA[<abc xmlns=\"\">testabc</abc>]]></root> </soapenv:Body></soapenv:Envelope> Samples \u00b6 Sample 440: Converting JSON to XML Using XSLT Sample 8: Introduction to Static and Dynamic Registry Resources and Using XSLT Transformations","title":"XSLT Mediator"},{"location":"references/xSLT-Mediator/#xslt-mediator","text":"The XSLT Mediator applies a specified XSLT transformation to a selected element of the current message payload. In addition, you can: Specify properties already included in the mediation flow to be added to the XSLT script as XSLT parameters. Specify features to be enabled/disabled in the XSLT transformation. Import external XSLT scripts to the main XSLT script of the XSLT mediator by adding them as resources. Info The XSLT mediator is a content aware mediator. Syntax | Configuration | Examples | Samples","title":"XSLT Mediator"},{"location":"references/xSLT-Mediator/#syntax","text":"<xslt key=\"string\" [source=\"xpath\"]> <property name=\"string\" (value=\"literal\" | expression=\"xpath\")/>* <feature name=\"string\" value=\"true| false\" />* <resource location=\"string\" key=\"string\"/>* </xslt>","title":"Syntax"},{"location":"references/xSLT-Mediator/#configuration","text":"The parameters available for configuring the XSLT mediator are as follows. Parameter Name Description Key Type You can select one of the following options. Static Key : If this is selected, an existing key can be selected from the registry for the Key parameter. Dynamic Key : If this is selected, the key can be entered dynamically in the Key parameter. Key This specifies the registry key to refer the XSLT to. This supports static and dynamic keys. Source This determines the element to which the given XSLT transformation should be applied via an XPath expression. If the source element is not specified, the XSLT transformation is applied to the first child of the SOAP body. !!! info Tip You can click NameSpaces to add namespaces if you are providing an expression. Then the Namespace Editor panel would appear where you can provide any number of namespace prefixes and URLs used in the XPath expression. Properties of the XSLT mediator This section is used to inject properties set in the mediation flow to the XSLT script as XSLT parameters. These are referred from the XSLT in transformation using the get-property(prop-name) XPath extension function. Parameters relating to the properties are as follows. Property Name : The name of the property to be passed into the transformations. Property Type : This specifies whether the property is given as a static value or an XPath expression. Value/Expression - This defines the static value or the XPath expression. Action - This parameter allows the property to be removed from the XSLT script if required. !!! info For example, define the transform.xslt.result.disableBuild property as shown below, to escape building the message at the XSLT transformation. It avoids replacing encoded values with real characters. E.g., If you do not add this property, \" &#10; \" in your XML content will be replaced by a new line, when the XML content is built at the XSLT mediator. Features of the XSLT mediator This section is used to specify features to be enabled/disabled in the XSLT transformation. For example, adding the http://ws.apache.org/ns/synapse/transform/feature/dom feature turns on DOM-based transformations instead of serializing elements into byte streams and/or temporary files. This approach can improve performance but might not work for all transformations. Parameters relating to the features are as follows. Feature Name : The name of the feature to be enabled/disabled in the XSLT transformation. Feature Value : This specified whether the feature is enabled or not. Select True to enable the feature or False to disable it. Action : This allows you to remove the feature from the XSLT transformation if required. Resources of the XSLT mediator This section is used to import external XSLT scripts to the main XSLT scripts defined in the XSLT mediator. The XSLT scripts to be imported are first added as resources in the registry. Parameters relating to the resources are as follows. Location : The location where the XSLT script to be imported is saved as a resource. Key : The registry key to which the XSLT should be referred. Browse for the relevant key in the Configuration registry or the Governance registry. Action : This allows you to remove the imported XSLT script added as a resource if required.","title":"Configuration"},{"location":"references/xSLT-Mediator/#examples","text":"Example 1 - Applying a XSLT transformation to a element selected based on an XPath expression Example 2 - Adding properties as XSLT parameters Example 3 - Adding XSLT imports as resources Example 4 - Adding CDATA to be displayed in the output","title":"Examples"},{"location":"references/xSLT-Mediator/#example-1-applying-a-xslt-transformation-to-a-element-selected-based-on-an-xpath-expression","text":"In this example, the XSLT can be picked by the key transform/example.xslt and the XSLT would be applied to a part of the message that is specified as an XPath expression. In this case, it is applied to s11:Body/child the message. <xslt xmlns=\"http://ws.apache.org/ns/synapse\" key=\"transform/example.xslt\" source=\"s11:Body/child\" />","title":"Example 1 - Applying a XSLT transformation to a element selected based on an XPath expression"},{"location":"references/xSLT-Mediator/#example-2-adding-properties-as-xslt-parameters","text":"In this example, a property named PARAM_NAME is added to the XSLT script as an XSLT parameter. A XPath expression is used to assign this property the value of the ORDER_ID property in the default scope. <xslt key=\"keyToXSLTFile\"> <property expression=\"$ctx:ORDER_ID\" name=\"PARAM_NAME\"> </property></xslt> The XSLT script with the PARAM_NAME property added would look as follows. <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:param name=\"PARAM_NAME\"></xsl:param> <xsl:template match=\"/\"> <orders xmlns=\"http://services.samples\"> <xsl:attribute name=\"id\"> <xsl:value-of select=\"$PARAM_NAME\"> </xsl:value-of></xsl:attribute> </orders> </xsl:template> </xsl:stylesheet>","title":"Example 2 - \u00a0Adding properties as XSLT parameters"},{"location":"references/xSLT-Mediator/#example-3-adding-xslt-imports-as-resources","text":"In this example, two XSLT files saved in the registry under conf:/ as resources are imported to the main XSLT script of the XSLT mediator. xslt1.xslt <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:template match=\"//people/person\" name=\"FILL_PPL\"> <client> <firstname> <xsl:value-of select=\"firstname\"> </xsl:value-of></firstname> <lastname> <xsl:value-of select=\"lastname\"> </xsl:value-of></lastname> <age> <xsl:value-of select=\"age\"> </xsl:value-of></age> <country> <xsl:value-of select=\"country\"> </xsl:value-of></country> </client> </xsl:template> </xsl:stylesheet> xslt2.xslt <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:import href=\"xslt1.xslt\" /> <xsl:template match=\"/\"> <clients> <xsl:for-each select=\"//people/person\"> <xsl:call-template name=\"FILL_PPL\"></xsl:call-template> </xsl:for-each> </clients> </xsl:template> </xsl:stylesheet> \\<xsl:include href=\"xslt1.xslt\"> element indicates that the xslt1.xslt is included in xslt2.xslt . These two files can be imported to the script of the XSLT mediator as follows. <xslt key=\"conf:/xslt2.xslt\"> <resource key=\"conf:/xslt1.xslt\" location=\"xslt1.xslt\"> </resource></xslt> The following SOAP request can be used to test the above configuration of the XSLT mediator included in a proxy configuration. <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header> <soapenv:Body> <people> <person> <firstname>Isuru</firstname> <lastname>Udana</lastname> <gender>Male</gender> <age>26</age> <country>SriLanka</country> </person> <person> <firstname>Ishan</firstname> <lastname>Jayawardena</lastname> <gender>Male</gender> <age>26</age> <country>SriLanka</country> </person> </people> </soapenv:Body> </soapenv:Header></soapenv:Envelope>","title":"Example 3 - Adding XSLT imports as resources"},{"location":"references/xSLT-Mediator/#example-4-adding-cdata-to-be-displayed-in-the-output","text":"Follow the steps below to add CDATA to display them in the output without processing them via the XSLT transformation. Create a file named XMLInputFactory.properties inside the <EI_HOME> directory, and include the following property in it: javax.xml.stream.isCoalescing= false Add the <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" indent=\"yes\"/> attribute to the XSL stylesheet. In the XSL stylesheet, wrap the encoded CDATA within the <xsl:text> elements with the disable-output-escaping=\"yes\" parameter as shown below. <xsl:text disable-output-escaping=\"yes\">&lt;![CDATA[</xsl:text> <xsl:copy-of select=\"*\"/> <xsl:text disable-output-escaping=\"yes\">]]&gt;</xsl:text> The following is an example of a XSL stylesheet, which includes CDATA to be displayed in the output. <xsl:stylesheet version=\"2.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" indent=\"yes\"/> <xsl:template match=\"/\"> <root> <xsl:copy> <xsl:text disable-output-escaping=\"yes\">&lt;![CDATA[</xsl:text> <xsl:copy-of select=\"*\"/> <xsl:text disable-output-escaping=\"yes\">]]&gt;</xsl:text> </xsl:copy> </root> </xsl:template> </xsl:stylesheet> You can use the following Synapse configuration to process the above XSL stylesheet via a XSLT mediator. In the above configuration, the XSL stylesheet is defined as a local entry named XSLTTest , and it is referred in the XSLT mediator configuration via the key attribute within the proxy service named XSLTProxy . <proxy name=\"XSLTProxy\" startOnLoad=\"true\" transports=\"http https\"> <description/> <target> <inSequence> <xslt key=\"XSLTTest\"/> <log level=\"full\"/> <respond/> </inSequence> </target> </proxy> <localEntry key=\"XSLTTest\"> <xsl:stylesheet version=\"2.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"> <xsl:output method=\"xml\" omit-xml-declaration=\"yes\" indent=\"yes\"/> <xsl:template match=\"/\"> <root> <xsl:copy> <xsl:text disable-output-escaping=\"yes\">&lt;![CDATA[</xsl:text> <xsl:copy-of select=\"*\"/> <xsl:text disable-output-escaping=\"yes\">]]&gt;</xsl:text> </xsl:copy> </root> </xsl:template> </xsl:stylesheet> <description/> </localEntry> For example, pass the following payload to the XSLTProxy proxy service of the above configuration. <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Header/> <soapenv:Body> <abc>testabc</abc> </soapenv:Body> </soapenv:Envelope> You view the output with the CDATA displayed as follows in the Console logs of the WSO2 EI server. INFO - LogMediator To: /services/XSLTProxy.XSLTProxyHttpSoap11Endpoint, WSAction: urn:mediate, SOAPAction: urn:mediate, MessageID: urn:uuid:266d380f-800f-479b-bee9-c30897efe562, Direction: request, Envelope: <?xml version='1.0' encoding='utf-8'?> <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\"> <soapenv:Body> <root xmlns=\"http://ws.apache.org/ns/synapse\"><![CDATA[<abc xmlns=\"\">testabc</abc>]]></root> </soapenv:Body></soapenv:Envelope>","title":"Example 4 - Adding CDATA to be displayed in the output"},{"location":"references/xSLT-Mediator/#samples","text":"Sample 440: Converting JSON to XML Using XSLT Sample 8: Introduction to Static and Dynamic Registry Resources and Using XSLT Transformations","title":"Samples"},{"location":"setup/adding_a_custom_proxy__path/","text":"Adding a Custom Proxy Path \u00b6 Adding a custom proxy path is useful when you have a proxy server fronting your Carbon server. In this scenario, the \"custom proxy path\" is used for mapping a proxy url with the actual url of your Carbon server, which allows clients to access the Carbon server with the proxy url. This feature is particularly useful when multiple WSO2 products are hosted under the same domain name. For example, consider that you have three WSO2 products; Application Server, API Manager and ESB, deployed in your production environment and you want all of them to be hosted with the \"wso2test.com\" domain. By using a reverse proxy and by configuring your servers with 'custom proxy paths' , you can host all products under a single domain and assign proxy paths for each product separately as shown below: Proxy URLs mapped to Carbon server URLs: https://10.100.1.1:\\<ListeningPort-apimanager>/carbon mapped to https://wso2test.com/apimanager . https://10.100.1.1:\\<ListeningPort-esb>/carbon mapped to https://wso2test.com/esb . https://10.100.1.1:\\<ListeningPort-appserver>/carbon mapped to https://wso2test.com/appserver . Note Note the following: This functionality is only available for WSO2 products that are based on Carbon 4.3.0 or a later Carbon version. See the WSO2 product release matrix for more information about WSO2 Carbon platform releases. Once you have configured your products with a proxy server, it will no longer be possible to access the product behind the proxy. See the section given below on configuring products to use the proxy server for more information. In the above example, \"apimanager\", \"esb\" and \"appserver\" are the \"proxy context paths\" of the respective products, which are configured in the carbon.xml file (stored in <PRODUCT_HOME>/repository/conf/ directory) for each product. When a client sends a request to the proxy entry url path, e.g. https://wso2test.com/apimanager , the request is directed to the back-end service url ( https://10.100.1.1:\\<PortNumber>/carbon ) where the original service lies. Eventually, the client has to be served via the requested proxy entry url path. The mapping between the proxy url path and the back-end service url path is resolved by the reverse proxy server fronting the back-end service. Info Prior to this solution, it was necessary to host these products as sub domains of the \"wso2.com\" domain as: https://apim.wso2.com , https://esb.wso2.com , https://as.wso2.com . Access WSO2 products through a custom proxy path \u00b6 This functionality will be demonstrated in this documentation using two WSO2 product servers as examples; WSO2 Application Server and WSO2 ESB as the back-end servers, and nginx as the reverse proxy. Follow the steps given below. Step 1: Install and configure a reverse proxy Step 2: Configure products with proxy context path Step 3: Start the Product Step 1: Install and configure a reverse proxy \u00b6 Download nginx server . Install the nginx server in your deployment server by executing the following command: sudo apt-get install nginx Create a folder called \"ssl\" inside /etc/nginx, and create the ssl certificates inside this folder by executing the following commands: sudo mkdir /etc/nginx/ssl cd /etc/nginx/ssl The next step is to create the server key and certificates. First create the private key as shown below. Note that a pass phrase is prompted when creating the private key. sudo openssl genrsa -des3 -out server.key 1024 Next, create the certificate signing request as shown below. sudo openssl req -new -key server.key -out server.csr Fill in the required details. Most important entry is the Common Name. Enter the domain name or the ip address if there is no domain name. Next step is to sign the SSL certificate using the following command: sudo openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt The certificate is now created. The last step is to set up the virtual host displaying the new certificate. Create a copy of the default, \" sites-enabled\" configuration using the following command: sudo cp /etc/nginx/sites-available/default /etc/nginx/sites-available/wso2 Now, create a symbolic between the \" sites-enabled\" directory and the \"sites-available\" directory using the following command: sudo ln -s /etc/nginx/sites-available/wso2 /etc/nginx/sites-enabled/wso2 The host is now activated. Open the /etc/nginx/sites-enabled/wso2 file and enter the following configurations. #Configurations for listener 8243. server { listen 8243; server_name wso2test.com; client_max_body_size 100M; root /usr/share/nginx/www; index index.html index.htm; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; #with portOffset 0 running AS location /appserver/ { proxy_pass https://wso2test.com:9443/; proxy_redirect https://wso2test.com:8243/ https://wso2test.com:8243/appserver/; proxy_cookie_path / /appserver; } #with portOffset 10 running ESB location /esb/ { proxy_pass https://wso2test.com:9453/; proxy_redirect https://wso2test.com:8243/ https://wso2test.com:8243/esb/; proxy_cookie_path / /esb; } } #Configurations for listener 8280. server { listen 8280; server_name wso2test.com; client_max_body_size 100M; root /usr/share/nginx/www; index index.html index.htm; #with portOffset 0 running AS location /appserver/ { proxy_pass http://wso2test.com:9763/; proxy_redirect http://wso2test.com:8280/ http://wso2test.com:8280/appserver/; proxy_cookie_path / /appserver; } #with portOffset 10 running ESB location /esb/ { proxy_pass http://wso2test.com:9773/; proxy_redirect http://wso2test.com:8280/ http://wso2test.com:8280/esb/; proxy_cookie_path / /esb; } } !!! note According to the nginx configuration, https requests with the /appserver/\\* pattern are directed to the /\\* pattern and then when the service is served to the client, it resolves the url pattern to /appserver/\\*. This works the same for http requests. Save the file and restart the nginx server using the following command to complete the nginx configuration: sudo service nginx restart In the above configuration, the https and http requests are listening on 8243 and 8280 ports respectively. Server name is set to wso2test.com. To test this in a local machine, you need to add wso2test.com and as.wso2.com to the /etc/hosts file as shown below. 127.0.0.1 wso2test.com 127.0.0.1 as.wso2test.com 127.0.0.1 esb.wso2test.com Step 2: Configure products with proxy context path \u00b6 Download WSO2 Application Server and WSO2 ESB. Open the carbon.xml file stored in the <PRODUCT_HOME>/repository/conf/ directory and set the HostName to what you defined in the nginx configuration as shown below (for both products): <HostName>wso2test.com</HostName> Now, set the MgtHostName as shown below. For Application Server: <MgtHostName>as.wso2test.com</MgtHostName> For ESB: <MgtHostName>esb.wso2test.com</MgtHostName> Set the \"ProxyContextPath\" as shown below. This is the proxy path string, which will appear in the management console, web apps and services urls. For Application Server: <ProxyContextPath>appserver</ProxyContextPath> For ESB: <ProxyContextPath>esb</ProxyContextPath> Since you need to run both products (AS and ESB) simultaneously, set port offsets as shown below. For Application Server: <Offset>0</Offset> For ESB: <Offset>10</Offset> According to the nginx configuration, the https, http requests are listening on 8243 and 8280 ports. However, by default WSO2 products are listening on 9443 (WSO2 Application Server) and 9453 (WSO2 ESB). Therefore, the listening ports of the reverse proxy should be configured as proxy ports in Application Server and ESB respectively. T o enable proxy ports, open the <PRODUCT_HOME>/repository/conf/tomcat/catalina-server.xml file and add the \"proxyPort\" entries. !!! note Note that after you define proxy ports (8243 and 8280) in the ` catalina-server.xml ` file, it will no longer be possible to access the products using the normal ports (9443 and 9453). For example, the \"proxyPort\" entries for Application Server are as follows: <Connector protocol=\"org.apache.coyote.http11.Http11NioProtocol\" port=\"9763\" proxyPort=\"8280\" redirectPort=\"9443\" bindOnInit=\"false\" maxHttpHeaderSize=\"8192\" acceptorThreadCount=\"2\" maxThreads=\"250\" minSpareThreads=\"50\" disableUploadTimeout=\"false\" connectionUploadTimeout=\"120000\" maxKeepAliveRequests=\"200\" acceptCount=\"200\" server=\"WSO2 Carbon Server\" compression=\"on\" compressionMinSize=\"2048\" noCompressionUserAgents=\"gozilla, traviata\" compressableMimeType=\"text/html,text/javascript,application/xjavascript,application/javascript,application/xml,text/css, application/xslt+xml,text/xsl,image/gif,image/jpg,image/jpeg\" URIEncoding=\"UTF-8\"/> <!-- optional attributes: proxyPort=\"443\" --> <Connector protocol=\"org.apache.coyote.http11.Http11NioProtocol\" port=\"9443\" proxyPort=\"8243\" bindOnInit=\"false\" sslProtocol=\"TLS\" maxHttpHeaderSize=\"8192\" Step 3: Start the Product \u00b6 Start the server and enter the following url in a browser: For Application Server: https://wso2test.com:8243/appserver/carbon/ For ESB: https://wso2test.com:8243/esb/carbon/ Give the admin credentials and log in to the server. You'll find the proxy path for admin console, services, webapps changed for each product as shown below. For \u201c/appserver\u201d proxy path: https://wso2test.com:8243/appserver/carbon/admin/index.jsp. For \"/esb\" proxy path: https://wso2test.com:8243/esb/carbon/admin/index.jsp.","title":"Adding a Custom Proxy Path"},{"location":"setup/adding_a_custom_proxy__path/#adding-a-custom-proxy-path","text":"Adding a custom proxy path is useful when you have a proxy server fronting your Carbon server. In this scenario, the \"custom proxy path\" is used for mapping a proxy url with the actual url of your Carbon server, which allows clients to access the Carbon server with the proxy url. This feature is particularly useful when multiple WSO2 products are hosted under the same domain name. For example, consider that you have three WSO2 products; Application Server, API Manager and ESB, deployed in your production environment and you want all of them to be hosted with the \"wso2test.com\" domain. By using a reverse proxy and by configuring your servers with 'custom proxy paths' , you can host all products under a single domain and assign proxy paths for each product separately as shown below: Proxy URLs mapped to Carbon server URLs: https://10.100.1.1:\\<ListeningPort-apimanager>/carbon mapped to https://wso2test.com/apimanager . https://10.100.1.1:\\<ListeningPort-esb>/carbon mapped to https://wso2test.com/esb . https://10.100.1.1:\\<ListeningPort-appserver>/carbon mapped to https://wso2test.com/appserver . Note Note the following: This functionality is only available for WSO2 products that are based on Carbon 4.3.0 or a later Carbon version. See the WSO2 product release matrix for more information about WSO2 Carbon platform releases. Once you have configured your products with a proxy server, it will no longer be possible to access the product behind the proxy. See the section given below on configuring products to use the proxy server for more information. In the above example, \"apimanager\", \"esb\" and \"appserver\" are the \"proxy context paths\" of the respective products, which are configured in the carbon.xml file (stored in <PRODUCT_HOME>/repository/conf/ directory) for each product. When a client sends a request to the proxy entry url path, e.g. https://wso2test.com/apimanager , the request is directed to the back-end service url ( https://10.100.1.1:\\<PortNumber>/carbon ) where the original service lies. Eventually, the client has to be served via the requested proxy entry url path. The mapping between the proxy url path and the back-end service url path is resolved by the reverse proxy server fronting the back-end service. Info Prior to this solution, it was necessary to host these products as sub domains of the \"wso2.com\" domain as: https://apim.wso2.com , https://esb.wso2.com , https://as.wso2.com .","title":"Adding a Custom Proxy Path"},{"location":"setup/adding_a_custom_proxy__path/#access-wso2-products-through-a-custom-proxy-path","text":"This functionality will be demonstrated in this documentation using two WSO2 product servers as examples; WSO2 Application Server and WSO2 ESB as the back-end servers, and nginx as the reverse proxy. Follow the steps given below. Step 1: Install and configure a reverse proxy Step 2: Configure products with proxy context path Step 3: Start the Product","title":"Access WSO2 products through a custom proxy path"},{"location":"setup/adding_a_custom_proxy__path/#step-1-install-and-configure-a-reverse-proxy","text":"Download nginx server . Install the nginx server in your deployment server by executing the following command: sudo apt-get install nginx Create a folder called \"ssl\" inside /etc/nginx, and create the ssl certificates inside this folder by executing the following commands: sudo mkdir /etc/nginx/ssl cd /etc/nginx/ssl The next step is to create the server key and certificates. First create the private key as shown below. Note that a pass phrase is prompted when creating the private key. sudo openssl genrsa -des3 -out server.key 1024 Next, create the certificate signing request as shown below. sudo openssl req -new -key server.key -out server.csr Fill in the required details. Most important entry is the Common Name. Enter the domain name or the ip address if there is no domain name. Next step is to sign the SSL certificate using the following command: sudo openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt The certificate is now created. The last step is to set up the virtual host displaying the new certificate. Create a copy of the default, \" sites-enabled\" configuration using the following command: sudo cp /etc/nginx/sites-available/default /etc/nginx/sites-available/wso2 Now, create a symbolic between the \" sites-enabled\" directory and the \"sites-available\" directory using the following command: sudo ln -s /etc/nginx/sites-available/wso2 /etc/nginx/sites-enabled/wso2 The host is now activated. Open the /etc/nginx/sites-enabled/wso2 file and enter the following configurations. #Configurations for listener 8243. server { listen 8243; server_name wso2test.com; client_max_body_size 100M; root /usr/share/nginx/www; index index.html index.htm; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; #with portOffset 0 running AS location /appserver/ { proxy_pass https://wso2test.com:9443/; proxy_redirect https://wso2test.com:8243/ https://wso2test.com:8243/appserver/; proxy_cookie_path / /appserver; } #with portOffset 10 running ESB location /esb/ { proxy_pass https://wso2test.com:9453/; proxy_redirect https://wso2test.com:8243/ https://wso2test.com:8243/esb/; proxy_cookie_path / /esb; } } #Configurations for listener 8280. server { listen 8280; server_name wso2test.com; client_max_body_size 100M; root /usr/share/nginx/www; index index.html index.htm; #with portOffset 0 running AS location /appserver/ { proxy_pass http://wso2test.com:9763/; proxy_redirect http://wso2test.com:8280/ http://wso2test.com:8280/appserver/; proxy_cookie_path / /appserver; } #with portOffset 10 running ESB location /esb/ { proxy_pass http://wso2test.com:9773/; proxy_redirect http://wso2test.com:8280/ http://wso2test.com:8280/esb/; proxy_cookie_path / /esb; } } !!! note According to the nginx configuration, https requests with the /appserver/\\* pattern are directed to the /\\* pattern and then when the service is served to the client, it resolves the url pattern to /appserver/\\*. This works the same for http requests. Save the file and restart the nginx server using the following command to complete the nginx configuration: sudo service nginx restart In the above configuration, the https and http requests are listening on 8243 and 8280 ports respectively. Server name is set to wso2test.com. To test this in a local machine, you need to add wso2test.com and as.wso2.com to the /etc/hosts file as shown below. 127.0.0.1 wso2test.com 127.0.0.1 as.wso2test.com 127.0.0.1 esb.wso2test.com","title":"Step 1: Install and configure a reverse proxy"},{"location":"setup/adding_a_custom_proxy__path/#step-2-configure-products-with-proxy-context-path","text":"Download WSO2 Application Server and WSO2 ESB. Open the carbon.xml file stored in the <PRODUCT_HOME>/repository/conf/ directory and set the HostName to what you defined in the nginx configuration as shown below (for both products): <HostName>wso2test.com</HostName> Now, set the MgtHostName as shown below. For Application Server: <MgtHostName>as.wso2test.com</MgtHostName> For ESB: <MgtHostName>esb.wso2test.com</MgtHostName> Set the \"ProxyContextPath\" as shown below. This is the proxy path string, which will appear in the management console, web apps and services urls. For Application Server: <ProxyContextPath>appserver</ProxyContextPath> For ESB: <ProxyContextPath>esb</ProxyContextPath> Since you need to run both products (AS and ESB) simultaneously, set port offsets as shown below. For Application Server: <Offset>0</Offset> For ESB: <Offset>10</Offset> According to the nginx configuration, the https, http requests are listening on 8243 and 8280 ports. However, by default WSO2 products are listening on 9443 (WSO2 Application Server) and 9453 (WSO2 ESB). Therefore, the listening ports of the reverse proxy should be configured as proxy ports in Application Server and ESB respectively. T o enable proxy ports, open the <PRODUCT_HOME>/repository/conf/tomcat/catalina-server.xml file and add the \"proxyPort\" entries. !!! note Note that after you define proxy ports (8243 and 8280) in the ` catalina-server.xml ` file, it will no longer be possible to access the products using the normal ports (9443 and 9453). For example, the \"proxyPort\" entries for Application Server are as follows: <Connector protocol=\"org.apache.coyote.http11.Http11NioProtocol\" port=\"9763\" proxyPort=\"8280\" redirectPort=\"9443\" bindOnInit=\"false\" maxHttpHeaderSize=\"8192\" acceptorThreadCount=\"2\" maxThreads=\"250\" minSpareThreads=\"50\" disableUploadTimeout=\"false\" connectionUploadTimeout=\"120000\" maxKeepAliveRequests=\"200\" acceptCount=\"200\" server=\"WSO2 Carbon Server\" compression=\"on\" compressionMinSize=\"2048\" noCompressionUserAgents=\"gozilla, traviata\" compressableMimeType=\"text/html,text/javascript,application/xjavascript,application/javascript,application/xml,text/css, application/xslt+xml,text/xsl,image/gif,image/jpg,image/jpeg\" URIEncoding=\"UTF-8\"/> <!-- optional attributes: proxyPort=\"443\" --> <Connector protocol=\"org.apache.coyote.http11.Http11NioProtocol\" port=\"9443\" proxyPort=\"8243\" bindOnInit=\"false\" sslProtocol=\"TLS\" maxHttpHeaderSize=\"8192\"","title":"Step 2: Configure products with proxy context path"},{"location":"setup/adding_a_custom_proxy__path/#step-3-start-the-product","text":"Start the server and enter the following url in a browser: For Application Server: https://wso2test.com:8243/appserver/carbon/ For ESB: https://wso2test.com:8243/esb/carbon/ Give the admin credentials and log in to the server. You'll find the proxy path for admin console, services, webapps changed for each product as shown below. For \u201c/appserver\u201d proxy path: https://wso2test.com:8243/appserver/carbon/admin/index.jsp. For \"/esb\" proxy path: https://wso2test.com:8243/esb/carbon/admin/index.jsp.","title":"Step 3: Start the Product"},{"location":"setup/backup_recovery/","text":"Backup and Recovery \u00b6 We recommend that you use a proper artifact management system such as Puppet to back up and manage your artifacts before deploying them in the WSO2 Carbon runtime. Also, use the WSO2 Update Manager (WUM) tool, which is a command-line utility that allows you to get the latest updates (bug fixes and security fixes) of a particular product release. Recovery recommendations \u00b6 Be sure to determine the following depending on your business-continuity requirements: Recovery Time Objective (RTO): How long does it take to recover to the RPO. Backup Frequency: How frequently you should take backups. If your RPO is one day, your backup frequency should be daily. Disaster Recovery Site: The place where the latest copy of your backup is. This can be from a different shelf in your data center to a completely different geographical location. We also recommend the following: Align your artifact deployment and recovery processes. Schedule disaster recovery drills to test the recoverability of the system. Test your artifacts in an environment that is identical to the production environment before deploying them into production. Recovery strategy \u00b6 The following steps include how to recover your setup using the backups: Recover the hot-deployment artifacts by replacing the /repository directory with the backed up copy. To recover the databases, follow the recovery strategy recommended by the databases you are using. For information on supported and tested databases, see Tested Database Management Systems.","title":"Backup and Recovery"},{"location":"setup/backup_recovery/#backup-and-recovery","text":"We recommend that you use a proper artifact management system such as Puppet to back up and manage your artifacts before deploying them in the WSO2 Carbon runtime. Also, use the WSO2 Update Manager (WUM) tool, which is a command-line utility that allows you to get the latest updates (bug fixes and security fixes) of a particular product release.","title":"Backup and Recovery"},{"location":"setup/backup_recovery/#recovery-recommendations","text":"Be sure to determine the following depending on your business-continuity requirements: Recovery Time Objective (RTO): How long does it take to recover to the RPO. Backup Frequency: How frequently you should take backups. If your RPO is one day, your backup frequency should be daily. Disaster Recovery Site: The place where the latest copy of your backup is. This can be from a different shelf in your data center to a completely different geographical location. We also recommend the following: Align your artifact deployment and recovery processes. Schedule disaster recovery drills to test the recoverability of the system. Test your artifacts in an environment that is identical to the production environment before deploying them into production.","title":"Recovery recommendations"},{"location":"setup/backup_recovery/#recovery-strategy","text":"The following steps include how to recover your setup using the backups: Recover the hot-deployment artifacts by replacing the /repository directory with the backed up copy. To recover the databases, follow the recovery strategy recommended by the databases you are using. For information on supported and tested databases, see Tested Database Management Systems.","title":"Recovery strategy"},{"location":"setup/changing_default_ports/","text":"Changing the Default Ports \u00b6 When you run multiple WSO2 products, multiple instances of the same product, or multiple WSO2 product clusters on the same server or virtual machines (VMs), you must change their default ports with an offset value to avoid port conflicts. Port offset defines the number by which all ports defined in the runtime such as the HTTP/S ports will be changed. For example, if the default HTTP port is 9763 and the port offset is 1, the effective HTTP port will change to 9764. For each additional WSO2 product instance, you set the port offset to a unique value. The default port offset value is 0. There are two ways to set an offset to a port: Pass the port offset to the server during startup. The following command starts the server with the default port incremented by 3 :./wso2server.sh -DportOffset=3 Set the Ports section of <PRODUCT_HOME>/repository/conf/carbon.xml as follows: <Offset>3</Offset> When you set the server-level port offset in WSO2 AS as shown above, all the ports used by the server will change automatically. However, this may not be the case with some WSO2 products such as WSO2 APIM and WSO2 AM. See the product documentation for instructions that are specific to your product. Default Ports of WSO2 Products \u00b6 This page describes the default ports that are used for each WSO2 product when the port offset is 0. Common ports Management console ports LDAP server ports KDC ports JMX monitoring ports Clustering ports Random ports Product-specific ports API Manager Data Analytics Server Ports inherited from WSO2 BAM Ports used by the Spark Analytics Engine Business Process Server Complex Event Processor Elastic Load Balancer Enterprise Service Bus Enterprise Integrator ESB ports EI-Analytics ports EI-Business Process ports EI-Broker ports EI-Micro Integrator ports EI-MSF4J ports Identity Server Message Broker Machine Learner Storage Server Enterprise Mobility Manager IoT Server Default ports Ports required for mobile devices to communicate with the server and the respective notification servers. Common ports \u00b6 The following ports are common to all WSO2 products that provide the given feature. Some features are bundled in the WSO2 Carbon platform itself and therefore are available in all WSO2 products by default. Management console ports | LDAP server ports | KDC ports | JMX monitoring ports | Clustering ports | Random ports | ESB ports | EI-Analytics ports | EI-Business Process ports | EI-Broker ports | EI-Micro Integrator ports | EI-MSF4J ports | Default ports | Ports required for mobile devices to communicate with the server and the respective notification servers. Management console ports \u00b6 WSO2 products that provide a management console (except WSO2 Enterprise Integrator) use the following servlet transport ports: 9443 - HTTPS servlet transport (the default URL of the management console is https://localhost:9443/carbon ) 9763 - HTTP servlet transport WSO2 Enterprise Integrator (WSO2 EI) uses the following ports to access the management console: 9443 - HTTPS servlet transport for the ESB runtime (the default URL of the management console is https://localhost:9443/carbon ) 9445 - HTTPS servlet transport for the EI-Business Process runtime (the default URL of the management console is https://localhost:9445/carbon ) 9444 - Used for the EI-Analytics management console LDAP server ports \u00b6 Provided by default in the WSO2 Carbon platform. 10389 - Used in WSO2 products that provide an embedded LDAP server KDC ports \u00b6 8000 - Used to expose the Kerberos key distribution center server JMX monitoring ports \u00b6 WSO2 Carbon platform uses TCP ports to monitor a running Carbon instance using a JMX client such as JConsole. By default, JMX is enabled in all products. You can disable it using <PRODUCT_HOME>/repository/conf/etc/jmx.xml file. 11111 - RMIRegistry port. Used to monitor Carbon remotely 9999 - RMIServer port. Used along with the RMIRegistry port when Carbon is monitored from a JMX client that is behind a firewall Clustering ports \u00b6 To cluster any running Carbon instance, either one of the following ports must be opened. 45564 - Opened if the membership scheme is multicast 4000 - Opened if the membership scheme is wka Random ports \u00b6 Certain ports are randomly opened during server startup. This is due to specific properties and configurations that become effective when the product is started. Note that the IDs of these random ports will change every time the server is started. A random TCP port will open at server startup because of the -Dcom.sun.management.jmxremote property set in the server startup script. This property is used for the JMX monitoring facility in JVM. A random UDP port is opened at server startup due to the log4j appender ( SyslogAppender ), which is configured in the <PRODUCT_HOME>/repository/conf/log4j.properties file. Product-specific ports \u00b6 Some WSO2 products will have additional ports as explained below. [ API Manager ] [ Data Analytics Server ] [ Business Process Server ] [ Complex Event Processor ] [ Elastic Load Balancer ] [ Enterprise Service Bus ] [ Enterprise Integrator ] [ Identity Server ] [ Message Broker ] [ Machine Learner ] [ Storage Server ] [ Enterprise Mobility Manager ] [ IoT Server ] API Manager \u00b6 5672 - Used by the internal Message Broker. 7611 - Authenticate data published when Thrift data publisher is used for throttling. 7612 - Publish Analytics to the API Manager Analytics server. 7711 - Port for secure transport when Thrift data publisher is used for throttling. 7711 + Port offset of the APIM Analytics Server - Thrift SSL port for secure transport when publishing analytics to the API Manager Analytics server. 8280, 8243 - NIO/PT transport ports. 9611 - Publish data to the Traffic Manager. Required when binary data publisher for throttling. 9711 - Authenticate data published to the Traffic Manager. Required when binary data publisher for throttling. 10397 - Thrift client and server ports. 9099 - Web Socket ports. Note If you change the default API Manager ports with a port offset, most of its ports will be changed automatically according to the offset except a few exceptions described in the API Manager documentation . Data Analytics Server \u00b6 Given below are the specific ports used by WSO2 DAS. Ports inherited from WSO2 BAM \u00b6 WSO2 DAS inherits the following port configurations used in its predecessor, WSO2 Business Activity Monitor (BAM) . 7711 - Thrift SSL port for secure transport, where the client is authenticated to use WSO2 DAS. 7611 - Thrift TCP port where WSO2 DAS receives events from clients. Ports used by the Spark Analytics Engine \u00b6 The Spark Analytics engine is used in 3 separate modes in WSO2 DAS as follows. Local mode Cluster mode Client mode Default port configurations for these modes are as follows. Info For more information on these ports, go to Apache Spark Documentation . Ports available for all modes The following ports are available for all three modes explained above. Description Port number spark.ui.port 4040 spark.history.ui.port 18080 spark.blockManager.port 12000 spark.broadcast.port 12500 spark.driver.port 13000 spark.executor.port 13500 spark.fileserver.port 14000 spark.replClassServer.port 14500 Ports available for the cluster mode The following ports are available only for the cluster mode. Description Port number spark.master.port 7077 spark.master.rest.port 6066 spark.master.webui.port 8081 spark.worker.port 11000 spark.worker.webui.port 11500 Business Process Server \u00b6 2199 - RMI registry port (datasources provider port) Complex Event Processor \u00b6 9160 - Cassandra port on which Thrift listens to clients 7711 - Thrift SSL port for secure transport, where the client is authenticated to CEP 7611 - Thrift TCP port to receive events from clients to CEP 11224 - Thrift TCP port for HA management of CEP Elastic Load Balancer \u00b6 8280, 8243 - NIO/PT transport ports Enterprise Service Bus \u00b6 Non-blocking HTTP/S transport ports: Used to accept message mediation requests. If you want to send a request to an API or a proxy service for example, you must use these ports. ESB_HOME}/repository/conf/axis2/axis2.xml file. 8243 - Passthrough or NIO HTTPS transport 8280 - Passthrough or NIO HTTP transport Enterprise Integrator \u00b6 Listed below are the default ports that are used in WSO2 Enterprise Integrator (WSO2 EI) when the port offset is 0. ESB ports \u00b6 9443 - HTTPS servlet transport (the default URL of the management console is https://localhost:9443/carbon ) Non-blocking HTTP/S transport ports: Used to accept message mediation requests. For example, if you want to send a request to an API or a proxy service, you must use these ports: <EI_HOME>/conf/axis2/axis2.xml file. 8243 - Passthrough or NIO HTTPS transport 8280 - Passthrough or NIO HTTP transport EI-Analytics ports \u00b6 9643 - The port on which the Analytics dashboard opens 9161 - Cassandra port on which Thrift listens to clients 7712 - Thrift SSL port for secure transport, where the client is authenticated for the Analytics profile 7612 - Thrift TCP port to receive events from clients to DAS EI-Business Process ports \u00b6 9445 - HTTPS servlet transport (the default URL of the management console is https://localhost:9445/carbon ) 9765 - HTTP servlet transport EI-Broker ports \u00b6 9446 - HTTPS servlet transport (the default URL of the management console is https://localhost:9446/carbon ) 9766 - HTTP servlet transport EI-Broker uses the following JMS ports to communicate with external clients over the JMS transport. 5675 - Port for listening for messages on TCP when the AMQP transport is used. 8675 - Port for listening for messages on TCP/SSL when the AMQP Transport is used. 1886 - Port for listening for messages on TCP when the MQTT transport is used. 8836 - Port for listening for messages on TCP/SSL when the MQTT Transport is used. 7614 - The port for Apache Thrift Server. EI-Micro Integrator ports \u00b6 8290 - HTTP servlet transport 8253 - HTTPS servlet transport EI-MSF4J ports \u00b6 9090 - HTTP servlet transport Identity Server \u00b6 8000 - KDCServerPort. Port which KDC (Kerberos Key Distribution Center) server runs 10500 - ThriftEntitlementReceivePort Message Broker \u00b6 Message Broker uses the following JMS ports to communicate with external clients over the JMS transport. 5672 - Port for listening for messages on TCP when the AMQP transport is used. 8672 - Port for listening for messages on TCP/SSL when the AMQP Transport is used. 1883 - Port for listening for messages on TCP when the MQTT transport is used. 8833 - Port for listening for messages on TCP/SSL when the MQTT Transport is used. 7611 - The port for Apache Thrift Server. Machine Learner \u00b6 7077 - The default port for Apache Spark. 54321 - The default port for H2O. 4040 - The default port for Spark UI. Storage Server \u00b6 Cassandra: 7000 - For Inter node communication within cluster nodes 7001 - For inter node communication within cluster nodes vis SSL 9160 - For Thrift client connections 7199 - For JMX HDFS: 54310 - Port used to connect to the default file system. 54311 - Port used by the MapRed job tracker 50470 - Name node secure HTTP server port 50475 - Data node secure HTTP server port 50010 - Data node server port for data transferring 50075 - Data node HTTP server port 50020 - Data node IPC server port Enterprise Mobility Manager \u00b6 The following ports need to be opened for Android and iOS devices so that it can connect to Google Cloud Messaging (GCM)/Firebase Cloud Messaging (FCM) and APNS (Apple Push Notification Service), and enroll to WSO2 EMM. Android: The ports to open are 5228, 5229 and 5230. GCM/FCM typically only uses 5228, but it sometimes uses 5229 and 5230. GCM/FCM does not provide specific IPs, so it is recommended to allow the firewall to accept outgoing connections to all IP addresses contained in the IP blocks listed in Google's ASN of 15169. iOS: 5223 - TCP port used by devices to communicate to APNs servers 2195 - TCP port used to send notifications to APNs 2196 - TCP port used by the APNs feedback service 443 - TCP port used as a fallback on Wi-Fi, only when devices are unable to communicate to APNs on port 5223 The APNs servers use load balancing. The devices will not always connect to the same public IP address for notifications. The entire 17.0.0.0/8 address block is assigned to Apple, so it is best to allow this range in the firewall settings. API Manager: Info The following WSO2 API Manager ports are only applicable to WSO2 EMM 1.1.0 onwards. 10397 - Thrift client and server ports 8280, 8243 - NIO/PT transport ports IoT Server \u00b6 The following ports need to be opened for WSO2 IoT Server, and Android and iOS devices so that it can connect to Google Cloud Messaging (GCM)/Firebase Cloud Messaging (FCM) and APNS (Apple Push Notification Service), and enroll to WSO2 IoT Server. Default ports \u00b6 8243 HTTPS gateway port. 9443 HTTPS port for the core profile. 8280 HTTP gateway port. 9763 HTTP port for the core profile. 1886 Default MQTT port. 9445 HTTPS port for the analytics profile. 9765 HTTP port for the analytics profile. 1039 HTTP port for the analytics profile Ports required for mobile devices to communicate with the server and the respective notification servers. \u00b6 Android 5228 5229 5230 The ports to open are 5228, 5229 and 5230. Google Cloud Messaging (GCM) and Firebase Cloud Messaging (FCM) typically only uses 5228, but it sometimes uses 5229 and 5230. GCM/FCM does not provide specific IPs, so it is recommended to allow the firewall to accept outgoing connections to all IP addresses contained in the IP blocks listed in Google's ASN of 15169. iOS 5223 Transmission Control Protocol (TCP) port used by devices to communicate to APNs servers. 2195 TCP port used to send notifications to APNs. 2196 TCP port used by the APNs feedback service. 443 TCP port used as a fallback on Wi-Fi, only when devices are unable to communicate to APNs on port 5223. The APNs servers use load balancing. The devices will not always connect to the same public IP address for notifications. The entire 17.0.0.0/8 address block is assigned to Apple, so it is best to allow this range in the firewall settings.","title":"Changing the Default Ports"},{"location":"setup/changing_default_ports/#changing-the-default-ports","text":"When you run multiple WSO2 products, multiple instances of the same product, or multiple WSO2 product clusters on the same server or virtual machines (VMs), you must change their default ports with an offset value to avoid port conflicts. Port offset defines the number by which all ports defined in the runtime such as the HTTP/S ports will be changed. For example, if the default HTTP port is 9763 and the port offset is 1, the effective HTTP port will change to 9764. For each additional WSO2 product instance, you set the port offset to a unique value. The default port offset value is 0. There are two ways to set an offset to a port: Pass the port offset to the server during startup. The following command starts the server with the default port incremented by 3 :./wso2server.sh -DportOffset=3 Set the Ports section of <PRODUCT_HOME>/repository/conf/carbon.xml as follows: <Offset>3</Offset> When you set the server-level port offset in WSO2 AS as shown above, all the ports used by the server will change automatically. However, this may not be the case with some WSO2 products such as WSO2 APIM and WSO2 AM. See the product documentation for instructions that are specific to your product.","title":"Changing the Default Ports"},{"location":"setup/changing_default_ports/#default-ports-of-wso2-products","text":"This page describes the default ports that are used for each WSO2 product when the port offset is 0. Common ports Management console ports LDAP server ports KDC ports JMX monitoring ports Clustering ports Random ports Product-specific ports API Manager Data Analytics Server Ports inherited from WSO2 BAM Ports used by the Spark Analytics Engine Business Process Server Complex Event Processor Elastic Load Balancer Enterprise Service Bus Enterprise Integrator ESB ports EI-Analytics ports EI-Business Process ports EI-Broker ports EI-Micro Integrator ports EI-MSF4J ports Identity Server Message Broker Machine Learner Storage Server Enterprise Mobility Manager IoT Server Default ports Ports required for mobile devices to communicate with the server and the respective notification servers.","title":"Default Ports of WSO2 Products"},{"location":"setup/changing_default_ports/#common-ports","text":"The following ports are common to all WSO2 products that provide the given feature. Some features are bundled in the WSO2 Carbon platform itself and therefore are available in all WSO2 products by default. Management console ports | LDAP server ports | KDC ports | JMX monitoring ports | Clustering ports | Random ports | ESB ports | EI-Analytics ports | EI-Business Process ports | EI-Broker ports | EI-Micro Integrator ports | EI-MSF4J ports | Default ports | Ports required for mobile devices to communicate with the server and the respective notification servers.","title":"Common ports"},{"location":"setup/changing_default_ports/#management-console-ports","text":"WSO2 products that provide a management console (except WSO2 Enterprise Integrator) use the following servlet transport ports: 9443 - HTTPS servlet transport (the default URL of the management console is https://localhost:9443/carbon ) 9763 - HTTP servlet transport WSO2 Enterprise Integrator (WSO2 EI) uses the following ports to access the management console: 9443 - HTTPS servlet transport for the ESB runtime (the default URL of the management console is https://localhost:9443/carbon ) 9445 - HTTPS servlet transport for the EI-Business Process runtime (the default URL of the management console is https://localhost:9445/carbon ) 9444 - Used for the EI-Analytics management console","title":"Management console ports"},{"location":"setup/changing_default_ports/#ldap-server-ports","text":"Provided by default in the WSO2 Carbon platform. 10389 - Used in WSO2 products that provide an embedded LDAP server","title":"LDAP server ports"},{"location":"setup/changing_default_ports/#kdc-ports","text":"8000 - Used to expose the Kerberos key distribution center server","title":"KDC ports"},{"location":"setup/changing_default_ports/#jmx-monitoring-ports","text":"WSO2 Carbon platform uses TCP ports to monitor a running Carbon instance using a JMX client such as JConsole. By default, JMX is enabled in all products. You can disable it using <PRODUCT_HOME>/repository/conf/etc/jmx.xml file. 11111 - RMIRegistry port. Used to monitor Carbon remotely 9999 - RMIServer port. Used along with the RMIRegistry port when Carbon is monitored from a JMX client that is behind a firewall","title":"JMX monitoring ports"},{"location":"setup/changing_default_ports/#clustering-ports","text":"To cluster any running Carbon instance, either one of the following ports must be opened. 45564 - Opened if the membership scheme is multicast 4000 - Opened if the membership scheme is wka","title":"Clustering ports"},{"location":"setup/changing_default_ports/#random-ports","text":"Certain ports are randomly opened during server startup. This is due to specific properties and configurations that become effective when the product is started. Note that the IDs of these random ports will change every time the server is started. A random TCP port will open at server startup because of the -Dcom.sun.management.jmxremote property set in the server startup script. This property is used for the JMX monitoring facility in JVM. A random UDP port is opened at server startup due to the log4j appender ( SyslogAppender ), which is configured in the <PRODUCT_HOME>/repository/conf/log4j.properties file.","title":"Random ports"},{"location":"setup/changing_default_ports/#product-specific-ports","text":"Some WSO2 products will have additional ports as explained below. [ API Manager ] [ Data Analytics Server ] [ Business Process Server ] [ Complex Event Processor ] [ Elastic Load Balancer ] [ Enterprise Service Bus ] [ Enterprise Integrator ] [ Identity Server ] [ Message Broker ] [ Machine Learner ] [ Storage Server ] [ Enterprise Mobility Manager ] [ IoT Server ]","title":"Product-specific ports"},{"location":"setup/changing_default_ports/#api-manager","text":"5672 - Used by the internal Message Broker. 7611 - Authenticate data published when Thrift data publisher is used for throttling. 7612 - Publish Analytics to the API Manager Analytics server. 7711 - Port for secure transport when Thrift data publisher is used for throttling. 7711 + Port offset of the APIM Analytics Server - Thrift SSL port for secure transport when publishing analytics to the API Manager Analytics server. 8280, 8243 - NIO/PT transport ports. 9611 - Publish data to the Traffic Manager. Required when binary data publisher for throttling. 9711 - Authenticate data published to the Traffic Manager. Required when binary data publisher for throttling. 10397 - Thrift client and server ports. 9099 - Web Socket ports. Note If you change the default API Manager ports with a port offset, most of its ports will be changed automatically according to the offset except a few exceptions described in the API Manager documentation .","title":"API Manager"},{"location":"setup/changing_default_ports/#data-analytics-server","text":"Given below are the specific ports used by WSO2 DAS.","title":"Data Analytics Server"},{"location":"setup/changing_default_ports/#ports-inherited-from-wso2-bam","text":"WSO2 DAS inherits the following port configurations used in its predecessor, WSO2 Business Activity Monitor (BAM) . 7711 - Thrift SSL port for secure transport, where the client is authenticated to use WSO2 DAS. 7611 - Thrift TCP port where WSO2 DAS receives events from clients.","title":"Ports inherited from WSO2 BAM"},{"location":"setup/changing_default_ports/#ports-used-by-the-spark-analytics-engine","text":"The Spark Analytics engine is used in 3 separate modes in WSO2 DAS as follows. Local mode Cluster mode Client mode Default port configurations for these modes are as follows. Info For more information on these ports, go to Apache Spark Documentation . Ports available for all modes The following ports are available for all three modes explained above. Description Port number spark.ui.port 4040 spark.history.ui.port 18080 spark.blockManager.port 12000 spark.broadcast.port 12500 spark.driver.port 13000 spark.executor.port 13500 spark.fileserver.port 14000 spark.replClassServer.port 14500 Ports available for the cluster mode The following ports are available only for the cluster mode. Description Port number spark.master.port 7077 spark.master.rest.port 6066 spark.master.webui.port 8081 spark.worker.port 11000 spark.worker.webui.port 11500","title":"Ports used by the Spark Analytics Engine"},{"location":"setup/changing_default_ports/#business-process-server","text":"2199 - RMI registry port (datasources provider port)","title":"Business Process Server"},{"location":"setup/changing_default_ports/#complex-event-processor","text":"9160 - Cassandra port on which Thrift listens to clients 7711 - Thrift SSL port for secure transport, where the client is authenticated to CEP 7611 - Thrift TCP port to receive events from clients to CEP 11224 - Thrift TCP port for HA management of CEP","title":"Complex Event Processor"},{"location":"setup/changing_default_ports/#elastic-load-balancer","text":"8280, 8243 - NIO/PT transport ports","title":"Elastic Load Balancer"},{"location":"setup/changing_default_ports/#enterprise-service-bus","text":"Non-blocking HTTP/S transport ports: Used to accept message mediation requests. If you want to send a request to an API or a proxy service for example, you must use these ports. ESB_HOME}/repository/conf/axis2/axis2.xml file. 8243 - Passthrough or NIO HTTPS transport 8280 - Passthrough or NIO HTTP transport","title":"Enterprise Service Bus"},{"location":"setup/changing_default_ports/#enterprise-integrator","text":"Listed below are the default ports that are used in WSO2 Enterprise Integrator (WSO2 EI) when the port offset is 0.","title":"Enterprise Integrator"},{"location":"setup/changing_default_ports/#esb-ports","text":"9443 - HTTPS servlet transport (the default URL of the management console is https://localhost:9443/carbon ) Non-blocking HTTP/S transport ports: Used to accept message mediation requests. For example, if you want to send a request to an API or a proxy service, you must use these ports: <EI_HOME>/conf/axis2/axis2.xml file. 8243 - Passthrough or NIO HTTPS transport 8280 - Passthrough or NIO HTTP transport","title":"ESB ports"},{"location":"setup/changing_default_ports/#ei-analytics-ports","text":"9643 - The port on which the Analytics dashboard opens 9161 - Cassandra port on which Thrift listens to clients 7712 - Thrift SSL port for secure transport, where the client is authenticated for the Analytics profile 7612 - Thrift TCP port to receive events from clients to DAS","title":"EI-Analytics ports"},{"location":"setup/changing_default_ports/#ei-business-process-ports","text":"9445 - HTTPS servlet transport (the default URL of the management console is https://localhost:9445/carbon ) 9765 - HTTP servlet transport","title":"EI-Business Process ports"},{"location":"setup/changing_default_ports/#ei-broker-ports","text":"9446 - HTTPS servlet transport (the default URL of the management console is https://localhost:9446/carbon ) 9766 - HTTP servlet transport EI-Broker uses the following JMS ports to communicate with external clients over the JMS transport. 5675 - Port for listening for messages on TCP when the AMQP transport is used. 8675 - Port for listening for messages on TCP/SSL when the AMQP Transport is used. 1886 - Port for listening for messages on TCP when the MQTT transport is used. 8836 - Port for listening for messages on TCP/SSL when the MQTT Transport is used. 7614 - The port for Apache Thrift Server.","title":"EI-Broker ports"},{"location":"setup/changing_default_ports/#ei-micro-integrator-ports","text":"8290 - HTTP servlet transport 8253 - HTTPS servlet transport","title":"EI-Micro Integrator ports"},{"location":"setup/changing_default_ports/#ei-msf4j-ports","text":"9090 - HTTP servlet transport","title":"EI-MSF4J ports"},{"location":"setup/changing_default_ports/#identity-server","text":"8000 - KDCServerPort. Port which KDC (Kerberos Key Distribution Center) server runs 10500 - ThriftEntitlementReceivePort","title":"Identity Server"},{"location":"setup/changing_default_ports/#message-broker","text":"Message Broker uses the following JMS ports to communicate with external clients over the JMS transport. 5672 - Port for listening for messages on TCP when the AMQP transport is used. 8672 - Port for listening for messages on TCP/SSL when the AMQP Transport is used. 1883 - Port for listening for messages on TCP when the MQTT transport is used. 8833 - Port for listening for messages on TCP/SSL when the MQTT Transport is used. 7611 - The port for Apache Thrift Server.","title":"Message Broker"},{"location":"setup/changing_default_ports/#machine-learner","text":"7077 - The default port for Apache Spark. 54321 - The default port for H2O. 4040 - The default port for Spark UI.","title":"Machine Learner"},{"location":"setup/changing_default_ports/#storage-server","text":"Cassandra: 7000 - For Inter node communication within cluster nodes 7001 - For inter node communication within cluster nodes vis SSL 9160 - For Thrift client connections 7199 - For JMX HDFS: 54310 - Port used to connect to the default file system. 54311 - Port used by the MapRed job tracker 50470 - Name node secure HTTP server port 50475 - Data node secure HTTP server port 50010 - Data node server port for data transferring 50075 - Data node HTTP server port 50020 - Data node IPC server port","title":"Storage Server"},{"location":"setup/changing_default_ports/#enterprise-mobility-manager","text":"The following ports need to be opened for Android and iOS devices so that it can connect to Google Cloud Messaging (GCM)/Firebase Cloud Messaging (FCM) and APNS (Apple Push Notification Service), and enroll to WSO2 EMM. Android: The ports to open are 5228, 5229 and 5230. GCM/FCM typically only uses 5228, but it sometimes uses 5229 and 5230. GCM/FCM does not provide specific IPs, so it is recommended to allow the firewall to accept outgoing connections to all IP addresses contained in the IP blocks listed in Google's ASN of 15169. iOS: 5223 - TCP port used by devices to communicate to APNs servers 2195 - TCP port used to send notifications to APNs 2196 - TCP port used by the APNs feedback service 443 - TCP port used as a fallback on Wi-Fi, only when devices are unable to communicate to APNs on port 5223 The APNs servers use load balancing. The devices will not always connect to the same public IP address for notifications. The entire 17.0.0.0/8 address block is assigned to Apple, so it is best to allow this range in the firewall settings. API Manager: Info The following WSO2 API Manager ports are only applicable to WSO2 EMM 1.1.0 onwards. 10397 - Thrift client and server ports 8280, 8243 - NIO/PT transport ports","title":"Enterprise Mobility Manager"},{"location":"setup/changing_default_ports/#iot-server","text":"The following ports need to be opened for WSO2 IoT Server, and Android and iOS devices so that it can connect to Google Cloud Messaging (GCM)/Firebase Cloud Messaging (FCM) and APNS (Apple Push Notification Service), and enroll to WSO2 IoT Server.","title":"IoT Server"},{"location":"setup/changing_default_ports/#default-ports","text":"8243 HTTPS gateway port. 9443 HTTPS port for the core profile. 8280 HTTP gateway port. 9763 HTTP port for the core profile. 1886 Default MQTT port. 9445 HTTPS port for the analytics profile. 9765 HTTP port for the analytics profile. 1039 HTTP port for the analytics profile","title":"Default ports"},{"location":"setup/changing_default_ports/#ports-required-for-mobile-devices-to-communicate-with-the-server-and-the-respective-notification-servers","text":"Android 5228 5229 5230 The ports to open are 5228, 5229 and 5230. Google Cloud Messaging (GCM) and Firebase Cloud Messaging (FCM) typically only uses 5228, but it sometimes uses 5229 and 5230. GCM/FCM does not provide specific IPs, so it is recommended to allow the firewall to accept outgoing connections to all IP addresses contained in the IP blocks listed in Google's ASN of 15169. iOS 5223 Transmission Control Protocol (TCP) port used by devices to communicate to APNs servers. 2195 TCP port used to send notifications to APNs. 2196 TCP port used by the APNs feedback service. 443 TCP port used as a fallback on Wi-Fi, only when devices are unable to communicate to APNs on port 5223. The APNs servers use load balancing. The devices will not always connect to the same public IP address for notifications. The entire 17.0.0.0/8 address block is assigned to Apple, so it is best to allow this range in the firewall settings.","title":"Ports required for mobile devices to communicate with the server and the respective notification servers."},{"location":"setup/configuring_proxy_servers/","text":"Working with Proxy Servers \u00b6 When using the ESB profile of WSO2 Enterprise Integrator (WSO2 EI), there can be scenarios where you need to configure the ESB to route messages through a proxy server. For example, if the ESB is behind a firewall, your proxy service might need to talk to a server through a proxy server as illustrated in the following diagram: For such scenarios, you need to configure the ESB transport sender to forward messages through a proxy server. Routing messages through a proxy server For non-blocking service calls For blocking service calls Configuring proxy profiles in the ESB profile Routing messages through a proxy server \u00b6 See the instructions given below. For non-blocking service calls \u00b6 To configure the ESB profile to route messages through a proxy server (for non-blocking service calls), add the parameters given below to the axis2.xml file (stored in the <EI_HOME>/conf/axis2/ directory) and update the values. This configuration ensures that all HTTP requests pass through the configured proxy server. <transportSender name=\"http\" class=\"org.apache.synapse.transport.passthru.PassThroughHttpSender\"> <parameter name=\"non-blocking\" locked=\"false\">true</parameter> <parameter name=\"http.proxyHost\" locked=\"false\"></parameter> <parameter name=\"http.proxyPort\" locked=\"false\"></parameter> </transportSender> The parameters are described below. non-blocking Specifies whether or not 'non-blocking' mode is enabled for the transport sender. Be sure that this parameter is set to true . http.proxyHost The host name of the proxy server. http.proxyPort The port (number) in the proxy server. For blocking service calls \u00b6 To configure the ESB profile to route messages through a proxy server (for blocking service calls), add the parameters given below to the axis2_blocking_client.xml file (stored in the <EI_HOME>/conf/axis2/ directory) and update the values. This configuration ensures that all HTTP requests pass through the configured proxy server. <parameter name=\"Proxy\"> <parameters> <ProxyHost></ProxyHost> <ProxyPort></ProxyPort> <ProxyUser></ProxyUser> <ProxyPassword></ProxyPassword> </parameters> </parameter> The parameters are described below. ProxyHost The host name of the proxy server. ProxyPort The port (number) in the proxy server. ProxyUser The user name for connecting to the proxy server. ProxyPassword The password for connecting to the proxy server. Info Bypass the proxy server for blocking calls? In the case of blocking service calls, you can apply a system property in the ESB profile to bypass the proxy server and route messages directly to the hosts that should receive the messages. Explained below are two methods of applying the system property: Set the system property in the product startup script that is located in the <PRODUCT_HOME>/bin/ directory as shown below. Note that the list of host names are separated by the pipe symbol ('|'). -Dhttp.nonProxyHosts =10.|localhost|127.0.0.1|.\\.domain.com \\ Pass the system property when you start the server as shown below. ./integrator.sh -Dhttp.nonProxyHosts =10.|localhost|127.0.0.1|.\\.domain.com Info A proxy server might require HTTP basic authentication before it handles communication from the ESB profile. Configuring proxy profiles in the ESB profile \u00b6 When using the ESB profile, there can be scenarios where you need to configure multiple proxy servers to route messages to different endpoints as illustrated in the following diagram. When you need to route messages to different endpoints through multiple proxy servers, you can configure proxy profiles. To configure proxy profiles in the ESB profile: Edit the <EI_HOME>/conf/axis2/axis2.xml file, add the proxyProfiles parameter in the <transportSender> configuration of the http transport, and then define multiple profiles based on the number of proxy servers you need to have. !!! info When you define a profile, it is mandatory to specify the ` targetHosts ` , ` proxyHost ` and ` proxyPort ` parameters for each profile. The following is a sample proxy profile configuration that you can have in the <transportSender> configuration of the HTTP transport: <parameter name=\"proxyProfiles\"> <profile> <targetHosts>example.com, .*.sample.com</targetHosts> <proxyHost>localhost</proxyHost> <proxyPort>3128</proxyPort> <proxyUserName>squidUser</proxyUserName> <proxyPassword>password</proxyPassword> <bypass>xxx.sample.com</bypass> </profile> <profile> <targetHosts>localhost</targetHosts> <proxyHost>localhost</proxyHost> <proxyPort>7443</proxyPort> </profile> <profile> <targetHosts>*</targetHosts> <proxyHost>localhost</proxyHost> <proxyPort>7443</proxyPort> <bypass>test.com, direct.com</bypass> </profile> </parameter> When you configure a proxy profile, following are details of the parameters that you need to define in a <profile> : Parameter Description Required targetHosts A host name or a comma-separated list of host names for a target endpoint. Host names can be specified as regular expressions that match a pattern. When targetHosts is specified as an asterisks (*), it will match all the hosts in the profile Yes proxyHost The host name of the proxy server. Yes proxyPort The port number of the proxy server. Yes proxyUserName The user name for the proxy server authentication. No proxyPassword The password for the proxy server authentication. No bypass A host name or a comma-separated list of host names that should not be sent via the proxy server. For example, if you want all requests to *. sample.com to be sent via a proxy server, but need to directly send requests to hello.sample.com , instead of going through the proxy server, you can add hello.sample.com as a bypass host name. You can specify host names as regular expressions that match a pattern. No","title":"Working with Proxy Servers"},{"location":"setup/configuring_proxy_servers/#working-with-proxy-servers","text":"When using the ESB profile of WSO2 Enterprise Integrator (WSO2 EI), there can be scenarios where you need to configure the ESB to route messages through a proxy server. For example, if the ESB is behind a firewall, your proxy service might need to talk to a server through a proxy server as illustrated in the following diagram: For such scenarios, you need to configure the ESB transport sender to forward messages through a proxy server. Routing messages through a proxy server For non-blocking service calls For blocking service calls Configuring proxy profiles in the ESB profile","title":"Working with Proxy Servers"},{"location":"setup/configuring_proxy_servers/#routing-messages-through-a-proxy-server","text":"See the instructions given below.","title":"Routing messages through a proxy server"},{"location":"setup/configuring_proxy_servers/#for-non-blocking-service-calls","text":"To configure the ESB profile to route messages through a proxy server (for non-blocking service calls), add the parameters given below to the axis2.xml file (stored in the <EI_HOME>/conf/axis2/ directory) and update the values. This configuration ensures that all HTTP requests pass through the configured proxy server. <transportSender name=\"http\" class=\"org.apache.synapse.transport.passthru.PassThroughHttpSender\"> <parameter name=\"non-blocking\" locked=\"false\">true</parameter> <parameter name=\"http.proxyHost\" locked=\"false\"></parameter> <parameter name=\"http.proxyPort\" locked=\"false\"></parameter> </transportSender> The parameters are described below. non-blocking Specifies whether or not 'non-blocking' mode is enabled for the transport sender. Be sure that this parameter is set to true . http.proxyHost The host name of the proxy server. http.proxyPort The port (number) in the proxy server.","title":"For non-blocking service calls"},{"location":"setup/configuring_proxy_servers/#for-blocking-service-calls","text":"To configure the ESB profile to route messages through a proxy server (for blocking service calls), add the parameters given below to the axis2_blocking_client.xml file (stored in the <EI_HOME>/conf/axis2/ directory) and update the values. This configuration ensures that all HTTP requests pass through the configured proxy server. <parameter name=\"Proxy\"> <parameters> <ProxyHost></ProxyHost> <ProxyPort></ProxyPort> <ProxyUser></ProxyUser> <ProxyPassword></ProxyPassword> </parameters> </parameter> The parameters are described below. ProxyHost The host name of the proxy server. ProxyPort The port (number) in the proxy server. ProxyUser The user name for connecting to the proxy server. ProxyPassword The password for connecting to the proxy server. Info Bypass the proxy server for blocking calls? In the case of blocking service calls, you can apply a system property in the ESB profile to bypass the proxy server and route messages directly to the hosts that should receive the messages. Explained below are two methods of applying the system property: Set the system property in the product startup script that is located in the <PRODUCT_HOME>/bin/ directory as shown below. Note that the list of host names are separated by the pipe symbol ('|'). -Dhttp.nonProxyHosts =10.|localhost|127.0.0.1|.\\.domain.com \\ Pass the system property when you start the server as shown below. ./integrator.sh -Dhttp.nonProxyHosts =10.|localhost|127.0.0.1|.\\.domain.com Info A proxy server might require HTTP basic authentication before it handles communication from the ESB profile.","title":"For blocking service calls"},{"location":"setup/configuring_proxy_servers/#configuring-proxy-profiles-in-the-esb-profile","text":"When using the ESB profile, there can be scenarios where you need to configure multiple proxy servers to route messages to different endpoints as illustrated in the following diagram. When you need to route messages to different endpoints through multiple proxy servers, you can configure proxy profiles. To configure proxy profiles in the ESB profile: Edit the <EI_HOME>/conf/axis2/axis2.xml file, add the proxyProfiles parameter in the <transportSender> configuration of the http transport, and then define multiple profiles based on the number of proxy servers you need to have. !!! info When you define a profile, it is mandatory to specify the ` targetHosts ` , ` proxyHost ` and ` proxyPort ` parameters for each profile. The following is a sample proxy profile configuration that you can have in the <transportSender> configuration of the HTTP transport: <parameter name=\"proxyProfiles\"> <profile> <targetHosts>example.com, .*.sample.com</targetHosts> <proxyHost>localhost</proxyHost> <proxyPort>3128</proxyPort> <proxyUserName>squidUser</proxyUserName> <proxyPassword>password</proxyPassword> <bypass>xxx.sample.com</bypass> </profile> <profile> <targetHosts>localhost</targetHosts> <proxyHost>localhost</proxyHost> <proxyPort>7443</proxyPort> </profile> <profile> <targetHosts>*</targetHosts> <proxyHost>localhost</proxyHost> <proxyPort>7443</proxyPort> <bypass>test.com, direct.com</bypass> </profile> </parameter> When you configure a proxy profile, following are details of the parameters that you need to define in a <profile> : Parameter Description Required targetHosts A host name or a comma-separated list of host names for a target endpoint. Host names can be specified as regular expressions that match a pattern. When targetHosts is specified as an asterisks (*), it will match all the hosts in the profile Yes proxyHost The host name of the proxy server. Yes proxyPort The port number of the proxy server. Yes proxyUserName The user name for the proxy server authentication. No proxyPassword The password for the proxy server authentication. No bypass A host name or a comma-separated list of host names that should not be sent via the proxy server. For example, if you want all requests to *. sample.com to be sent via a proxy server, but need to directly send requests to hello.sample.com , instead of going through the proxy server, you can add hello.sample.com as a bypass host name. You can specify host names as regular expressions that match a pattern. No","title":"Configuring proxy profiles in the ESB profile"},{"location":"setup/configuring_task_scheduling_comp/","text":"Configuring the Task Scheduling Component \u00b6 Info The Task Scheduling component in a WSO2 product allows you to define specific tasks and to invoke them periodiclly. This functionality is used by WSO2 products such as WSO2 Enterprise Integrator (WSO2 EI), WSO2 Enterprise Service Bus (WSO2 ESB), and WSO2 Data Services Server (WSO2 DSS). Follow the instructions given on this page to configure and set up this component . Then, see the respective product's documentation for details on how to use task scheduling. The task scheduling component is configured in the tasks-config.xml file (stored in the <PRODUCT_HOME>/conf/etc/ directory for WSO2 EI, and in the <PRODUCT_HOME>/repository/conf/etc/ directory for other products). Given below are the settings that you can configure in the tasks-config.xml file. Step 1: Setting the task server mode Step 2: Configuring a cluster of task servers Setting the task server count Setting the default location resolver Step 1: Setting the task server mode \u00b6 Select one of the following values for the <taskServerMode> element in the tasks-config.xml file: AUTO : This is the default task handling mode. This setting detects if clustering is enabled in the server and automatically switches to CLUSTERED mode. STANDALONE : This setting is used when a single instance of the product server is installed. That is, tasks will be managed locally within the server. CLUSTERED : This setting is used when a cluster of product servers are put together. This requires Axis2 clustering to work. With this setting, if one of the servers in the cluster fail, the tasks will be rescheduled in one of the remaining server nodes. !!! note Find out more about [clustering WSO2 products](https://docs.wso2.com/display/CLUSTER44x/Clustering+WSO2+Products) . Step 2: Configuring a cluster of task servers \u00b6 If you have enabled the CLUSTERED task server mode in step 1 , the following configuration elements in the tasks-config.xml file will be effective: Setting the task server count \u00b6 Use the parameter shown below to specify the number of servers (in the cluster) that can handle tasks. The task server count is set to \"1\" by default, which indicates that at least one node in the cluster is required for task handling. <taskServerCount>1</taskServerCount> Note that a product cluster begins the process of scheduling tasks only after the given number of servers are activated. For example, consider a situation where ten tasks are saved and scheduled in your product and there are five task-handling servers. As the individual servers become active, we do not want the first active server to schedule all the tasks. Instead, all five servers should become active and share the ten tasks between them. Setting the default location resolver \u00b6 The default location resolver controls how the scheduled tasks are shared by multiple sever nodes of a cluster. Note that the task server count should be a value larger than '1' for this location resolver setting to be effective. For example, if there are 5 task-handling servers in the cluster, the location resolver determines how the tasks get shared among the 5 servers. Info Handling task failover The location resolver parameter also controls how task failover is handled in a clustered environment. A scheduled task will only run in one of the nodes (at a given time) in a clustered environment. The task will failover to another node only if the first node fails. This parameter determines how nodes are selected for task failover. One of the following options can be used for the location resolver . RoundRobinTaskLocationResolver : Cluster nodes are selected on a round-robin basis and the tasks are allocated. This location resolver is enabled in the tasks-config.xml file by default as shown below. <defaultLocationResolver> <locationResolverClass>org.wso2.carbon.ntask.core.impl.RoundRobinTaskLocationResolver</locationResolverClass> </defaultLocationResolver> RandomTaskLocationResolver : Cluster nodes are randomly selected and the tasks are allocated. If you want to enable this location resolver, you need to change the default configuration (shown above) in the tasks-config.xml file as shown below. <defaultLocationResolver> <locationResolverClass>org.wso2.carbon.ntask.core.impl.RandomTaskLocationResolver</locationResolverClass> </defaultLocationResolver> RuleBasedLocationResolver : This allows you to set the criteria for selecting the cluster nodes to which the tasks should be allocated. The [task-type-pattern],[task-name-pattern], and [address-pattern of the server node] can be used as criteria. For example, with this setting, a scheduled task that matches a particular [task-type-pattern], and [task-name-pattern] will be allocated to the server node with a particular [address-pattern]. If multiple server nodes in the cluster match the [address-pattern], the nodes are selected on a round robin basis. This criteria is specified in the configuration using the <property> element. Therefore, you can define multiple properties containing different criteria. Before you enable this location resolver, you need to first comment out the default location resolver that is already enabled in the task-config.xml file. You can then uncomment the following code block, and update the property values as required. <defaultLocationResolver> <locationResolverClass>org.wso2.carbon.ntask.core.impl.RuleBasedLocationResolver</locationResolverClass> <properties> <property name=\"rule-1\">HIVE_TASK,HTTP_SCRIPT.*,192.168.1.*</property> <property name=\"rule-2\">HIVE_TASK,.*,192.168.2.*</property> <property name=\"rule-5\">.*,.*,.*</property> </properties> </defaultLocationResolver> As shown above, the property names (rule-1, rule-2 and rule-5) define a sequence for the list of properties in the configuration. Therefore, scheduled tasks will evaluate the criteria specified in each property according to the sequence. That is, rule-1 is checked before rule-2. In other words, the scheduled task will first check if it matches the criteria in rule-1, and if it does not, it will check rule-2. !!! note The ` RuleBasedLocationResolver ` allows you to address scenarios where tasks are required to be executed in specific server nodes first. Then, it can fail-over to another set of server nodes if the first (preferred) one is not available.","title":"Configuring the Task Scheduling Component"},{"location":"setup/configuring_task_scheduling_comp/#configuring-the-task-scheduling-component","text":"Info The Task Scheduling component in a WSO2 product allows you to define specific tasks and to invoke them periodiclly. This functionality is used by WSO2 products such as WSO2 Enterprise Integrator (WSO2 EI), WSO2 Enterprise Service Bus (WSO2 ESB), and WSO2 Data Services Server (WSO2 DSS). Follow the instructions given on this page to configure and set up this component . Then, see the respective product's documentation for details on how to use task scheduling. The task scheduling component is configured in the tasks-config.xml file (stored in the <PRODUCT_HOME>/conf/etc/ directory for WSO2 EI, and in the <PRODUCT_HOME>/repository/conf/etc/ directory for other products). Given below are the settings that you can configure in the tasks-config.xml file. Step 1: Setting the task server mode Step 2: Configuring a cluster of task servers Setting the task server count Setting the default location resolver","title":"Configuring the Task Scheduling Component"},{"location":"setup/configuring_task_scheduling_comp/#step-1-setting-the-task-server-mode","text":"Select one of the following values for the <taskServerMode> element in the tasks-config.xml file: AUTO : This is the default task handling mode. This setting detects if clustering is enabled in the server and automatically switches to CLUSTERED mode. STANDALONE : This setting is used when a single instance of the product server is installed. That is, tasks will be managed locally within the server. CLUSTERED : This setting is used when a cluster of product servers are put together. This requires Axis2 clustering to work. With this setting, if one of the servers in the cluster fail, the tasks will be rescheduled in one of the remaining server nodes. !!! note Find out more about [clustering WSO2 products](https://docs.wso2.com/display/CLUSTER44x/Clustering+WSO2+Products) .","title":"Step 1: Setting the task server mode"},{"location":"setup/configuring_task_scheduling_comp/#step-2-configuring-a-cluster-of-task-servers","text":"If you have enabled the CLUSTERED task server mode in step 1 , the following configuration elements in the tasks-config.xml file will be effective:","title":"Step 2: Configuring a cluster of task servers"},{"location":"setup/configuring_task_scheduling_comp/#setting-the-task-server-count","text":"Use the parameter shown below to specify the number of servers (in the cluster) that can handle tasks. The task server count is set to \"1\" by default, which indicates that at least one node in the cluster is required for task handling. <taskServerCount>1</taskServerCount> Note that a product cluster begins the process of scheduling tasks only after the given number of servers are activated. For example, consider a situation where ten tasks are saved and scheduled in your product and there are five task-handling servers. As the individual servers become active, we do not want the first active server to schedule all the tasks. Instead, all five servers should become active and share the ten tasks between them.","title":"Setting the task server count"},{"location":"setup/configuring_task_scheduling_comp/#setting-the-default-location-resolver","text":"The default location resolver controls how the scheduled tasks are shared by multiple sever nodes of a cluster. Note that the task server count should be a value larger than '1' for this location resolver setting to be effective. For example, if there are 5 task-handling servers in the cluster, the location resolver determines how the tasks get shared among the 5 servers. Info Handling task failover The location resolver parameter also controls how task failover is handled in a clustered environment. A scheduled task will only run in one of the nodes (at a given time) in a clustered environment. The task will failover to another node only if the first node fails. This parameter determines how nodes are selected for task failover. One of the following options can be used for the location resolver . RoundRobinTaskLocationResolver : Cluster nodes are selected on a round-robin basis and the tasks are allocated. This location resolver is enabled in the tasks-config.xml file by default as shown below. <defaultLocationResolver> <locationResolverClass>org.wso2.carbon.ntask.core.impl.RoundRobinTaskLocationResolver</locationResolverClass> </defaultLocationResolver> RandomTaskLocationResolver : Cluster nodes are randomly selected and the tasks are allocated. If you want to enable this location resolver, you need to change the default configuration (shown above) in the tasks-config.xml file as shown below. <defaultLocationResolver> <locationResolverClass>org.wso2.carbon.ntask.core.impl.RandomTaskLocationResolver</locationResolverClass> </defaultLocationResolver> RuleBasedLocationResolver : This allows you to set the criteria for selecting the cluster nodes to which the tasks should be allocated. The [task-type-pattern],[task-name-pattern], and [address-pattern of the server node] can be used as criteria. For example, with this setting, a scheduled task that matches a particular [task-type-pattern], and [task-name-pattern] will be allocated to the server node with a particular [address-pattern]. If multiple server nodes in the cluster match the [address-pattern], the nodes are selected on a round robin basis. This criteria is specified in the configuration using the <property> element. Therefore, you can define multiple properties containing different criteria. Before you enable this location resolver, you need to first comment out the default location resolver that is already enabled in the task-config.xml file. You can then uncomment the following code block, and update the property values as required. <defaultLocationResolver> <locationResolverClass>org.wso2.carbon.ntask.core.impl.RuleBasedLocationResolver</locationResolverClass> <properties> <property name=\"rule-1\">HIVE_TASK,HTTP_SCRIPT.*,192.168.1.*</property> <property name=\"rule-2\">HIVE_TASK,.*,192.168.2.*</property> <property name=\"rule-5\">.*,.*,.*</property> </properties> </defaultLocationResolver> As shown above, the property names (rule-1, rule-2 and rule-5) define a sequence for the list of properties in the configuration. Therefore, scheduled tasks will evaluate the criteria specified in each property according to the sequence. That is, rule-1 is checked before rule-2. In other words, the scheduled task will first check if it matches the criteria in rule-1, and if it does not, it will check rule-2. !!! note The ` RuleBasedLocationResolver ` allows you to address scenarios where tasks are required to be executed in specific server nodes first. Then, it can fail-over to another set of server nodes if the first (preferred) one is not available.","title":"Setting the default location resolver"},{"location":"setup/configuring_timestamp_conversion_for_rdbms/","text":"Configuring Timestamp Conversion for RDBMS \u00b6 Note Note that the following functionality is only applicable to data integration in WSO2 Enterprise Integrator. By default, a timestamp is always converted to the UTC time zone before inserting the timestamp data to the database. When retrieving the data, the server will convert the timestamp back to the server's time zone. This can sometimes lead to inconsistencies where the time zones applicable to timestamps (for inserting data and retrieving data) are different. Therefore, you can disable UTC conversion for RDBMSs as follows: Open the server startup script, which is stored in the <EI_HOME>/bin directory. For Linux: integrator .sh For Windows: integrator .bat Set the following property to true: -Ddss.legacy.timezone.mode=true This configuration ensures that data is entered into RDBMSs using the server time zone without converting to UTC.","title":"Configuring Timestamp Conversion for RDBMS"},{"location":"setup/configuring_timestamp_conversion_for_rdbms/#configuring-timestamp-conversion-for-rdbms","text":"Note Note that the following functionality is only applicable to data integration in WSO2 Enterprise Integrator. By default, a timestamp is always converted to the UTC time zone before inserting the timestamp data to the database. When retrieving the data, the server will convert the timestamp back to the server's time zone. This can sometimes lead to inconsistencies where the time zones applicable to timestamps (for inserting data and retrieving data) are different. Therefore, you can disable UTC conversion for RDBMSs as follows: Open the server startup script, which is stored in the <EI_HOME>/bin directory. For Linux: integrator .sh For Windows: integrator .bat Set the following property to true: -Ddss.legacy.timezone.mode=true This configuration ensures that data is entered into RDBMSs using the server time zone without converting to UTC.","title":"Configuring Timestamp Conversion for RDBMS"},{"location":"setup/customizing_error_pages/","text":"Customizing Error Pages \u00b6 WSO2 Carbon servers display errors, exceptions, and HTTP status codes in full detail. These are known as Verbose error messages. These error messages contain technical details such as stack traces. There may also disclose other sensitive details. Attackers may fingerprint the server, based on the information disclosed in error messages. Alternatively, attackers may attempt to trigger specific error messages to obtain technical information about the server. You can avoid these situations by configuring the server to display generic, non-detailed error messages in Apache Tomcat. From Carbon 4.4.6 onwards, the pages that should be displayed on a certain throwable exception, error or an HTTP status code are specified in the <CARBON_HOME>repository/conf/tomcat/carbon/WEB-INF/web. xml file. You can customize those error pages as preferred. For example, if you try to access a resource that is not available in the Carbon server (e.g., https://10.100.5.72:9443/abc ), you will view the error page as follows: \" Error 404 - Not Found \". You can customize the above error message by following the instructions given below. Download and install Apache Maven . Create a Maven project using your IDE. Create a directory named resources inside the <PROJECT_HOME>/src/main/ directory, and then create another directory named web inside it. !!! tip ` <PROJECT_HOME> ` denotes the top-level directory of your Maven project. Create a new HTML error page (e.g. new_ error_404.html file) as shown below. This contains the customized error page. <html> <head> <meta http-equiv=\"content-type\" content=\"text/html; charset=ISO-8859-1\"> <title>404 - Error not found</title> </head> <body> <h1>Sorry, this resource is not found.</h1> </body> </html> Add the new_ error_404.html file inside the <PROJECT_HOME>/src/main/web directory. Add the following property below the <version> element in the <PROJECT_HOME>/pom. xml file: <packaging>bundle</packaging> Add the following configurations inside the <plugins> element in the <PROJECT_HOME>/pom. xml file. <plugin> <groupId>org.apache.felix</groupId> <artifactId>maven-bundle-plugin</artifactId> <extensions>true</extensions> <configuration> <instructions> <Bundle-SymbolicName>${project.artifactId}</Bundle-SymbolicName> <Bundle-Name>${project.artifactId}</Bundle-Name> <Import-Package> org.osgi.framework, org.osgi.service.http, org.wso2.carbon.ui, javax.servlet.*;version=\"2.4.0\", *;resolution:=optional </Import-Package> <Fragment-Host>org.wso2.carbon.ui</Fragment-Host> <Carbon-Component>UIBundle</Carbon-Component> </instructions> </configuration> </plugin> Add the following configurations inside the <dependencies> element in the <PROJECT_HOME>/pom. xml file: <dependency> <groupId>org.apache.felix</groupId> <artifactId>org.apache.felix.framework</artifactId> <version>1.0.3</version> </dependency> Build the Maven project by executing the following command: mvn clean install Once the project is built, copy the JAR file (from the <PROJECT_HOME>/target/ directory) to the <CARBON_HOME>/repository/components/dropins/ directory. Change the following configurations in the <CARBON_HOME>/repository/conf/tomcat/carbon/WEB-INF/web.xml file. <error-page> <error-code>404</error-code> <location>/carbon/new_error_404.html</location> </error-page> !!! tip You need to replicate this configuration, and change the values of the ` <error-code> ` and ` <location> ` elements accordingly for each new HTML error page you add. Restart the WSO2 Carbon server. Access the following URL again, to test the error page you customized: https://10.100.5.72:9443/abc. You will view the new error page with the following content: \" Sorry, this resource is not found. \"","title":"Customizing Error Pages"},{"location":"setup/customizing_error_pages/#customizing-error-pages","text":"WSO2 Carbon servers display errors, exceptions, and HTTP status codes in full detail. These are known as Verbose error messages. These error messages contain technical details such as stack traces. There may also disclose other sensitive details. Attackers may fingerprint the server, based on the information disclosed in error messages. Alternatively, attackers may attempt to trigger specific error messages to obtain technical information about the server. You can avoid these situations by configuring the server to display generic, non-detailed error messages in Apache Tomcat. From Carbon 4.4.6 onwards, the pages that should be displayed on a certain throwable exception, error or an HTTP status code are specified in the <CARBON_HOME>repository/conf/tomcat/carbon/WEB-INF/web. xml file. You can customize those error pages as preferred. For example, if you try to access a resource that is not available in the Carbon server (e.g., https://10.100.5.72:9443/abc ), you will view the error page as follows: \" Error 404 - Not Found \". You can customize the above error message by following the instructions given below. Download and install Apache Maven . Create a Maven project using your IDE. Create a directory named resources inside the <PROJECT_HOME>/src/main/ directory, and then create another directory named web inside it. !!! tip ` <PROJECT_HOME> ` denotes the top-level directory of your Maven project. Create a new HTML error page (e.g. new_ error_404.html file) as shown below. This contains the customized error page. <html> <head> <meta http-equiv=\"content-type\" content=\"text/html; charset=ISO-8859-1\"> <title>404 - Error not found</title> </head> <body> <h1>Sorry, this resource is not found.</h1> </body> </html> Add the new_ error_404.html file inside the <PROJECT_HOME>/src/main/web directory. Add the following property below the <version> element in the <PROJECT_HOME>/pom. xml file: <packaging>bundle</packaging> Add the following configurations inside the <plugins> element in the <PROJECT_HOME>/pom. xml file. <plugin> <groupId>org.apache.felix</groupId> <artifactId>maven-bundle-plugin</artifactId> <extensions>true</extensions> <configuration> <instructions> <Bundle-SymbolicName>${project.artifactId}</Bundle-SymbolicName> <Bundle-Name>${project.artifactId}</Bundle-Name> <Import-Package> org.osgi.framework, org.osgi.service.http, org.wso2.carbon.ui, javax.servlet.*;version=\"2.4.0\", *;resolution:=optional </Import-Package> <Fragment-Host>org.wso2.carbon.ui</Fragment-Host> <Carbon-Component>UIBundle</Carbon-Component> </instructions> </configuration> </plugin> Add the following configurations inside the <dependencies> element in the <PROJECT_HOME>/pom. xml file: <dependency> <groupId>org.apache.felix</groupId> <artifactId>org.apache.felix.framework</artifactId> <version>1.0.3</version> </dependency> Build the Maven project by executing the following command: mvn clean install Once the project is built, copy the JAR file (from the <PROJECT_HOME>/target/ directory) to the <CARBON_HOME>/repository/components/dropins/ directory. Change the following configurations in the <CARBON_HOME>/repository/conf/tomcat/carbon/WEB-INF/web.xml file. <error-page> <error-code>404</error-code> <location>/carbon/new_error_404.html</location> </error-page> !!! tip You need to replicate this configuration, and change the values of the ` <error-code> ` and ` <location> ` elements accordingly for each new HTML error page you add. Restart the WSO2 Carbon server. Access the following URL again, to test the error page you customized: https://10.100.5.72:9443/abc. You will view the new error page with the following content: \" Sorry, this resource is not found. \"","title":"Customizing Error Pages"},{"location":"setup/deploying_wso2_ei/","text":"Deploying WSO2 Enterprise Integrator \u00b6 The following sections provide information and instructions on how to cluster the ESB profile of WSO2 Enterprise Integrator (WSO2 EI) with a third-party load balancer. The deployment pattern \u00b6 This deployment scenario uses a two-node ESB cluster. That is, two ESB nodes are configured to serve requests with high availability and scalability. As depicted by the following diagram, the product nodes in the cluster are fronted by an external third-party load balancer, which routes requests to the two nodes on a round-robin basis. Installing WSO2 Enterprise Integrator \u00b6 Follow the instructions on downloading and installing WSO2 EI on a single machine. Setting up the load balancer \u00b6 Follow the instructions on setting up a load balancer for a two-node deployment of WSO2 EI. Setting up the primary database \u00b6 By default, the embedded H2 database is configured as the product's primary database. Select your preferred database type from the following list and follow the given instructions: Setting up MySQL Setting up MSSQL Setting up Oracle Setting up Oracle RAC Setting up IBM Informix Setting up IBM DB2 Setting up Maria DB Setting up embedded Derby Setting up remote Derby Setting up PostgreSQL Setting up remote H2 Updating keystores \u00b6 Create a new SSL certificate and import it to the primary keyStore and trustStore (which are located in the /repository/security/ directory). Your primary keystore can now be configured for SSL communication. Create a new keystore to use as the internal keystore (for the purpose of data encryption/decryption in internal data stores). Configuring the WSO2 EI nodes \u00b6 Follow the steps given below to configure the two nodes in the WSO2 EI deployment. Open the esb.toml file of both the nodes. This file is located in the /conf/ directory. To specify the default settings that should be applicable to the nodes, update the following toml parameters with the required values. // The config section that groups the parameters that identify the server. [server] // The hostname of the server. hostname = \"localhost\" // If you are running both product nodes (of your cluster) on the same VM, set a port offset for on the servers. offset = 0 Find more parameters for deployment settings. To define the clustering configurations that specify how the two nodes communicate with one another, add the following to the esb.toml file and update the values. // The config section that groups the parameters that define cluster coordination. [clustering] // Specify the port on which the current server node is running. local_member_port = 4000 // Specify the hostname of the current server node. local_member_host = \"10.100.5.86\" // Specify the IP address:port of each of the nodes in the cluster as shown below. Be sure to use the same port number and hostname you specified above. members = [\"10.100.5.86:4000\",\"10.100.5.86:4001\"] Find more parameters to define clustering. To enable the two nodes to access the shared database, go to the section on setting up the primary database , select your database type, and verify the parameters specific to your database. To manage the users and roles in the system, update the [user_store] section in the esb.toml file. If you are using primary database of your product as the user store, use the following: // The config section that groups the parameters that define the user store (which is the shared DB in this example) connected to the server. [user_store] // Specify \"database\" as the database type to infer the connection details of your shared DB. type = \"database\" Find more parameters to configure the primary database. If you have set up a separate database, LDAP, or Active Directory user store, see the section under changing the default user store . To change the credentials of the admin user of each node in the cluster, update the following parameters in the esb.toml file. In the below example, a plain text password is used. If required, you can encrypt the password . // The config section that groups the parameters defining the system administrator. [super_admin] // Specify the username of connecting to the server. username = \"admin\" // Specify the password for connecting to the server. password = \"admin\" // Set this property to 'true'. This ensures that the admin account is created in the user store. create_admin_account = true Find more parameters to configure the system administrator. If you have separated the internal keystore of your product, update the [keystore.internal] section in the esb.toml file. See Configuring Keystores for instructions. Optional configurations \u00b6 See the topics given below. Changing the default user store \u00b6 By default, the primary database that you configure will be the user store (that stores all user information) for the server. This will be a JDBC user store. If required, you can configure another database, or an LDAP/Active Directory as your user store and connect it to the product server. Find out more about: Setting up a JDBC user store Setting up a read-only LDAP Setting up a read-write LDAP Setting up a read-write Active Directory Configuring Deployment Synchronization \u00b6 Configure deployment synchronization so that all the integration artifacts you develop can be shared between all the nodes in the cluster. You need to use the deployment synchronization mechanism that is suitable for your environment. See Configuring Deployment Synchronization for instructions. Configuring Analytics \u00b6 If you have already done the configurations explained above, you have the option of applying the following configurations for your deployment. If you wish to view reports, statistics, and graphs related to the message mediation that happens through the ESB, you need to configure the Analytics profile of WSO2 EI. Follow configuring WSO2 EI Analytics in a production setup. Verify your deployment \u00b6 Ensure that you have taken into account the respective security hardening factors (e.g., changing and encrypting the default passwords, configuring JVM security etc.) before deploying WSO2 EI. For more information, see the Production Deployment Checklist . Starting the ESB servers \u00b6 Start the server using the following standard start-up script. On Linux/MacOS : cd <EI_HOME>/bin/ sh intergrator.sh ON Windows : cd <EI_HOME>\\bin\\wso2server.bat","title":"Deploying WSO2 Enterprise Integrator"},{"location":"setup/deploying_wso2_ei/#deploying-wso2-enterprise-integrator","text":"The following sections provide information and instructions on how to cluster the ESB profile of WSO2 Enterprise Integrator (WSO2 EI) with a third-party load balancer.","title":"Deploying WSO2 Enterprise Integrator"},{"location":"setup/deploying_wso2_ei/#the-deployment-pattern","text":"This deployment scenario uses a two-node ESB cluster. That is, two ESB nodes are configured to serve requests with high availability and scalability. As depicted by the following diagram, the product nodes in the cluster are fronted by an external third-party load balancer, which routes requests to the two nodes on a round-robin basis.","title":"The deployment pattern"},{"location":"setup/deploying_wso2_ei/#installing-wso2-enterprise-integrator","text":"Follow the instructions on downloading and installing WSO2 EI on a single machine.","title":"Installing WSO2 Enterprise Integrator"},{"location":"setup/deploying_wso2_ei/#setting-up-the-load-balancer","text":"Follow the instructions on setting up a load balancer for a two-node deployment of WSO2 EI.","title":"Setting up the load balancer"},{"location":"setup/deploying_wso2_ei/#setting-up-the-primary-database","text":"By default, the embedded H2 database is configured as the product's primary database. Select your preferred database type from the following list and follow the given instructions: Setting up MySQL Setting up MSSQL Setting up Oracle Setting up Oracle RAC Setting up IBM Informix Setting up IBM DB2 Setting up Maria DB Setting up embedded Derby Setting up remote Derby Setting up PostgreSQL Setting up remote H2","title":"Setting up the primary database"},{"location":"setup/deploying_wso2_ei/#updating-keystores","text":"Create a new SSL certificate and import it to the primary keyStore and trustStore (which are located in the /repository/security/ directory). Your primary keystore can now be configured for SSL communication. Create a new keystore to use as the internal keystore (for the purpose of data encryption/decryption in internal data stores).","title":"Updating keystores"},{"location":"setup/deploying_wso2_ei/#configuring-the-wso2-ei-nodes","text":"Follow the steps given below to configure the two nodes in the WSO2 EI deployment. Open the esb.toml file of both the nodes. This file is located in the /conf/ directory. To specify the default settings that should be applicable to the nodes, update the following toml parameters with the required values. // The config section that groups the parameters that identify the server. [server] // The hostname of the server. hostname = \"localhost\" // If you are running both product nodes (of your cluster) on the same VM, set a port offset for on the servers. offset = 0 Find more parameters for deployment settings. To define the clustering configurations that specify how the two nodes communicate with one another, add the following to the esb.toml file and update the values. // The config section that groups the parameters that define cluster coordination. [clustering] // Specify the port on which the current server node is running. local_member_port = 4000 // Specify the hostname of the current server node. local_member_host = \"10.100.5.86\" // Specify the IP address:port of each of the nodes in the cluster as shown below. Be sure to use the same port number and hostname you specified above. members = [\"10.100.5.86:4000\",\"10.100.5.86:4001\"] Find more parameters to define clustering. To enable the two nodes to access the shared database, go to the section on setting up the primary database , select your database type, and verify the parameters specific to your database. To manage the users and roles in the system, update the [user_store] section in the esb.toml file. If you are using primary database of your product as the user store, use the following: // The config section that groups the parameters that define the user store (which is the shared DB in this example) connected to the server. [user_store] // Specify \"database\" as the database type to infer the connection details of your shared DB. type = \"database\" Find more parameters to configure the primary database. If you have set up a separate database, LDAP, or Active Directory user store, see the section under changing the default user store . To change the credentials of the admin user of each node in the cluster, update the following parameters in the esb.toml file. In the below example, a plain text password is used. If required, you can encrypt the password . // The config section that groups the parameters defining the system administrator. [super_admin] // Specify the username of connecting to the server. username = \"admin\" // Specify the password for connecting to the server. password = \"admin\" // Set this property to 'true'. This ensures that the admin account is created in the user store. create_admin_account = true Find more parameters to configure the system administrator. If you have separated the internal keystore of your product, update the [keystore.internal] section in the esb.toml file. See Configuring Keystores for instructions.","title":"Configuring the WSO2 EI nodes"},{"location":"setup/deploying_wso2_ei/#optional-configurations","text":"See the topics given below.","title":"Optional configurations"},{"location":"setup/deploying_wso2_ei/#changing-the-default-user-store","text":"By default, the primary database that you configure will be the user store (that stores all user information) for the server. This will be a JDBC user store. If required, you can configure another database, or an LDAP/Active Directory as your user store and connect it to the product server. Find out more about: Setting up a JDBC user store Setting up a read-only LDAP Setting up a read-write LDAP Setting up a read-write Active Directory","title":"Changing the default user store"},{"location":"setup/deploying_wso2_ei/#configuring-deployment-synchronization","text":"Configure deployment synchronization so that all the integration artifacts you develop can be shared between all the nodes in the cluster. You need to use the deployment synchronization mechanism that is suitable for your environment. See Configuring Deployment Synchronization for instructions.","title":"Configuring Deployment Synchronization"},{"location":"setup/deploying_wso2_ei/#configuring-analytics","text":"If you have already done the configurations explained above, you have the option of applying the following configurations for your deployment. If you wish to view reports, statistics, and graphs related to the message mediation that happens through the ESB, you need to configure the Analytics profile of WSO2 EI. Follow configuring WSO2 EI Analytics in a production setup.","title":"Configuring Analytics"},{"location":"setup/deploying_wso2_ei/#verify-your-deployment","text":"Ensure that you have taken into account the respective security hardening factors (e.g., changing and encrypting the default passwords, configuring JVM security etc.) before deploying WSO2 EI. For more information, see the Production Deployment Checklist .","title":"Verify your deployment"},{"location":"setup/deploying_wso2_ei/#starting-the-esb-servers","text":"Start the server using the following standard start-up script. On Linux/MacOS : cd <EI_HOME>/bin/ sh intergrator.sh ON Windows : cd <EI_HOME>\\bin\\wso2server.bat","title":"Starting the ESB servers"},{"location":"setup/deployment_checklist/","text":"Production Deployment Guidelines \u00b6 The requirements for deploying WSO2 products can change based on the deployment scenario and pattern. The recommendations in this topic are for general production use, assuming moderate load conditions. For situations where a high volume of traffic is expected and if there are large deployments, these guidelines may not be sufficient. See Troubleshooting in Production Environments for information on how to obtain and analyze information to solve production issues. The following are the topics addressed in this section. Common guidelines and checklist \u00b6 The following table lists out the common guidelines and details pertaining to them. These are common to all products and are followed for making an installed WSO2 product ready for production. Guideline Details Security hardening Guidelines for hardening the security of a WSO2 deployment in a production environment can be discussed under three high-level categories: Product-level security OS-level security Network-level security Related links See Security Guidelines for Production Deployment for the detailed list of security-related recommendations. Hostname By default, WSO2 products identify the hostname of the current machine through the Java API. However, this value sometimes yields erroneous results on some environments. Therefore, users are recommended to configure the hostname by setting the HostName parameter in the <PRODUCT_HOME>/repository/conf/carbon.xml file. <HostName> your.host.name </HostName> To configure hostnames for WSDLs and endpoints, users are recommended to add the following parameter in the <transportReceiver> section in the <PRODUCT_HOME>/repository/conf/axis2/axis2.xml file as shown below. <parameter name= \"WSDLEPRPrefix\" locked= \"false\" > [http]://your.host.name:[port] </parameter> Related links See the topics on changing hostnames shown below: Setting up hostnames and ports Changing the hostname See Working with Transports for information on transports in WSO2 products. Registry and governance All WSO2 products make use of an instance of a registry to store configurations. The registry uses a database as the persistent storage. By default, the registry uses an embedded H2 database. This embedded database might yield a lower performance and is less reliable compared to a standard database like MySQL when there are a large number of deployed artifacts. Hence, you should look at associated trade-offs, and we recommend that you switch to a database like Oracle, MySQL or MSSQL. Moreover, it is worth noting that the default setup does not include database backup procedures. The production setup should obviously need to have regular database backup procedures configured. When the registry database is pointed to a remote database, multiple running instances of the same product can boot up and run against the same configuration stored in the registry. This, in turn, helps with governance. Related links See here for more information on sharing a registry space across multiple WSO2 product instances. User stores WSO2 products offer three choices to store user details: Using a database Using an LDAP server Using an Active Directory service The default is to use the embedded H2 database, with the user store schema. Like in the case of the registry database, you can switch to a database like Oracle, MySQL or MSSQL. You can point to an existing LDAP or an Active Directory to make use of existing user bases and grant access privileges for WSO2 products based on those user stores. Related links See Configuring User Stores for more information on user stores, how they work, and how to configure them. Monitoring with JMX WSO2 Products supportJMXformonitoring. By default, JMX uses port 9999. You can configure this to the desired port by setting the JMX port parameter in the <PRODUCT_HOME>/repository/conf/carbon.xml file. <Ports> <JMX> 9999 </JMX> </Ports> Related links See JMX-Based Monitoring for information on monitoring WSO2 products using JMX. Tuning WSO2 products Most of the performance tuning recommendations are common to all WSO2 products. However, each WSO2 product may have additional guidelines for optimizing the performance of product-specific features. Related links See Performance Tuning for the general guidelines, which are common to all WSO2 products. For performance tuning guidelines that are specific to each product, go to the product documentation for each product listed below and search for performance tuning guidelines. Listed below are the main WSO2 products: API Manager Data Analytics Server Enterprise Integrator profiles ESB profile Business Process Management profile Message Broker profile Analytics profile Micro Integrator profile IOT Server Identity Server Enterprise Service Bus The following are now legacy products of WSO2: Application Server Business Rules Server Enterprise Mobility Manager Enterprise Store Governance Registry Firewalls The following ports must be accessed when operating within a firewall. 9443 - Used by the management console and services that use the servlet transport, and is defined in the <PRODUCT_HOME>/repository/conf/tomcat/catalina-server.xml file. 9763 - Used by the services that use servlet transport, and is defined in the <PRODUCT_HOME>/repository/conf/tomcat/catalina-server.xml file. 9999 - Used for JMX monitoring, defined in the <PRODUCT_HOME>/repository/conf/carbon.xml file. 8280 - Default HTTP port used by ESB for proxy services, and is defined in the <PRODUCT_HOME>/repository/conf/axis2/axis2.xml file. 8243 - Default HTTPS port used by ESB for proxy services, and is defined in the <PRODUCT_HOME>/repository/conf/axis2/axis2.xml file. Related links See Default Ports of WSO2 Products for a list of common and product-specific ports used by WSO2 products. Proxy servers If the product is hosted behind a proxy such as ApacheHTTPD, users can configure products to use the proxy server by providing the following system properties at the start-up. -Dhttp. proxyHost =xxxx -Dhttp. proxyPort =xxxx Alternatively, this can be done by adding the following configurations in the <PRODUCT_HOME>/repository/conf/axis2/axis2.xml file. <parameter name= \"Proxy\" > <Configuration> <proxyhost> you.proxy.host </proxyhost> <proxyport> your.proxy.port </proxyport> </configuration> </parameter> High availability For high availability, WSO2 products must run on a cluster . This enables the WSO2 products to still work in the case of failover. Databases used for the repository, user management, and business activity monitoring can also be configured in a cluster or can use replication management provided by the RDBMS. Related links See Overview for more details on clustering, what it is, how it helps and other related information. See Separating the Worker and Manager Nodes for information on clustering WSO2 products by separating worker and manager concerns between the nodes. Data backup and archiving For data backup and for archiving of data, use the functionality provided by the RDBMS. Security guidelines and checklist \u00b6 Given below are the common security guidelines for deploying a WSO2 product in a production environment. Also, see the production deployment guidelines and any other product-specific guidelines that might come in the respective product's documentation. WSO2 product-level security \u00b6 Guideline Details Apply security updates Apply all the security patches relevant to your product version. If your WSO2 product version is supported by WSO2 Update Manager (WUM), you need to use WUM to get the latest fixes. If your WSO2 product is listed as a WUM-supported product here , follow the instructions in Getting Started with WUM . If you are using an older WSO2 product version, which is not WUM-supported, you need to download the security patches relevant to your product from the WSO2 Security Patch Release page and apply them to your system manually. The instructions are given in WSO2 Patch Application Process . !!! note Note the following: WSO2 releases security patch notifications monthly via the Support Portal and the above mentioned WSO2 Security Patch Releases page. However, for highly critical issues, patches are issued immediately to customers. The WSO2 Security Patch Release page has all the security patches for the latest product versions. WSO2 does not issue patches publicly for older product versions. Community users are encouraged to use the latest product version to receive all the security fixes. Change default keystores Change the default key stores and create new keys for all the cryptographic operations. WSO2 products by default come with a self-signed SSL key. Since these keys are public, it is recommended to configure your own keys for security purposes. Consider the following guidelines when creating the keystores: Select a key size of at least 2048 bits. Use an SHA256 certificate. Make sure that WSO2 default certificates do not exist in any of the keystores in your production environment. For example, be sure to delete the default public certificate in the default trust store that is shipped with the product. See the recommendations for using keystores in WSO2 products for information. See Creating New Keystores for information on how to create and configure your own keys. Encrypt passwords in configuration files WSO2 products use a tool called Secure Vault to encrypt the plain-text passwords in configuration files. See Securing Passwords in Configuration Files for instructions. Change default ports All the default ports used by WSO2 products are listed in Default Ports of WSO2 Products . For example, the default HTTPS port is 9443 and the HTTP port is 9763. Also, Axis2 services are exposed over the following ports: 8243 and 8280. To change a default port, update the <Offset> element in the carbon. xml file as explained in Changing the Default Ports . Enable read-only access to external user stores (LDAPs etc.) If your WSO2 product is connecting to an external user store, such as Microsoft Active Directory, for the purpose of reading and retrieving user information, be sure to enable read-only access to that user store. For example, see Configuring a Read-Only LDAP User Store under Configuring User Stores for instructions. Always communicate (with external user stores) over TLS All connections from your WSO2 product to external databases, userstores (LDAP), or other services, should be over TLS, to ensure adequate network-level protection. Therefore, be sure to use external systems (user stores, databases) that are TLS-enabled. Connect to data stores using a less privileged user When connecting the WSO2 product to external databases or user stores (LDAP), be sure to go through a user who does not have permission to change the data store's schema. Be sure not to use the root user of the data store because all permissions are generally granted to the root user. Configure strong HTTP(S) security To have strong transport-level security, use TLS 1.2 and disable SSL, TLS 1.0 and 1.1. The TLS protocol and strong ciphers are configured for an HTTP connector in the catalina -server. xml file (using the sslEnabledProtocols and ciphers attributes). See the following links for instructions: Configuring Transport-Level Security Supported Cipher Suites !!! note Note the following: When deciding on the TLS protocol and the ciphers, consider the compatibility with existing client applications. Imposing maximum security might cause functional problems with client applications. Apply ciphers with 256 bits key length if you have applied the Unlimited strength policy. Note that Unlimited strength policy is recommended. Also, consider the following factors when deciding on the ciphers: DES/3DES are deprecated and should not be used. MD5 should not be used, due to known collision attacks. RC4 should not be used, due to crypto-analytical attacks. DSS is limited to a small 1024 bit key size. Cipher-suites that do not provide Perfect Forward Secrecy/ Forward Secrecy (PFS/FS). GCM based ciphers are recommended over CBC ciphers. Remove weak ciphers for PassThrough transport !!! note Applicable only to products that use the PassThrough transport, such as WSO2 API Manager and WSO2 Enterprise Integrator (ESB profile). Remove any weak ciphers from the PassThrough transport and ensure that the server does not accept connections using those weak ciphers. The PassThrough transport is configured using the axis2. xml file (stored in the <PRODUCT_HOME>/repository/conf/axis2/ directory. You need to add the PreferredCiphers parameter under the \"Transport Ins (Listeners)\" section along with the list of relevant cipher suites. See Configuring the PassThrough Transport for instructions. Update the HTTP Response header \"Server\" value By default, all WSO2 products pass \"WSO2 Carbon Server\" as the server value in HTTP headers when sending HTTP responses. This means that information about the WSO2 product stack will be exposed through HTTP responses. It is recommended to change this by configuring the server name in the catalina -server. xml file. See Configuring Transport Level Security for instructions. Enabling HTTP Strict Transport Security Headers (HSTS) Be sure that HTTP Strict Transport Security (HSTS) is enabled for all the applications deployed in your WSO2 server. This includes the management console, and any other web applications and/or Jaggery applications. Note that (for products based on Carbon 4.4.11 or later versions) HSTS is disabled for the applications with which the product is shipped by default. This is because HSTS validation can interrupt the development processes by validating signatures of self-signed certificates. See the topic on Enabling HTTP Strict Transport Security Headers for instructions. Preventing browser caching If there are dynamic pages in your application with sensitive information, you need to prevent browser caching. This can be done by making sure that the applications deployed in your server will return the relevant HTTP response headers. Note that cache prevention headers are enabled for the applications with which the product is shipped by default. Therefore, you need to manually enable cache prevention headers only for all the new applications that you deploy in your server. See the topic on Preventing browser caching for instructions. Increase Ephemeral Diffie-Hellman Key size Before starting the server, open the product startup script ( wso2server.sh in Linux and wso2server.bat in Windows) and enter the following with the other Java properties: -Djdk. tls . ephemeralDHKeySize = 2048 \\ Disable client-initiated renegotiation Before starting the server, open the product startup script ( wso2server.sh in Linux and wso2server.bat in Windows) and enter the following with the other Java properties: -Djdk. tls . rejectClientInitiatedRenegotiation = true \\ Enable HostName Verification If your product is using Carbon Kernel 4.4.17 or a later version, make sure that hostname verification is enabled in the product startup script ( wso2server.sh in Linux and wso2server.bat in Windows) with the Strict mode. That is, you need to enable the following parameter: -Dhttpclient. hostnameVerifier =Strict \\ In Carbon versions prior to 4.4.17, be sure that hostname verification is enabled by setting the following property to false . -Dorg. wso2 . ignoreHostnameVerification = false \\ See Enabling HostName Verification for instructions. Enable additional XSS Protection XSS attacks are prevented on the latest WSO2 products by default. This is due to output encoding of the displaying values. However, if additional protection is required, an input validation valve can be configured. See Mitigating Cross Site Scripting Attacks for instructions. Increase JSESSIONID length If required, increase the session ID length by changing the sessionIDLength attribute of the session manager in the context.xml file (stored in the <PRODUCT_HOME>/repository/conf/tomcat/context.xml directory) as shown below. The default value is 16 bytes. <Manager className= \"org.wso2.carbon.webapp.mgt.CarbonTomcatSessionManager\" sessionIdLength= \"16\" ></Manager> Change default admin credentials All WSO2 products have the Administrator account configured by default. The default user nameand password of the administrator account is \"admin\". To change the administrator credentials, you need to first sign in tothe management console of the product as \"admin\", and then use the Change Password option under Home->Configure->User Management->Users in the navigator. See Changing a Password for more information on how to change the password of the administrator. Restrict access to the management console Make sure that the permission for signing into the management console is granted only to the users that need to use the management console. For example, in products such as WSO2 Identity Server and WSO2 API Manager, the majority of users only need to login to the connected service providers via the WSO2 product. Therefore, such users should not have permission to log into the management console. You can make sure that only administrative users have access to the product's management console. Further, instead of granting all permission to one administrator, you can distribute the responsibilities among administrators by assigning different permissions for conducting various tasks. See Managing Users, Roles and Permissions for instructions. Enable log rotation and monitoring Ensure that you have a relevant log rotation scheme to manage logs. Log4J properties for WSO2 products can be configured in the <PRODUCT_HOME>/repository/conf/log4j.properties file. To roll the wso2carbon.log based on size, the following configurations can be used. log4j. appender . CARBON_LOGFILE =org. apache . log4j . RollingFileAppender log4j. appender . CARBON_LOGFILE =${carbon. home }/repository/logs/${instance. log }/wso2carbon${instance. log }. log log4j. appender . CARBON_LOGFILE . MaxFileSize =1000KB log4j. appender . CARBON_LOGFILE . MaxBackupIndex = 10 See Monitoring Logs for details on how to configure logging details in WSO2 products. Prevent Log Forging Log forging can be prevented by appending a UUID to the log message. See Monitoring Logs for more information on configuring the log4j.properties file. Set appropriate JVM parameters The recommended JDK version is JDK 1.7 or 1.8. See the installation pre-requisites for more information. For JDK 1.7, set the appropriate Heap and Permgen values for the JVM based on your deployment scenario. These can be set in the <PRODUCT_HOME>/bin/wso2server.sh file. You do not need to set this in JDK 1.8 as the MaxPermSize value has been removed from Hotspot JVM. For example -Xms512m -Xmx2048m -XX:MaxPermSize=1024m !!! tip Tip : To run the JVM with 2 GB (-Xmx2048m), you should ideally have about 4GB of memory on the physical machine. OS-level security \u00b6 This section provides the list of OS-level security guidelines for your production environment. Guideline Details Run WSO2 processes with a specific OS-level user Use a separate OS-level user to run WSO2 products. Make sure that the user is only granted the permissions required for running the product for that particular user. Do not use the root/administrator user of your OS because the root/administrator is granted all privileges by default. Minimize software to avoid vulnerability Make sure that you only install the software/packages that are relevant to your WSO2 product's deployment. Also, continuously monitor the software that you install. See the Installation Prerequisites to identify the minimum software your WSO2 product will need. Enable the Firewall Enable a firewall at the OS level (for example, iptables ). This will provide protection for inbound and outbound connections of your WSO2 product. Make sure that you only open the required outbound and inbound ports from the OS-level firewall. Restrict access to TCP ports used for clustering Apply a firewall at host-level to disallow access to TCP ports used for clustering (port 4000, 4001, \u2026 by default) from unrecognized hosts. These ports should be accessible only from other members of the WSO2 product cluster. Use Secure Shell(SSH) Use Secure Shell (SSH) when interacting with servers and executing commands. Adhere to the following best practices when you configure SSH: Change the default ssh port to a higher value. Disable the root or administrator. Enable login with user keys. Display a legal banner or a security banner with security warnings before SSH authentication. Keep the system up-to-date If there are security updates available for the packages installed in your OS (including the Java runtime), be sure to perform the necessary testing in a staging environment, and then proceed to install them for your OS. Monitor user activities Monitor the activities of your OS users. You can do this by enabling OS-level logs and by reviewing them regularly. You can also set up a centralized log monitoring system for this purpose. Session Data Cleanup !!! note Note: This security guideline is specific only to WSO2 Identity Server. In a production environment, there is a possibility for a deadlock/database lock to occur when running a session data cleanup task in high load scenarios. To mitigate this, configure the following property to clean data in chunks. Configure this property in the <IS_HOME>/repository/conf/identity/identity.xml file under <SessionDataCleanUp> with the required chunk size. This value is in the number of records and depends on the database type and server capacity. It also depends on the amount of load generated by single sign-on (SSO). A higher value increases the chances of deadlocks and a lower value increases the time it takes for a cleanup. <DeleteChunkSize> 50000 </DeleteChunkSize> For more information on configuring sessions in production, see Authentication Session Persistence in the WSO2 Identity Server documentation. Make regular backups Make sure to backup important files and archive them continuously. See Backup and Recovery Recommendations for more information. Network-level security \u00b6 This section provides the list of security guidelines for configuring the network. Guideline Details Create a failover setup When your WSO2 products are clustered, you need to regularly monitor the health of your server instances. For example, you need to monitor resource-level factors such as the server's resource utilization, response time anomalies, and the number of incoming network connections. Server monitoring will help you identify when additional server instances (failover instances) are required. You can also make decisions about network routing changes that you need to do in order to avoid server downtime. See Deployment and Clustering/Key Concepts for information on configuring failover. See Monitoring WSO2 products for information on the monitoring options for WSO2 products. Maintain network level logging Be sure to maintain and monitor logs for your proxy servers, load balancers, and other network devices. Check open ports and services Periodically check for open ports using port scanning tools and make sure that only the necessary ports are open to both internal and external networks. Be sure that only the ports relevant to your WSO2 products are open for communication. If there are other ports started, be sure to monitor them. See Default Ports of WSO2 Products for the full list of ports in all WSO2 products. Configure device-level security All your network devices should be periodically checked for anomalies. For example, you need to verify routing tables and firewall rules. Also, make sure that the default credentials are changed before the first use of those devices. Apply firmware updates Firmware updates for your network devices should be applied regularly. Block the /services and /carbon contexts from the DMZ Access to the \"/services\" and \"/carbon\" contexts should be blocked from the DMZ level (i.e., from the proxy server, load balancer and/or firewall). The \"/services\" context is used in WSO2 products to expose admin services. These admin services are used for performing administrative operations using SOAP requests. The \"/carbon\" context is used in WSO2 products to expose the management console (administration console) of the product. The management console is a user interface for performing some of the administrative operations of a product. In addition to the \"/services\" and \"/carbon\" contexts, be sure to expose only the required applications in your product to users beyond the DMZ level in your network. !!! note It is recommended to use a whitelisting approach when allowing access to resources in your product from the DMZ level.","title":"Product Deployment Checklist"},{"location":"setup/deployment_checklist/#production-deployment-guidelines","text":"The requirements for deploying WSO2 products can change based on the deployment scenario and pattern. The recommendations in this topic are for general production use, assuming moderate load conditions. For situations where a high volume of traffic is expected and if there are large deployments, these guidelines may not be sufficient. See Troubleshooting in Production Environments for information on how to obtain and analyze information to solve production issues. The following are the topics addressed in this section.","title":"Production Deployment Guidelines"},{"location":"setup/deployment_checklist/#common-guidelines-and-checklist","text":"The following table lists out the common guidelines and details pertaining to them. These are common to all products and are followed for making an installed WSO2 product ready for production. Guideline Details Security hardening Guidelines for hardening the security of a WSO2 deployment in a production environment can be discussed under three high-level categories: Product-level security OS-level security Network-level security Related links See Security Guidelines for Production Deployment for the detailed list of security-related recommendations. Hostname By default, WSO2 products identify the hostname of the current machine through the Java API. However, this value sometimes yields erroneous results on some environments. Therefore, users are recommended to configure the hostname by setting the HostName parameter in the <PRODUCT_HOME>/repository/conf/carbon.xml file. <HostName> your.host.name </HostName> To configure hostnames for WSDLs and endpoints, users are recommended to add the following parameter in the <transportReceiver> section in the <PRODUCT_HOME>/repository/conf/axis2/axis2.xml file as shown below. <parameter name= \"WSDLEPRPrefix\" locked= \"false\" > [http]://your.host.name:[port] </parameter> Related links See the topics on changing hostnames shown below: Setting up hostnames and ports Changing the hostname See Working with Transports for information on transports in WSO2 products. Registry and governance All WSO2 products make use of an instance of a registry to store configurations. The registry uses a database as the persistent storage. By default, the registry uses an embedded H2 database. This embedded database might yield a lower performance and is less reliable compared to a standard database like MySQL when there are a large number of deployed artifacts. Hence, you should look at associated trade-offs, and we recommend that you switch to a database like Oracle, MySQL or MSSQL. Moreover, it is worth noting that the default setup does not include database backup procedures. The production setup should obviously need to have regular database backup procedures configured. When the registry database is pointed to a remote database, multiple running instances of the same product can boot up and run against the same configuration stored in the registry. This, in turn, helps with governance. Related links See here for more information on sharing a registry space across multiple WSO2 product instances. User stores WSO2 products offer three choices to store user details: Using a database Using an LDAP server Using an Active Directory service The default is to use the embedded H2 database, with the user store schema. Like in the case of the registry database, you can switch to a database like Oracle, MySQL or MSSQL. You can point to an existing LDAP or an Active Directory to make use of existing user bases and grant access privileges for WSO2 products based on those user stores. Related links See Configuring User Stores for more information on user stores, how they work, and how to configure them. Monitoring with JMX WSO2 Products supportJMXformonitoring. By default, JMX uses port 9999. You can configure this to the desired port by setting the JMX port parameter in the <PRODUCT_HOME>/repository/conf/carbon.xml file. <Ports> <JMX> 9999 </JMX> </Ports> Related links See JMX-Based Monitoring for information on monitoring WSO2 products using JMX. Tuning WSO2 products Most of the performance tuning recommendations are common to all WSO2 products. However, each WSO2 product may have additional guidelines for optimizing the performance of product-specific features. Related links See Performance Tuning for the general guidelines, which are common to all WSO2 products. For performance tuning guidelines that are specific to each product, go to the product documentation for each product listed below and search for performance tuning guidelines. Listed below are the main WSO2 products: API Manager Data Analytics Server Enterprise Integrator profiles ESB profile Business Process Management profile Message Broker profile Analytics profile Micro Integrator profile IOT Server Identity Server Enterprise Service Bus The following are now legacy products of WSO2: Application Server Business Rules Server Enterprise Mobility Manager Enterprise Store Governance Registry Firewalls The following ports must be accessed when operating within a firewall. 9443 - Used by the management console and services that use the servlet transport, and is defined in the <PRODUCT_HOME>/repository/conf/tomcat/catalina-server.xml file. 9763 - Used by the services that use servlet transport, and is defined in the <PRODUCT_HOME>/repository/conf/tomcat/catalina-server.xml file. 9999 - Used for JMX monitoring, defined in the <PRODUCT_HOME>/repository/conf/carbon.xml file. 8280 - Default HTTP port used by ESB for proxy services, and is defined in the <PRODUCT_HOME>/repository/conf/axis2/axis2.xml file. 8243 - Default HTTPS port used by ESB for proxy services, and is defined in the <PRODUCT_HOME>/repository/conf/axis2/axis2.xml file. Related links See Default Ports of WSO2 Products for a list of common and product-specific ports used by WSO2 products. Proxy servers If the product is hosted behind a proxy such as ApacheHTTPD, users can configure products to use the proxy server by providing the following system properties at the start-up. -Dhttp. proxyHost =xxxx -Dhttp. proxyPort =xxxx Alternatively, this can be done by adding the following configurations in the <PRODUCT_HOME>/repository/conf/axis2/axis2.xml file. <parameter name= \"Proxy\" > <Configuration> <proxyhost> you.proxy.host </proxyhost> <proxyport> your.proxy.port </proxyport> </configuration> </parameter> High availability For high availability, WSO2 products must run on a cluster . This enables the WSO2 products to still work in the case of failover. Databases used for the repository, user management, and business activity monitoring can also be configured in a cluster or can use replication management provided by the RDBMS. Related links See Overview for more details on clustering, what it is, how it helps and other related information. See Separating the Worker and Manager Nodes for information on clustering WSO2 products by separating worker and manager concerns between the nodes. Data backup and archiving For data backup and for archiving of data, use the functionality provided by the RDBMS.","title":"Common guidelines and checklist"},{"location":"setup/deployment_checklist/#security-guidelines-and-checklist","text":"Given below are the common security guidelines for deploying a WSO2 product in a production environment. Also, see the production deployment guidelines and any other product-specific guidelines that might come in the respective product's documentation.","title":"Security guidelines and checklist"},{"location":"setup/deployment_checklist/#wso2-product-level-security","text":"Guideline Details Apply security updates Apply all the security patches relevant to your product version. If your WSO2 product version is supported by WSO2 Update Manager (WUM), you need to use WUM to get the latest fixes. If your WSO2 product is listed as a WUM-supported product here , follow the instructions in Getting Started with WUM . If you are using an older WSO2 product version, which is not WUM-supported, you need to download the security patches relevant to your product from the WSO2 Security Patch Release page and apply them to your system manually. The instructions are given in WSO2 Patch Application Process . !!! note Note the following: WSO2 releases security patch notifications monthly via the Support Portal and the above mentioned WSO2 Security Patch Releases page. However, for highly critical issues, patches are issued immediately to customers. The WSO2 Security Patch Release page has all the security patches for the latest product versions. WSO2 does not issue patches publicly for older product versions. Community users are encouraged to use the latest product version to receive all the security fixes. Change default keystores Change the default key stores and create new keys for all the cryptographic operations. WSO2 products by default come with a self-signed SSL key. Since these keys are public, it is recommended to configure your own keys for security purposes. Consider the following guidelines when creating the keystores: Select a key size of at least 2048 bits. Use an SHA256 certificate. Make sure that WSO2 default certificates do not exist in any of the keystores in your production environment. For example, be sure to delete the default public certificate in the default trust store that is shipped with the product. See the recommendations for using keystores in WSO2 products for information. See Creating New Keystores for information on how to create and configure your own keys. Encrypt passwords in configuration files WSO2 products use a tool called Secure Vault to encrypt the plain-text passwords in configuration files. See Securing Passwords in Configuration Files for instructions. Change default ports All the default ports used by WSO2 products are listed in Default Ports of WSO2 Products . For example, the default HTTPS port is 9443 and the HTTP port is 9763. Also, Axis2 services are exposed over the following ports: 8243 and 8280. To change a default port, update the <Offset> element in the carbon. xml file as explained in Changing the Default Ports . Enable read-only access to external user stores (LDAPs etc.) If your WSO2 product is connecting to an external user store, such as Microsoft Active Directory, for the purpose of reading and retrieving user information, be sure to enable read-only access to that user store. For example, see Configuring a Read-Only LDAP User Store under Configuring User Stores for instructions. Always communicate (with external user stores) over TLS All connections from your WSO2 product to external databases, userstores (LDAP), or other services, should be over TLS, to ensure adequate network-level protection. Therefore, be sure to use external systems (user stores, databases) that are TLS-enabled. Connect to data stores using a less privileged user When connecting the WSO2 product to external databases or user stores (LDAP), be sure to go through a user who does not have permission to change the data store's schema. Be sure not to use the root user of the data store because all permissions are generally granted to the root user. Configure strong HTTP(S) security To have strong transport-level security, use TLS 1.2 and disable SSL, TLS 1.0 and 1.1. The TLS protocol and strong ciphers are configured for an HTTP connector in the catalina -server. xml file (using the sslEnabledProtocols and ciphers attributes). See the following links for instructions: Configuring Transport-Level Security Supported Cipher Suites !!! note Note the following: When deciding on the TLS protocol and the ciphers, consider the compatibility with existing client applications. Imposing maximum security might cause functional problems with client applications. Apply ciphers with 256 bits key length if you have applied the Unlimited strength policy. Note that Unlimited strength policy is recommended. Also, consider the following factors when deciding on the ciphers: DES/3DES are deprecated and should not be used. MD5 should not be used, due to known collision attacks. RC4 should not be used, due to crypto-analytical attacks. DSS is limited to a small 1024 bit key size. Cipher-suites that do not provide Perfect Forward Secrecy/ Forward Secrecy (PFS/FS). GCM based ciphers are recommended over CBC ciphers. Remove weak ciphers for PassThrough transport !!! note Applicable only to products that use the PassThrough transport, such as WSO2 API Manager and WSO2 Enterprise Integrator (ESB profile). Remove any weak ciphers from the PassThrough transport and ensure that the server does not accept connections using those weak ciphers. The PassThrough transport is configured using the axis2. xml file (stored in the <PRODUCT_HOME>/repository/conf/axis2/ directory. You need to add the PreferredCiphers parameter under the \"Transport Ins (Listeners)\" section along with the list of relevant cipher suites. See Configuring the PassThrough Transport for instructions. Update the HTTP Response header \"Server\" value By default, all WSO2 products pass \"WSO2 Carbon Server\" as the server value in HTTP headers when sending HTTP responses. This means that information about the WSO2 product stack will be exposed through HTTP responses. It is recommended to change this by configuring the server name in the catalina -server. xml file. See Configuring Transport Level Security for instructions. Enabling HTTP Strict Transport Security Headers (HSTS) Be sure that HTTP Strict Transport Security (HSTS) is enabled for all the applications deployed in your WSO2 server. This includes the management console, and any other web applications and/or Jaggery applications. Note that (for products based on Carbon 4.4.11 or later versions) HSTS is disabled for the applications with which the product is shipped by default. This is because HSTS validation can interrupt the development processes by validating signatures of self-signed certificates. See the topic on Enabling HTTP Strict Transport Security Headers for instructions. Preventing browser caching If there are dynamic pages in your application with sensitive information, you need to prevent browser caching. This can be done by making sure that the applications deployed in your server will return the relevant HTTP response headers. Note that cache prevention headers are enabled for the applications with which the product is shipped by default. Therefore, you need to manually enable cache prevention headers only for all the new applications that you deploy in your server. See the topic on Preventing browser caching for instructions. Increase Ephemeral Diffie-Hellman Key size Before starting the server, open the product startup script ( wso2server.sh in Linux and wso2server.bat in Windows) and enter the following with the other Java properties: -Djdk. tls . ephemeralDHKeySize = 2048 \\ Disable client-initiated renegotiation Before starting the server, open the product startup script ( wso2server.sh in Linux and wso2server.bat in Windows) and enter the following with the other Java properties: -Djdk. tls . rejectClientInitiatedRenegotiation = true \\ Enable HostName Verification If your product is using Carbon Kernel 4.4.17 or a later version, make sure that hostname verification is enabled in the product startup script ( wso2server.sh in Linux and wso2server.bat in Windows) with the Strict mode. That is, you need to enable the following parameter: -Dhttpclient. hostnameVerifier =Strict \\ In Carbon versions prior to 4.4.17, be sure that hostname verification is enabled by setting the following property to false . -Dorg. wso2 . ignoreHostnameVerification = false \\ See Enabling HostName Verification for instructions. Enable additional XSS Protection XSS attacks are prevented on the latest WSO2 products by default. This is due to output encoding of the displaying values. However, if additional protection is required, an input validation valve can be configured. See Mitigating Cross Site Scripting Attacks for instructions. Increase JSESSIONID length If required, increase the session ID length by changing the sessionIDLength attribute of the session manager in the context.xml file (stored in the <PRODUCT_HOME>/repository/conf/tomcat/context.xml directory) as shown below. The default value is 16 bytes. <Manager className= \"org.wso2.carbon.webapp.mgt.CarbonTomcatSessionManager\" sessionIdLength= \"16\" ></Manager> Change default admin credentials All WSO2 products have the Administrator account configured by default. The default user nameand password of the administrator account is \"admin\". To change the administrator credentials, you need to first sign in tothe management console of the product as \"admin\", and then use the Change Password option under Home->Configure->User Management->Users in the navigator. See Changing a Password for more information on how to change the password of the administrator. Restrict access to the management console Make sure that the permission for signing into the management console is granted only to the users that need to use the management console. For example, in products such as WSO2 Identity Server and WSO2 API Manager, the majority of users only need to login to the connected service providers via the WSO2 product. Therefore, such users should not have permission to log into the management console. You can make sure that only administrative users have access to the product's management console. Further, instead of granting all permission to one administrator, you can distribute the responsibilities among administrators by assigning different permissions for conducting various tasks. See Managing Users, Roles and Permissions for instructions. Enable log rotation and monitoring Ensure that you have a relevant log rotation scheme to manage logs. Log4J properties for WSO2 products can be configured in the <PRODUCT_HOME>/repository/conf/log4j.properties file. To roll the wso2carbon.log based on size, the following configurations can be used. log4j. appender . CARBON_LOGFILE =org. apache . log4j . RollingFileAppender log4j. appender . CARBON_LOGFILE =${carbon. home }/repository/logs/${instance. log }/wso2carbon${instance. log }. log log4j. appender . CARBON_LOGFILE . MaxFileSize =1000KB log4j. appender . CARBON_LOGFILE . MaxBackupIndex = 10 See Monitoring Logs for details on how to configure logging details in WSO2 products. Prevent Log Forging Log forging can be prevented by appending a UUID to the log message. See Monitoring Logs for more information on configuring the log4j.properties file. Set appropriate JVM parameters The recommended JDK version is JDK 1.7 or 1.8. See the installation pre-requisites for more information. For JDK 1.7, set the appropriate Heap and Permgen values for the JVM based on your deployment scenario. These can be set in the <PRODUCT_HOME>/bin/wso2server.sh file. You do not need to set this in JDK 1.8 as the MaxPermSize value has been removed from Hotspot JVM. For example -Xms512m -Xmx2048m -XX:MaxPermSize=1024m !!! tip Tip : To run the JVM with 2 GB (-Xmx2048m), you should ideally have about 4GB of memory on the physical machine.","title":"WSO2 product-level security"},{"location":"setup/deployment_checklist/#os-level-security","text":"This section provides the list of OS-level security guidelines for your production environment. Guideline Details Run WSO2 processes with a specific OS-level user Use a separate OS-level user to run WSO2 products. Make sure that the user is only granted the permissions required for running the product for that particular user. Do not use the root/administrator user of your OS because the root/administrator is granted all privileges by default. Minimize software to avoid vulnerability Make sure that you only install the software/packages that are relevant to your WSO2 product's deployment. Also, continuously monitor the software that you install. See the Installation Prerequisites to identify the minimum software your WSO2 product will need. Enable the Firewall Enable a firewall at the OS level (for example, iptables ). This will provide protection for inbound and outbound connections of your WSO2 product. Make sure that you only open the required outbound and inbound ports from the OS-level firewall. Restrict access to TCP ports used for clustering Apply a firewall at host-level to disallow access to TCP ports used for clustering (port 4000, 4001, \u2026 by default) from unrecognized hosts. These ports should be accessible only from other members of the WSO2 product cluster. Use Secure Shell(SSH) Use Secure Shell (SSH) when interacting with servers and executing commands. Adhere to the following best practices when you configure SSH: Change the default ssh port to a higher value. Disable the root or administrator. Enable login with user keys. Display a legal banner or a security banner with security warnings before SSH authentication. Keep the system up-to-date If there are security updates available for the packages installed in your OS (including the Java runtime), be sure to perform the necessary testing in a staging environment, and then proceed to install them for your OS. Monitor user activities Monitor the activities of your OS users. You can do this by enabling OS-level logs and by reviewing them regularly. You can also set up a centralized log monitoring system for this purpose. Session Data Cleanup !!! note Note: This security guideline is specific only to WSO2 Identity Server. In a production environment, there is a possibility for a deadlock/database lock to occur when running a session data cleanup task in high load scenarios. To mitigate this, configure the following property to clean data in chunks. Configure this property in the <IS_HOME>/repository/conf/identity/identity.xml file under <SessionDataCleanUp> with the required chunk size. This value is in the number of records and depends on the database type and server capacity. It also depends on the amount of load generated by single sign-on (SSO). A higher value increases the chances of deadlocks and a lower value increases the time it takes for a cleanup. <DeleteChunkSize> 50000 </DeleteChunkSize> For more information on configuring sessions in production, see Authentication Session Persistence in the WSO2 Identity Server documentation. Make regular backups Make sure to backup important files and archive them continuously. See Backup and Recovery Recommendations for more information.","title":"OS-level security"},{"location":"setup/deployment_checklist/#network-level-security","text":"This section provides the list of security guidelines for configuring the network. Guideline Details Create a failover setup When your WSO2 products are clustered, you need to regularly monitor the health of your server instances. For example, you need to monitor resource-level factors such as the server's resource utilization, response time anomalies, and the number of incoming network connections. Server monitoring will help you identify when additional server instances (failover instances) are required. You can also make decisions about network routing changes that you need to do in order to avoid server downtime. See Deployment and Clustering/Key Concepts for information on configuring failover. See Monitoring WSO2 products for information on the monitoring options for WSO2 products. Maintain network level logging Be sure to maintain and monitor logs for your proxy servers, load balancers, and other network devices. Check open ports and services Periodically check for open ports using port scanning tools and make sure that only the necessary ports are open to both internal and external networks. Be sure that only the ports relevant to your WSO2 products are open for communication. If there are other ports started, be sure to monitor them. See Default Ports of WSO2 Products for the full list of ports in all WSO2 products. Configure device-level security All your network devices should be periodically checked for anomalies. For example, you need to verify routing tables and firewall rules. Also, make sure that the default credentials are changed before the first use of those devices. Apply firmware updates Firmware updates for your network devices should be applied regularly. Block the /services and /carbon contexts from the DMZ Access to the \"/services\" and \"/carbon\" contexts should be blocked from the DMZ level (i.e., from the proxy server, load balancer and/or firewall). The \"/services\" context is used in WSO2 products to expose admin services. These admin services are used for performing administrative operations using SOAP requests. The \"/carbon\" context is used in WSO2 products to expose the management console (administration console) of the product. The management console is a user interface for performing some of the administrative operations of a product. In addition to the \"/services\" and \"/carbon\" contexts, be sure to expose only the required applications in your product to users beyond the DMZ level in your network. !!! note It is recommended to use a whitelisting approach when allowing access to resources in your product from the DMZ level.","title":"Network-level security"},{"location":"setup/deployment_synchronization/","text":"Use the following deployment synchronization recommendations based on the rate of change of artifacts that happen in your cluster: If your artifacts are changing frequently, use Network File Share (NFS). If your artifacts are changing at a medium rate, use Remote Synchronization (Rsync). If your artifacts are changing less frequently (for example, once a week) use a Configuration Management System (Puppet, Chef etc.). Using Network File share \u00b6 You can use a common shared file system such as Network File System (NFS) or any other shared file system as the content synchronization mechanism. You need to mount the /repository/deployment/server directory of the two nodes to the shared file system to share all the artifacts between both nodes. Using Remote Synchronization \u00b6 Deployment synchronization can be done using rsync, which is a file copying tool. These changes must be done in the manager node and in the same directory. Create a file called workers-list.txt that lists all the worker nodes in the deployment. The following is a sample of the file where there are two worker nodes. Different nodes are separated by new lines. ubuntu@192.168.1.1:~/setup/192.168.1.1/as/as_worker/repository/deployment/server ubuntu@192.168.1.2:~/setup/192.168.1.2/as/as_worker/repository/deployment/server Create a file to synchronize the /repository/deployment/server folders between the manager and all worker nodes. You must create your own SSH key and define it as the pem_file. Alternatively, you can use an existing SSH key. Specify the manager_server_dir depending on the location in your local machine. Change the logs.txt file path and the lock location based on where they are located in your machine. #!/bin/sh manager_server_dir=~/wso2as-5.2.1/repository/deployment/server/ pem_file=~/.ssh/carbon-440-test.pem #delete the lock on exit trap 'rm -rf /var/lock/depsync-lock' EXIT mkdir /tmp/carbon-rsync-logs/ #keep a lock to stop parallel runs if mkdir /var/lock/depsync-lock; then echo \"Locking succeeded\" >&2 else echo \"Lock failed - exit\" >&2 exit 1 fi #get the workers-list.txt pushd `dirname $0` > /dev/null SCRIPTPATH=`pwd` popd > /dev/null echo $SCRIPTPATH for x in `cat ${SCRIPTPATH}/workers-list.txt` do echo ================================================== >> /tmp/carbon-rsync-logs/logs.txt; echo Syncing $x; rsync --delete -arve \"ssh -i $pem_file -o StrictHostKeyChecking=no\" $manager_server_dir $x >> /tmp/carbon-rsync-logs/logs.txt echo ================================================== >> /tmp/carbon-rsync-logs/logs.txt; done Create a Cron job that executes the above file every minute for deployment synchronization. Do this by running the following command in your command line. Note: You can run the Cron job on one given node (master) at a given time. If you switch it to another node, you must stop the Cron job on the existing node and start a new Cron job on the new node after updating it with the latest files so far. * * * * * /home/ubuntu/setup/rsync-for-depsync/rsync-for-depsync.sh Configuring the WSO2 EI server \u00b6 Depending on the deployment pattern you are setting up, you may have one or several WSO2 EI nodes installed in your deployment. See Deploying WSO2 Enterprise Integrator for instructions on how to set up your deployment.","title":"Configuring Deployment Synchronization"},{"location":"setup/deployment_synchronization/#using-network-file-share","text":"You can use a common shared file system such as Network File System (NFS) or any other shared file system as the content synchronization mechanism. You need to mount the /repository/deployment/server directory of the two nodes to the shared file system to share all the artifacts between both nodes.","title":"Using Network File share"},{"location":"setup/deployment_synchronization/#using-remote-synchronization","text":"Deployment synchronization can be done using rsync, which is a file copying tool. These changes must be done in the manager node and in the same directory. Create a file called workers-list.txt that lists all the worker nodes in the deployment. The following is a sample of the file where there are two worker nodes. Different nodes are separated by new lines. ubuntu@192.168.1.1:~/setup/192.168.1.1/as/as_worker/repository/deployment/server ubuntu@192.168.1.2:~/setup/192.168.1.2/as/as_worker/repository/deployment/server Create a file to synchronize the /repository/deployment/server folders between the manager and all worker nodes. You must create your own SSH key and define it as the pem_file. Alternatively, you can use an existing SSH key. Specify the manager_server_dir depending on the location in your local machine. Change the logs.txt file path and the lock location based on where they are located in your machine. #!/bin/sh manager_server_dir=~/wso2as-5.2.1/repository/deployment/server/ pem_file=~/.ssh/carbon-440-test.pem #delete the lock on exit trap 'rm -rf /var/lock/depsync-lock' EXIT mkdir /tmp/carbon-rsync-logs/ #keep a lock to stop parallel runs if mkdir /var/lock/depsync-lock; then echo \"Locking succeeded\" >&2 else echo \"Lock failed - exit\" >&2 exit 1 fi #get the workers-list.txt pushd `dirname $0` > /dev/null SCRIPTPATH=`pwd` popd > /dev/null echo $SCRIPTPATH for x in `cat ${SCRIPTPATH}/workers-list.txt` do echo ================================================== >> /tmp/carbon-rsync-logs/logs.txt; echo Syncing $x; rsync --delete -arve \"ssh -i $pem_file -o StrictHostKeyChecking=no\" $manager_server_dir $x >> /tmp/carbon-rsync-logs/logs.txt echo ================================================== >> /tmp/carbon-rsync-logs/logs.txt; done Create a Cron job that executes the above file every minute for deployment synchronization. Do this by running the following command in your command line. Note: You can run the Cron job on one given node (master) at a given time. If you switch it to another node, you must stop the Cron job on the existing node and start a new Cron job on the new node after updating it with the latest files so far. * * * * * /home/ubuntu/setup/rsync-for-depsync/rsync-for-depsync.sh","title":"Using Remote Synchronization"},{"location":"setup/deployment_synchronization/#configuring-the-wso2-ei-server","text":"Depending on the deployment pattern you are setting up, you may have one or several WSO2 EI nodes installed in your deployment. See Deploying WSO2 Enterprise Integrator for instructions on how to set up your deployment.","title":"Configuring the WSO2 EI server"},{"location":"setup/enabling_CORS_for_data_services/","text":"Enabling CORS for Data Services \u00b6 You can enable Cross Origin Resource Sharing for data services deployed in the ESB of WSO2 EI. As explained below, you have the option of enabling CORS for selected data services or for all the data services. To enable CORS: Download the following JARs: \" cors-filter-2.4.jar \" and \" java-property-utils-1.9.1.jar \". Copy the JARs to the <EI_HOME>/lib/ directory. Add the following configurations to the <EI_HOME>/conf/tomcat/web.xml file. <filter> <filter-name>CORS</filter-name> <filter-class>com.thetransactioncompany.cors.CORSFilter</filter-class> </filter> <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/*</url-pattern> </filter-mapping> Edit the <filter-mapping> section in the above configuration to specify whether CORS should be enabled for all data services or only selected data services. To enable CORS for a selected data service, add the service's url as the url pattern: <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/services/example/*</url-pattern> </filter-mapping> To enable CORS for multiple data service, you add the urls in a comma separated list: <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/services/sampleservice1/*,/services/sampleservice2/*</url-pattern> </filter-mapping> Alternatively, you can add two separate filter mappings: <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/services/sampleservice1/*</url-pattern> </filter-mapping> <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/services/sampleservice2/*</url-pattern> </filter-mapping> Restart the server.","title":"Enabling CORS for Data Services"},{"location":"setup/enabling_CORS_for_data_services/#enabling-cors-for-data-services","text":"You can enable Cross Origin Resource Sharing for data services deployed in the ESB of WSO2 EI. As explained below, you have the option of enabling CORS for selected data services or for all the data services. To enable CORS: Download the following JARs: \" cors-filter-2.4.jar \" and \" java-property-utils-1.9.1.jar \". Copy the JARs to the <EI_HOME>/lib/ directory. Add the following configurations to the <EI_HOME>/conf/tomcat/web.xml file. <filter> <filter-name>CORS</filter-name> <filter-class>com.thetransactioncompany.cors.CORSFilter</filter-class> </filter> <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/*</url-pattern> </filter-mapping> Edit the <filter-mapping> section in the above configuration to specify whether CORS should be enabled for all data services or only selected data services. To enable CORS for a selected data service, add the service's url as the url pattern: <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/services/example/*</url-pattern> </filter-mapping> To enable CORS for multiple data service, you add the urls in a comma separated list: <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/services/sampleservice1/*,/services/sampleservice2/*</url-pattern> </filter-mapping> Alternatively, you can add two separate filter mappings: <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/services/sampleservice1/*</url-pattern> </filter-mapping> <filter-mapping> <filter-name>CORS</filter-name> <url-pattern>/services/sampleservice2/*</url-pattern> </filter-mapping> Restart the server.","title":"Enabling CORS for Data Services"},{"location":"setup/enabling_SSL_tunneling_thru_proxy_server/","text":"Enabling SSL Tunneling through a Proxy Server \u00b6 If your proxy service connects to a back-end server through a proxy server, you can enable secure socket layer (SSL) tunneling through the proxy server to prevent any intermediate proxy services from interfering with the communication. SSL tunneling is available when your proxy service uses either the HTTP PassThrough transport or the HTTP-NIO transport . The following section walks you through the steps to enable SSL tunneling through a proxy server. Here we will use Squid as the caching and forwarding HTTP web proxy. Setting up Squid Configuring SSL tunneling Setting up Squid \u00b6 Follow the steps below to set up Squid: Install Squid as described here . Add the following lines in the <SQUID_HOME>/etc/squid3/squid.conf file: acl SSL_ports port 443 8443 8448 8248 8280 acl Safe_ports port 80 # http acl Safe_ports port 21 # ftp acl Safe_ports port 443 # https acl Safe_ports port 70 # gopher acl Safe_ports port 210 # wais acl Safe_ports port 1025\u201365535 # unregistered ports acl Safe_ports port 280 # http-mgmt acl Safe_ports port 488 # gss-http acl Safe_ports port 591 # filemaker acl Safe_ports port 777 # multiling http acl CONNECT method CONNECT auth_param basic program /usr/lib/squid3/basic_ncsa_auth /etc/squid3/basic_pw auth_param basic children 5 auth_param basic realm Squid proxy-caching web server auth_param basic credentialsttl 2 hours auth_param basic casesensitive off acl ncsa_users proxy_auth REQUIRED http_access allow ncsa_users http_port 3128 Configuring SSL tunneling \u00b6 Follow the steps below to configure SSL tunneling through the proxy server: In <EI_HOME>/conf/axis2/axis2.xml , add the following parameters to the transportSender configuration for PassThroughHttpSender , PassThroughHttpSSLSender , HttpCoreNIOSender , and HttpCoreNIOSSLSender : <parameter name=\"http.proxyHost\" locked=\"false\"> hostName </parameter> <parameter name=\"http.proxyPort\" locked=\"false\"> portNumber </parameter> where hostName and portNumber specify the host name and port number of the proxy server. Uncomment the following parameter in the PassThroughHttpSSLSender and HttpCoreNIOSSLSender configurations and change the value to AllowAll . <parameter name=\"HostnameVerifier\">AllowAll</parameter> For example, if the host name and port number of proxy server is localhost:8080 , your transportSender configurations for PassThroughHttPSender and PassThroughHttpSSLSender would be as follows: PassThroughHTTPSender ``` html/xml true localhost 8080 **PassThroughHttpSSLSender** ``` html/xml <transportSender name=\"https\" class=\"org.apache.synapse.transport.passthru.PassThroughHttpSSLSender\"> <parameter name=\"non-blocking\" locked=\"false\">true</parameter> <parameter name=\"keystore\" locked=\"false\"> <KeyStore> <Location>repository/resources/security/wso2carbon.jks</Location> <Type>JKS</Type> <Password>wso2carbon</Password> <KeyPassword>wso2carbon</KeyPassword> </KeyStore> </parameter> <parameter name=\"truststore\" locked=\"false\"> <TrustStore> <Location>repository/resources/security/client-truststore.jks</Location> <Type>JKS</Type> <Password>wso2carbon</Password> </TrustStore> </parameter> <parameter name=\"http.proxyHost\" locked=\"false\">localhost</parameter> <parameter name=\"http.proxyPort\" locked=\"false\">8080</parameter> <parameter name=\"HostnameVerifier\">AllowAll</parameter> </transportSender>","title":"Enabling SSL Tunneling through a Proxy Server"},{"location":"setup/enabling_SSL_tunneling_thru_proxy_server/#enabling-ssl-tunneling-through-a-proxy-server","text":"If your proxy service connects to a back-end server through a proxy server, you can enable secure socket layer (SSL) tunneling through the proxy server to prevent any intermediate proxy services from interfering with the communication. SSL tunneling is available when your proxy service uses either the HTTP PassThrough transport or the HTTP-NIO transport . The following section walks you through the steps to enable SSL tunneling through a proxy server. Here we will use Squid as the caching and forwarding HTTP web proxy. Setting up Squid Configuring SSL tunneling","title":"Enabling SSL Tunneling through a Proxy Server"},{"location":"setup/enabling_SSL_tunneling_thru_proxy_server/#setting-up-squid","text":"Follow the steps below to set up Squid: Install Squid as described here . Add the following lines in the <SQUID_HOME>/etc/squid3/squid.conf file: acl SSL_ports port 443 8443 8448 8248 8280 acl Safe_ports port 80 # http acl Safe_ports port 21 # ftp acl Safe_ports port 443 # https acl Safe_ports port 70 # gopher acl Safe_ports port 210 # wais acl Safe_ports port 1025\u201365535 # unregistered ports acl Safe_ports port 280 # http-mgmt acl Safe_ports port 488 # gss-http acl Safe_ports port 591 # filemaker acl Safe_ports port 777 # multiling http acl CONNECT method CONNECT auth_param basic program /usr/lib/squid3/basic_ncsa_auth /etc/squid3/basic_pw auth_param basic children 5 auth_param basic realm Squid proxy-caching web server auth_param basic credentialsttl 2 hours auth_param basic casesensitive off acl ncsa_users proxy_auth REQUIRED http_access allow ncsa_users http_port 3128","title":"Setting up Squid"},{"location":"setup/enabling_SSL_tunneling_thru_proxy_server/#configuring-ssl-tunneling","text":"Follow the steps below to configure SSL tunneling through the proxy server: In <EI_HOME>/conf/axis2/axis2.xml , add the following parameters to the transportSender configuration for PassThroughHttpSender , PassThroughHttpSSLSender , HttpCoreNIOSender , and HttpCoreNIOSSLSender : <parameter name=\"http.proxyHost\" locked=\"false\"> hostName </parameter> <parameter name=\"http.proxyPort\" locked=\"false\"> portNumber </parameter> where hostName and portNumber specify the host name and port number of the proxy server. Uncomment the following parameter in the PassThroughHttpSSLSender and HttpCoreNIOSSLSender configurations and change the value to AllowAll . <parameter name=\"HostnameVerifier\">AllowAll</parameter> For example, if the host name and port number of proxy server is localhost:8080 , your transportSender configurations for PassThroughHttPSender and PassThroughHttpSSLSender would be as follows: PassThroughHTTPSender ``` html/xml true localhost 8080 **PassThroughHttpSSLSender** ``` html/xml <transportSender name=\"https\" class=\"org.apache.synapse.transport.passthru.PassThroughHttpSSLSender\"> <parameter name=\"non-blocking\" locked=\"false\">true</parameter> <parameter name=\"keystore\" locked=\"false\"> <KeyStore> <Location>repository/resources/security/wso2carbon.jks</Location> <Type>JKS</Type> <Password>wso2carbon</Password> <KeyPassword>wso2carbon</KeyPassword> </KeyStore> </parameter> <parameter name=\"truststore\" locked=\"false\"> <TrustStore> <Location>repository/resources/security/client-truststore.jks</Location> <Type>JKS</Type> <Password>wso2carbon</Password> </TrustStore> </parameter> <parameter name=\"http.proxyHost\" locked=\"false\">localhost</parameter> <parameter name=\"http.proxyPort\" locked=\"false\">8080</parameter> <parameter name=\"HostnameVerifier\">AllowAll</parameter> </transportSender>","title":"Configuring SSL tunneling"},{"location":"setup/govern_ext_refs_across_env/","text":"Governing External References Across Environments \u00b6 Note This content is currently restricted to confluence admins as it is being updated. Some artifacts must change based on the environment where the application is deployed. For example, when you deploy a WSO2 Enterprise Integrator(WSO2 EI) application to Dev, QA, and Production environments, the service endpoints are different in each of those environments, so you must update the proxy services accordingly with the relevant endpoint values. This section provides information on how to manage and deploy artifacts across multiple environments. It focuses specifically on the management of endpoints in multiple environments. The endpoints are the environment dependent artifacts and are used as external references from within the proxy service (environment independent artifact) configuration. By doing this, the proxy service configuration does not need to be edited each time it is deployed in a different environment. Understanding the Users Best Practices for Migration Understanding the Users \u00b6 Users interacting with artifacts in each environment often have different roles and have access to different resources and tools. For example: Developer : uses WSO2 Enterprise Integrator(WSO2 EI) Tooling to create services and Composite Applications (Capps) and push project artifacts to a source code repository, such as GitHub. Typically, the developer has no access to QA or Production resources. DevOps or Operations team member : uses scripts and the WSO2 Management Console to pull the applications created by the developers from the source code repository and deploy them to the QA and Production environments. These users need to update the endpoints before they deploy in the different environments. Typically, they do not have WSO2 EI Tooling. Best Practices for Migration \u00b6 The following are the best practices that allow you to easily migrate applications across environments: We recommend you create one Composite Application (CApp) for each deployment environment, namely HelloWorldDevResources and HelloWorldQAResources. This allows you to deploy and manage them separately. Additionally, you need to define a Composite Application for the application itself, i.e., the proxy service. Whenever you create a proxy service use the endpoint as a reference name rather than defining it inline within the proxy service. This approach ensures that the proxy service can be deployed from one environment to another without having to do any environment specific configuration changes. Ensure to use the same name for the endpoint across all environments. Ensure the endpoint values are present and accurate in all environments prior to deploying an application using those endpoints. You can either manually edit the endpoint values prior to deploying the application, or make this an automatic part of your deployment process. Maven Users \u00b6 Maven can be used to build and deploy your artifacts across environments. When using Maven, you are also able to define the endpoints as variables and pass the URL value at the time of building the project. Details on how you can assign endpoint values at the time of building is available at http://susinda.blogspot.ae/2017/01/wso2-esb-how-to-assign-endpoints-at.html . Tip In WSO2 EI Tooling, a Maven Multi Module(MMM) project is used to contain all the project information. For more information on Maven Multi Module projects, see http://www.sonatype.com/books/mvnex-book/reference/multimodule.html .","title":"Governing External References Across Environments"},{"location":"setup/govern_ext_refs_across_env/#governing-external-references-across-environments","text":"Note This content is currently restricted to confluence admins as it is being updated. Some artifacts must change based on the environment where the application is deployed. For example, when you deploy a WSO2 Enterprise Integrator(WSO2 EI) application to Dev, QA, and Production environments, the service endpoints are different in each of those environments, so you must update the proxy services accordingly with the relevant endpoint values. This section provides information on how to manage and deploy artifacts across multiple environments. It focuses specifically on the management of endpoints in multiple environments. The endpoints are the environment dependent artifacts and are used as external references from within the proxy service (environment independent artifact) configuration. By doing this, the proxy service configuration does not need to be edited each time it is deployed in a different environment. Understanding the Users Best Practices for Migration","title":"Governing External References Across Environments"},{"location":"setup/govern_ext_refs_across_env/#understanding-the-users","text":"Users interacting with artifacts in each environment often have different roles and have access to different resources and tools. For example: Developer : uses WSO2 Enterprise Integrator(WSO2 EI) Tooling to create services and Composite Applications (Capps) and push project artifacts to a source code repository, such as GitHub. Typically, the developer has no access to QA or Production resources. DevOps or Operations team member : uses scripts and the WSO2 Management Console to pull the applications created by the developers from the source code repository and deploy them to the QA and Production environments. These users need to update the endpoints before they deploy in the different environments. Typically, they do not have WSO2 EI Tooling.","title":"Understanding the Users"},{"location":"setup/govern_ext_refs_across_env/#best-practices-for-migration","text":"The following are the best practices that allow you to easily migrate applications across environments: We recommend you create one Composite Application (CApp) for each deployment environment, namely HelloWorldDevResources and HelloWorldQAResources. This allows you to deploy and manage them separately. Additionally, you need to define a Composite Application for the application itself, i.e., the proxy service. Whenever you create a proxy service use the endpoint as a reference name rather than defining it inline within the proxy service. This approach ensures that the proxy service can be deployed from one environment to another without having to do any environment specific configuration changes. Ensure to use the same name for the endpoint across all environments. Ensure the endpoint values are present and accurate in all environments prior to deploying an application using those endpoints. You can either manually edit the endpoint values prior to deploying the application, or make this an automatic part of your deployment process.","title":"Best Practices for Migration"},{"location":"setup/govern_ext_refs_across_env/#maven-users","text":"Maven can be used to build and deploy your artifacts across environments. When using Maven, you are also able to define the endpoints as variables and pass the URL value at the time of building the project. Details on how you can assign endpoint values at the time of building is available at http://susinda.blogspot.ae/2017/01/wso2-esb-how-to-assign-endpoints-at.html . Tip In WSO2 EI Tooling, a Maven Multi Module(MMM) project is used to contain all the project information. For more information on Maven Multi Module projects, see http://www.sonatype.com/books/mvnex-book/reference/multimodule.html .","title":"Maven Users"},{"location":"setup/install_in_docker/","text":"Installing WSO2 EI on Docker \u00b6 To run your Micro Integrator solutions on Docker or Kubernetes, you need to first create an immutable docker image with the required synapse artifacts, configurations, and third-party dependencies by using the base Docker image of WSO2 Micro Integrator. You can then deploy and run the immutable images on Docker or Kubernetes. One advantage of having an immutable docker image is that you can easily implement a CI/CD pipeline to systematically test the solution before deploying in production. The Micro Integrator Docker image (with the latest products) is available in the WSO2 Docker Registry . Note that you need a valid WSO2 subscription to use the Docker image with updates. Therefore, you need to provide your log in credentials when downloading the Docker image. If you do not already have a subscription, you can get a free trial subscription . Micro Integrator Docker image (with updates) docker.wso2.com/micro-integrator:1.0.0 Log in to WSO2 Docker Registry docker login docker.wso2.com The community version of Micro Integrator's base Docker image is available on DockerHub . Base Docker Image and Tag (community version) wso2/micro-integrator:1.0.0 Given below are the basic steps you need to follow to run the Micro Integrator on Docker: Export the integration artifacts into a CAR file. Create the Dockerfile as shown below. This file contains instructions to download the base Docker image of WSO2 Micro Integrator from DockerHub (community version) or the WSO2 Docker Registry (includes updates), and to copy the integration artifacts to the Micro Integrator. FROM <docker_image_name>:1.0.0 COPY <directoy_path>/<capp_name> /home/wso2ei/wso2mi/repository/deployment/server/carbonapps The information specified in the Docker file is as follows: FROM The 'FROM' tag in the docker file specifies the WSO2 Micro Integrator version that should be downloaded. You can use the updated Docker image or the community version as shown below. The version is 1.0.0 of the WSO2 Micro Integrator. If required, you can use an earlier version by replacing 'latest' with the version number. Example 1: Docker image with updates FROM docker. wso2 . com /micro-integrator: 1.0. 0 Example 2: Docker image without updates (community version) FROM wso2/micro-integrator: 1.0. 0 COPY The 'COPY' tag in the docker file specifies the directory path to your composite application, followed by the location in your Docker instance to which the composite application should be copied. Example 1 COPY carbonapps /home/wso2ei/wso2mi/repository/deployment/server/carbonapps If you have multiple composite application that you want to deploy in Docker using a single Docker image, add another entry to the Dockerfile. For example: Example 2 COPY carbonapps /home/wso2ei/wso2mi/repository/deployment/server/carbonapps COPY <sample_carbon_app> /home/wso2ei/wso2mi/repository/deployment/server/carbonapps Create an immutable Docker image for your integration artifacts on WSO2 Micro Integrator by executing the following command from the location of your Dockerfile. docker build -t sample_docker_image . Start a Docker container by running the Docker image as shown below. docker run -d -p 8290:8290 sample_docker_image","title":"Install in Docker"},{"location":"setup/install_in_docker/#installing-wso2-ei-on-docker","text":"To run your Micro Integrator solutions on Docker or Kubernetes, you need to first create an immutable docker image with the required synapse artifacts, configurations, and third-party dependencies by using the base Docker image of WSO2 Micro Integrator. You can then deploy and run the immutable images on Docker or Kubernetes. One advantage of having an immutable docker image is that you can easily implement a CI/CD pipeline to systematically test the solution before deploying in production. The Micro Integrator Docker image (with the latest products) is available in the WSO2 Docker Registry . Note that you need a valid WSO2 subscription to use the Docker image with updates. Therefore, you need to provide your log in credentials when downloading the Docker image. If you do not already have a subscription, you can get a free trial subscription . Micro Integrator Docker image (with updates) docker.wso2.com/micro-integrator:1.0.0 Log in to WSO2 Docker Registry docker login docker.wso2.com The community version of Micro Integrator's base Docker image is available on DockerHub . Base Docker Image and Tag (community version) wso2/micro-integrator:1.0.0 Given below are the basic steps you need to follow to run the Micro Integrator on Docker: Export the integration artifacts into a CAR file. Create the Dockerfile as shown below. This file contains instructions to download the base Docker image of WSO2 Micro Integrator from DockerHub (community version) or the WSO2 Docker Registry (includes updates), and to copy the integration artifacts to the Micro Integrator. FROM <docker_image_name>:1.0.0 COPY <directoy_path>/<capp_name> /home/wso2ei/wso2mi/repository/deployment/server/carbonapps The information specified in the Docker file is as follows: FROM The 'FROM' tag in the docker file specifies the WSO2 Micro Integrator version that should be downloaded. You can use the updated Docker image or the community version as shown below. The version is 1.0.0 of the WSO2 Micro Integrator. If required, you can use an earlier version by replacing 'latest' with the version number. Example 1: Docker image with updates FROM docker. wso2 . com /micro-integrator: 1.0. 0 Example 2: Docker image without updates (community version) FROM wso2/micro-integrator: 1.0. 0 COPY The 'COPY' tag in the docker file specifies the directory path to your composite application, followed by the location in your Docker instance to which the composite application should be copied. Example 1 COPY carbonapps /home/wso2ei/wso2mi/repository/deployment/server/carbonapps If you have multiple composite application that you want to deploy in Docker using a single Docker image, add another entry to the Dockerfile. For example: Example 2 COPY carbonapps /home/wso2ei/wso2mi/repository/deployment/server/carbonapps COPY <sample_carbon_app> /home/wso2ei/wso2mi/repository/deployment/server/carbonapps Create an immutable Docker image for your integration artifacts on WSO2 Micro Integrator by executing the following command from the location of your Dockerfile. docker build -t sample_docker_image . Start a Docker container by running the Docker image as shown below. docker run -d -p 8290:8290 sample_docker_image","title":"Installing WSO2 EI on Docker"},{"location":"setup/install_in_kubernetes/","text":"Installing WSO2 EI on Kubernetes \u00b6 Kubernetes is an open source container orchestration system for automating, scaling, and managing your application deployments. To run your Micro Integrator solutions on Kubernetes, you need to first create an immutable docker image with the required synapse artifacts, configurations, and third-party dependencies by using the base Docker image of WSO2 Micro Integrator as explained above . You can then use the immutable Docker image to create pods and to configure the kubernetes deployments according to your requirements. Follow the Quick Start Guide to run a simple use case on a Kubernetes cluster.","title":"Install in Kubernetes"},{"location":"setup/install_in_kubernetes/#installing-wso2-ei-on-kubernetes","text":"Kubernetes is an open source container orchestration system for automating, scaling, and managing your application deployments. To run your Micro Integrator solutions on Kubernetes, you need to first create an immutable docker image with the required synapse artifacts, configurations, and third-party dependencies by using the base Docker image of WSO2 Micro Integrator as explained above . You can then use the immutable Docker image to create pods and to configure the kubernetes deployments according to your requirements. Follow the Quick Start Guide to run a simple use case on a Kubernetes cluster.","title":"Installing WSO2 EI on Kubernetes"},{"location":"setup/install_in_vm/","text":"Installing in a VM \u00b6 Follow the steps given below to install and run the Micro Integrator on a VM. Installing the WSO2 Micro Integrator \u00b6 Follow the steps below: Go to the WSO2 Micro Integrator product page and download. Note that there are several options for installing the product in various environments. Use the available links for more information on each option. If you used the installer, double-click to open the installation wizard, which will guide you through the installation. When you finish, the product will be installed and ready for use. Accessing the HOME directory \u00b6 Let's call the installation location of your product the MI_HOME directory. If you used the installer to install the product, this is located in a place specific to your OS as shown below: OS Home directory Mac OS /Library/WSO2/MicroIntegrator/1.0.0 Windows C:\\Program Files\\WSO2\\MicroIntegrator\\1.0.0 Ubuntu /usr/lib/wso2/MicroIntegrator/1.0.0 CentOS /usr/lib64/MicroIntegrator/1.0.0 Uninstalling the product \u00b6 If you used the installer to install the product, you can uninstall by following the steps given below: OS Instructions Mac OS Open a terminal and run the following command as the root user: sudo bash /Library/WSO2/MicroIntegrator/ 1.0. 0 /uninstall. sh Windows Go to the Start Menu -> Programs -> WSO2 -> Uninstall Micro Integrator 1.0.0 or search Uninstall Micro Integrator 1.0.0 and click the shortcut icon. This will uninstall the product from your computer. Ubuntu Open a terminal and run the following command: sudo apt-get purge wso2mi- 1.0. 0 CentOS Open a terminal and run the following command: sudo yum remove wso2ei- 1.0. 0 -x86_ 64 Running the Micro Integrator \u00b6 Start the WSO2 Micro Integrator by following the instructions given below. Using the installer \u00b6 On MacOS/Linux/CentOS , open a terminal and execute the command given below. sudo wso2mi-1.0.0 The operation log keeps running until the profile starts, which usually takes several seconds. Wait until the profile fully boots up and displays a message similar to \" WSO2 Carbon started in n seconds. \" On Windows , go to Start Menu -> Programs -> WSO2 -> Micro Integrator. This will open a terminal and start the relevant profile. Using the binary distribution \u00b6 Before you execute the product startup script, be sure to set the JAVA HOME in your machine. Use a JDK that is compatible with WSO2 Micro Integrator . Open a terminal and navigate to the <MI_HOME>/bin/ directory, where <MI_HOME> is the home directory of your product distribution. Execute the relevant command. On MacOS/Linux/CentOS sh micro-integrator.sh If you have installed the product using the installer , and you want to manually run the product startup script from the /bin directory, you need to use the 'sudo launcher_micro-integrator' command. This script automatically assigns the JAVA HOME of your VM to the root user of your Micro Integrator instance. On Windows micro-integrator.bat By default, the HTTP listener port is 8290 and the default HTTPS listener port is 8253. Stopping the Micro In tegrator \u00b6 To stop the Micro Integrator runtime, press Ctrl+C in the command window.","title":"Install in a VM"},{"location":"setup/install_in_vm/#installing-in-a-vm","text":"Follow the steps given below to install and run the Micro Integrator on a VM.","title":"Installing in a VM"},{"location":"setup/install_in_vm/#installing-the-wso2-micro-integrator","text":"Follow the steps below: Go to the WSO2 Micro Integrator product page and download. Note that there are several options for installing the product in various environments. Use the available links for more information on each option. If you used the installer, double-click to open the installation wizard, which will guide you through the installation. When you finish, the product will be installed and ready for use.","title":"Installing the WSO2 Micro Integrator"},{"location":"setup/install_in_vm/#accessing-the-home-directory","text":"Let's call the installation location of your product the MI_HOME directory. If you used the installer to install the product, this is located in a place specific to your OS as shown below: OS Home directory Mac OS /Library/WSO2/MicroIntegrator/1.0.0 Windows C:\\Program Files\\WSO2\\MicroIntegrator\\1.0.0 Ubuntu /usr/lib/wso2/MicroIntegrator/1.0.0 CentOS /usr/lib64/MicroIntegrator/1.0.0","title":"Accessing the HOME directory"},{"location":"setup/install_in_vm/#uninstalling-the-product","text":"If you used the installer to install the product, you can uninstall by following the steps given below: OS Instructions Mac OS Open a terminal and run the following command as the root user: sudo bash /Library/WSO2/MicroIntegrator/ 1.0. 0 /uninstall. sh Windows Go to the Start Menu -> Programs -> WSO2 -> Uninstall Micro Integrator 1.0.0 or search Uninstall Micro Integrator 1.0.0 and click the shortcut icon. This will uninstall the product from your computer. Ubuntu Open a terminal and run the following command: sudo apt-get purge wso2mi- 1.0. 0 CentOS Open a terminal and run the following command: sudo yum remove wso2ei- 1.0. 0 -x86_ 64","title":"Uninstalling the product"},{"location":"setup/install_in_vm/#running-the-micro-integrator","text":"Start the WSO2 Micro Integrator by following the instructions given below.","title":"Running the Micro Integrator"},{"location":"setup/install_in_vm/#using-the-installer","text":"On MacOS/Linux/CentOS , open a terminal and execute the command given below. sudo wso2mi-1.0.0 The operation log keeps running until the profile starts, which usually takes several seconds. Wait until the profile fully boots up and displays a message similar to \" WSO2 Carbon started in n seconds. \" On Windows , go to Start Menu -> Programs -> WSO2 -> Micro Integrator. This will open a terminal and start the relevant profile.","title":"Using the installer"},{"location":"setup/install_in_vm/#using-the-binary-distribution","text":"Before you execute the product startup script, be sure to set the JAVA HOME in your machine. Use a JDK that is compatible with WSO2 Micro Integrator . Open a terminal and navigate to the <MI_HOME>/bin/ directory, where <MI_HOME> is the home directory of your product distribution. Execute the relevant command. On MacOS/Linux/CentOS sh micro-integrator.sh If you have installed the product using the installer , and you want to manually run the product startup script from the /bin directory, you need to use the 'sudo launcher_micro-integrator' command. This script automatically assigns the JAVA HOME of your VM to the root user of your Micro Integrator instance. On Windows micro-integrator.bat By default, the HTTP listener port is 8290 and the default HTTPS listener port is 8253.","title":"Using the binary distribution"},{"location":"setup/install_in_vm/#stopping-the-micro-in-tegrator","text":"To stop the Micro Integrator runtime, press Ctrl+C in the command window.","title":"Stopping the Micro In tegrator"},{"location":"setup/network_os_performance/","text":"Network and OS Level Performance Tuning \u00b6 When it comes to performance, the OS that the server runs plays an important role. This page describes the parameters that you can configure to optimize the network and OS performance. If you are running MacOS Sierra and experience long startup times for WSO2 products, try mapping your Mac hostname to 127.0.0.1 and ::1 in the /etc/hosts file as described in this blog post . Following are the files and parameters you can configure to optimize performance: Configure the following parameters in the /etc/sysctl.conf file of Linux for maximum concurrency. These parameters can be set to specify a larger port range, a more effective TCP connection timeout value, and a number of other important settings at the OS-level based on your requirement. Since all these settings apply at the OS level, changing settings can affect other programs running on the server. The sample values specified here might not be the optimal values for your production system. You need to apply the values and run a performance test to find the best values for your production system. Parameter Description Recommended Value net.ipv4.tcp_fin_timeout This is the length of time (in seconds) that TCP takes to receive a final FIN before the socket is closed. Setting this is required to prevent DoS attacks. 30 net.ipv4.tcp_tw_recycle This enables fast recycling of TIME_WAIT sockets. !!! note Note Change this with caution and ONLY in internal networks where the network connectivity speeds are faster. It is not recommended to use net.ipv4.tcp_tw_recycle = 1 when working with network address translation (NAT), such as if you are deploying products in EC2 or any other environment configured with NAT. 1 net.ipv4.tcp_tw_reuse This allows reuse of sockets in TIME_WAIT state for new connections when it is safe from the network stack\u2019s perspective. 1 net.core.rmem_default This sets the default OS receive buffer size for all types of connections. 524288 net.core.wmem_default This sets the default OS send buffer size for all types of connections. 524288 net.core.rmem_max This sets the maximum OS receive buffer size for all types of connections. 67108864 net.core.wmem_max This sets the maximum OS send buffer size for all types of connections. 67108864 net.ipv4.tcp_rmem This specifies the receive buffer space for each TCP connection and has three values that hold the following information: The first value is the minimum receive buffer space for each TCP connection, and this buffer is always allocated to a TCP socket, even under high pressure on the system. The second value is the default receive buffer space allocated for each TCP socket. This value overrides the /proc/sys/net/core/rmem_default value used by other protocols. The last value is the maximum receive buffer space allocated for a TCP socket. 4096 87380 16777216 net.ipv4.tcp_wmem This specifies the send buffer space for each TCP connection and has three values that hold the following information: The first value is the minimum TCP send buffer space available for a single TCP socket. The second value is the default send buffer space allowed for a single TCP socket to use. The third value is the maximum TCP send buffer space. Every TCP socket has the specified amount of buffer space to use before the buffer is filled up, and each of the three values are used under different conditions. 4096 65536 16777216 net.ipv4.ip_local_port_range This defines the local port range that is used by TCP and UDP to choose the local port. The first number is the first local port allowed for TCP and UDP traffic, and the second number is the last port number. If your Linux server is opening a large number of outgoing network connections, you need to increase the default local port range. In Linux, the default range of IP port numbers allowed for TCP and UDP traffic is small, and if this range is not changed accordingly, a server can come under fire if it runs out of ports. 1024 65535 fs.file-max This is the maximum number of file handles that the kernel can allocate. The kernel has a built-in limit on the number of files that a process can open. If you need to increase this limit, you can increase the fs.file-max value although it can take up some system memory. 2097152 Configure the following parameters in the /etc/security/limits.conf file of Linux if you need to alter the maximum number of open files allowed for system users. * soft nofile 4096 * hard nofile 65535 The * character denotes that the limit is applicable to all system users in the server, and the values specified above are the default values for normal system usage. The hard limit is used to enforce hard resource limits and the soft limit is to enforce soft resource limits. The hard limit is set by the super user and is enforced by the Kernel. You cannot increase the hard limit unless you have super user privileges. You can increase or decrease the soft limit as necessary, but the maximum limit you can increase this is up to the hard limit value that is set. Configure the following settings in the /etc/security/limits.conf file of Linux if you need to alter the maximum number of processes a system user is allowed to run at a given time. Each carbon server instance you run requires upto 1024 threads with the default thread pool configuration. Therefore, you need to increase both the hard and soft nproc value by 1024 per carbon server. * soft nproc 20000 * hard nproc 20000 The * character denotes that the limit is applicable to all system users in the server, and the values specified above are the default values for normal system usage.","title":"Tuning Network and OS Performance"},{"location":"setup/network_os_performance/#network-and-os-level-performance-tuning","text":"When it comes to performance, the OS that the server runs plays an important role. This page describes the parameters that you can configure to optimize the network and OS performance. If you are running MacOS Sierra and experience long startup times for WSO2 products, try mapping your Mac hostname to 127.0.0.1 and ::1 in the /etc/hosts file as described in this blog post . Following are the files and parameters you can configure to optimize performance: Configure the following parameters in the /etc/sysctl.conf file of Linux for maximum concurrency. These parameters can be set to specify a larger port range, a more effective TCP connection timeout value, and a number of other important settings at the OS-level based on your requirement. Since all these settings apply at the OS level, changing settings can affect other programs running on the server. The sample values specified here might not be the optimal values for your production system. You need to apply the values and run a performance test to find the best values for your production system. Parameter Description Recommended Value net.ipv4.tcp_fin_timeout This is the length of time (in seconds) that TCP takes to receive a final FIN before the socket is closed. Setting this is required to prevent DoS attacks. 30 net.ipv4.tcp_tw_recycle This enables fast recycling of TIME_WAIT sockets. !!! note Note Change this with caution and ONLY in internal networks where the network connectivity speeds are faster. It is not recommended to use net.ipv4.tcp_tw_recycle = 1 when working with network address translation (NAT), such as if you are deploying products in EC2 or any other environment configured with NAT. 1 net.ipv4.tcp_tw_reuse This allows reuse of sockets in TIME_WAIT state for new connections when it is safe from the network stack\u2019s perspective. 1 net.core.rmem_default This sets the default OS receive buffer size for all types of connections. 524288 net.core.wmem_default This sets the default OS send buffer size for all types of connections. 524288 net.core.rmem_max This sets the maximum OS receive buffer size for all types of connections. 67108864 net.core.wmem_max This sets the maximum OS send buffer size for all types of connections. 67108864 net.ipv4.tcp_rmem This specifies the receive buffer space for each TCP connection and has three values that hold the following information: The first value is the minimum receive buffer space for each TCP connection, and this buffer is always allocated to a TCP socket, even under high pressure on the system. The second value is the default receive buffer space allocated for each TCP socket. This value overrides the /proc/sys/net/core/rmem_default value used by other protocols. The last value is the maximum receive buffer space allocated for a TCP socket. 4096 87380 16777216 net.ipv4.tcp_wmem This specifies the send buffer space for each TCP connection and has three values that hold the following information: The first value is the minimum TCP send buffer space available for a single TCP socket. The second value is the default send buffer space allowed for a single TCP socket to use. The third value is the maximum TCP send buffer space. Every TCP socket has the specified amount of buffer space to use before the buffer is filled up, and each of the three values are used under different conditions. 4096 65536 16777216 net.ipv4.ip_local_port_range This defines the local port range that is used by TCP and UDP to choose the local port. The first number is the first local port allowed for TCP and UDP traffic, and the second number is the last port number. If your Linux server is opening a large number of outgoing network connections, you need to increase the default local port range. In Linux, the default range of IP port numbers allowed for TCP and UDP traffic is small, and if this range is not changed accordingly, a server can come under fire if it runs out of ports. 1024 65535 fs.file-max This is the maximum number of file handles that the kernel can allocate. The kernel has a built-in limit on the number of files that a process can open. If you need to increase this limit, you can increase the fs.file-max value although it can take up some system memory. 2097152 Configure the following parameters in the /etc/security/limits.conf file of Linux if you need to alter the maximum number of open files allowed for system users. * soft nofile 4096 * hard nofile 65535 The * character denotes that the limit is applicable to all system users in the server, and the values specified above are the default values for normal system usage. The hard limit is used to enforce hard resource limits and the soft limit is to enforce soft resource limits. The hard limit is set by the super user and is enforced by the Kernel. You cannot increase the hard limit unless you have super user privileges. You can increase or decrease the soft limit as necessary, but the maximum limit you can increase this is up to the hard limit value that is set. Configure the following settings in the /etc/security/limits.conf file of Linux if you need to alter the maximum number of processes a system user is allowed to run at a given time. Each carbon server instance you run requires upto 1024 threads with the default thread pool configuration. Therefore, you need to increase both the hard and soft nproc value by 1024 per carbon server. * soft nproc 20000 * hard nproc 20000 The * character denotes that the limit is applicable to all system users in the server, and the values specified above are the default values for normal system usage.","title":"Network and OS Level Performance Tuning"},{"location":"setup/registry_mounting/","text":"Optional: Mounting a Registry \u00b6 Add the following configurations to the <EI_HOME>/conf/registry.xml file of each ESB node to configure the shared registry database and mounting details. This ensures that the shared registry for governance and configurations (i.e., the REGISTRY_DB database) mount on both ESB nodes. Before you begin \u00b6 Note the following when adding these configurations: The existing dbConfig called wso2registry must not be removed . The datasource you specify in the <dbConfig name=\"sharedregistry\"> tag must match the JNDI Config name you specify in the <EI_HOME>/conf/datasources/master-datasources.xml file. The registry mount path denotes the type of registry. For example, \u201d /_system/config \u201d refers to configuration Registry, and \" /_system/governance \" refers to the governance registry. The \\<dbconfig> entry enables you to identify the datasource you configured in the <EI_HOME>/conf/datasources/master-datasources.xml file. The unique name \"s haredregistry \" refers to that datasource entry. The <remoteInstance> section refers to an external registry mount. Specify the read-only/read-write nature of this instance, caching configurations and the registry root location in this section. Also, specify the cache ID in the <remoteInstance> section. This enables caching to function properly in the clustered environment. Cache ID is the same as the JDBC connection URL of the registry database. This value is the Cache ID of the remote instance. It should be in the format of $database_username@$database_url , where $database_username is the username of the remote instance database and $database_url is the remote instance database URL. This cacheID denotes the enabled cache. In this case, the database it should connect to is REGISTRY_DB , which is the database shared across all the nodes. You can find that in the mounting configurations of the same datasource that is being used. Define a unique name in the <id> tag for each remote instance. This is then referred to from mount configurations. In the above example, the unique ID for the remote instance is \" instanceId \". Specify the actual mount path and target mount path in each of the mounting configurations. The target path can be any meaningful name. In this instance, it is \" /_system/eiconfig \". Update the configurations \u00b6 Open the esb.toml file and addd the following configuration section. param1=\"\" param2=\"\"","title":"Optional: Mounting a Registry"},{"location":"setup/registry_mounting/#optional-mounting-a-registry","text":"Add the following configurations to the <EI_HOME>/conf/registry.xml file of each ESB node to configure the shared registry database and mounting details. This ensures that the shared registry for governance and configurations (i.e., the REGISTRY_DB database) mount on both ESB nodes.","title":"Optional: Mounting a Registry"},{"location":"setup/registry_mounting/#before-you-begin","text":"Note the following when adding these configurations: The existing dbConfig called wso2registry must not be removed . The datasource you specify in the <dbConfig name=\"sharedregistry\"> tag must match the JNDI Config name you specify in the <EI_HOME>/conf/datasources/master-datasources.xml file. The registry mount path denotes the type of registry. For example, \u201d /_system/config \u201d refers to configuration Registry, and \" /_system/governance \" refers to the governance registry. The \\<dbconfig> entry enables you to identify the datasource you configured in the <EI_HOME>/conf/datasources/master-datasources.xml file. The unique name \"s haredregistry \" refers to that datasource entry. The <remoteInstance> section refers to an external registry mount. Specify the read-only/read-write nature of this instance, caching configurations and the registry root location in this section. Also, specify the cache ID in the <remoteInstance> section. This enables caching to function properly in the clustered environment. Cache ID is the same as the JDBC connection URL of the registry database. This value is the Cache ID of the remote instance. It should be in the format of $database_username@$database_url , where $database_username is the username of the remote instance database and $database_url is the remote instance database URL. This cacheID denotes the enabled cache. In this case, the database it should connect to is REGISTRY_DB , which is the database shared across all the nodes. You can find that in the mounting configurations of the same datasource that is being used. Define a unique name in the <id> tag for each remote instance. This is then referred to from mount configurations. In the above example, the unique ID for the remote instance is \" instanceId \". Specify the actual mount path and target mount path in each of the mounting configurations. The target path can be any meaningful name. In this instance, it is \" /_system/eiconfig \".","title":"Before you begin"},{"location":"setup/registry_mounting/#update-the-configurations","text":"Open the esb.toml file and addd the following configuration section. param1=\"\" param2=\"\"","title":"Update the configurations"},{"location":"setup/setting_up_jdbc_userstore/","text":"Setting up a JDBC User Store \u00b6","title":"Setting up a JDBC User Store"},{"location":"setup/setting_up_jdbc_userstore/#setting-up-a-jdbc-user-store","text":"","title":"Setting up a JDBC User Store"},{"location":"setup/setting_up_lb/","text":"Setting up a Load balancer \u00b6 The load balancer automatically distributes incoming traffic across multiple WSO2 product instances. It enables you to achieve greater levels of fault tolerance in your cluster and provides the required balancing of load needed to distribute traffic. Before you begin \u00b6 Note the following: These configurations are not required if your clustering setup does not have a load balancer. The load balancer ports of the deployment pattern that is shown above are HTTP 80 and HTTPS 443. If your system uses any other ports, be sure to replace 80 and 443 values with the corresponding ports when you follow the configuration steps in this section. The load balancer directs requests to the server on a round robin basis. For example, the load balancer will direct requests to node 1 ( xxx.xxx.xxx.xx1 ) of the ESB cluster as follows: HTTP requests will be directed to node 1 using the http://xxx.xxx.xxx.xx1/<service> URL via HTTP 80 port. HTTPS requests will be directed to node 1 using the https://xxx.xxx.xxx.xx1/<service> URL via HTTPS 443 port. The management console of node 1 will be accessed using the https://xxx.xxx.xxx.xx1/carbon/ URL via HTTPS 443 port. It is recommended to use NGINX Plus as your load balancer of choice. Configuring the load balancer \u00b6 Follow the steps below to configure NGINX Plus version 1.7.11 or NGINX community version 1.9.2 as the load balancer. Install NGINX Plus or the NGINX community version on your cluster network. Create a VHost file named ei.http.conf in the /etc/nginx/conf.d directory and add the following configurations. This configures NGINX Plus to direct the HTTP requests to the two ESB nodes (xxx.xxx.xxx.xx1 and xxx.xxx.xxx.xx2) via the HTTP 80 port using the http://ei.wso2.com/ URL. If you are setting up NGINX on a Mac OS, you will not have the conf.d directory. Follow the steps given below to add the VHost files mentioned in this step and the preceding steps: 1. Create a directory named conf in the nginx directory, and create the ei.http.conf file in it. 2. Open the nginx/nginx.conf file and add the following entry before the final }. This includes all the files in the conf directory into the NGINX server: include conf/*.conf; ``` upstream ssl.wso2.ei.com { server xxx.xxx.xxx.xx1:8243; server xxx.xxx.xxx.xx2:8243; ip_hash; } server { listen 443; server_name ei.wso2.com; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; location / { proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_read_timeout 5m; proxy_send_timeout 5m; proxy_pass https://ssl.wso2.ei.com; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } ``` Create a VHost file (ei.https.conf) in the nginx/conf.d directory or in the nginx/conf directory if you are on a Mac OS and add the following configurations. This configures NGINX Plus to direct the HTTPS requests to the two ESB nodes (xxx.xxx.xxx.xx1 and xxx.xxx.xxx.xx2) via the HTTPS 443 port using the https://ei.wso2.com/ URL. NGINX Community version upstream ssl.wso2.ei.com { server xxx.xxx.xxx.xx1:8243; server xxx.xxx.xxx.xx2:8243; ip_hash; } server { listen 443; server_name ei.wso2.com; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; location / { proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_read_timeout 5m; proxy_send_timeout 5m; proxy_pass https://ssl.wso2.ei.com; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } NGINX Plus upstream ssl.wso2.ei.com { server xxx.xxx.xxx.xx1:8243; server xxx.xxx.xxx.xx2:8243; sticky learn create=$upstream_cookie_jsessionid lookup=$cookie_jsessionid zone=client_sessions:1m; } server { listen 443; server_name ei.wso2.com; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; location / { proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_read_timeout 5m; proxy_send_timeout 5m; proxy_pass https://ssl.wso2.ei.com; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } Configure NGINX to access the management console as https://ui.ei.wso2.com/carbon via the HTTPS 443 port. To do this, create a VHost file ( ui.ei.https.conf ) in the nginx/conf.d/ directory or in the nginx/conf directory if you are on a Mac OS and add the following configurations into it. Make sure that the SSL files you create in step 6 are in the /etc/nginx/ssl/ directory. If it is not, update the path given below. Execute the following command if you do not need to restart the server when you are simply making a modification to the VHost file: sudo service nginx reload Configuring the WSO2 EI server \u00b6 Depending on the deployment pattern you are setting up, you may have one or several WSO2 EI nodes installed in your deployment. See Deploying WSO2 Enterprise Integrator for instructions on connecting your server to the load balancer.","title":"Setting up a Load Balancer"},{"location":"setup/setting_up_lb/#setting-up-a-load-balancer","text":"The load balancer automatically distributes incoming traffic across multiple WSO2 product instances. It enables you to achieve greater levels of fault tolerance in your cluster and provides the required balancing of load needed to distribute traffic.","title":"Setting up a Load balancer"},{"location":"setup/setting_up_lb/#before-you-begin","text":"Note the following: These configurations are not required if your clustering setup does not have a load balancer. The load balancer ports of the deployment pattern that is shown above are HTTP 80 and HTTPS 443. If your system uses any other ports, be sure to replace 80 and 443 values with the corresponding ports when you follow the configuration steps in this section. The load balancer directs requests to the server on a round robin basis. For example, the load balancer will direct requests to node 1 ( xxx.xxx.xxx.xx1 ) of the ESB cluster as follows: HTTP requests will be directed to node 1 using the http://xxx.xxx.xxx.xx1/<service> URL via HTTP 80 port. HTTPS requests will be directed to node 1 using the https://xxx.xxx.xxx.xx1/<service> URL via HTTPS 443 port. The management console of node 1 will be accessed using the https://xxx.xxx.xxx.xx1/carbon/ URL via HTTPS 443 port. It is recommended to use NGINX Plus as your load balancer of choice.","title":"Before you begin"},{"location":"setup/setting_up_lb/#configuring-the-load-balancer","text":"Follow the steps below to configure NGINX Plus version 1.7.11 or NGINX community version 1.9.2 as the load balancer. Install NGINX Plus or the NGINX community version on your cluster network. Create a VHost file named ei.http.conf in the /etc/nginx/conf.d directory and add the following configurations. This configures NGINX Plus to direct the HTTP requests to the two ESB nodes (xxx.xxx.xxx.xx1 and xxx.xxx.xxx.xx2) via the HTTP 80 port using the http://ei.wso2.com/ URL. If you are setting up NGINX on a Mac OS, you will not have the conf.d directory. Follow the steps given below to add the VHost files mentioned in this step and the preceding steps: 1. Create a directory named conf in the nginx directory, and create the ei.http.conf file in it. 2. Open the nginx/nginx.conf file and add the following entry before the final }. This includes all the files in the conf directory into the NGINX server: include conf/*.conf; ``` upstream ssl.wso2.ei.com { server xxx.xxx.xxx.xx1:8243; server xxx.xxx.xxx.xx2:8243; ip_hash; } server { listen 443; server_name ei.wso2.com; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; location / { proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_read_timeout 5m; proxy_send_timeout 5m; proxy_pass https://ssl.wso2.ei.com; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } ``` Create a VHost file (ei.https.conf) in the nginx/conf.d directory or in the nginx/conf directory if you are on a Mac OS and add the following configurations. This configures NGINX Plus to direct the HTTPS requests to the two ESB nodes (xxx.xxx.xxx.xx1 and xxx.xxx.xxx.xx2) via the HTTPS 443 port using the https://ei.wso2.com/ URL. NGINX Community version upstream ssl.wso2.ei.com { server xxx.xxx.xxx.xx1:8243; server xxx.xxx.xxx.xx2:8243; ip_hash; } server { listen 443; server_name ei.wso2.com; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; location / { proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_read_timeout 5m; proxy_send_timeout 5m; proxy_pass https://ssl.wso2.ei.com; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } NGINX Plus upstream ssl.wso2.ei.com { server xxx.xxx.xxx.xx1:8243; server xxx.xxx.xxx.xx2:8243; sticky learn create=$upstream_cookie_jsessionid lookup=$cookie_jsessionid zone=client_sessions:1m; } server { listen 443; server_name ei.wso2.com; ssl on; ssl_certificate /etc/nginx/ssl/server.crt; ssl_certificate_key /etc/nginx/ssl/server.key; location / { proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_read_timeout 5m; proxy_send_timeout 5m; proxy_pass https://ssl.wso2.ei.com; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } Configure NGINX to access the management console as https://ui.ei.wso2.com/carbon via the HTTPS 443 port. To do this, create a VHost file ( ui.ei.https.conf ) in the nginx/conf.d/ directory or in the nginx/conf directory if you are on a Mac OS and add the following configurations into it. Make sure that the SSL files you create in step 6 are in the /etc/nginx/ssl/ directory. If it is not, update the path given below. Execute the following command if you do not need to restart the server when you are simply making a modification to the VHost file: sudo service nginx reload","title":"Configuring the load balancer"},{"location":"setup/setting_up_lb/#configuring-the-wso2-ei-server","text":"Depending on the deployment pattern you are setting up, you may have one or several WSO2 EI nodes installed in your deployment. See Deploying WSO2 Enterprise Integrator for instructions on connecting your server to the load balancer.","title":"Configuring the WSO2 EI server"},{"location":"setup/setting_up_ro_ldap/","text":"Setting up a Read-Only LDAP \u00b6","title":"Setting up a Read-Only LDAP"},{"location":"setup/setting_up_ro_ldap/#setting-up-a-read-only-ldap","text":"","title":"Setting up a Read-Only LDAP"},{"location":"setup/setting_up_rw_ad/","text":"Setting up a Read Write Active Directory \u00b6","title":"Setting up a Read-Write Active Directory"},{"location":"setup/setting_up_rw_ad/#setting-up-a-read-write-active-directory","text":"","title":"Setting up a Read Write Active Directory"},{"location":"setup/setting_up_rw_ldap/","text":"Setting up a Read Write LDAP \u00b6","title":"Setting up a Read-Write LDAP"},{"location":"setup/setting_up_rw_ldap/#setting-up-a-read-write-ldap","text":"","title":"Setting up a Read Write LDAP"},{"location":"setup/troubleshooting_deployment/","text":"Troubleshooting in Production Environments \u00b6 The following sections provide information on how to troubleshoot various problems that may arise for deployment in production environments. Analyzing a stack trace \u00b6 When your Java process starts to spin your CPU, you must immediately analyze the issue using the following two commands and obtain the invaluable information required to tackle the issue. This is done based on the process ID (pid). jstack <pid> > thread-dump.txt ps -C java -L -o pcpu,cpu,nice,state,cputime,pid,tid > thread-usage.txt OS X users can alternatively use the command ps M <PID> instead. These commands provide you with the thread-dump.txt file and the thread-usage.txt file. After obtaining these two files, do the following. Find the thread ID (the one that belongs to the corresponding PID) that takes up the highest CPU usage by examining the thread-usage.txt file. %CPU CPU NI S TIME PID TID .......... 0.0 - 0 S 00:00:00 1519 1602 0.0 - 0 S 00:00:00 1519 1603 24.8 - 0 R 00:06:19 1519 1604 2.4 - 0 S 00:00:37 1519 1605 0.0 - 0 S 00:00:00 1519 1606 .......... In this example, the thread ID that takes up the highest CPU usage is 1604. Convert the decimal value (in this case 1604) to hexadecimal. You can use an online converter to do this. The hexadecimal value for 1604 is 644. Search the thread-dump.txt file for the hexadecimal obtained in order to identify the thread that spins. In this case, the hexadecimal value to search for is 644. The thread-dump.txt file should have that value as a thread ID of one thread. That thread usually has a stack trace, and that's the lead you need to find the issue. In this example, the stack trace of the thread that spins is as follows. \"HTTPS-Sender I/O dispatcher-1\" prio=10 tid=0x00007fb54c010000 nid=0x644 runnable [0x00007fb534e20000] java.lang.Thread.State: RUNNABLE at org.apache.http.impl.nio.reactor.IOSessionImpl.getEventMask(IOSessionImpl.java:139) - locked <0x00000006cd91fef8> (a org.apache.http.impl.nio.reactor.IOSessionImpl) at org.apache.http.nio.reactor.ssl.SSLIOSession.updateEventMask(SSLIOSession.java:300) at org.apache.http.nio.reactor.ssl.SSLIOSession.inboundTransport(SSLIOSession.java:402) - locked <0x00000006cd471df8> (a org.apache.http.nio.reactor.ssl.SSLIOSession) at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:121) at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:160) at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:342) at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:320) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:106) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:604) at java.lang.Thread.run(Thread.java:722) Capturing the state of the system \u00b6 Carbondump is a tool used to collect all the necessary data from a running WSO2 product instance at the time of an error. The carbondump generates a ZIP archive with the collected data that helps to analyze the system and determine the problem that caused the error. Therefore, it is recommended that you run this tool as soon as an error occurs in the WSO2 product instance. When using the tool, you have to provide the process ID (pid) of the product instance and the <PRODUCT_HOME> location, which is where your unzipped Carbon distribution files reside. The command takes the following format: sh carbondump.sh [-carbonHome path] [-pid of the carbon instance] For example, In Linux: sh carbondump.sh -carbonHome /home/user/wso2carbon-3.0.0/ -pid 5151 In Windows: carbondump.bat -carbonHome c:\\wso2carbon-3.0.0\\ -pid 5151 The tool captures the following information about the system: Operating system information ** OS (kernel) version Installed modules lists and their information List of running tasks in the system Memory information of the Java process ** Java heap memory dump Histogram of the heap Objects waiting for finalization Java heap summary. GC algo used, etc. Statistics on permgen space of Java heap Information about the running Carbon instance ** Product name and version Carbon framework version (This includes the patched version) \\<PRODUCT_HOME>, \\<JAVA_HOME> configuration files log files H2 database files Thread dump Checksum values of all the files found in the $CARBON_HOME Viewing process threads in Solaris \u00b6 This information is useful to know in situations when the database processes are not fully utilizing the CPU's threading capabilities. It gives you a better understanding on how 11g and 10g takes advantage of threading and how you can validate those queries from the system. The following information provides insight on whether a Solaris process is parallelized and is taking advantage of the threading within the CPU. Open a command line in Solaris. Run prstat and have a look to the last column, labeled PROCESS/NLWP . NLWP is a reference to the number of lightweight processes and are the number of threads the process is currently using with Solaris as there is a one-to-one mapping between lightweight processes and user threads. A single thread process will show 1 there while a multi-threaded one will show a larger number. See the following code block for an example. PID USERNAME SIZE RSS STATE PRI NICE TIME CPU PROCESS/NLWP ... 12905 root 4472K 3640K cpu0 59 0 0:00:01 0.4% prstat/1 18403 monitor 474M 245M run 59 17 1:01:28 9.1% java/103 4102 oracle 12G 12G run 59 0 0:00:12 4.5% oracle/1 If you observe the PROCESS/NLWP value in the example above, you can identify that prstat and oracle are single thread processes, while java is a multi-threaded process. Alternatively, you can analyze individual thread activity of a multi-threaded process by using the -L and -p options, like prstat -L -p pid . This displays a line for each thread sorted by CPU activity. In that case, the last column is labeled PROCESS/LWPID , where LWPID is the thread ID. If more than one thread shows significant activity, your process is actively taking advantage of multi-threading. Checking the health of a cluster \u00b6 In Hazelcast, the health of a member in the cluster is determined by the heartbeats the member sends. If the well-known member does not receive a heartbeat within a given amount of time (this can be configured), then the node is assumed dead. By default, the given amount of time is 600 seconds (or 10mins), which might be too much for some scenarios. Failure detectors used in distributed systems can be unreliable. In these sort of scenarios, Hazelcast uses heartbeat monitoring as a fault detection mechanism and the nodes send heartbeats to other nodes. If a heartbeat message is not received by a given amount of time, Hazelcast assumes the node is dead. This is configured via the hazelcast.max.no.heartbeat.seconds property. The optimum value for this property depends on the system. Although the default is 600 seconds, it might be necessary to reduce the heartbeat to a lower value if nodes are to be declared dead in a shorter time frame. However, you must verify this in your system and adjust as necessary depending on your scenario. Warning : Reducing the value of this property to a lower value can result in nodes being considered as dead even if they are not. This results in multiple messages indicating that a node is leaving and rejoining the cluster. To configure the maximum time between heartbeats: Open the esb.toml file and add the following configuration: [config_heading] hazelcast.max.no.heartbeat.seconds= 300 Restart the servers.","title":"Troubleshooting Production Deployment"},{"location":"setup/troubleshooting_deployment/#troubleshooting-in-production-environments","text":"The following sections provide information on how to troubleshoot various problems that may arise for deployment in production environments.","title":"Troubleshooting in Production Environments"},{"location":"setup/troubleshooting_deployment/#analyzing-a-stack-trace","text":"When your Java process starts to spin your CPU, you must immediately analyze the issue using the following two commands and obtain the invaluable information required to tackle the issue. This is done based on the process ID (pid). jstack <pid> > thread-dump.txt ps -C java -L -o pcpu,cpu,nice,state,cputime,pid,tid > thread-usage.txt OS X users can alternatively use the command ps M <PID> instead. These commands provide you with the thread-dump.txt file and the thread-usage.txt file. After obtaining these two files, do the following. Find the thread ID (the one that belongs to the corresponding PID) that takes up the highest CPU usage by examining the thread-usage.txt file. %CPU CPU NI S TIME PID TID .......... 0.0 - 0 S 00:00:00 1519 1602 0.0 - 0 S 00:00:00 1519 1603 24.8 - 0 R 00:06:19 1519 1604 2.4 - 0 S 00:00:37 1519 1605 0.0 - 0 S 00:00:00 1519 1606 .......... In this example, the thread ID that takes up the highest CPU usage is 1604. Convert the decimal value (in this case 1604) to hexadecimal. You can use an online converter to do this. The hexadecimal value for 1604 is 644. Search the thread-dump.txt file for the hexadecimal obtained in order to identify the thread that spins. In this case, the hexadecimal value to search for is 644. The thread-dump.txt file should have that value as a thread ID of one thread. That thread usually has a stack trace, and that's the lead you need to find the issue. In this example, the stack trace of the thread that spins is as follows. \"HTTPS-Sender I/O dispatcher-1\" prio=10 tid=0x00007fb54c010000 nid=0x644 runnable [0x00007fb534e20000] java.lang.Thread.State: RUNNABLE at org.apache.http.impl.nio.reactor.IOSessionImpl.getEventMask(IOSessionImpl.java:139) - locked <0x00000006cd91fef8> (a org.apache.http.impl.nio.reactor.IOSessionImpl) at org.apache.http.nio.reactor.ssl.SSLIOSession.updateEventMask(SSLIOSession.java:300) at org.apache.http.nio.reactor.ssl.SSLIOSession.inboundTransport(SSLIOSession.java:402) - locked <0x00000006cd471df8> (a org.apache.http.nio.reactor.ssl.SSLIOSession) at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:121) at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:160) at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:342) at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:320) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:106) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:604) at java.lang.Thread.run(Thread.java:722)","title":"Analyzing a stack trace"},{"location":"setup/troubleshooting_deployment/#capturing-the-state-of-the-system","text":"Carbondump is a tool used to collect all the necessary data from a running WSO2 product instance at the time of an error. The carbondump generates a ZIP archive with the collected data that helps to analyze the system and determine the problem that caused the error. Therefore, it is recommended that you run this tool as soon as an error occurs in the WSO2 product instance. When using the tool, you have to provide the process ID (pid) of the product instance and the <PRODUCT_HOME> location, which is where your unzipped Carbon distribution files reside. The command takes the following format: sh carbondump.sh [-carbonHome path] [-pid of the carbon instance] For example, In Linux: sh carbondump.sh -carbonHome /home/user/wso2carbon-3.0.0/ -pid 5151 In Windows: carbondump.bat -carbonHome c:\\wso2carbon-3.0.0\\ -pid 5151 The tool captures the following information about the system: Operating system information ** OS (kernel) version Installed modules lists and their information List of running tasks in the system Memory information of the Java process ** Java heap memory dump Histogram of the heap Objects waiting for finalization Java heap summary. GC algo used, etc. Statistics on permgen space of Java heap Information about the running Carbon instance ** Product name and version Carbon framework version (This includes the patched version) \\<PRODUCT_HOME>, \\<JAVA_HOME> configuration files log files H2 database files Thread dump Checksum values of all the files found in the $CARBON_HOME","title":"Capturing the state of the system"},{"location":"setup/troubleshooting_deployment/#viewing-process-threads-in-solaris","text":"This information is useful to know in situations when the database processes are not fully utilizing the CPU's threading capabilities. It gives you a better understanding on how 11g and 10g takes advantage of threading and how you can validate those queries from the system. The following information provides insight on whether a Solaris process is parallelized and is taking advantage of the threading within the CPU. Open a command line in Solaris. Run prstat and have a look to the last column, labeled PROCESS/NLWP . NLWP is a reference to the number of lightweight processes and are the number of threads the process is currently using with Solaris as there is a one-to-one mapping between lightweight processes and user threads. A single thread process will show 1 there while a multi-threaded one will show a larger number. See the following code block for an example. PID USERNAME SIZE RSS STATE PRI NICE TIME CPU PROCESS/NLWP ... 12905 root 4472K 3640K cpu0 59 0 0:00:01 0.4% prstat/1 18403 monitor 474M 245M run 59 17 1:01:28 9.1% java/103 4102 oracle 12G 12G run 59 0 0:00:12 4.5% oracle/1 If you observe the PROCESS/NLWP value in the example above, you can identify that prstat and oracle are single thread processes, while java is a multi-threaded process. Alternatively, you can analyze individual thread activity of a multi-threaded process by using the -L and -p options, like prstat -L -p pid . This displays a line for each thread sorted by CPU activity. In that case, the last column is labeled PROCESS/LWPID , where LWPID is the thread ID. If more than one thread shows significant activity, your process is actively taking advantage of multi-threading.","title":"Viewing process threads in Solaris"},{"location":"setup/troubleshooting_deployment/#checking-the-health-of-a-cluster","text":"In Hazelcast, the health of a member in the cluster is determined by the heartbeats the member sends. If the well-known member does not receive a heartbeat within a given amount of time (this can be configured), then the node is assumed dead. By default, the given amount of time is 600 seconds (or 10mins), which might be too much for some scenarios. Failure detectors used in distributed systems can be unreliable. In these sort of scenarios, Hazelcast uses heartbeat monitoring as a fault detection mechanism and the nodes send heartbeats to other nodes. If a heartbeat message is not received by a given amount of time, Hazelcast assumes the node is dead. This is configured via the hazelcast.max.no.heartbeat.seconds property. The optimum value for this property depends on the system. Although the default is 600 seconds, it might be necessary to reduce the heartbeat to a lower value if nodes are to be declared dead in a shorter time frame. However, you must verify this in your system and adjust as necessary depending on your scenario. Warning : Reducing the value of this property to a lower value can result in nodes being considered as dead even if they are not. This results in multiple messages indicating that a node is leaving and rejoining the cluster. To configure the maximum time between heartbeats: Open the esb.toml file and add the following configuration: [config_heading] hazelcast.max.no.heartbeat.seconds= 300 Restart the servers.","title":"Checking the health of a cluster"},{"location":"setup/tuning_db_performance/","text":"Tuning Database Performance \u00b6","title":"Tuning Database Performance"},{"location":"setup/tuning_db_performance/#tuning-database-performance","text":"","title":"Tuning Database Performance"},{"location":"setup/tuning_jvm_performance/","text":"Java Virtual Machine Performance Tuning \u00b6 You can tune the Java Virtual Machine (JVM) settings to make a production system more efficient. You can configure the JVM parameters in the <EI_HOME>/bin/integrator.bat file (on Windows) or the <EI_HOME>/bin/integrator.sh file (on Linux/Solaris). Following are the most important JVM parameters you need to configure: Maximum Heap Memory Allocation (Xmx) - This parameter sets the maximum heap memory allocated for the JVM. Increasing the memory allocation increases the memory available for the server, which results in increasing the maximum TPS and reducing the latency. We recommend al least 2 GB of heap memory allocation for instances. For example, If you want to set 2 GB as the maximum heap memory size, you need to configure the parameter as follows: -Xms2048m -Xmx2048m Here, Xmx is the maximum memory allocation pool for a JVM. Xms is the initial memory allocation pool. Metaspace size - In JDK 1.8.*, class metadata is stored in the native heap, and this space is called Metaspace. By default class metadata allocation is only limited by the amount of available native memory. If you need to limit the amount of native memory used for class metadata, you need to set MaxMetaspaceSize. To ensure stability of a production system, you can set the MaxMetaspaceSize parameter to 1GB as follows: -XX:MaxMetaspaceSize=1g When an XML element has a large number of sub elements and the system tries to process all the sub elements, the system can become unstable due to a memory overhead. This is a security risk. To avoid this issue, you can define a maximum level of entity substitutions that the XML parser allows in the system. You do this using the entity expansion limit as follows in the <EI_HOME>/bin/integrator.bat file (for Windows) or the <EI_HOME>/bin/integrator.sh file (for Linux/Solaris). The default entity expansion limit is 64000. -DentityExpansionLimit=10000 In a clustered environment, the entity expansion limit has no dependency on the number of worker nodes.","title":"Tuning JVM Performance"},{"location":"setup/tuning_jvm_performance/#java-virtual-machine-performance-tuning","text":"You can tune the Java Virtual Machine (JVM) settings to make a production system more efficient. You can configure the JVM parameters in the <EI_HOME>/bin/integrator.bat file (on Windows) or the <EI_HOME>/bin/integrator.sh file (on Linux/Solaris). Following are the most important JVM parameters you need to configure: Maximum Heap Memory Allocation (Xmx) - This parameter sets the maximum heap memory allocated for the JVM. Increasing the memory allocation increases the memory available for the server, which results in increasing the maximum TPS and reducing the latency. We recommend al least 2 GB of heap memory allocation for instances. For example, If you want to set 2 GB as the maximum heap memory size, you need to configure the parameter as follows: -Xms2048m -Xmx2048m Here, Xmx is the maximum memory allocation pool for a JVM. Xms is the initial memory allocation pool. Metaspace size - In JDK 1.8.*, class metadata is stored in the native heap, and this space is called Metaspace. By default class metadata allocation is only limited by the amount of available native memory. If you need to limit the amount of native memory used for class metadata, you need to set MaxMetaspaceSize. To ensure stability of a production system, you can set the MaxMetaspaceSize parameter to 1GB as follows: -XX:MaxMetaspaceSize=1g When an XML element has a large number of sub elements and the system tries to process all the sub elements, the system can become unstable due to a memory overhead. This is a security risk. To avoid this issue, you can define a maximum level of entity substitutions that the XML parser allows in the system. You do this using the entity expansion limit as follows in the <EI_HOME>/bin/integrator.bat file (for Windows) or the <EI_HOME>/bin/integrator.sh file (for Linux/Solaris). The default entity expansion limit is 64000. -DentityExpansionLimit=10000 In a clustered environment, the entity expansion limit has no dependency on the number of worker nodes.","title":"Java Virtual Machine Performance Tuning"},{"location":"setup/upgrading/","text":"Upgrading from the previous WSO2 Enterprise Integrator \u00b6 |","title":"Upgrading from a Previous Release"},{"location":"setup/upgrading/#upgrading-from-the-previous-wso2-enterprise-integrator","text":"|","title":"Upgrading from the previous WSO2 Enterprise Integrator"},{"location":"setup/db/setting-up-Embedded-Derby/","text":"Setting up Embedded Derby \u00b6 The following section describes how to set up an IBM DB2 database to replace the default H2 database in your WSO2 product: Setting up the database \u00b6 Follow the steps below to set up an embedded Derby database: Download Apache Derby . Install Apache Derby on your computer. For instructions on installing Apache Derby, see the Apache Derby documentation . Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"mysql\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up Embedded Derby"},{"location":"setup/db/setting-up-Embedded-Derby/#setting-up-embedded-derby","text":"The following section describes how to set up an IBM DB2 database to replace the default H2 database in your WSO2 product:","title":"Setting up Embedded Derby"},{"location":"setup/db/setting-up-Embedded-Derby/#setting-up-the-database","text":"Follow the steps below to set up an embedded Derby database: Download Apache Derby . Install Apache Derby on your computer. For instructions on installing Apache Derby, see the Apache Derby documentation .","title":"Setting up the database"},{"location":"setup/db/setting-up-Embedded-Derby/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"mysql\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-Embedded-H2/","text":"Setting up Embedded H2 \u00b6 The following sections describe how to set up an embedded H2 database to replace the default H2 database in your WSO2 product: H2 is not recommended in production The embedded H2 database is NOT recommended in enterprise testing and production environments. It has lower performance, clustering limitations, and can cause file corruption failures. Please use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MS SQL instead. You can use the embedded H2 database in development environments and as the local registry in a registry mount. Setting up the database \u00b6 Download and install the H2 database engine on your computer. For instructions on installing DB2 Express-C, see H2 installation guide. Setting up the drivers \u00b6 WSO2 currently ships H2 database engine version h2-1.2.140.* and its related H2 database driver. If you want to use a different H2 database driver, take the following steps: Delete the following H2 database-related JAR file, which is shipped with WSO2 products: <PRODUCT_HOME>/repository/components/plugins/h2-database-engine_1.2.140.wso2v3.jar Find the JAR file of the new H2 database driver ( <H2_HOME>/bin/h2-*.jar , where <H2_HOME> is the H2 installation directory) and copy it to your WSO2 product's <PRODUCT_HOME>/repository/components/lib/ directory. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"mysql\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up Embedded H2"},{"location":"setup/db/setting-up-Embedded-H2/#setting-up-embedded-h2","text":"The following sections describe how to set up an embedded H2 database to replace the default H2 database in your WSO2 product: H2 is not recommended in production The embedded H2 database is NOT recommended in enterprise testing and production environments. It has lower performance, clustering limitations, and can cause file corruption failures. Please use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MS SQL instead. You can use the embedded H2 database in development environments and as the local registry in a registry mount.","title":"Setting up Embedded H2"},{"location":"setup/db/setting-up-Embedded-H2/#setting-up-the-database","text":"Download and install the H2 database engine on your computer. For instructions on installing DB2 Express-C, see H2 installation guide.","title":"Setting up the database"},{"location":"setup/db/setting-up-Embedded-H2/#setting-up-the-drivers","text":"WSO2 currently ships H2 database engine version h2-1.2.140.* and its related H2 database driver. If you want to use a different H2 database driver, take the following steps: Delete the following H2 database-related JAR file, which is shipped with WSO2 products: <PRODUCT_HOME>/repository/components/plugins/h2-database-engine_1.2.140.wso2v3.jar Find the JAR file of the new H2 database driver ( <H2_HOME>/bin/h2-*.jar , where <H2_HOME> is the H2 installation directory) and copy it to your WSO2 product's <PRODUCT_HOME>/repository/components/lib/ directory.","title":"Setting up the drivers"},{"location":"setup/db/setting-up-Embedded-H2/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"mysql\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-IBM-DB2/","text":"Setting up IBM DB2 \u00b6 The following sections describe how to set up an IBM DB2 database to replace the default H2 database in your WSO2 product: Prerequisites \u00b6 Download the latest version of DB2 Express-C and install it on your computer. For instructions on installing DB2 Express-C, see this ebook . Setting up the database and users \u00b6 Create the database using either DB2 command processor or DB2 control center as described below. Using the DB2 command processor \u00b6 Run DB2 console and execute the db2start command on a CLI to open DB2. Create the database using the following command: create database <DB_NAME> Before issuing an SQL statement, establish the connection to the database using the following command: connect to <DB_NAME> user <USER_ID> using <PASSWORD> Grant required permissions for users as follows: connect to DB_NAME grant <AUTHORITY> on database to user <USER_ID> For example: {width=\"550\"} For more information on DB2 commands, see the DB2 Express-C Guide . Using the DB2 control center \u00b6 Open the DB2 control center using the db2cc command as follows: Right-click All Databases in the control center tree (inside the object browser), click Create Database , and then click Standard and follow the steps in the Create New Database wizard. {width=\"580\"} Click User and Group Objects in the control center tree to create users for the newly created database. {width=\"780\"} Give the required permissions to the newly created users. {width=\"420\"} Setting up DB2 JDBC drivers \u00b6 Copy the DB2 JDBC drivers ( db2jcc.jar and db2jcc_license_c0u.jar ) from <DB2_HOME>/SQLLIB/java/ directory to the <PRODUCT_HOME>/repository/components/lib/ directory. <DB2_HOME> refers to the installation directory of DB2 Express-C, and \\< PRODUCT _HOME> refers to the directory where you run the WSO2 product instance. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"ibm_db\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:db2://SERVER_NAME:PORT/DB_NAME\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"regadmin\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up IBM DB2"},{"location":"setup/db/setting-up-IBM-DB2/#setting-up-ibm-db2","text":"The following sections describe how to set up an IBM DB2 database to replace the default H2 database in your WSO2 product:","title":"Setting up IBM DB2"},{"location":"setup/db/setting-up-IBM-DB2/#prerequisites","text":"Download the latest version of DB2 Express-C and install it on your computer. For instructions on installing DB2 Express-C, see this ebook .","title":"Prerequisites"},{"location":"setup/db/setting-up-IBM-DB2/#setting-up-the-database-and-users","text":"Create the database using either DB2 command processor or DB2 control center as described below.","title":"Setting up the database and users"},{"location":"setup/db/setting-up-IBM-DB2/#using-the-db2-command-processor","text":"Run DB2 console and execute the db2start command on a CLI to open DB2. Create the database using the following command: create database <DB_NAME> Before issuing an SQL statement, establish the connection to the database using the following command: connect to <DB_NAME> user <USER_ID> using <PASSWORD> Grant required permissions for users as follows: connect to DB_NAME grant <AUTHORITY> on database to user <USER_ID> For example: {width=\"550\"} For more information on DB2 commands, see the DB2 Express-C Guide .","title":"Using the DB2 command processor"},{"location":"setup/db/setting-up-IBM-DB2/#using-the-db2-control-center","text":"Open the DB2 control center using the db2cc command as follows: Right-click All Databases in the control center tree (inside the object browser), click Create Database , and then click Standard and follow the steps in the Create New Database wizard. {width=\"580\"} Click User and Group Objects in the control center tree to create users for the newly created database. {width=\"780\"} Give the required permissions to the newly created users. {width=\"420\"}","title":"Using the DB2 control center"},{"location":"setup/db/setting-up-IBM-DB2/#setting-up-db2-jdbc-drivers","text":"Copy the DB2 JDBC drivers ( db2jcc.jar and db2jcc_license_c0u.jar ) from <DB2_HOME>/SQLLIB/java/ directory to the <PRODUCT_HOME>/repository/components/lib/ directory. <DB2_HOME> refers to the installation directory of DB2 Express-C, and \\< PRODUCT _HOME> refers to the directory where you run the WSO2 product instance.","title":"Setting up DB2 JDBC drivers"},{"location":"setup/db/setting-up-IBM-DB2/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"ibm_db\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:db2://SERVER_NAME:PORT/DB_NAME\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"regadmin\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-IBM-Informix/","text":"Setting up IBM Informix \u00b6 The following sections describe how to set up IBM Informix to replace the default H2 database in your WSO2 product: Prerequisites \u00b6 Download the latest version of IBM Informix and install it on your computer. Creating the database \u00b6 Create the database and users in Informix. For instructions on creating the database and users, see Informix product documentation . Do the following changes to the default database when creating the Informix database. Define the page size as 4K or higher when creating the dbspace as shown in the following command (i.e. denoted by -k 4 ) : onspaces -c -S testspace4 -k 4 -p /usr/informix/logdir/data5.dat -o 100 -s 3000000 Add the following system environment variables. export DB_LOCALE=en_US.UTF-8 export CLIENT_LOCALE=en_US.UTF-8 Create an sbspace other than the dbspace by executing the following command: onspaces -c -S testspace4 -k 4 -p /usr/informix/logdir/data5.dat -o 100 -s 3000000 Add the following entry to the <INFORMIX_HOME>/etc/onconfig file, and replace the given example sbspace name (i.e. testspace4 ) with your sbspace name: SBSPACENAME testspace4 Setting up Informix JDBC drivers \u00b6 Download the Informix JDBC drivers and copy them to your WSO2 product's <PRODUCT_HOME>/repository/components/lib/ directory. Use Informix JDBC driver version 3.70.JC8, 4.10.JC2 or higher. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"ibm_informix\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:informix-sqli://localhost:1533/AM_DB;CLIENT_LOCALE=en_US.utf8;DB_LOCALE=en_us.utf8;IFX_USE_STRENC=true;\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up IBM Informix"},{"location":"setup/db/setting-up-IBM-Informix/#setting-up-ibm-informix","text":"The following sections describe how to set up IBM Informix to replace the default H2 database in your WSO2 product:","title":"Setting up IBM Informix"},{"location":"setup/db/setting-up-IBM-Informix/#prerequisites","text":"Download the latest version of IBM Informix and install it on your computer.","title":"Prerequisites"},{"location":"setup/db/setting-up-IBM-Informix/#creating-the-database","text":"Create the database and users in Informix. For instructions on creating the database and users, see Informix product documentation . Do the following changes to the default database when creating the Informix database. Define the page size as 4K or higher when creating the dbspace as shown in the following command (i.e. denoted by -k 4 ) : onspaces -c -S testspace4 -k 4 -p /usr/informix/logdir/data5.dat -o 100 -s 3000000 Add the following system environment variables. export DB_LOCALE=en_US.UTF-8 export CLIENT_LOCALE=en_US.UTF-8 Create an sbspace other than the dbspace by executing the following command: onspaces -c -S testspace4 -k 4 -p /usr/informix/logdir/data5.dat -o 100 -s 3000000 Add the following entry to the <INFORMIX_HOME>/etc/onconfig file, and replace the given example sbspace name (i.e. testspace4 ) with your sbspace name: SBSPACENAME testspace4","title":"Creating the database"},{"location":"setup/db/setting-up-IBM-Informix/#setting-up-informix-jdbc-drivers","text":"Download the Informix JDBC drivers and copy them to your WSO2 product's <PRODUCT_HOME>/repository/components/lib/ directory. Use Informix JDBC driver version 3.70.JC8, 4.10.JC2 or higher.","title":"Setting up Informix JDBC drivers"},{"location":"setup/db/setting-up-IBM-Informix/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"ibm_informix\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:informix-sqli://localhost:1533/AM_DB;CLIENT_LOCALE=en_US.utf8;DB_LOCALE=en_us.utf8;IFX_USE_STRENC=true;\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-MSSQL/","text":"Setting up Microsoft SQL \u00b6 The following sections describe how to set up Microsoft SQL to replace the default H2 database in your WSO2 product: Enable TCP/IP \u00b6 In the start menu, click Programs and launch Microsoft SQL Server 2012. Click Configuration Tools , and then click SQL Server Configuration Manager . Enable TCP/IP and disable Named Pipes from protocols of your Microsoft SQL server. Double click TCP/IP to open the TCP/IP properties window and set Listen All to Yes on the Protocol tab. On the IP Address tab, disable TCP Dynamic Ports by leaving it blank and give a valid TCP port, so that Microsoft SQL server will listen on that port. The best practice is to use port 1433, because you can use it in order processing services. Similarly, enable TCP/IP from SQL Native Client Configuration and disable Named Pipes . Also, check whether the port is set correctly to 1433. Restart Microsoft SQL server. Create the database and user \u00b6 Open the Microsoft SQL Management Studio to create a database and user. Click New Database from the Database menu and specify all the options to create a new database. Click New Login from the Logins menu, and specify all the necessary options. Grant permissions \u00b6 Assign newly created users the required grants/permissions to log in and create tables, to insert, index, select, update and delete data in tables in the newly created database. These are the minimum set of SQL server permissions. Setting up the JDBC driver \u00b6 Download and copy the sqljdbc4 Microsoft SQL JDBC driver file to the WSO2 product's /repository/components/lib/directory. Use com.microsoft.sqlserver.jdbc.SQLServerDriver as the in your datasource configuration in /repository/conf/datasources/master-datasources.xml file as explained in the next section. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"MSSQL\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:sqlserver://<IP>:1433;databaseName=wso2greg;SendStringParametersAsUnicode=false\" // The username for connecting to the database. By default, 'root' is the MSSQL username. username = \"regadmin\" // The password for connecting to the database. By default, 'root' is the MSSQL password. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up MSSQL"},{"location":"setup/db/setting-up-MSSQL/#setting-up-microsoft-sql","text":"The following sections describe how to set up Microsoft SQL to replace the default H2 database in your WSO2 product:","title":"Setting up Microsoft SQL"},{"location":"setup/db/setting-up-MSSQL/#enable-tcpip","text":"In the start menu, click Programs and launch Microsoft SQL Server 2012. Click Configuration Tools , and then click SQL Server Configuration Manager . Enable TCP/IP and disable Named Pipes from protocols of your Microsoft SQL server. Double click TCP/IP to open the TCP/IP properties window and set Listen All to Yes on the Protocol tab. On the IP Address tab, disable TCP Dynamic Ports by leaving it blank and give a valid TCP port, so that Microsoft SQL server will listen on that port. The best practice is to use port 1433, because you can use it in order processing services. Similarly, enable TCP/IP from SQL Native Client Configuration and disable Named Pipes . Also, check whether the port is set correctly to 1433. Restart Microsoft SQL server.","title":"Enable TCP/IP"},{"location":"setup/db/setting-up-MSSQL/#create-the-database-and-user","text":"Open the Microsoft SQL Management Studio to create a database and user. Click New Database from the Database menu and specify all the options to create a new database. Click New Login from the Logins menu, and specify all the necessary options.","title":"Create the database and user"},{"location":"setup/db/setting-up-MSSQL/#grant-permissions","text":"Assign newly created users the required grants/permissions to log in and create tables, to insert, index, select, update and delete data in tables in the newly created database. These are the minimum set of SQL server permissions.","title":"Grant permissions"},{"location":"setup/db/setting-up-MSSQL/#setting-up-the-jdbc-driver","text":"Download and copy the sqljdbc4 Microsoft SQL JDBC driver file to the WSO2 product's /repository/components/lib/directory. Use com.microsoft.sqlserver.jdbc.SQLServerDriver as the in your datasource configuration in /repository/conf/datasources/master-datasources.xml file as explained in the next section.","title":"Setting up the JDBC driver"},{"location":"setup/db/setting-up-MSSQL/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"MSSQL\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:sqlserver://<IP>:1433;databaseName=wso2greg;SendStringParametersAsUnicode=false\" // The username for connecting to the database. By default, 'root' is the MSSQL username. username = \"regadmin\" // The password for connecting to the database. By default, 'root' is the MSSQL password. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-MariaDB/","text":"Setting up MariaDB \u00b6 The following sections describe how to set up MariaDB to replace the default H2 database in your WSO2 product Setting up the database and users \u00b6 Follow the steps given below to set up MariaDB. See Tested DBMSs for information on the MariaDB versions that are tested with WSO2 products. Download, install and start MariaDB on your computer. See https://downloads.mariadb.org/ . You can install MariaDB standalone or as a galera cluster for high availability. Database clustering is independent of WSO2 product clustering. For instructions on installing MariaDB on MAC OS, go to Homebrew . Log in to MariaDB as the root user (or any other user with database creation privileges). mysql -u root -p Enter the password when prompted. In most systems, there is no default root password. Press the Enter key without typing anything if you have not changed the default root password. In the MySQL command prompt, create the database using the following command: create database regdb; Give authorization to the regadmin user as follows: GRANT ALL ON regdb.* TO regadmin@localhost IDENTIFIED BY \"regadmin\"; Once you have finalized the permissions, reload all the privileges by executing the following command: FLUSH PRIVILEGES; Log out from the MySQL prompt by executing the following command: quit; Setting up the drivers \u00b6 Download the MySQL Java connector JAR file , and copy it to the \\< PRODUCT_HOME>/repository/components/lib/ directory. Note that you must use the MySQL connector that is compatible with your MariaDB version. For example, mysql-connector-java-5.1.36-bi n.jar is compatible with MariaDB version 10.0.20. See Tested DBMSs for information on the WSO2 products that have been tested for compatibility with different versions of MariaDB and MySQL connectors. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"maria_db\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" To find additional parameters for configuring the database connection, see the configuration catalog .","title":"Setting up Maria DB"},{"location":"setup/db/setting-up-MariaDB/#setting-up-mariadb","text":"The following sections describe how to set up MariaDB to replace the default H2 database in your WSO2 product","title":"Setting up MariaDB"},{"location":"setup/db/setting-up-MariaDB/#setting-up-the-database-and-users","text":"Follow the steps given below to set up MariaDB. See Tested DBMSs for information on the MariaDB versions that are tested with WSO2 products. Download, install and start MariaDB on your computer. See https://downloads.mariadb.org/ . You can install MariaDB standalone or as a galera cluster for high availability. Database clustering is independent of WSO2 product clustering. For instructions on installing MariaDB on MAC OS, go to Homebrew . Log in to MariaDB as the root user (or any other user with database creation privileges). mysql -u root -p Enter the password when prompted. In most systems, there is no default root password. Press the Enter key without typing anything if you have not changed the default root password. In the MySQL command prompt, create the database using the following command: create database regdb; Give authorization to the regadmin user as follows: GRANT ALL ON regdb.* TO regadmin@localhost IDENTIFIED BY \"regadmin\"; Once you have finalized the permissions, reload all the privileges by executing the following command: FLUSH PRIVILEGES; Log out from the MySQL prompt by executing the following command: quit;","title":"Setting up the database and users"},{"location":"setup/db/setting-up-MariaDB/#setting-up-the-drivers","text":"Download the MySQL Java connector JAR file , and copy it to the \\< PRODUCT_HOME>/repository/components/lib/ directory. Note that you must use the MySQL connector that is compatible with your MariaDB version. For example, mysql-connector-java-5.1.36-bi n.jar is compatible with MariaDB version 10.0.20. See Tested DBMSs for information on the WSO2 products that have been tested for compatibility with different versions of MariaDB and MySQL connectors.","title":"Setting up the drivers"},{"location":"setup/db/setting-up-MariaDB/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"maria_db\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" To find additional parameters for configuring the database connection, see the configuration catalog .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-MySQL/","text":"Configuration a MySQL database \u00b6 By default, WSO2 products use the embedded H2 database as the database for storing user management and registry data. Given below are the steps you need to follow in order to use a MySQL database for this purpose. Setting up MySQL \u00b6 Following the steps below to create the necessary databases. Download and install MySQL Server . Download the MySQL JDBC driver . Download and unzip the WSO2 EI binary distribution. Unzip the downloaded MySQL driver, and copy the MySQL JDBC driver JAR (mysql-connector-java-x.x.xx-bin.jar) into the /lib/ directory of both ESB nodes. Execute the following command in a terminal/command window, where the username is the username you want to use to access the databases: mysql -u username -p When prompted, specify the password to access the databases with the username you specified. Creating the database \u00b6 Create the databases using the following commands: If you are using MySQL 5.7 or a later version, you need to use the mysql5.7.sql script instead of the mysql.sql script. This script has been tested on MySQL 5.7 and MySQL 8. mysql> create database WSO2_USER_DB; mysql> use WSO2_USER_DB; mysql> source <EI_HOME>/dbscripts/mysql.sql; mysql> grant all on WSO2_USER_DB.* TO regadmin@\"carbondb.mysql-wso2.com\" identified by \"regadmin\"; mysql> create database REGISTRY_DB; mysql> use REGISTRY_DB; mysql> source <EI_HOME>/dbscripts/mysql.sql; mysql> grant all on REGISTRY_DB.* TO regadmin@\"carbondb.mysql-wso2.com\" identified by \"regadmin\"; mysql> create database REGISTRY_LOCAL1; mysql> use REGISTRY_LOCAL1; mysql> source <EI_HOME>/dbscripts/mysql.sql; mysql> grant all on REGISTRY_LOCAL1.* TO regadmin@\"carbondb.mysql-wso2.com\" identified by \"regadmin\"; mysql> create database REGISTRY_LOCAL2; mysql> use REGISTRY_LOCAL2; mysql> source <EI_HOME>/dbscripts/mysql.sql; mysql> grant all on REGISTRY_LOCAL2.* TO regadmin@\"carbondb.mysql-wso2.com\" identified by \"regadmin\"; About using MySQL in different operating systems For users of Microsoft Windows, when creating the database in MySQL, it is important to specify the character set as latin1. Failure to do this may result in an error (error code: 1709) when starting your cluster. This error occurs in certain versions of MySQL (5.6.x) and is related to the UTF-8 encoding. MySQL originally used the latin1 character set by default, which stored characters in a 2-byte sequence. However, in recent versions, MySQL defaults to UTF-8 to be friendlier to international users. Hence, you must use latin1 as the character set as indicated below in the database creation commands to avoid this problem. Note that this may result in issues with non-latin characters (like Hebrew, Japanese, etc.). The following is how your database creation command should look: mysql> create database <DATABASE_NAME> character set latin1; For users of other operating systems, the standard database creation commands will suffice. For these operating systems, the following is how your database creation command should look: mysql> create database <DATABASE_NAME>; Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"mysql\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up MySQL"},{"location":"setup/db/setting-up-MySQL/#configuration-a-mysql-database","text":"By default, WSO2 products use the embedded H2 database as the database for storing user management and registry data. Given below are the steps you need to follow in order to use a MySQL database for this purpose.","title":"Configuration a MySQL database"},{"location":"setup/db/setting-up-MySQL/#setting-up-mysql","text":"Following the steps below to create the necessary databases. Download and install MySQL Server . Download the MySQL JDBC driver . Download and unzip the WSO2 EI binary distribution. Unzip the downloaded MySQL driver, and copy the MySQL JDBC driver JAR (mysql-connector-java-x.x.xx-bin.jar) into the /lib/ directory of both ESB nodes. Execute the following command in a terminal/command window, where the username is the username you want to use to access the databases: mysql -u username -p When prompted, specify the password to access the databases with the username you specified.","title":"Setting up MySQL"},{"location":"setup/db/setting-up-MySQL/#creating-the-database","text":"Create the databases using the following commands: If you are using MySQL 5.7 or a later version, you need to use the mysql5.7.sql script instead of the mysql.sql script. This script has been tested on MySQL 5.7 and MySQL 8. mysql> create database WSO2_USER_DB; mysql> use WSO2_USER_DB; mysql> source <EI_HOME>/dbscripts/mysql.sql; mysql> grant all on WSO2_USER_DB.* TO regadmin@\"carbondb.mysql-wso2.com\" identified by \"regadmin\"; mysql> create database REGISTRY_DB; mysql> use REGISTRY_DB; mysql> source <EI_HOME>/dbscripts/mysql.sql; mysql> grant all on REGISTRY_DB.* TO regadmin@\"carbondb.mysql-wso2.com\" identified by \"regadmin\"; mysql> create database REGISTRY_LOCAL1; mysql> use REGISTRY_LOCAL1; mysql> source <EI_HOME>/dbscripts/mysql.sql; mysql> grant all on REGISTRY_LOCAL1.* TO regadmin@\"carbondb.mysql-wso2.com\" identified by \"regadmin\"; mysql> create database REGISTRY_LOCAL2; mysql> use REGISTRY_LOCAL2; mysql> source <EI_HOME>/dbscripts/mysql.sql; mysql> grant all on REGISTRY_LOCAL2.* TO regadmin@\"carbondb.mysql-wso2.com\" identified by \"regadmin\"; About using MySQL in different operating systems For users of Microsoft Windows, when creating the database in MySQL, it is important to specify the character set as latin1. Failure to do this may result in an error (error code: 1709) when starting your cluster. This error occurs in certain versions of MySQL (5.6.x) and is related to the UTF-8 encoding. MySQL originally used the latin1 character set by default, which stored characters in a 2-byte sequence. However, in recent versions, MySQL defaults to UTF-8 to be friendlier to international users. Hence, you must use latin1 as the character set as indicated below in the database creation commands to avoid this problem. Note that this may result in issues with non-latin characters (like Hebrew, Japanese, etc.). The following is how your database creation command should look: mysql> create database <DATABASE_NAME> character set latin1; For users of other operating systems, the standard database creation commands will suffice. For these operating systems, the following is how your database creation command should look: mysql> create database <DATABASE_NAME>;","title":"Creating the database"},{"location":"setup/db/setting-up-MySQL/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"mysql\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-Oracle-RAC/","text":"Setting up Oracle RAC \u00b6 The following sections describe how to set up Oracle RAC to replace the default H2 database in your WSO2 product: Oracle Real Application Clusters (RAC) is an option that facilitates clustering and high availability in Oracle database environments. In the Oracle RAC environment, some of the commands used in oracle.sql are considered inefficient. Therefore, the product has a separate SQL script ( oracle_rac.sql ) for Oracle RAC. The Oracle RAC-friendly script is located in the dbscripts folder together with other .sql scripts. To test products on Oracle RAC, rename oracle_rac.sql to oracle.sql before running -Dsetup . Setting up the database and users \u00b6 Follow the steps below to set up an Oracle RAC database. Set environment variables \\< ORACLE_HOME> , PATH , and ORACLE_SID with the corresponding values ( /oracle/app/oracle/product/11.2.0/dbhome_1 , $PATH:<ORACLE_HOME>/bin , and orcl1 ) as follows: {width=\"600\"} Connect to Oracle using SQL*Plus as SYSDBA. {width=\"700\"} Create a database user and grant privileges to the user as shown below: Create user <USER_NAME> identified by password account unlock; grant connect to <USER_NAME>; grant create session, create table, create sequence, create trigger to <USER_NAME>; alter user <USER_NAME> quota <SPACE_QUOTA_SIZE_IN_MEGABYTES> on '<TABLE_SPACE_NAME>'; commit; Exit from the SQL*Plus session by executing the quit command. Setting up the JDBC driver \u00b6 Copy the Oracle JDBC libraries (for example, the <ORACLE_HOME>/jdbc/lib/ojdbc14.jar file) to the <PRODUCT_HOME>/repository/components/lib/ directory. Remove the old database driver from the <PRODUCT_HOME>/repository/components/dropins directory when you upgrade the database driver. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"oracle_rac\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:oracle:thin:@(DESCRIPTION=(LOAD_BALANCE=on)(ADDRESS=(PROTOCOL=TCP)(HOST=racnode1) (PORT=1521))(ADDRESS=(PROTOCOL=TCP)(HOST=racnode2) (PORT=1521))CONNECT_DATA=(SERVICE_NAME=rac)))\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up Oracle RAC"},{"location":"setup/db/setting-up-Oracle-RAC/#setting-up-oracle-rac","text":"The following sections describe how to set up Oracle RAC to replace the default H2 database in your WSO2 product: Oracle Real Application Clusters (RAC) is an option that facilitates clustering and high availability in Oracle database environments. In the Oracle RAC environment, some of the commands used in oracle.sql are considered inefficient. Therefore, the product has a separate SQL script ( oracle_rac.sql ) for Oracle RAC. The Oracle RAC-friendly script is located in the dbscripts folder together with other .sql scripts. To test products on Oracle RAC, rename oracle_rac.sql to oracle.sql before running -Dsetup .","title":"Setting up Oracle RAC"},{"location":"setup/db/setting-up-Oracle-RAC/#setting-up-the-database-and-users","text":"Follow the steps below to set up an Oracle RAC database. Set environment variables \\< ORACLE_HOME> , PATH , and ORACLE_SID with the corresponding values ( /oracle/app/oracle/product/11.2.0/dbhome_1 , $PATH:<ORACLE_HOME>/bin , and orcl1 ) as follows: {width=\"600\"} Connect to Oracle using SQL*Plus as SYSDBA. {width=\"700\"} Create a database user and grant privileges to the user as shown below: Create user <USER_NAME> identified by password account unlock; grant connect to <USER_NAME>; grant create session, create table, create sequence, create trigger to <USER_NAME>; alter user <USER_NAME> quota <SPACE_QUOTA_SIZE_IN_MEGABYTES> on '<TABLE_SPACE_NAME>'; commit; Exit from the SQL*Plus session by executing the quit command.","title":"Setting up the database and users"},{"location":"setup/db/setting-up-Oracle-RAC/#setting-up-the-jdbc-driver","text":"Copy the Oracle JDBC libraries (for example, the <ORACLE_HOME>/jdbc/lib/ojdbc14.jar file) to the <PRODUCT_HOME>/repository/components/lib/ directory. Remove the old database driver from the <PRODUCT_HOME>/repository/components/dropins directory when you upgrade the database driver.","title":"Setting up the JDBC driver"},{"location":"setup/db/setting-up-Oracle-RAC/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"oracle_rac\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:oracle:thin:@(DESCRIPTION=(LOAD_BALANCE=on)(ADDRESS=(PROTOCOL=TCP)(HOST=racnode1) (PORT=1521))(ADDRESS=(PROTOCOL=TCP)(HOST=racnode2) (PORT=1521))CONNECT_DATA=(SERVICE_NAME=rac)))\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-Oracle/","text":"Setting up Oracle \u00b6 The following sections describe how to set up an Oracle database to replace the default H2 database in your WSO2 product: Setting up the database and users \u00b6 Follow the steps below to set up an Oracle database. Create a new database by using the Oracle database configuration assistant (dbca) or manually. Make the necessary changes in the Oracle tnsnames.ora and listner.ora files in order to define addresses of the databases for establishing connections to the newly created database. After configuring the .ora files, start the Oracle instance using the following command: sudo /etc/init.d/oracle-xe restart Connect to Oracle using SQL*Plus as SYSDBA as follows: ./$<ORACLE_HOME>/config/scripts/sqlplus.sh sysadm/password as SYSDBA Connect to the instance with the username and password using the following command: connect As SYSDBA, create a database user and grant privileges to the user as shown below: Create user <USER_NAME> identified by <PASSWORD> account unlock; grant connect to <USER_NAME>; grant create session, create table, create sequence, create trigger to <USER_NAME>; alter user <USER_NAME> quota <SPACE_QUOTA_SIZE_IN_MEGABYTES> on '<TABLE_SPACE_NAME>'; commit; Exit from the SQL*Plus session by executing the quit command. Setting up the JDBC driver \u00b6 Copy the Oracle JDBC libraries (for example, \\< ORACLE_HOME/jdbc/lib/ojdbc14.jar) to the \\< PRODUCT_HOME>/repository/components/lib/ directory. Remove the old database driver from the <PRODUCT_HOME>/repository/components/dropins/ directory. If you get a \" timezone region not found\" error when using the ojdbc6.jar file with WSO2 servers, set the Java property as follows: export JAVA_OPTS=\"-Duser.timezone='+05:30'\" he value of this property should be the GMT difference of the country. If it is necessary to set this property permanently, define it inside the wso2server.sh as a new JAVA_OPT property. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"oracle\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:oracle:thin:@SERVER_NAME:PORT/SID\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up Oracle"},{"location":"setup/db/setting-up-Oracle/#setting-up-oracle","text":"The following sections describe how to set up an Oracle database to replace the default H2 database in your WSO2 product:","title":"Setting up Oracle"},{"location":"setup/db/setting-up-Oracle/#setting-up-the-database-and-users","text":"Follow the steps below to set up an Oracle database. Create a new database by using the Oracle database configuration assistant (dbca) or manually. Make the necessary changes in the Oracle tnsnames.ora and listner.ora files in order to define addresses of the databases for establishing connections to the newly created database. After configuring the .ora files, start the Oracle instance using the following command: sudo /etc/init.d/oracle-xe restart Connect to Oracle using SQL*Plus as SYSDBA as follows: ./$<ORACLE_HOME>/config/scripts/sqlplus.sh sysadm/password as SYSDBA Connect to the instance with the username and password using the following command: connect As SYSDBA, create a database user and grant privileges to the user as shown below: Create user <USER_NAME> identified by <PASSWORD> account unlock; grant connect to <USER_NAME>; grant create session, create table, create sequence, create trigger to <USER_NAME>; alter user <USER_NAME> quota <SPACE_QUOTA_SIZE_IN_MEGABYTES> on '<TABLE_SPACE_NAME>'; commit; Exit from the SQL*Plus session by executing the quit command.","title":"Setting up the database and users"},{"location":"setup/db/setting-up-Oracle/#setting-up-the-jdbc-driver","text":"Copy the Oracle JDBC libraries (for example, \\< ORACLE_HOME/jdbc/lib/ojdbc14.jar) to the \\< PRODUCT_HOME>/repository/components/lib/ directory. Remove the old database driver from the <PRODUCT_HOME>/repository/components/dropins/ directory. If you get a \" timezone region not found\" error when using the ojdbc6.jar file with WSO2 servers, set the Java property as follows: export JAVA_OPTS=\"-Duser.timezone='+05:30'\" he value of this property should be the GMT difference of the country. If it is necessary to set this property permanently, define it inside the wso2server.sh as a new JAVA_OPT property.","title":"Setting up the JDBC driver"},{"location":"setup/db/setting-up-Oracle/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"oracle\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:oracle:thin:@SERVER_NAME:PORT/SID\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-PostgreSQL/","text":"Setting up PostgreSQL \u00b6 The following sections describe how to set up PostgreSQL to replace the default H2 database in your WSO2 product: Setting up the database and login role \u00b6 Follow the steps below to set up a PostgreSQL database. Install PostgreSQL on your computer as follows: Start the PostgreSQL service using the following command: Create a database and the login role from a GUI using the PGAdminIII tool . To connect PGAdminIII to a PostgreSQL database server, locate the server from the object browser, right-click the client and click Connect . This will show you the databases, tablespaces, and login roles as follows: {width=\"800\"} To create a database, click Databases in the tree (inside the object browser), and click New Database . In the New Database dialog box, give a name to the database, e.g., gregdb and click OK . To create a login role, click Login Roles in the tree (inside the object browser), and click New Login Role . Enter the role name and a password. These values will be used in the product configurations as described in the following sections. In the sample configuration, gregadmin will be used as both the role name and the password. Optionally, enter other policies, such as the expiration time for the login and the connection limit. Click OK to finish creating the login role. Setting up the drivers \u00b6 Download the PostgreSQL JDBC4 driver . Copy the driver to your WSO2 product's \\< PRODUCT_HOME>/repository/components/lib directory. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"postgre_sql\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:postgresql://localhost:5432/gregdb\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up PostgreSQL"},{"location":"setup/db/setting-up-PostgreSQL/#setting-up-postgresql","text":"The following sections describe how to set up PostgreSQL to replace the default H2 database in your WSO2 product:","title":"Setting up PostgreSQL"},{"location":"setup/db/setting-up-PostgreSQL/#setting-up-the-database-and-login-role","text":"Follow the steps below to set up a PostgreSQL database. Install PostgreSQL on your computer as follows: Start the PostgreSQL service using the following command: Create a database and the login role from a GUI using the PGAdminIII tool . To connect PGAdminIII to a PostgreSQL database server, locate the server from the object browser, right-click the client and click Connect . This will show you the databases, tablespaces, and login roles as follows: {width=\"800\"} To create a database, click Databases in the tree (inside the object browser), and click New Database . In the New Database dialog box, give a name to the database, e.g., gregdb and click OK . To create a login role, click Login Roles in the tree (inside the object browser), and click New Login Role . Enter the role name and a password. These values will be used in the product configurations as described in the following sections. In the sample configuration, gregadmin will be used as both the role name and the password. Optionally, enter other policies, such as the expiration time for the login and the connection limit. Click OK to finish creating the login role.","title":"Setting up the database and login role"},{"location":"setup/db/setting-up-PostgreSQL/#setting-up-the-drivers","text":"Download the PostgreSQL JDBC4 driver . Copy the driver to your WSO2 product's \\< PRODUCT_HOME>/repository/components/lib directory.","title":"Setting up the drivers"},{"location":"setup/db/setting-up-PostgreSQL/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"postgre_sql\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:postgresql://localhost:5432/gregdb\" // The username for connecting to the database. username = \"regadmin\" // The password for connecting to the database. password = \"regadmin\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-Remote-Derby/","text":"Setting up Remote Derby \u00b6 The following sections describe how to set up a remote Derby database to replace the default H2 database in your WSO2 product: Setting up the database \u00b6 Follow the steps below to set up a remote Derby database. Download Apache Derby . Install Apache Derby on your computer. For instructions on installing Apache Derby, see the Apache Derby documentation . Go to the <DERBY_HOME>/bin / directory and run the Derby network server start script. Usually, it is named startNetworkServer . Setting up the drivers \u00b6 Copy the derby.jar , derbyclient.jar JAR and the derbynet.jar JAR from the \\< DERBY_HOME>/lib/ directory to the \\< PRODUCT_HOME>/repository/components/extensions/ directory (the classpath of the Carbon web application). Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"remote_derby\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up Remote Derby"},{"location":"setup/db/setting-up-Remote-Derby/#setting-up-remote-derby","text":"The following sections describe how to set up a remote Derby database to replace the default H2 database in your WSO2 product:","title":"Setting up Remote Derby"},{"location":"setup/db/setting-up-Remote-Derby/#setting-up-the-database","text":"Follow the steps below to set up a remote Derby database. Download Apache Derby . Install Apache Derby on your computer. For instructions on installing Apache Derby, see the Apache Derby documentation . Go to the <DERBY_HOME>/bin / directory and run the Derby network server start script. Usually, it is named startNetworkServer .","title":"Setting up the database"},{"location":"setup/db/setting-up-Remote-Derby/#setting-up-the-drivers","text":"Copy the derby.jar , derbyclient.jar JAR and the derbynet.jar JAR from the \\< DERBY_HOME>/lib/ directory to the \\< PRODUCT_HOME>/repository/components/extensions/ directory (the classpath of the Carbon web application).","title":"Setting up the drivers"},{"location":"setup/db/setting-up-Remote-Derby/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"remote_derby\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting-up-Remote-H2/","text":"Setting up Remote H2 \u00b6 The following sections describe how to set up a remote H2 database to replace the default H2 database in your WSO2 product: When to use the embedded H2 database? The embedded H2 database is NOT recommended in enterprise testing and production environments. It has lower performance, clustering limitations, and can cause file corruption failures. Please use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MS SQL instead. However, you can use the embedded H2 database as the local registry in a registry mount even in enterprise testing and production environments . Setting up the remote H2 database \u00b6 Follow the steps below to set up a Remote H2 database. Download and install the H2 database engine on your computer as follows: For instructions on installing, see the H2 installation guide . Go to the \\< H2_HOME>/bin directory and run the H2 network server starting script as follows, where \\< H2_HOME> is the H2 installation directory: Run the H2 database server with the following commands: For Linux: $ ./h2.sh For Windows: $ h2.bat The script starts the database engine and opens a pop-up window. Click Start Browser to open a web browser containing a client application, which you use to connect to a database. If a database does not already exist by the name you provided in the JDBC URL text box, H2 will automatically create a database. Setting up the drivers \u00b6 WSO2 currently ships H2 database engine version h2-1.2.140.* and its related H2 database driver. If you want to use a different H2 database driver, take the following steps: Delete the following H2 database-related JAR file, which is shipped with WSO2 products: <PRODUCT_HOME>/repository/components/plugins/h2-database-engine_1.2.140.wso2v3.jar Find the JAR file of the new H2 database driver ( <H2_HOME>/bin/h2-*.jar , where <H2_HOME> is the H2 installation directory) and copy it to your WSO2 product's <PRODUCT_HOME>/repository/components/lib directory. Connecting the database to the server \u00b6 To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"remote_H2\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Setting up Remote H2"},{"location":"setup/db/setting-up-Remote-H2/#setting-up-remote-h2","text":"The following sections describe how to set up a remote H2 database to replace the default H2 database in your WSO2 product: When to use the embedded H2 database? The embedded H2 database is NOT recommended in enterprise testing and production environments. It has lower performance, clustering limitations, and can cause file corruption failures. Please use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MS SQL instead. However, you can use the embedded H2 database as the local registry in a registry mount even in enterprise testing and production environments .","title":"Setting up Remote H2"},{"location":"setup/db/setting-up-Remote-H2/#setting-up-the-remote-h2-database","text":"Follow the steps below to set up a Remote H2 database. Download and install the H2 database engine on your computer as follows: For instructions on installing, see the H2 installation guide . Go to the \\< H2_HOME>/bin directory and run the H2 network server starting script as follows, where \\< H2_HOME> is the H2 installation directory: Run the H2 database server with the following commands: For Linux: $ ./h2.sh For Windows: $ h2.bat The script starts the database engine and opens a pop-up window. Click Start Browser to open a web browser containing a client application, which you use to connect to a database. If a database does not already exist by the name you provided in the JDBC URL text box, H2 will automatically create a database.","title":"Setting up the remote H2 database"},{"location":"setup/db/setting-up-Remote-H2/#setting-up-the-drivers","text":"WSO2 currently ships H2 database engine version h2-1.2.140.* and its related H2 database driver. If you want to use a different H2 database driver, take the following steps: Delete the following H2 database-related JAR file, which is shipped with WSO2 products: <PRODUCT_HOME>/repository/components/plugins/h2-database-engine_1.2.140.wso2v3.jar Find the JAR file of the new H2 database driver ( <H2_HOME>/bin/h2-*.jar , where <H2_HOME> is the H2 installation directory) and copy it to your WSO2 product's <PRODUCT_HOME>/repository/components/lib directory.","title":"Setting up the drivers"},{"location":"setup/db/setting-up-Remote-H2/#connecting-the-database-to-the-server","text":"To enable the two nodes to access the shared database, update the following parameters in the esb.toml file. // The config section that groups the parameters for the primary database that will be shared by both product nodes in the cluster. [database.shared.db] // Specify the type of database. type = \"remote_H2\" // Specify the connection URL of your database. The following default URL connects to the H2 database that is shipped with the product. url=\"jdbc:h2:./repository/database/WSO2SHARED_DB;DB_CLOSE_ON_EXIT=FALSE;LOCK_TIMEOUT=60000\" // The username for connecting to the database. By default, 'root' is the MySQL username. username = \"root\" // The password for connecting to the database. By default, 'root' is the MySQL password. password = \"root\" Find more parameters for connecting to the primary database and for tuning the primary database .","title":"Connecting the database to the server"},{"location":"setup/db/setting_up_db/","text":"See the topics given below. Setting up a MySQL database Setting up a MSSQL database Setting up Orable database","title":"Setting up db"},{"location":"setup/security/configuring_SAML2_SSO_across_prods/","text":"Configuring SAML2 Single-Sign-On Across Different WSO2 Products \u00b6 With the SAML2 relying party capabilities of Carbon, it is possible to set up Single Sign-On between different WSO2 Product instances where WSO2 Identity Server acts as the identity provider while other WSO2 products act as the relying party. This topic provides instructions on how to set up Single Sign-On between different WSO2 products. Step 1 - Installing the SAML2 relying party (SAML2 SSO Authenticator) feature in a Carbon Server \u00b6 Note You only need to do this step if \"SAML2 based Single Sign-On authenticator\" is not installed in your WSO2 product. SAML2 relying party components are not shipped with the default Carbon distribution. However, these bundles are packages that can be installed as a feature, which is available in the online-hosted P2 repository. Therefore, it is possible to install this feature with minimal effort through the Carbon Feature Manager. Add the WSO2 online P2 repository as a new repository. Usually, the hosted P2 repository is available at this URL: http://dist.wso2.org/p2/carbon/releases/ (Carbon-Release-Version ). Learn how to add a repository to the Identity Server here . Search for the word \"authenticator\". Select \"SAML2 based Single Sign-On authenticator\" from the result and click \"Install.\" See Installing Features . Step 2 - Configuring the Carbon Server to use the SAML2-based authentication instead of default username/password-based authentication \u00b6 After installing the SAML2 relying party components (SAML2 SSO authenticator), it is necessary to configure the SAML2 SSO authentication component to communicate with the Identity Server for user authentication. This can be configured in the <PRODUCT_HOME>/repository/conf/security/authenticators.xml file. This file will contain configurations for different authenticators. By default, it is shipped with a sample configuration for SAML2 SSO authenticator and requires minor modifications to prior to setup. <Authenticator name=\"SAML2SSOAuthenticator\" disabled=\"false\"> <Priority>10</Priority> <Config> <Parameter name=\"LoginPage\">/carbon/admin/login.jsp</Parameter> <Parameter name=\"ServiceProviderID\">carbonServer</Parameter> <Parameter name=\"IdentityProviderSSOServiceURL\">https://localhost:9443/samlsso</Parameter> <Parameter name=\"NameIDPolicyFormat\">urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified</Parameter> <Parameter name=\"IdPCertAlias\">wso2carbon</Parameter> </Config> </Authenticator> Authenticator disabled - This should be set to false . Priority - This is the priority level of the authenticator. In the Carbon Runtime, the authenticator with the highest priority will be picked up. This value should be greater than 5 in order to supersede the default username/password-based authenticator. Parameter LoginPage - This is the default login page URL of Carbon. All requests coming to this page will be intercepted for authentication. It is not necessary to change this value from the value given in the sample configuration. Parameter ServiceProviderID - This is the unique identifier for the Carbon Server in an SSO setup. This value should be used as the value of the issuer in the Identity Server configuration. Parameter IdentityProviderSSOServiceURL - This is the Identity Server URL to which the users will be redirected for authentication. It should have this format: https://(host-name):(port)/samlsso . Parameter NameIDPolicyFormat - This specifies the name identifier format that the Carbon server wants to receive in the subject of an assertion from a particular identity provider. Parameter IdPCertAlias - This is uncommented by default. This is the alias of the identity provider certificate. This is specifically used whenever a Carbon server uses IS as the identity provider. The configuration needs to be done at the relying party server's <PRODUCT_HOME>/repository/conf/security/authenticators.xml file. Step 3 - Configuring the Identity Server as the Single Sign-On provider \u00b6 Finally, you need to configure the Identity Server to act as the Single Sign-on provider. Each relying party should be registered as a service provider at the Identity Server-end. The following is a sample configuration for registering a Carbon server as a service provider. Sign in. Enter your username and password to log on to the Management Console . Navigate to the Main menu to access the Identity menu. Click Add under Service Providers . Fill in the Service Provider Name and provide a brief Description of the service provider. Only Service Provider Name is a required field. Click Register to add the new service provider. Expand the Inbound Authentication Configuration section, followed by the SAML2 Web SSO Configuration section and click Configure . Fill in the form that appears. Specify the Issuer . This should be equal to the ServiceProviderID value mentioned in the authenticators.xml of the relying party Carbon server. Specify the Assertion Consumer URL . This is the URL to which the browser should be redirected after the authentication is successful. It should have this format: https://(host-name):(port)/acs . Select Use fully qualified username in SAML Response if that feature is required. Select Enable Response Signing to sign the SAML2 Responses returned after the authentication. Select Enable Assertion Signing to sign the SAML2 Assertions returned after the authentication. SAML2 relying party components expect these assertions to be signed by the Identity Server. Select Enable Signature Validation in Authentication Requests and Logout Requests if you need this feature configured. Select Enable Single Logout so that all sessions are terminated once the user signs out from one server. You can enter a Custom Logout URL if required. Select Enable Attribute Profile to enable this and add a claim by entering the claim link and clicking the Add Claim button. Select Enable Audience Restriction to restrict the audience. You may add audience members using the Audience text box and clicking the Add Audience button. Expand the Local & Outbound Authentication Configuration section and do the following. Select Use tenant domain in local subject identifier to append the tenant domain to the local subject identifier. Select Use user store domain in local subject identifier to append the user store domain that the user resides in the local subject identifier. {width=\"429\"} Click the Update button to update the details of the service provider.","title":"Configuring SAML2 Single-Sign-On Across Different WSO2 Products"},{"location":"setup/security/configuring_SAML2_SSO_across_prods/#configuring-saml2-single-sign-on-across-different-wso2-products","text":"With the SAML2 relying party capabilities of Carbon, it is possible to set up Single Sign-On between different WSO2 Product instances where WSO2 Identity Server acts as the identity provider while other WSO2 products act as the relying party. This topic provides instructions on how to set up Single Sign-On between different WSO2 products.","title":"Configuring SAML2 Single-Sign-On Across Different WSO2 Products"},{"location":"setup/security/configuring_SAML2_SSO_across_prods/#step-1-installing-the-saml2-relying-party-saml2-sso-authenticator-feature-in-a-carbon-server","text":"Note You only need to do this step if \"SAML2 based Single Sign-On authenticator\" is not installed in your WSO2 product. SAML2 relying party components are not shipped with the default Carbon distribution. However, these bundles are packages that can be installed as a feature, which is available in the online-hosted P2 repository. Therefore, it is possible to install this feature with minimal effort through the Carbon Feature Manager. Add the WSO2 online P2 repository as a new repository. Usually, the hosted P2 repository is available at this URL: http://dist.wso2.org/p2/carbon/releases/ (Carbon-Release-Version ). Learn how to add a repository to the Identity Server here . Search for the word \"authenticator\". Select \"SAML2 based Single Sign-On authenticator\" from the result and click \"Install.\" See Installing Features .","title":"Step 1 - Installing the SAML2 relying party (SAML2 SSO Authenticator) feature in a Carbon Server"},{"location":"setup/security/configuring_SAML2_SSO_across_prods/#step-2-configuring-the-carbon-server-to-use-the-saml2-based-authentication-instead-of-default-usernamepassword-based-authentication","text":"After installing the SAML2 relying party components (SAML2 SSO authenticator), it is necessary to configure the SAML2 SSO authentication component to communicate with the Identity Server for user authentication. This can be configured in the <PRODUCT_HOME>/repository/conf/security/authenticators.xml file. This file will contain configurations for different authenticators. By default, it is shipped with a sample configuration for SAML2 SSO authenticator and requires minor modifications to prior to setup. <Authenticator name=\"SAML2SSOAuthenticator\" disabled=\"false\"> <Priority>10</Priority> <Config> <Parameter name=\"LoginPage\">/carbon/admin/login.jsp</Parameter> <Parameter name=\"ServiceProviderID\">carbonServer</Parameter> <Parameter name=\"IdentityProviderSSOServiceURL\">https://localhost:9443/samlsso</Parameter> <Parameter name=\"NameIDPolicyFormat\">urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified</Parameter> <Parameter name=\"IdPCertAlias\">wso2carbon</Parameter> </Config> </Authenticator> Authenticator disabled - This should be set to false . Priority - This is the priority level of the authenticator. In the Carbon Runtime, the authenticator with the highest priority will be picked up. This value should be greater than 5 in order to supersede the default username/password-based authenticator. Parameter LoginPage - This is the default login page URL of Carbon. All requests coming to this page will be intercepted for authentication. It is not necessary to change this value from the value given in the sample configuration. Parameter ServiceProviderID - This is the unique identifier for the Carbon Server in an SSO setup. This value should be used as the value of the issuer in the Identity Server configuration. Parameter IdentityProviderSSOServiceURL - This is the Identity Server URL to which the users will be redirected for authentication. It should have this format: https://(host-name):(port)/samlsso . Parameter NameIDPolicyFormat - This specifies the name identifier format that the Carbon server wants to receive in the subject of an assertion from a particular identity provider. Parameter IdPCertAlias - This is uncommented by default. This is the alias of the identity provider certificate. This is specifically used whenever a Carbon server uses IS as the identity provider. The configuration needs to be done at the relying party server's <PRODUCT_HOME>/repository/conf/security/authenticators.xml file.","title":"Step 2 - Configuring the Carbon Server to use the SAML2-based authentication instead of default username/password-based authentication"},{"location":"setup/security/configuring_SAML2_SSO_across_prods/#step-3-configuring-the-identity-server-as-the-single-sign-on-provider","text":"Finally, you need to configure the Identity Server to act as the Single Sign-on provider. Each relying party should be registered as a service provider at the Identity Server-end. The following is a sample configuration for registering a Carbon server as a service provider. Sign in. Enter your username and password to log on to the Management Console . Navigate to the Main menu to access the Identity menu. Click Add under Service Providers . Fill in the Service Provider Name and provide a brief Description of the service provider. Only Service Provider Name is a required field. Click Register to add the new service provider. Expand the Inbound Authentication Configuration section, followed by the SAML2 Web SSO Configuration section and click Configure . Fill in the form that appears. Specify the Issuer . This should be equal to the ServiceProviderID value mentioned in the authenticators.xml of the relying party Carbon server. Specify the Assertion Consumer URL . This is the URL to which the browser should be redirected after the authentication is successful. It should have this format: https://(host-name):(port)/acs . Select Use fully qualified username in SAML Response if that feature is required. Select Enable Response Signing to sign the SAML2 Responses returned after the authentication. Select Enable Assertion Signing to sign the SAML2 Assertions returned after the authentication. SAML2 relying party components expect these assertions to be signed by the Identity Server. Select Enable Signature Validation in Authentication Requests and Logout Requests if you need this feature configured. Select Enable Single Logout so that all sessions are terminated once the user signs out from one server. You can enter a Custom Logout URL if required. Select Enable Attribute Profile to enable this and add a claim by entering the claim link and clicking the Add Claim button. Select Enable Audience Restriction to restrict the audience. You may add audience members using the Audience text box and clicking the Add Audience button. Expand the Local & Outbound Authentication Configuration section and do the following. Select Use tenant domain in local subject identifier to append the tenant domain to the local subject identifier. Select Use user store domain in local subject identifier to append the user store domain that the user resides in the local subject identifier. {width=\"429\"} Click the Update button to update the details of the service provider.","title":"Step 3 - Configuring the Identity Server as the Single Sign-On provider"},{"location":"setup/security/configuring_keystores/","text":"Configuring Keystores for WSO2 EI \u00b6 WSO2 Carbon-based products are shipped with a default keystore named wso2carbon.jks, which is stored in the /repository/resources/security directory. This is the primary keystore of the product, and it is used, by default, for all of the following requirements. Encrypting/decrypting passwords and other confidential information, which are maintained in various configuration files as well as internal data stores. Note that it is recommended to separate the keystore for encrypting information in internal data stores . Signing messages when the WSO2 product communicates with external parties (such as SAML, OIDC id_token signing). Changing the default primary keystore \u00b6 Follow the steps given below to change the default primary keystore that is shipped with the product. Create a new keystore and copy it to the /repository/security/ directory. Note that CA-signed certificates are recommended for this keystore because it is used for communicating with external parties. Open the ei.toml file, add the following config section and update the parameters of our internal keystore. [keystore.tls] file_name=\"wso2carbon.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"wso2carbon\" key_password=\"wso2carbon\" Separating the internal keystore \u00b6 By default, the primary keystore is used for internal data encryption (encrypting data in internal data stores and configuration files) as well as for signing messages that are communicated with external parties. Why separate the internal keystore? It is sometimes a common requirement to have separate keystores for communicating messages with external parties (such as SAML, OIDC id_token signing) and for encrypting information in internal data stores. This is because, for the first scenario of signing messages, the keystore certificates need to be frequently renewed. However, for encrypting information in internal data stores, the keystore certificates should not be changed frequently because the data that is already encrypted will become unusable every time the certificate changes. Follow the steps given below to separate the keystore that is used for encrypting data in internal data stores. Create a new keystore and copy it to the /repository/security/ directory. Note that you do not require CA-signed certificates for this keystore because it will not be used for communicating with external parties. Open the ei.toml file, add the following config section and update the parameters of our internal keystore. [keystore.internal] file_name=\"wso2carbon.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"wso2carbon\" key_password=\"wso2carbon\" Optional: Changing the default truststore \u00b6 The truststore is used for storing the digital certificates that are used for SSL communication over the network. If required, you can change the default truststore (which is the truststore.jks file stored in the /repository/security/ directory) as follows: Create a new keystore and copy it to the /repository/security/ directory. Note that you do not require CA-signed certificates for this keystore because it will not be used for communicating with external parties. Open the ei.toml file, add the following config section and update the parameters of our internal keystore. [truststore] file_name=\"wso2carbon.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"symmetric.key.value\" algorithm=\"\" Import all the required certificates.","title":"Configuring Keystores"},{"location":"setup/security/configuring_keystores/#configuring-keystores-for-wso2-ei","text":"WSO2 Carbon-based products are shipped with a default keystore named wso2carbon.jks, which is stored in the /repository/resources/security directory. This is the primary keystore of the product, and it is used, by default, for all of the following requirements. Encrypting/decrypting passwords and other confidential information, which are maintained in various configuration files as well as internal data stores. Note that it is recommended to separate the keystore for encrypting information in internal data stores . Signing messages when the WSO2 product communicates with external parties (such as SAML, OIDC id_token signing).","title":"Configuring Keystores for WSO2 EI"},{"location":"setup/security/configuring_keystores/#changing-the-default-primary-keystore","text":"Follow the steps given below to change the default primary keystore that is shipped with the product. Create a new keystore and copy it to the /repository/security/ directory. Note that CA-signed certificates are recommended for this keystore because it is used for communicating with external parties. Open the ei.toml file, add the following config section and update the parameters of our internal keystore. [keystore.tls] file_name=\"wso2carbon.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"wso2carbon\" key_password=\"wso2carbon\"","title":"Changing the default primary keystore"},{"location":"setup/security/configuring_keystores/#separating-the-internal-keystore","text":"By default, the primary keystore is used for internal data encryption (encrypting data in internal data stores and configuration files) as well as for signing messages that are communicated with external parties. Why separate the internal keystore? It is sometimes a common requirement to have separate keystores for communicating messages with external parties (such as SAML, OIDC id_token signing) and for encrypting information in internal data stores. This is because, for the first scenario of signing messages, the keystore certificates need to be frequently renewed. However, for encrypting information in internal data stores, the keystore certificates should not be changed frequently because the data that is already encrypted will become unusable every time the certificate changes. Follow the steps given below to separate the keystore that is used for encrypting data in internal data stores. Create a new keystore and copy it to the /repository/security/ directory. Note that you do not require CA-signed certificates for this keystore because it will not be used for communicating with external parties. Open the ei.toml file, add the following config section and update the parameters of our internal keystore. [keystore.internal] file_name=\"wso2carbon.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"wso2carbon\" key_password=\"wso2carbon\"","title":"Separating the internal keystore"},{"location":"setup/security/configuring_keystores/#optional-changing-the-default-truststore","text":"The truststore is used for storing the digital certificates that are used for SSL communication over the network. If required, you can change the default truststore (which is the truststore.jks file stored in the /repository/security/ directory) as follows: Create a new keystore and copy it to the /repository/security/ directory. Note that you do not require CA-signed certificates for this keystore because it will not be used for communicating with external parties. Open the ei.toml file, add the following config section and update the parameters of our internal keystore. [truststore] file_name=\"wso2carbon.jks\" type=\"JKS\" password=\"wso2carbon\" alias=\"symmetric.key.value\" algorithm=\"\" Import all the required certificates.","title":"Optional: Changing the default truststore"},{"location":"setup/security/creating_keystores/","text":"Creating New keystores \u00b6 WSO2 Carbon-based products are shipped with a default keystore named wso2carbon.jks , which is stored in the /repository/resources/security directory. There are two ways to create keystores for a WSO2 product. You can either generate a keystore using an already existing public key certificate (CA-signed), or you can create the public key certificate at the time of generating the keystore. See the instructions given below. Creating a keystore using an existing certificate \u00b6 Secure Sockets Layer (SSL) is a protocol that is used to secure communication between systems. This protocol uses a public key, a private key and a random symmetric key to encrypt data. As SSL is widely used in many systems, certificates may already exist that can be reused. In such situations, you can use an already existing CA-signed certificate to generate your keystore for SSL by using OpenSSL and Java keytool. First, you must export certificates to the PKCS12/PFX format. Give strong passwords whenever required. In WSO2 products, it is a must to have the same password for both the keystore and private key. Execute the following command to export the entries of a trust chain into a keystore of .pfx format: openssl pkcs12 -export -in <certificate file>.crt -inkey <private>.key -name \"<alias>\" -certfile <additional certificate file> -out <pfx keystore name>.pfx Convert the PKCS12/PFX formatted keystore to a Java keystore using the following command: openssl pkcs12 -export -in <certificate file>.crt -inkey <private>.key -name \"<alias>\" -certfile <additional certificate file> -out <pfx keystore name>.pfx Creating a new keystore using a new certificate \u00b6 You can follow the steps in this section to create a new keystore with a private key and a new public key certificate. We will be using the keytool that is available with your JDK installation. Note that the pubic key certificate we generate for the keystore is self-signed. Therefore, if you need a public key certificate that is CA-signed, you need to generate a CA-signed certificate and import it to the keystore as explained in the next section. Alternatively, you can choose the option of generating a new keystore using a CA-signed public certificate as explained previously. Open a command prompt and go to the /repository/resources/security/ directory. All keystores should be stored here. Create the keystore that includes the private key by executing the following command: keytool -genkey -alias newcert -keyalg RSA -keysize 2048 -keystore newkeystore.jks -dname \"CN=<testdomain.org>, OU=Home,O=Home,L=SL,S=WS,C=LK\" -storepass mypassword -keypass mypassword This command will create a keystore with the following details: Keystore name: newkeystore.jks Alias of public certificate: newcert Keystore password: mypassword Private key password: mypassword (this is required to be the same as keystore password) Note that if you did not specify values for the '-keypass' and the '-storepass' in the above command, you will be asked to give a value for the '-storepass' (password of the keystore). As a best practice, use a password generator to generate a strong password. You will then be asked to enter a value for -keypass. Click Enter because we need the same password for both the keystore and the key. Also, if you did not specify values for -dname, you will be asked to provide those details individually. Open the /repository/resources/security/ directory and check if the new keystore file is created. Make a backup of it and move it to a secure location. This is important as it is the only place with your private key. You now have a keystore (.jks file) with a private key and a self-signed public key certificate.","title":"Creating New Keystores"},{"location":"setup/security/creating_keystores/#creating-new-keystores","text":"WSO2 Carbon-based products are shipped with a default keystore named wso2carbon.jks , which is stored in the /repository/resources/security directory. There are two ways to create keystores for a WSO2 product. You can either generate a keystore using an already existing public key certificate (CA-signed), or you can create the public key certificate at the time of generating the keystore. See the instructions given below.","title":"Creating New keystores"},{"location":"setup/security/creating_keystores/#creating-a-keystore-using-an-existing-certificate","text":"Secure Sockets Layer (SSL) is a protocol that is used to secure communication between systems. This protocol uses a public key, a private key and a random symmetric key to encrypt data. As SSL is widely used in many systems, certificates may already exist that can be reused. In such situations, you can use an already existing CA-signed certificate to generate your keystore for SSL by using OpenSSL and Java keytool. First, you must export certificates to the PKCS12/PFX format. Give strong passwords whenever required. In WSO2 products, it is a must to have the same password for both the keystore and private key. Execute the following command to export the entries of a trust chain into a keystore of .pfx format: openssl pkcs12 -export -in <certificate file>.crt -inkey <private>.key -name \"<alias>\" -certfile <additional certificate file> -out <pfx keystore name>.pfx Convert the PKCS12/PFX formatted keystore to a Java keystore using the following command: openssl pkcs12 -export -in <certificate file>.crt -inkey <private>.key -name \"<alias>\" -certfile <additional certificate file> -out <pfx keystore name>.pfx","title":"Creating a keystore using an existing certificate"},{"location":"setup/security/creating_keystores/#creating-a-new-keystore-using-a-new-certificate","text":"You can follow the steps in this section to create a new keystore with a private key and a new public key certificate. We will be using the keytool that is available with your JDK installation. Note that the pubic key certificate we generate for the keystore is self-signed. Therefore, if you need a public key certificate that is CA-signed, you need to generate a CA-signed certificate and import it to the keystore as explained in the next section. Alternatively, you can choose the option of generating a new keystore using a CA-signed public certificate as explained previously. Open a command prompt and go to the /repository/resources/security/ directory. All keystores should be stored here. Create the keystore that includes the private key by executing the following command: keytool -genkey -alias newcert -keyalg RSA -keysize 2048 -keystore newkeystore.jks -dname \"CN=<testdomain.org>, OU=Home,O=Home,L=SL,S=WS,C=LK\" -storepass mypassword -keypass mypassword This command will create a keystore with the following details: Keystore name: newkeystore.jks Alias of public certificate: newcert Keystore password: mypassword Private key password: mypassword (this is required to be the same as keystore password) Note that if you did not specify values for the '-keypass' and the '-storepass' in the above command, you will be asked to give a value for the '-storepass' (password of the keystore). As a best practice, use a password generator to generate a strong password. You will then be asked to enter a value for -keypass. Click Enter because we need the same password for both the keystore and the key. Also, if you did not specify values for -dname, you will be asked to provide those details individually. Open the /repository/resources/security/ directory and check if the new keystore file is created. Make a backup of it and move it to a secure location. This is important as it is the only place with your private key. You now have a keystore (.jks file) with a private key and a self-signed public key certificate.","title":"Creating a new keystore using a new certificate"},{"location":"setup/security/encrypting_plain_text/","text":"Encrypting Passwords in Configuration files \u00b6 The instructions on this page explain how plain text passwords in configuration files can be encrypted using the secure vault implementation that is built into WSO2 products. Note that you can customize the default secure vault configurations in the product by implementing a new secret repository, call back handler etc. Read more about the Secure Vault implementation in WSO2 products. Before you begin \u00b6 If you are using Windows, you need to have Ant (http://ant.apache.org/) installed before using the Cipher Tool. Encrypting passwords \u00b6 Open the ei.toml file and add the following configuration section as shown below. Give an alias for the information type that you want to encrypt followed by the actual value. [secrets] foo = \"[R!#etoqe]\" bar = \"[T@h3rerqr]\" Open a terminal, navigate to the /bin/ directory, and execute the following command (You must first enable the Cipher tool for the product by executing the -Dconfigure command with the cipher tool script as shown below). On Linux: ./ciphertool.sh -Dconfigure On Windows: ./ciphertool.bat -Dconfigure Go back to the ei.toml file and see that the alias passwords are encrypted. [secrets] foo = \"qf24n82l48gl=3o42nfmakr83b5y3\" bar = \"02jehlqnghfbat54oqhwcxeqorm2o\" Using encrypting Passwords \u00b6 Once you have encrypted password values, you can refer them from the relevant configuration sections in the ei.toml file. For example, if you have encrypted the admin password of your server, update the code [super_admin] config section in the ei.toml file as shown below. [super_admin] username=\"admin\" password=\"$ref{alias}\" Changing encrypted passwords \u00b6 To change any password which we have encrypted already, follow the below steps: Be sure to shut down the server. Open a command prompt and go to the /bin directory, where the cipher tool scripts (for Windows and Linux) are stored. Execute the following command for your OS: On Linux: ./ciphertool.sh -Dconfigure On Windows: ./ciphertool.bat -Dconfigure It will prompt for the primary keystore password. Enter the keystore password (which is \"wso2carbon\" for the default keystore). The alias values of all the passwords that you encrypted will now be shown in a numbered list. The system will then prompt you to select the alias of the password which you want to change. Enter the list number of the password alias. The system will then prompt you (twice) to enter the new password. Enter your new password. Resolving encrypted passwords \u00b6 If you have secured the plain text passwords in configuration files using Secure Vault, the keystore password and private key password of the product's primary keystore will serve as the root passwords for Secure Vault. This is because the keystore passwords are needed to initialise the values encrypted by the Secret Manager in the Secret Repository. Therefore, the Secret Callback handler is used to resolve these passwords. Read about the Secure Vault implementation in WSO2 products. Also, see how passwords in configuration files are encrypted using Secure Vault. The default secret CallbackHandler in a WSO2 product provides two options for reading these encrypted passwords when you start the server: Enter password in command-line \u00b6 Start the server by running the product start up script from the /bin/ directory as shown below. ./wso2server.sh When you run the startup script, the following message will be prompted before starting the server: \"[Enter KeyStore and Private Key Password :]\". This is because, in order to connect to the default user store, the encrypted passwords should be decrypted. The administrator starting the server must provide the private key and keystore passwords using the command-line. Note that passwords are hidden from the terminal and log files. Start server as a background job \u00b6 If you start the WSO2 Carbon server as a background job, you will not be able to provide password values on the command line. Therefore, you must start the server in \"daemon\" mode as explained below. Create a new file in the directory. The file should be named according to your OS as explained below. For Linux: The file name should be password-tmp. For Windows: The file name should be password-tmp.txt. Add \"wso2carbon\" (the primary keystore password) to the new file and save. By default, the password provider assumes that both private key and keystore passwords are the same. If not, the private key password must be entered in the second line of the file. Now, start the server as a background process by running the following command. ./wso2server.sh start Start the server by running the product start-up script from the /bin directory by executing the following command: daemon. sh wso2server.sh -start","title":"Encrypting Plain Text"},{"location":"setup/security/encrypting_plain_text/#encrypting-passwords-in-configuration-files","text":"The instructions on this page explain how plain text passwords in configuration files can be encrypted using the secure vault implementation that is built into WSO2 products. Note that you can customize the default secure vault configurations in the product by implementing a new secret repository, call back handler etc. Read more about the Secure Vault implementation in WSO2 products.","title":"Encrypting Passwords in Configuration files"},{"location":"setup/security/encrypting_plain_text/#before-you-begin","text":"If you are using Windows, you need to have Ant (http://ant.apache.org/) installed before using the Cipher Tool.","title":"Before you begin"},{"location":"setup/security/encrypting_plain_text/#encrypting-passwords","text":"Open the ei.toml file and add the following configuration section as shown below. Give an alias for the information type that you want to encrypt followed by the actual value. [secrets] foo = \"[R!#etoqe]\" bar = \"[T@h3rerqr]\" Open a terminal, navigate to the /bin/ directory, and execute the following command (You must first enable the Cipher tool for the product by executing the -Dconfigure command with the cipher tool script as shown below). On Linux: ./ciphertool.sh -Dconfigure On Windows: ./ciphertool.bat -Dconfigure Go back to the ei.toml file and see that the alias passwords are encrypted. [secrets] foo = \"qf24n82l48gl=3o42nfmakr83b5y3\" bar = \"02jehlqnghfbat54oqhwcxeqorm2o\"","title":"Encrypting passwords"},{"location":"setup/security/encrypting_plain_text/#using-encrypting-passwords","text":"Once you have encrypted password values, you can refer them from the relevant configuration sections in the ei.toml file. For example, if you have encrypted the admin password of your server, update the code [super_admin] config section in the ei.toml file as shown below. [super_admin] username=\"admin\" password=\"$ref{alias}\"","title":"Using encrypting Passwords"},{"location":"setup/security/encrypting_plain_text/#changing-encrypted-passwords","text":"To change any password which we have encrypted already, follow the below steps: Be sure to shut down the server. Open a command prompt and go to the /bin directory, where the cipher tool scripts (for Windows and Linux) are stored. Execute the following command for your OS: On Linux: ./ciphertool.sh -Dconfigure On Windows: ./ciphertool.bat -Dconfigure It will prompt for the primary keystore password. Enter the keystore password (which is \"wso2carbon\" for the default keystore). The alias values of all the passwords that you encrypted will now be shown in a numbered list. The system will then prompt you to select the alias of the password which you want to change. Enter the list number of the password alias. The system will then prompt you (twice) to enter the new password. Enter your new password.","title":"Changing encrypted passwords"},{"location":"setup/security/encrypting_plain_text/#resolving-encrypted-passwords","text":"If you have secured the plain text passwords in configuration files using Secure Vault, the keystore password and private key password of the product's primary keystore will serve as the root passwords for Secure Vault. This is because the keystore passwords are needed to initialise the values encrypted by the Secret Manager in the Secret Repository. Therefore, the Secret Callback handler is used to resolve these passwords. Read about the Secure Vault implementation in WSO2 products. Also, see how passwords in configuration files are encrypted using Secure Vault. The default secret CallbackHandler in a WSO2 product provides two options for reading these encrypted passwords when you start the server:","title":"Resolving encrypted passwords"},{"location":"setup/security/encrypting_plain_text/#enter-password-in-command-line","text":"Start the server by running the product start up script from the /bin/ directory as shown below. ./wso2server.sh When you run the startup script, the following message will be prompted before starting the server: \"[Enter KeyStore and Private Key Password :]\". This is because, in order to connect to the default user store, the encrypted passwords should be decrypted. The administrator starting the server must provide the private key and keystore passwords using the command-line. Note that passwords are hidden from the terminal and log files.","title":"Enter password in command-line"},{"location":"setup/security/encrypting_plain_text/#start-server-as-a-background-job","text":"If you start the WSO2 Carbon server as a background job, you will not be able to provide password values on the command line. Therefore, you must start the server in \"daemon\" mode as explained below. Create a new file in the directory. The file should be named according to your OS as explained below. For Linux: The file name should be password-tmp. For Windows: The file name should be password-tmp.txt. Add \"wso2carbon\" (the primary keystore password) to the new file and save. By default, the password provider assumes that both private key and keystore passwords are the same. If not, the private key password must be entered in the second line of the file. Now, start the server as a background process by running the following command. ./wso2server.sh start Start the server by running the product start-up script from the /bin directory by executing the following command: daemon. sh wso2server.sh -start","title":"Start server as a background job"},{"location":"setup/security/gdpr_ei/","text":"General Data Protection Regulation (GDPR) for WSO2 EI \u00b6 WSO2 Enterprise Integrator (WSO2 EI) consists of four profiles (ESB, Message Broker, Business Process Server, and Analytics) that can persist a user's personally identifiable information (PII) in various sources, namely log files and RDBMSs. However, organizations that use WSO2 EI have a legal obligation to remove all instances of a user's PII from the system if the relevant user requests. For example, consider a situation where an employee resigns from the organization and, thereby, requests the organization to remove all instances of one's PII from the organization's system. You can fulfill this requirement by anonymizing the user's PII in the system, or (in some cases) by completely removing such PII from the system. See the topics given below for instructions on how to remove PII from each profile of WSO2 EI. What is GDPR? How WSO2 EI persists a user's PII Tools for removing PII in WSO2 EI Prerequisites for removing PII Removing PII from the ESB profile Anonymizing PII references Deleting original (archived) log files Removing PII from the BPS profile Anonymizing PII in the BPMN (activiti) component Deleting original (archived) log files Removing Human Task and BPEL process instances Removing PII from the Analytics profile What is GDPR? \u00b6 The General Data Protection Regulation (GDPR) is a new legal framework that was formalized by the European Union (EU) in 2016. It comes into effect from 28, May 2018. GDPR requires any organization that processes Personally Identifiable Information (PII) of individuals who live in Europe, to be compliant with the regulations. Organizations that fail to demonstrate GDPR compliance are subjected to financial penalties. Info Do you want to learn more about GDPR? If you are new to GDPR, we recommend that you take a look at our tutorial series on Creating a Winning GDPR Strategy. Part 1 - Introduction to GDPR Part 2 - 7 Steps for GDPR Compliance Part 3 - Identity and Access Management to the Rescue Part 4 - GDPR Compliant Consent Design For more resources on GDPR, see the white papers, case studies, solution briefs, webinars, and talks published on our WSO2 GDPR homepage . You can also find the original GDPR legal text here . How WSO2 EI persists a user's PII \u00b6 Each profile of WSO2 EI persists user information in various different sources as explained below. ESB Profile The ESB profile can persist PII in various log files (carbon logs, audit logs, API logs, and service-specific logs) depending on the mediation logic defined. The ESB does not persist a user's PII in any RDBMS by default. BPS Profile The BPS profile of WSO2 EI contains three main components: BPMN , BPEL , and Human Tasks . These components will persist a user's PII in various ways as explained below. If you have workflows defined in any one of the above components or all of them, PII of users will get stored in the relevant RDBMS . !!! info What are the RDBMSs used by the BPS profile? The BPS profile uses two separate databases dedicated to BPMN data and BPEL/Human Task data respectively. By default, these are two H2 databases that are shipped with the product. You can find out more about these databases from here . All three components will also persist user information in log files (carbon logs and audit logs) . Message Broker Profile The Message Broker profile does not persist any PII in any way. Analytics Profile The Analytics profile of WSO2 EI uses event streams, which contain user information (PII) in its schemas. This data is stored in two separate RDBMSs dedicated for the Analytics profile. By default, when you start the Analytics profile, two H2 databases will be created for this purpose. However, when you move to a production environment, it is recommended to change these to industry-grade RDBMSs. You need to create the connection to the new databases from the profile by configuring the analytics-datasources.xml file (stored in the <EI_HOME>/wso2/analytics/conf/datasources/ directory. Tools for removing PII in WSO2 EI \u00b6 The following tools are shipped with WSO2 EI: WSO2 EI is shipped with the Forget-Me Tool , which can anonymize a user's PII in log files and RDBMSs by replacing all occurrences of the deleted user with either a randomly generated UUID value or a specified pseudonym. This tool is stored in the <EI_HOME>/wso2/tools/forget-me/ directory of WSO2 EI. Find out about all the capabilities of the Forget-Me tool from here . Important! In the case of log files, note that the Forget-Me Tool does not replace PII values in the actual log files. Instead, the tool will create a new set of log files with anonymized PII values. The organization can then remove the original log files. !!! info If you want to use the Forget-Me tool to remove PII in multiple WSO2 products at the same time, you can use the standalone version of the tool. For information on how to build and run the Forget-Me tool in standalone mode, see [Removing References to Deleted User Identities in WSO2 Products](https://docs.wso2.com/display/ADMIN44x/Removing+References+to+Deleted+User+Identities+in+WSO2+Products) in the WSO2 Administration Guide. In addition to this tool, the BPS profile also contains a set of SQL scripts that can remove PII by completely removing any process instances associated with a particular user. This method is only applicable to BPEL and Human Task processes. Prerequisites for removing PII \u00b6 As explained in How WSO2 EI persists a user's PII , the ESB profile and the BPS profile will store user information in log files. Note that we can only remove a deleted user's PII from archived log files, and not the live log files that are connected to the system. Therefore, before you start removing PII stored by the ESB profile and the BPMN component, be sure that the relevant user has been inactive in the system for a sufficient amount of time. This will ensure that all of the user's PII contained in log files are successfully archived. You can then follow the instructions given below to remove the user's PII references from the archived log files. Removing PII from the ESB profile \u00b6 Tip Before you begin , Find out about how the ESB profile stores a user's PII . See the prerequisites for removing PII from the ESB . In the instructions given below, we will use a proxy service that logs PII to demonstrate how PII can be removed from the ESB profile. However, please note that it is not recommended to log PII from a proxy service. Anonymizing PII references \u00b6 You can use the Forget-Me Tool to remove references to personally identifiable information (PII) from logs in the ESB profile. For example, consider a proxy service that logs the username that is sent through a payload. A Log mediator can be used for this as shown below. <log level=\"custom\"> <property expression=\"//Authentication/username\" name=\"USER_NAME\"/> </log> The user name that is used when you invoke this query will be logged in the following log files: wso2carbon.log file, audit.log file, warn.log, and the service-specific log file that is enabled for the proxy service. [EI-Core] INFO - LogMediator USER_NAME = Sam Let's look at how to anonymize the username value in log files. Every log statement follows the same pattern where the \"USER_NAME\" keyword is followed by an actual username (in this example it is \"Sam\"). The regex pattern of this log statement will be as shown below. The Forget-Me Tool will use the below regex pattern to anonymize the username. This pattern should be added to the ei-patterns.xml file (stored in the <EI_HOME>/wso2/tools/forget-me/conf/log-config/ directory). <pattern key=\"pattern3\"> <detectPattern>(.)*(USER_NAME)(.)*${username}(.)*</detectPattern> <replacePattern>${username}</replacePattern> </pattern> Update the config.json file (stored in the <EI_HOME>/wso2/tools/forget-me/conf/ directory) as shown below. This file contains references to all the log files (except any service-specific log file ) in the system that store the above user information. If you have enabled a service-specific log file, you need to add that file name (see the element descriptions given below). { \"processors\" : [ \"log-file\" ], \"directories\": [ { \"dir\": \"log-config\", \"type\": \"log-file\", \"processor\" : \"log-file\", \"log-file-path\" : \"<EI_HOME>/repository/logs\", \"log-file-name-regex\" : \"(audit.log|warn.log|wso2carbon.log)(.)*\" } ] } The elements in the above configuration are explained below. \"processors\" : The processors listed for this element specifies whether the tool will on log files, RDBMSs, or analytics streams. In the case of the ESB profile, we only need to remove PII from log files, and therefore, the processor is set to \"log-file\". \"directories\" : This element lists the directories that correspond to the processors. In the case of the ESB profile, we need to specify the directories that store log files. \"log-file-path\" : This specifies the directory path to the log files. Note that all the relevant log files are stored in the <EI_HOME>/repository/logs/ directory. !!! note Be sure to replace the \"log-file-path\" value with the correct absolute path to the location where the log files are stored. If you are **on Windows** , be sure to use the forward slash (\"/\") instead of the back slash (\"\\\\\"). For example: ` C:/Users/Administrator/Desktop/wso2ei-6.2.0/repository/log ` . \"log-file-name-regex\" : This gives the list of log files (stored in the log-file-path) that will persist the user's PII. Note that the above log-file-name-regex includes the audit.log, warn.log, and wso2carbon.log files, as well as the archived files of the same logs. If you have enabled a service-specific log file , be sure to add the file name to this list. Open a command prompt and navigate to the <EI_HOME>/bin directory. Execute the following command to anonymize the user information that was added to the ei-patterns.xml file. On Linux: ./forgetme.sh -U Sam On Windows: forgetme.bat -U This will result in the following: Copies will be created of all the log files specified in the config.json file. The following is the format of the log copy : anon-<time_stamp>-<original_log_name>.log . For example , anon-1520946791793-warn.log . The PII will be anonymized in the copies. The log files will display the user information as a pseudonym. [EI-Core] INFO - LogMediator USER_NAME = 86c3bfd9-f97c-4b08-9f15-772dcb0c1c Note F or the list of commands you can run using the Forget-Me tool, see this link . Deleting original (archived) log files \u00b6 Note that the PII is not removed from the original log files. It is the responsibility of the organization to remove the original log files that contain the user's PII. Removing PII from the BPS profile \u00b6 Tip Before you begin , Find out about how the BPS profile stores a user's PII . See the prerequisites for removing PII in the BPS profile . Let's look at how to anonymize/remove personally identifiable information (PII) stored by the three main components of the BPS profile ( BPMN component, BPEL component, and the Human Task component). Anonymizing PII in the BPMN ( activiti ) component \u00b6 The PII references stored by the BPMN component can be removed from log files as well as the BPMN-specific database by using the Forget-Me Tool . Follow the steps given below. Add the relevant drivers for your BPMN-specific database to the <EI_HOME>/wso2/tools/forget-me/lib directory. For example, if you have changed your BPMN database from the default H2 database to MySQL, copy the MySQL driver to this given directory. Open the activiti - datasources . xml file (stored in the \\<EI_HOME>/wso2/tools/forget-me/conf/datasources/ directory), and specify the details of the RDBMS that stores the metadata from BPMN workflows. Update the config.json file ( stored in the <EI_HOME>/wso2/tools/forget-me/conf/ directory) as shown below. This file contains references to all the log files in the system, and the RDBMS that stores the user information form BPMN workflows. { \"processors\" : [ \"log-file\", \"rdbms\" ], \"directories\": [ { \"dir\": \"log-config\", \"type\": \"log-file\", \"processor\" : \"log-file\", \"log-file-path\" : \"<EI_HOME>/wso2/business-process/repository/logs\", \"log-file-name-regex\" : \"(audit.log|warn.log|wso2carbon.log)(.)*\" }, { \"dir\": \"sql\", \"type\": \"rdbms\", \"processor\" : \"rdbms\" } ], \"extensions\": [ { \"dir\": \"datasources\", \"type\": \"datasource\", \"processor\" : \"rdbms\" } ] } The elements in the above configuration are explained below. \"processors\" : The processors listed for this element specifies whether the tool will run for log files, RDBMSs, or analytics streams. In the case of the BPMN component of the BPS profile, we need to remove PII from log files, as well as the BPMN-specific database. Therefore, the processor is set to \"log-file\",\"rdbms\". \"directories\" : This element lists the directories that correspond to the processors. In the case of the BPMN component, we need to specify the directories that store log files, as well as the directory of the SQL scripts for the BPMN database. Therefore, the above configuration contains two directories: \"log-config\" and \"sql\". \"log-file-path\" : This specifies the directory path to the logs. In this example, all the relevant log files for BPS are stored in the <EI_HOME>/wso2/business-process/repository/logs/ directory. !!! note Be sure to replace the \"log-file-path\" value with the correct absolute path to the location where the log files are stored. If you are **on Windows** , be sure to use the forward slash (\"/\") instead of the back slash (\"\\\\\"). For example : ` C:/Users/Administrator/Desktop/wso2ei-6.2.0/repository/log ` . \"log-file-name-regex\" : This gives the list of log files (stored in the log-file-path) that will persist the user's PII. Note that the above log-file-name-regex includes the audit.log, warn.log, and wso2carbon.log files, as well as the archived files of the same logs. Open a command prompt and navigate to the <EI_HOME>/bin directory. Run the tool using the following command: On Linux: ./forgetme.sh -U <USERNAME> On Windows: forgetme.bat -U <USERNAME> This will result in the following: Copies will be created of all the log files specified in the config.json file. The following is the format of the log copy : anon-<time_stamp>-<original_log_name>.log . For example , anon-1520946791793-warn.log . The PII will be anonymized in the copies. The log files will display the user information as a pseudonym. The user's PII will be removed from the BPMN database. Note For the list of commands you can run using the Forget-Me tool, see this link . Deleting original (archived) log files \u00b6 Note that the PII is not removed from the original log files. It is the responsibility of the organization to remove the original log files that contain the user's PII. Removing Human Task and BPEL process instances \u00b6 If you are using Human Tasks and BPEL workflows in your BPS profile, you can remove a user's personally identifiable information (PII) from the BPS instance by removing all process instances and task instances (associated with message exchanges) from the server. WSO2 EI is shipped with a set of SQL scripts (stored in the bpel and humantask folders in the <EI_HOME>/wso2/business-process/repository/resources/cleanup-scripts directory) that you can use for removing process instances and task instances from the BPS profile. There are two ways of doing this: Remove all completed tasks/processes. This can be configured to a particular period. Identify the processes/tasks that are specific to a given user ID, and remove them individually. For instructions, see BPS database cleanup . Removing PII from the Analytics profile \u00b6 Tip Before you begin , find out about how the Analytics profile stores a user's PII . Shown below is an example data stream (used by the ESB profile) for product analytics. Note that the username, email and the date of birth are personally identifiable information (PII) of the user. Stream Name Attribute List org.wso2.gdpr.students username email dateOfBirth org.wso2.gdpr.students.marks username marks These PII references can be removed from the Analytics database by using the Forget-Me Tool . Follow the steps given below. Add the relevant drivers for your Analytics-specific databases to the <EI_HOME>/wso2/tools/forget-me/lib directory. For example, if you have changed your Analytics databases from the default H2 instances to MySQL, copy the MySQL driver to this given directory. Create a folder named 'streams' in the <EI_HOME>/wso2/tools/forget-me/conf/ directory. Create a new file named streams.json with the content shown below, and store it in the /streams directory that you created in the previous step. This file holds the details of the streams and the attributes with PII that we need to remove from the database. { \"streams\": [ { \"streamName\": \"org.wso2.gdpr.students\", \"attributes\": [\"username\", \"email\", \"dateOfBirth\"], \"id\": \"username\" }, { \"streamName\": \"org.wso2.gdpr.students.marks\", \"attributes\": [\"username\"], \"id\": \"username\" } ] } The above configuration includes the following: Stream Name : The name of the stream. Attributes: The list of attributes that contain PII. id : The ID attribute, which holds the value that needs to be anonymized (replaced with a pseudonym). Update the config.json file ( stored in the <EI_HOME>/wso2/tools/forget-me/conf/ directory) as shown below. { \"processors\": [ \"analytics-streams\" ], \"directories\": [ { \"dir\": \"analytics-streams\", \"type\": \"analytics-streams\", \"processor\": \"analytics-streams\" } ] } Open a command prompt and navigate to the <EI_HOME>/bin directory. Run the tool using the following command: On Linux: ./forgetme.sh -U <USERNAME> -carbon <EI_ANALYTICS_HOME> On Windows: forgetme.bat -U <USERNAME> -carbon <EI_ANALYTICS_HOME> Note For the list of commands you can run using the Forget-Me tool, see this link .","title":"General Data Protection Regulation (GDPR) for WSO2 EI"},{"location":"setup/security/gdpr_ei/#general-data-protection-regulation-gdpr-for-wso2-ei","text":"WSO2 Enterprise Integrator (WSO2 EI) consists of four profiles (ESB, Message Broker, Business Process Server, and Analytics) that can persist a user's personally identifiable information (PII) in various sources, namely log files and RDBMSs. However, organizations that use WSO2 EI have a legal obligation to remove all instances of a user's PII from the system if the relevant user requests. For example, consider a situation where an employee resigns from the organization and, thereby, requests the organization to remove all instances of one's PII from the organization's system. You can fulfill this requirement by anonymizing the user's PII in the system, or (in some cases) by completely removing such PII from the system. See the topics given below for instructions on how to remove PII from each profile of WSO2 EI. What is GDPR? How WSO2 EI persists a user's PII Tools for removing PII in WSO2 EI Prerequisites for removing PII Removing PII from the ESB profile Anonymizing PII references Deleting original (archived) log files Removing PII from the BPS profile Anonymizing PII in the BPMN (activiti) component Deleting original (archived) log files Removing Human Task and BPEL process instances Removing PII from the Analytics profile","title":"General Data Protection Regulation (GDPR) for WSO2 EI"},{"location":"setup/security/gdpr_ei/#what-is-gdpr","text":"The General Data Protection Regulation (GDPR) is a new legal framework that was formalized by the European Union (EU) in 2016. It comes into effect from 28, May 2018. GDPR requires any organization that processes Personally Identifiable Information (PII) of individuals who live in Europe, to be compliant with the regulations. Organizations that fail to demonstrate GDPR compliance are subjected to financial penalties. Info Do you want to learn more about GDPR? If you are new to GDPR, we recommend that you take a look at our tutorial series on Creating a Winning GDPR Strategy. Part 1 - Introduction to GDPR Part 2 - 7 Steps for GDPR Compliance Part 3 - Identity and Access Management to the Rescue Part 4 - GDPR Compliant Consent Design For more resources on GDPR, see the white papers, case studies, solution briefs, webinars, and talks published on our WSO2 GDPR homepage . You can also find the original GDPR legal text here .","title":"What is GDPR?"},{"location":"setup/security/gdpr_ei/#how-wso2-ei-persists-a-users-pii","text":"Each profile of WSO2 EI persists user information in various different sources as explained below. ESB Profile The ESB profile can persist PII in various log files (carbon logs, audit logs, API logs, and service-specific logs) depending on the mediation logic defined. The ESB does not persist a user's PII in any RDBMS by default. BPS Profile The BPS profile of WSO2 EI contains three main components: BPMN , BPEL , and Human Tasks . These components will persist a user's PII in various ways as explained below. If you have workflows defined in any one of the above components or all of them, PII of users will get stored in the relevant RDBMS . !!! info What are the RDBMSs used by the BPS profile? The BPS profile uses two separate databases dedicated to BPMN data and BPEL/Human Task data respectively. By default, these are two H2 databases that are shipped with the product. You can find out more about these databases from here . All three components will also persist user information in log files (carbon logs and audit logs) . Message Broker Profile The Message Broker profile does not persist any PII in any way. Analytics Profile The Analytics profile of WSO2 EI uses event streams, which contain user information (PII) in its schemas. This data is stored in two separate RDBMSs dedicated for the Analytics profile. By default, when you start the Analytics profile, two H2 databases will be created for this purpose. However, when you move to a production environment, it is recommended to change these to industry-grade RDBMSs. You need to create the connection to the new databases from the profile by configuring the analytics-datasources.xml file (stored in the <EI_HOME>/wso2/analytics/conf/datasources/ directory.","title":"How WSO2 EI persists a user's PII"},{"location":"setup/security/gdpr_ei/#tools-for-removing-pii-in-wso2-ei","text":"The following tools are shipped with WSO2 EI: WSO2 EI is shipped with the Forget-Me Tool , which can anonymize a user's PII in log files and RDBMSs by replacing all occurrences of the deleted user with either a randomly generated UUID value or a specified pseudonym. This tool is stored in the <EI_HOME>/wso2/tools/forget-me/ directory of WSO2 EI. Find out about all the capabilities of the Forget-Me tool from here . Important! In the case of log files, note that the Forget-Me Tool does not replace PII values in the actual log files. Instead, the tool will create a new set of log files with anonymized PII values. The organization can then remove the original log files. !!! info If you want to use the Forget-Me tool to remove PII in multiple WSO2 products at the same time, you can use the standalone version of the tool. For information on how to build and run the Forget-Me tool in standalone mode, see [Removing References to Deleted User Identities in WSO2 Products](https://docs.wso2.com/display/ADMIN44x/Removing+References+to+Deleted+User+Identities+in+WSO2+Products) in the WSO2 Administration Guide. In addition to this tool, the BPS profile also contains a set of SQL scripts that can remove PII by completely removing any process instances associated with a particular user. This method is only applicable to BPEL and Human Task processes.","title":"Tools for removing PII in WSO2 EI"},{"location":"setup/security/gdpr_ei/#prerequisites-for-removing-pii","text":"As explained in How WSO2 EI persists a user's PII , the ESB profile and the BPS profile will store user information in log files. Note that we can only remove a deleted user's PII from archived log files, and not the live log files that are connected to the system. Therefore, before you start removing PII stored by the ESB profile and the BPMN component, be sure that the relevant user has been inactive in the system for a sufficient amount of time. This will ensure that all of the user's PII contained in log files are successfully archived. You can then follow the instructions given below to remove the user's PII references from the archived log files.","title":"Prerequisites for removing PII"},{"location":"setup/security/gdpr_ei/#removing-pii-from-the-esb-profile","text":"Tip Before you begin , Find out about how the ESB profile stores a user's PII . See the prerequisites for removing PII from the ESB . In the instructions given below, we will use a proxy service that logs PII to demonstrate how PII can be removed from the ESB profile. However, please note that it is not recommended to log PII from a proxy service.","title":"Removing PII from the ESB profile"},{"location":"setup/security/gdpr_ei/#anonymizing-pii-references","text":"You can use the Forget-Me Tool to remove references to personally identifiable information (PII) from logs in the ESB profile. For example, consider a proxy service that logs the username that is sent through a payload. A Log mediator can be used for this as shown below. <log level=\"custom\"> <property expression=\"//Authentication/username\" name=\"USER_NAME\"/> </log> The user name that is used when you invoke this query will be logged in the following log files: wso2carbon.log file, audit.log file, warn.log, and the service-specific log file that is enabled for the proxy service. [EI-Core] INFO - LogMediator USER_NAME = Sam Let's look at how to anonymize the username value in log files. Every log statement follows the same pattern where the \"USER_NAME\" keyword is followed by an actual username (in this example it is \"Sam\"). The regex pattern of this log statement will be as shown below. The Forget-Me Tool will use the below regex pattern to anonymize the username. This pattern should be added to the ei-patterns.xml file (stored in the <EI_HOME>/wso2/tools/forget-me/conf/log-config/ directory). <pattern key=\"pattern3\"> <detectPattern>(.)*(USER_NAME)(.)*${username}(.)*</detectPattern> <replacePattern>${username}</replacePattern> </pattern> Update the config.json file (stored in the <EI_HOME>/wso2/tools/forget-me/conf/ directory) as shown below. This file contains references to all the log files (except any service-specific log file ) in the system that store the above user information. If you have enabled a service-specific log file, you need to add that file name (see the element descriptions given below). { \"processors\" : [ \"log-file\" ], \"directories\": [ { \"dir\": \"log-config\", \"type\": \"log-file\", \"processor\" : \"log-file\", \"log-file-path\" : \"<EI_HOME>/repository/logs\", \"log-file-name-regex\" : \"(audit.log|warn.log|wso2carbon.log)(.)*\" } ] } The elements in the above configuration are explained below. \"processors\" : The processors listed for this element specifies whether the tool will on log files, RDBMSs, or analytics streams. In the case of the ESB profile, we only need to remove PII from log files, and therefore, the processor is set to \"log-file\". \"directories\" : This element lists the directories that correspond to the processors. In the case of the ESB profile, we need to specify the directories that store log files. \"log-file-path\" : This specifies the directory path to the log files. Note that all the relevant log files are stored in the <EI_HOME>/repository/logs/ directory. !!! note Be sure to replace the \"log-file-path\" value with the correct absolute path to the location where the log files are stored. If you are **on Windows** , be sure to use the forward slash (\"/\") instead of the back slash (\"\\\\\"). For example: ` C:/Users/Administrator/Desktop/wso2ei-6.2.0/repository/log ` . \"log-file-name-regex\" : This gives the list of log files (stored in the log-file-path) that will persist the user's PII. Note that the above log-file-name-regex includes the audit.log, warn.log, and wso2carbon.log files, as well as the archived files of the same logs. If you have enabled a service-specific log file , be sure to add the file name to this list. Open a command prompt and navigate to the <EI_HOME>/bin directory. Execute the following command to anonymize the user information that was added to the ei-patterns.xml file. On Linux: ./forgetme.sh -U Sam On Windows: forgetme.bat -U This will result in the following: Copies will be created of all the log files specified in the config.json file. The following is the format of the log copy : anon-<time_stamp>-<original_log_name>.log . For example , anon-1520946791793-warn.log . The PII will be anonymized in the copies. The log files will display the user information as a pseudonym. [EI-Core] INFO - LogMediator USER_NAME = 86c3bfd9-f97c-4b08-9f15-772dcb0c1c Note F or the list of commands you can run using the Forget-Me tool, see this link .","title":"Anonymizing PII references"},{"location":"setup/security/gdpr_ei/#deleting-original-archived-log-files","text":"Note that the PII is not removed from the original log files. It is the responsibility of the organization to remove the original log files that contain the user's PII.","title":"Deleting original (archived) log files"},{"location":"setup/security/gdpr_ei/#removing-pii-from-the-bps-profile","text":"Tip Before you begin , Find out about how the BPS profile stores a user's PII . See the prerequisites for removing PII in the BPS profile . Let's look at how to anonymize/remove personally identifiable information (PII) stored by the three main components of the BPS profile ( BPMN component, BPEL component, and the Human Task component).","title":"Removing PII from the BPS profile"},{"location":"setup/security/gdpr_ei/#anonymizing-pii-in-the-bpmn-activiti-component","text":"The PII references stored by the BPMN component can be removed from log files as well as the BPMN-specific database by using the Forget-Me Tool . Follow the steps given below. Add the relevant drivers for your BPMN-specific database to the <EI_HOME>/wso2/tools/forget-me/lib directory. For example, if you have changed your BPMN database from the default H2 database to MySQL, copy the MySQL driver to this given directory. Open the activiti - datasources . xml file (stored in the \\<EI_HOME>/wso2/tools/forget-me/conf/datasources/ directory), and specify the details of the RDBMS that stores the metadata from BPMN workflows. Update the config.json file ( stored in the <EI_HOME>/wso2/tools/forget-me/conf/ directory) as shown below. This file contains references to all the log files in the system, and the RDBMS that stores the user information form BPMN workflows. { \"processors\" : [ \"log-file\", \"rdbms\" ], \"directories\": [ { \"dir\": \"log-config\", \"type\": \"log-file\", \"processor\" : \"log-file\", \"log-file-path\" : \"<EI_HOME>/wso2/business-process/repository/logs\", \"log-file-name-regex\" : \"(audit.log|warn.log|wso2carbon.log)(.)*\" }, { \"dir\": \"sql\", \"type\": \"rdbms\", \"processor\" : \"rdbms\" } ], \"extensions\": [ { \"dir\": \"datasources\", \"type\": \"datasource\", \"processor\" : \"rdbms\" } ] } The elements in the above configuration are explained below. \"processors\" : The processors listed for this element specifies whether the tool will run for log files, RDBMSs, or analytics streams. In the case of the BPMN component of the BPS profile, we need to remove PII from log files, as well as the BPMN-specific database. Therefore, the processor is set to \"log-file\",\"rdbms\". \"directories\" : This element lists the directories that correspond to the processors. In the case of the BPMN component, we need to specify the directories that store log files, as well as the directory of the SQL scripts for the BPMN database. Therefore, the above configuration contains two directories: \"log-config\" and \"sql\". \"log-file-path\" : This specifies the directory path to the logs. In this example, all the relevant log files for BPS are stored in the <EI_HOME>/wso2/business-process/repository/logs/ directory. !!! note Be sure to replace the \"log-file-path\" value with the correct absolute path to the location where the log files are stored. If you are **on Windows** , be sure to use the forward slash (\"/\") instead of the back slash (\"\\\\\"). For example : ` C:/Users/Administrator/Desktop/wso2ei-6.2.0/repository/log ` . \"log-file-name-regex\" : This gives the list of log files (stored in the log-file-path) that will persist the user's PII. Note that the above log-file-name-regex includes the audit.log, warn.log, and wso2carbon.log files, as well as the archived files of the same logs. Open a command prompt and navigate to the <EI_HOME>/bin directory. Run the tool using the following command: On Linux: ./forgetme.sh -U <USERNAME> On Windows: forgetme.bat -U <USERNAME> This will result in the following: Copies will be created of all the log files specified in the config.json file. The following is the format of the log copy : anon-<time_stamp>-<original_log_name>.log . For example , anon-1520946791793-warn.log . The PII will be anonymized in the copies. The log files will display the user information as a pseudonym. The user's PII will be removed from the BPMN database. Note For the list of commands you can run using the Forget-Me tool, see this link .","title":"Anonymizing PII in the BPMN ( activiti ) component"},{"location":"setup/security/gdpr_ei/#deleting-original-archived-log-files_1","text":"Note that the PII is not removed from the original log files. It is the responsibility of the organization to remove the original log files that contain the user's PII.","title":"Deleting original (archived) log files"},{"location":"setup/security/gdpr_ei/#removing-human-task-and-bpel-process-instances","text":"If you are using Human Tasks and BPEL workflows in your BPS profile, you can remove a user's personally identifiable information (PII) from the BPS instance by removing all process instances and task instances (associated with message exchanges) from the server. WSO2 EI is shipped with a set of SQL scripts (stored in the bpel and humantask folders in the <EI_HOME>/wso2/business-process/repository/resources/cleanup-scripts directory) that you can use for removing process instances and task instances from the BPS profile. There are two ways of doing this: Remove all completed tasks/processes. This can be configured to a particular period. Identify the processes/tasks that are specific to a given user ID, and remove them individually. For instructions, see BPS database cleanup .","title":"Removing Human Task and BPEL process instances"},{"location":"setup/security/gdpr_ei/#removing-pii-from-the-analytics-profile","text":"Tip Before you begin , find out about how the Analytics profile stores a user's PII . Shown below is an example data stream (used by the ESB profile) for product analytics. Note that the username, email and the date of birth are personally identifiable information (PII) of the user. Stream Name Attribute List org.wso2.gdpr.students username email dateOfBirth org.wso2.gdpr.students.marks username marks These PII references can be removed from the Analytics database by using the Forget-Me Tool . Follow the steps given below. Add the relevant drivers for your Analytics-specific databases to the <EI_HOME>/wso2/tools/forget-me/lib directory. For example, if you have changed your Analytics databases from the default H2 instances to MySQL, copy the MySQL driver to this given directory. Create a folder named 'streams' in the <EI_HOME>/wso2/tools/forget-me/conf/ directory. Create a new file named streams.json with the content shown below, and store it in the /streams directory that you created in the previous step. This file holds the details of the streams and the attributes with PII that we need to remove from the database. { \"streams\": [ { \"streamName\": \"org.wso2.gdpr.students\", \"attributes\": [\"username\", \"email\", \"dateOfBirth\"], \"id\": \"username\" }, { \"streamName\": \"org.wso2.gdpr.students.marks\", \"attributes\": [\"username\"], \"id\": \"username\" } ] } The above configuration includes the following: Stream Name : The name of the stream. Attributes: The list of attributes that contain PII. id : The ID attribute, which holds the value that needs to be anonymized (replaced with a pseudonym). Update the config.json file ( stored in the <EI_HOME>/wso2/tools/forget-me/conf/ directory) as shown below. { \"processors\": [ \"analytics-streams\" ], \"directories\": [ { \"dir\": \"analytics-streams\", \"type\": \"analytics-streams\", \"processor\": \"analytics-streams\" } ] } Open a command prompt and navigate to the <EI_HOME>/bin directory. Run the tool using the following command: On Linux: ./forgetme.sh -U <USERNAME> -carbon <EI_ANALYTICS_HOME> On Windows: forgetme.bat -U <USERNAME> -carbon <EI_ANALYTICS_HOME> Note For the list of commands you can run using the Forget-Me tool, see this link .","title":"Removing PII from the Analytics profile"},{"location":"setup/security/importing_ssl_certificate/","text":"Adding SSL certificates to keystores \u00b6 Now, let's look at how you can get a CA-signed certificate for your keystores. Note that you do not need to create a new keystore everytime you need add a CA-signed certificate. Step 1: Generating SSL certificates for keystore \u00b6 First, you need to generate a certificate signing request (CSR) for your keystore (.jks file). This CSR file can then be certified by a certification authority (CA), which is an entity that issues digital certificates. These certificates certify the ownership of a public key. Execute the following command to generate the CSR: keytool -certreq -alias certalias -file newcertreq.csr -keystore newkeystore.jks As mentioned before, use the same alias that you used during the keystore creation process. You will be asked to give the keystore password. Once the password is given, the command will output the newcertreq.csr file to the /repository/resources/security/ directory. This is the CSR that you must submit to a CA. You must provide this CSR file to the CA. For testing purposes, try the 90 days trial SSL certificate from Comodo. It is preferable to have a wildcard certificate or multiple domain certificates if you wish to have multiple subdomains like gateway.sampledomain.org , publisher.sampledomain.org , identity.sampledomain.org , etc., for the deployment. For such requirements, you must modify the CSR request by adding subject alternative names. Most of the SSL providers give instructions to generate the CSR in such cases. After accepting the request, a signed certificate is provided along with a root certificate and several intermediate certificates (depending on the CA) as a bundle (.zip file). Sample certificates provided by the CA (Comodo) Root certificate of the CA: AddTrustExternalCARoot.crt Intermediate certificates: COMODORSAAddTrustCA.crt, COMODORSADomainValidationSecureServerCA.crt SSL Certificate signed by CA: test_sampleapp_org.crt Step 2: Importing SSL certificates to a keystore \u00b6 Follow the steps given below to import the CA-signed certificate to your keystore. Before importing the CA-signed certificate to the keystore, you must add the root CA certificate and the two (related) intermediate certificates by executing the commands given below. Note that the sample certificates given above are used as examples. keytool -import -v -trustcacerts -alias ExternalCARoot -file AddTrustExternalCARoot.crt -keystore newkeystore.jks -storepass mypassword keytool -import -v -trustcacerts -alias TrustCA -file COMODORSAAddTrustCA.crt -keystore newkeystore.jks -storepass mypassword keytool -import -v -trustcacerts -alias SecureServerCA -file COMODORSADomainValidationSecureServerCA.crt -keystore newkeystore.jks -storepass mypassword Optionally we can append the -storepass option to avoid having to enter the password when prompted later in the interactive mode. After you add the root certificate and all other intermediate certificates, add the CA-signed SSL certificate to the keystore by executing the following command: keytool -import -v -alias newcert -file <test_sampleapp_org.crt> -keystore newkeystore.jks -keypass mypassword -storepass mypassword In this command, use the same alias (i.e., 'newcert') that you used while creating the keystore Now you have a Java keystore, which includes a CA-signed public key certificate that can be used for SSL in a production environment. Next, you may need to add the same CA-signed public key certificate to the client-truststore.jks file. This will provide security and trust for backend communication/inter-system communication of WSO2 products via SSL. Step 3: Importing SSL certificates to a truststore \u00b6 In SSL handshake, the client needs to verify the certificate presented by the server. For this purpose, the client usually stores the certificates it trusts, in a trust store. To enable secure and trusted backend communication, all WSO2 products are shipped with a trust store named client-truststore.jks, which resides in the same directory as the default keystore ( /repository/resources/security/). Follow the steps given below to import the same CA-signed public key certificate (which you obtained in the previous step) into your WSO2 product's default truststore (client-truststore.jks). Get a copy of the client-truststore.jks file from the /repository/resources/security/ directory. Export the public key from your .jks file using the following command. keytool -export -alias certalias -keystore newkeystore.jks -file <public key name>.pem Import the public key you extracted in the previous step to the client-truststore.jks file using the following command. keytool -import -alias certalias -file <public key name>.pem -keystore client-truststore.jks -storepass wso2carbon > Note that 'wso2carbon' is the keystore password of the default client-truststore.jks file. Now, you have an SSL certificate stored in a Java keystore and a public key added to the client-truststore.jks file. Note that both these files should be in the /repository/resources/security/ directory. You can now replace the default wso2carbon.jks keystore in your product with the newly created keystore by updating the relevant configuration files in your product.","title":"Adding SSL certificates to keystores"},{"location":"setup/security/importing_ssl_certificate/#adding-ssl-certificates-to-keystores","text":"Now, let's look at how you can get a CA-signed certificate for your keystores. Note that you do not need to create a new keystore everytime you need add a CA-signed certificate.","title":"Adding SSL certificates to keystores"},{"location":"setup/security/importing_ssl_certificate/#step-1-generating-ssl-certificates-for-keystore","text":"First, you need to generate a certificate signing request (CSR) for your keystore (.jks file). This CSR file can then be certified by a certification authority (CA), which is an entity that issues digital certificates. These certificates certify the ownership of a public key. Execute the following command to generate the CSR: keytool -certreq -alias certalias -file newcertreq.csr -keystore newkeystore.jks As mentioned before, use the same alias that you used during the keystore creation process. You will be asked to give the keystore password. Once the password is given, the command will output the newcertreq.csr file to the /repository/resources/security/ directory. This is the CSR that you must submit to a CA. You must provide this CSR file to the CA. For testing purposes, try the 90 days trial SSL certificate from Comodo. It is preferable to have a wildcard certificate or multiple domain certificates if you wish to have multiple subdomains like gateway.sampledomain.org , publisher.sampledomain.org , identity.sampledomain.org , etc., for the deployment. For such requirements, you must modify the CSR request by adding subject alternative names. Most of the SSL providers give instructions to generate the CSR in such cases. After accepting the request, a signed certificate is provided along with a root certificate and several intermediate certificates (depending on the CA) as a bundle (.zip file). Sample certificates provided by the CA (Comodo) Root certificate of the CA: AddTrustExternalCARoot.crt Intermediate certificates: COMODORSAAddTrustCA.crt, COMODORSADomainValidationSecureServerCA.crt SSL Certificate signed by CA: test_sampleapp_org.crt","title":"Step 1: Generating SSL certificates for keystore"},{"location":"setup/security/importing_ssl_certificate/#step-2-importing-ssl-certificates-to-a-keystore","text":"Follow the steps given below to import the CA-signed certificate to your keystore. Before importing the CA-signed certificate to the keystore, you must add the root CA certificate and the two (related) intermediate certificates by executing the commands given below. Note that the sample certificates given above are used as examples. keytool -import -v -trustcacerts -alias ExternalCARoot -file AddTrustExternalCARoot.crt -keystore newkeystore.jks -storepass mypassword keytool -import -v -trustcacerts -alias TrustCA -file COMODORSAAddTrustCA.crt -keystore newkeystore.jks -storepass mypassword keytool -import -v -trustcacerts -alias SecureServerCA -file COMODORSADomainValidationSecureServerCA.crt -keystore newkeystore.jks -storepass mypassword Optionally we can append the -storepass option to avoid having to enter the password when prompted later in the interactive mode. After you add the root certificate and all other intermediate certificates, add the CA-signed SSL certificate to the keystore by executing the following command: keytool -import -v -alias newcert -file <test_sampleapp_org.crt> -keystore newkeystore.jks -keypass mypassword -storepass mypassword In this command, use the same alias (i.e., 'newcert') that you used while creating the keystore Now you have a Java keystore, which includes a CA-signed public key certificate that can be used for SSL in a production environment. Next, you may need to add the same CA-signed public key certificate to the client-truststore.jks file. This will provide security and trust for backend communication/inter-system communication of WSO2 products via SSL.","title":"Step 2: Importing SSL certificates to a keystore"},{"location":"setup/security/importing_ssl_certificate/#step-3-importing-ssl-certificates-to-a-truststore","text":"In SSL handshake, the client needs to verify the certificate presented by the server. For this purpose, the client usually stores the certificates it trusts, in a trust store. To enable secure and trusted backend communication, all WSO2 products are shipped with a trust store named client-truststore.jks, which resides in the same directory as the default keystore ( /repository/resources/security/). Follow the steps given below to import the same CA-signed public key certificate (which you obtained in the previous step) into your WSO2 product's default truststore (client-truststore.jks). Get a copy of the client-truststore.jks file from the /repository/resources/security/ directory. Export the public key from your .jks file using the following command. keytool -export -alias certalias -keystore newkeystore.jks -file <public key name>.pem Import the public key you extracted in the previous step to the client-truststore.jks file using the following command. keytool -import -alias certalias -file <public key name>.pem -keystore client-truststore.jks -storepass wso2carbon > Note that 'wso2carbon' is the keystore password of the default client-truststore.jks file. Now, you have an SSL certificate stored in a Java keystore and a public key added to the client-truststore.jks file. Note that both these files should be in the /repository/resources/security/ directory. You can now replace the default wso2carbon.jks keystore in your product with the newly created keystore by updating the relevant configuration files in your product.","title":"Step 3: Importing SSL certificates to a truststore"},{"location":"setup/security/mitigating_csrf/","text":"Mitigating Cross Site Request Forgery Attacks \u00b6 The following sections describe the impact of the Cross Site Request Forgery (CSRF) attack and how to mitigate it. How can CSRF attacks be harmful? \u00b6 Cross Site Request Forgery (CSRF) attacks trick you to send a malicious request, by forcing you to execute unwanted actions on an already authenticated web browser. The session in which you logged in to the web application on the browser is used to bypass the authentication step during this attack. If you are already authenticated on the website, the site or application cannot distinguish between a forged request and a legitimate request. Therefore, it is also known as \"session riding\". The attack includes maliciously tricking you to click a URL or HTML content that will consequently send a request to the website. For example: You send a request to an online banking application to transfer $100 to another bank account. An example URL including the parameters (i.e. account number and transfer amount) of a request is similar to the following: https://bank.com/transfer.do?acct=10220048&amount=100 The attacker uses this same URL by replacing the actual account number with a malicious account number. Then the attacker disguises this URL by including it in a clickable image and sends it to you in an email with other content. You may unknowingly click on this URL, which will send a transfer request to the bank to transfer money to the malicious bank account. Mitigating CSRF attacks \u00b6 OWASP CSRFGuard is an OWASP flagship project that provides synchronizer token pattern-based CSRF protection in a comprehensive and customizable manner. You can use the best practices and configuration recommendations of OWASP CSRFGuard to mitigate CSRF attacks in applications hosted on the WSO2 platform. Fine-tuned configuration values of CSRFGuard increases security, based on the security requirements of the specific application. CSRFGuard offers complete protection over CSRF scenarios by covering HTTP POST, HTTP GET as well as AJAX-based requests. You can protect forms based on HTTP POST and HTTP GET methods by injecting CSRF tokens into the \u201caction\u201d of the form, or by embedding a token in a hidden field. You can protect HTTP GET requests sent as a result of resource inclusions and links can by appending a relevant token in the \u201chref\u201d or \u201csrc\u201d attributes. Include these tokens manually using provided JSP tag library or by using a JavaScript based automated injection mechanism. AJAX requests are protected by injecting an additional header, which contains a CSRF token. Configuring applications in WSO2 product to mitigate CSRF attacks \u00b6 Before you begin , note the following: If your WSO2 product is based on Carbon 4.4.6 or a later version, t he configurations for mitigating CSRF attacks are enabled by default for all the applications that are built into the product. Therefore, you need to apply these configurations manually, only if you have any custom applications deployed in your product. If your WSO2 product is based on a Carbon version prior to version 4.4.6, the configurations for mitigating CSRF attacks should be applied to all applications manually. Important! Some updates of JDK 1.8 (for example, JDK1.8.0_151 ) are affected by a known issue related to GZIP decoding, which may prevent these CSRF-related configurations from working for your product. Therefore, until this issue is fixed, we recommend one of the following approaches: Be sure that your product is running on JDK1.8.0_144 or JDK1.8.0_077 . We have verified that these JDK versions are not affected by the known issue . Alternatively, you can disable GZIP decoding for your product by following the steps given below. This will ensure that your product is not affected by the known issue . Open the catalina-server.xml file from the <PRODUCT_HOME>/repository/conf/tomcat/ directory. Set the compression parameter (under each of the connector configurations) to false as shown below: compression=\"off\" Restart the server. See the following for instructions on manually updating CSRF configurations in WSO2 products: Securing web applications \u00b6 Follow the steps below to secure web applications. Add the following configurations in the web.xml file of your application. <!-- OWASP CSRFGuard context listener used to read CSRF configuration --> <listener> <listener-class>org.owasp.csrfguard.CsrfGuardServletContextListener</listener-class> </listener> <!-- OWASP CSRFGuard session listener used to generate per-session CSRF token --> <listener> <listener-class>org.owasp.csrfguard.CsrfGuardHttpSessionListener</listener-class> </listener> <!-- OWASP CSRFGuard per-application configuration property file location--> <context-param> <param-name>Owasp.CsrfGuard.Config</param-name> <param-value>/repository/conf/security/Owasp.CsrfGuard.properties</param-value> </context-param> <!-- OWASP CSRFGuard filter used to validate CSRF token--> <filter> <filter-name>CSRFGuard</filter-name> <filter-class>org.owasp.csrfguard.CsrfGuardFilter</filter-class> </filter> <!-- OWASP CSRFGuard filter mapping used to validate CSRF token--> <filter-mapping> <filter-name>CSRFGuard</filter-name> <url-pattern>/*</url-pattern> </filter-mapping> <!-- OWASP CSRFGuard servlet that serves dynamic token injection JavaScript (application can customize the URL pattern as required)--> <servlet> <servlet-name>JavaScriptServlet</servlet-name> <servlet-class>org.owasp.csrfguard.servlet.JavaScriptServlet</servlet-class> </servlet> <servlet-mapping> <servlet-name>JavaScriptServlet</servlet-name> <url-pattern>/csrf.js</url-pattern> </servlet-mapping> Include the following JavaScriptServlet as the first JavaScript inclusion of the <head> element, in the HTML template of all pages of the application that you need to protect. \u2026 <html> <head> \u2026 <script type=\u201dtext/javascript\u201d src=\u201d/csrf.js\u201d></script> <!-- other JavaScript inclusions should follow \u201ccsrf.js\u201d inclusion --> <script type=\u201dtext/javascript\u201d src=\u201d/main.js\u201d></script> \u2026 </head> <body> ... </body> </html> Open the esb.toml file add the following configuration section [config_heading] //Defines the hashing algorithm used to generate the CSRF token. org.owasp.csrfguard.PRNG=SHA1PRNG //Defines the length of the CSRF token. org.owasp.csrfguard.TokenLength=32 //Invalidates the user session, if a CSRF attack attempt was blocked by CSRFGuard. org.owasp.csrfguard.action.Invalidate=org.owasp.csrfguard.action.Invalidate Securing Jaggery applications \u00b6 Follow the steps below to secure Jaggery applications. Add the following configurations in the jaggery.conf file of your application. \"listeners\" : [ { \"class\" : \"org.owasp.csrfguard.CsrfGuardServletContextListener\" }, { \"class\" : \"org.owasp.csrfguard.CsrfGuardHttpSessionListener\" } ], \"servlets\" : [ { \"name\" : \"JavaScriptServlet\", \"class\" : \"org.owasp.csrfguard.servlet.JavaScriptServlet\" } ], \"servletMappings\" : [ { \"name\" : \"JavaScriptServlet\", \"url\" : \"/csrf.js\" } ], \"contextParams\" : [ { \"name\" : \"Owasp.CsrfGuard.Config\", \"value\" : \"/repository/conf/security/Owasp.CsrfGuard.dashboard.properties\" } ] Include the following JavaScriptServlet as the first JavaScript inclusion of the <head> element in the HTML template of all pages of the application that you need to protect. <html> <head> \u2026 <script type=\u201dtext/javascript\u201d src=\u201d/csrf.js\u201d></script> <!-- other JavaScript inclusions should follow \u201ccsrf.js\u201d inclusion --> <script type=\u201dtext/javascript\u201d src=\u201d/main.js\u201d></script> \u2026 </head> <body> ... </body> </html> Open the esb.toml file add the following configuration section [config_heading] //Defines the hashing algorithm used to generate the CSRF token. org.owasp.csrfguard.PRNG=SHA1PRNG //Defines the length of the CSRF token. org.owasp.csrfguard.TokenLength=32 //Invalidates the user session, if a CSRF attack attempt was blocked by CSRFGuard. org.owasp.csrfguard.action.Invalidate=org.owasp.csrfguard.action.Invalidate","title":"Mitigating Cross Site Request Forgery Attacks"},{"location":"setup/security/mitigating_csrf/#mitigating-cross-site-request-forgery-attacks","text":"The following sections describe the impact of the Cross Site Request Forgery (CSRF) attack and how to mitigate it.","title":"Mitigating Cross Site Request Forgery Attacks"},{"location":"setup/security/mitigating_csrf/#how-can-csrf-attacks-be-harmful","text":"Cross Site Request Forgery (CSRF) attacks trick you to send a malicious request, by forcing you to execute unwanted actions on an already authenticated web browser. The session in which you logged in to the web application on the browser is used to bypass the authentication step during this attack. If you are already authenticated on the website, the site or application cannot distinguish between a forged request and a legitimate request. Therefore, it is also known as \"session riding\". The attack includes maliciously tricking you to click a URL or HTML content that will consequently send a request to the website. For example: You send a request to an online banking application to transfer $100 to another bank account. An example URL including the parameters (i.e. account number and transfer amount) of a request is similar to the following: https://bank.com/transfer.do?acct=10220048&amount=100 The attacker uses this same URL by replacing the actual account number with a malicious account number. Then the attacker disguises this URL by including it in a clickable image and sends it to you in an email with other content. You may unknowingly click on this URL, which will send a transfer request to the bank to transfer money to the malicious bank account.","title":"How can CSRF attacks be harmful?"},{"location":"setup/security/mitigating_csrf/#mitigating-csrf-attacks","text":"OWASP CSRFGuard is an OWASP flagship project that provides synchronizer token pattern-based CSRF protection in a comprehensive and customizable manner. You can use the best practices and configuration recommendations of OWASP CSRFGuard to mitigate CSRF attacks in applications hosted on the WSO2 platform. Fine-tuned configuration values of CSRFGuard increases security, based on the security requirements of the specific application. CSRFGuard offers complete protection over CSRF scenarios by covering HTTP POST, HTTP GET as well as AJAX-based requests. You can protect forms based on HTTP POST and HTTP GET methods by injecting CSRF tokens into the \u201caction\u201d of the form, or by embedding a token in a hidden field. You can protect HTTP GET requests sent as a result of resource inclusions and links can by appending a relevant token in the \u201chref\u201d or \u201csrc\u201d attributes. Include these tokens manually using provided JSP tag library or by using a JavaScript based automated injection mechanism. AJAX requests are protected by injecting an additional header, which contains a CSRF token.","title":"Mitigating CSRF attacks"},{"location":"setup/security/mitigating_csrf/#configuring-applications-in-wso2-product-to-mitigate-csrf-attacks","text":"Before you begin , note the following: If your WSO2 product is based on Carbon 4.4.6 or a later version, t he configurations for mitigating CSRF attacks are enabled by default for all the applications that are built into the product. Therefore, you need to apply these configurations manually, only if you have any custom applications deployed in your product. If your WSO2 product is based on a Carbon version prior to version 4.4.6, the configurations for mitigating CSRF attacks should be applied to all applications manually. Important! Some updates of JDK 1.8 (for example, JDK1.8.0_151 ) are affected by a known issue related to GZIP decoding, which may prevent these CSRF-related configurations from working for your product. Therefore, until this issue is fixed, we recommend one of the following approaches: Be sure that your product is running on JDK1.8.0_144 or JDK1.8.0_077 . We have verified that these JDK versions are not affected by the known issue . Alternatively, you can disable GZIP decoding for your product by following the steps given below. This will ensure that your product is not affected by the known issue . Open the catalina-server.xml file from the <PRODUCT_HOME>/repository/conf/tomcat/ directory. Set the compression parameter (under each of the connector configurations) to false as shown below: compression=\"off\" Restart the server. See the following for instructions on manually updating CSRF configurations in WSO2 products:","title":"Configuring applications in WSO2 product to mitigate CSRF attacks"},{"location":"setup/security/mitigating_csrf/#securing-web-applications","text":"Follow the steps below to secure web applications. Add the following configurations in the web.xml file of your application. <!-- OWASP CSRFGuard context listener used to read CSRF configuration --> <listener> <listener-class>org.owasp.csrfguard.CsrfGuardServletContextListener</listener-class> </listener> <!-- OWASP CSRFGuard session listener used to generate per-session CSRF token --> <listener> <listener-class>org.owasp.csrfguard.CsrfGuardHttpSessionListener</listener-class> </listener> <!-- OWASP CSRFGuard per-application configuration property file location--> <context-param> <param-name>Owasp.CsrfGuard.Config</param-name> <param-value>/repository/conf/security/Owasp.CsrfGuard.properties</param-value> </context-param> <!-- OWASP CSRFGuard filter used to validate CSRF token--> <filter> <filter-name>CSRFGuard</filter-name> <filter-class>org.owasp.csrfguard.CsrfGuardFilter</filter-class> </filter> <!-- OWASP CSRFGuard filter mapping used to validate CSRF token--> <filter-mapping> <filter-name>CSRFGuard</filter-name> <url-pattern>/*</url-pattern> </filter-mapping> <!-- OWASP CSRFGuard servlet that serves dynamic token injection JavaScript (application can customize the URL pattern as required)--> <servlet> <servlet-name>JavaScriptServlet</servlet-name> <servlet-class>org.owasp.csrfguard.servlet.JavaScriptServlet</servlet-class> </servlet> <servlet-mapping> <servlet-name>JavaScriptServlet</servlet-name> <url-pattern>/csrf.js</url-pattern> </servlet-mapping> Include the following JavaScriptServlet as the first JavaScript inclusion of the <head> element, in the HTML template of all pages of the application that you need to protect. \u2026 <html> <head> \u2026 <script type=\u201dtext/javascript\u201d src=\u201d/csrf.js\u201d></script> <!-- other JavaScript inclusions should follow \u201ccsrf.js\u201d inclusion --> <script type=\u201dtext/javascript\u201d src=\u201d/main.js\u201d></script> \u2026 </head> <body> ... </body> </html> Open the esb.toml file add the following configuration section [config_heading] //Defines the hashing algorithm used to generate the CSRF token. org.owasp.csrfguard.PRNG=SHA1PRNG //Defines the length of the CSRF token. org.owasp.csrfguard.TokenLength=32 //Invalidates the user session, if a CSRF attack attempt was blocked by CSRFGuard. org.owasp.csrfguard.action.Invalidate=org.owasp.csrfguard.action.Invalidate","title":"Securing web applications"},{"location":"setup/security/mitigating_csrf/#securing-jaggery-applications","text":"Follow the steps below to secure Jaggery applications. Add the following configurations in the jaggery.conf file of your application. \"listeners\" : [ { \"class\" : \"org.owasp.csrfguard.CsrfGuardServletContextListener\" }, { \"class\" : \"org.owasp.csrfguard.CsrfGuardHttpSessionListener\" } ], \"servlets\" : [ { \"name\" : \"JavaScriptServlet\", \"class\" : \"org.owasp.csrfguard.servlet.JavaScriptServlet\" } ], \"servletMappings\" : [ { \"name\" : \"JavaScriptServlet\", \"url\" : \"/csrf.js\" } ], \"contextParams\" : [ { \"name\" : \"Owasp.CsrfGuard.Config\", \"value\" : \"/repository/conf/security/Owasp.CsrfGuard.dashboard.properties\" } ] Include the following JavaScriptServlet as the first JavaScript inclusion of the <head> element in the HTML template of all pages of the application that you need to protect. <html> <head> \u2026 <script type=\u201dtext/javascript\u201d src=\u201d/csrf.js\u201d></script> <!-- other JavaScript inclusions should follow \u201ccsrf.js\u201d inclusion --> <script type=\u201dtext/javascript\u201d src=\u201d/main.js\u201d></script> \u2026 </head> <body> ... </body> </html> Open the esb.toml file add the following configuration section [config_heading] //Defines the hashing algorithm used to generate the CSRF token. org.owasp.csrfguard.PRNG=SHA1PRNG //Defines the length of the CSRF token. org.owasp.csrfguard.TokenLength=32 //Invalidates the user session, if a CSRF attack attempt was blocked by CSRFGuard. org.owasp.csrfguard.action.Invalidate=org.owasp.csrfguard.action.Invalidate","title":"Securing Jaggery applications"},{"location":"setup/security/mitigating_css/","text":"Mitigating Cross Site Scripting Attacks \u00b6 The following sections describe the impact of the XSS attack and the approaches you can use to mitigate it. Note that XSS attacks are prevented on the latest WSO2 products by default. This is due to output encoding of the displaying values. However, if additional protection is required, an input validation valve can be configured as explained below. How can XSS attacks be harmful? \u00b6 Cross Site Scripting (XSS) attacks use web applications to inject malicious scripts or a malicious payload, generally in the form of a client side script, into trusted legitimate web applications. XSS Attackers can gain elevated access privileges to sensitive page content, session cookies, and a variety of other information with respect to web applications that are maintained by the web browser on behalf of the user. Mitigating XSS attacks \u00b6 You can use the following approach to mitigate XSS attacks. The XSS Valve acts as a filter to differentiate between the malicious scripts from the legitimate scripts by carrying out a specific validation on the URL patterns. To configure the XSS valve: Open the esb.toml file and add the following configurations: [xss_prevention] enabled=true rule_allowed=true pattern=commonauth Restart the product server.","title":"Mitigating Cross Site Scripting"},{"location":"setup/security/mitigating_css/#mitigating-cross-site-scripting-attacks","text":"The following sections describe the impact of the XSS attack and the approaches you can use to mitigate it. Note that XSS attacks are prevented on the latest WSO2 products by default. This is due to output encoding of the displaying values. However, if additional protection is required, an input validation valve can be configured as explained below.","title":"Mitigating Cross Site Scripting Attacks"},{"location":"setup/security/mitigating_css/#how-can-xss-attacks-be-harmful","text":"Cross Site Scripting (XSS) attacks use web applications to inject malicious scripts or a malicious payload, generally in the form of a client side script, into trusted legitimate web applications. XSS Attackers can gain elevated access privileges to sensitive page content, session cookies, and a variety of other information with respect to web applications that are maintained by the web browser on behalf of the user.","title":"How can XSS attacks be harmful?"},{"location":"setup/security/mitigating_css/#mitigating-xss-attacks","text":"You can use the following approach to mitigate XSS attacks. The XSS Valve acts as a filter to differentiate between the malicious scripts from the legitimate scripts by carrying out a specific validation on the URL patterns. To configure the XSS valve: Open the esb.toml file and add the following configurations: [xss_prevention] enabled=true rule_allowed=true pattern=commonauth Restart the product server.","title":"Mitigating XSS attacks"},{"location":"setup/security/renewing_ca_signed_certificate_in_keystore/","text":"Renewing a CA-Signed Certificate in a Keystore \u00b6 A digital certificate has a validity period, after which the certificate expires. Once a certificate expires, it is no longer valid, and it can cause the client-server communication to fail at the SSL handshake level. Therefore, it is important to plan certificate renewal ahead of time. Neglecting certificate renewal can eventually lead to a catastrophic situation such as major service outage. Following are a few important points to keep in mind when you are renewing an expired certificate: Use the same certificate authority that you used when you first got the public certificate. If you use a different certificate authority for certificate renewal, you will have to import the new CA-certificate as well as the intermediate certificates to the keystore and the client\u2019s trust store. If the certificate authority\u2019s certificate is not in the keystore, you will get the following error when you try to import the CA-signed certificate to the keystore: keytool error: java.lang.Exception: Failed to establish chain from reply To overcome the above error, be sure to first import the CA-signed certificate as well as the intermediate certificates to the keystore in the correct order. Now let's take a look at each high level step in detail . Step 1: Check the validity period of the certificate \u00b6 Follow one of the steps below to view the validity period of a certificate: If you have a public hostname, go to https://www.sslshopper.com/ssl-checker.html and specify the hostname of your server. SSL hopper lists all the information about the server certificate. If you have a java keystore, execute the following keytool command to view the certificate information: keytool -list -keystore <keystore_name.jks> -alias <cert_alias> -v This prompts for the keystore password. Once you specify the password, you can view the certificate information in a human readable format where the validity period is displayed as follows: Valid from: Sun Jun 18 19:26:25 IST 2017 until: Sat Jun 19 19:26:25 IST 2027 If you have the certificate file, execute the following openssl command: x509 -in <certname.cer> -text -noout This displays the validity as follows: Validity Not Before: Jun 18 13:56:25 2017 GMT Not After : Jun 19 13:56:25 2027 GMT If it is a website, you can view the certificate information via the browser. All major browsers provide the capability to view certificate information. Once you view the validity period of a certificate and if it says that the certificate is about to expire or has already expired, the next step you should generate a Certificate Signing Request (CSR) and get a new certificate generated from the CA. Step 2: Generate a certificate signing request \u00b6 Depending on the type of keystore you have, follow one of the steps below to generate a CSR: If you have a java keystore, execute the following command: keytool -certreq -alias <cert_alias> -file <CSR.csr> -keystore <keystore_name.jks> If you want generate a CSR with a subject alternative name (SAN), be sure to use the -ext attribute in the keytool command to specify required SAN. Following is a sample keytool command that includes a SAN: keytool -certreq -alias test -file test.csr -keystore test.jks -ext SAN=dns:test.example.com If you have the private key and public key, execute the following command: openssl x509 -x509toreq -in <cert_name.crt> -out <CSR.csr> -signkey <private_key.key> Once you generate the CSR, you need to submit the CSR to your certificate authority to get a new CA-signed certificate. For testing purposes you can go to http://www.getacert.com/signacert.html and submit your CSR to obtain a new CA-signed certificate for free. After you obtain a new certificate, you have to import the new certificate to a keystore if you are using a java keystore. Step 3: Import the new certificate to a keystore \u00b6 Execute the following command to import a new certificate to a keystore: keytool -import -v -trustcacerts -alias <current_alias> -file <ca_signed_cert.cer> -keystore <keystore_name.jks> If you want to view information related to the renewed certificate,execute the following keytool command: keytool -list -keystore <keystore_name.jks> -alias <cert_alias> -v","title":"Renewing a CA-signed Certificate"},{"location":"setup/security/renewing_ca_signed_certificate_in_keystore/#renewing-a-ca-signed-certificate-in-a-keystore","text":"A digital certificate has a validity period, after which the certificate expires. Once a certificate expires, it is no longer valid, and it can cause the client-server communication to fail at the SSL handshake level. Therefore, it is important to plan certificate renewal ahead of time. Neglecting certificate renewal can eventually lead to a catastrophic situation such as major service outage. Following are a few important points to keep in mind when you are renewing an expired certificate: Use the same certificate authority that you used when you first got the public certificate. If you use a different certificate authority for certificate renewal, you will have to import the new CA-certificate as well as the intermediate certificates to the keystore and the client\u2019s trust store. If the certificate authority\u2019s certificate is not in the keystore, you will get the following error when you try to import the CA-signed certificate to the keystore: keytool error: java.lang.Exception: Failed to establish chain from reply To overcome the above error, be sure to first import the CA-signed certificate as well as the intermediate certificates to the keystore in the correct order. Now let's take a look at each high level step in detail .","title":"Renewing a CA-Signed Certificate in a Keystore"},{"location":"setup/security/renewing_ca_signed_certificate_in_keystore/#step-1-check-the-validity-period-of-the-certificate","text":"Follow one of the steps below to view the validity period of a certificate: If you have a public hostname, go to https://www.sslshopper.com/ssl-checker.html and specify the hostname of your server. SSL hopper lists all the information about the server certificate. If you have a java keystore, execute the following keytool command to view the certificate information: keytool -list -keystore <keystore_name.jks> -alias <cert_alias> -v This prompts for the keystore password. Once you specify the password, you can view the certificate information in a human readable format where the validity period is displayed as follows: Valid from: Sun Jun 18 19:26:25 IST 2017 until: Sat Jun 19 19:26:25 IST 2027 If you have the certificate file, execute the following openssl command: x509 -in <certname.cer> -text -noout This displays the validity as follows: Validity Not Before: Jun 18 13:56:25 2017 GMT Not After : Jun 19 13:56:25 2027 GMT If it is a website, you can view the certificate information via the browser. All major browsers provide the capability to view certificate information. Once you view the validity period of a certificate and if it says that the certificate is about to expire or has already expired, the next step you should generate a Certificate Signing Request (CSR) and get a new certificate generated from the CA.","title":"Step 1: Check the validity period of the certificate"},{"location":"setup/security/renewing_ca_signed_certificate_in_keystore/#step-2-generate-a-certificate-signing-request","text":"Depending on the type of keystore you have, follow one of the steps below to generate a CSR: If you have a java keystore, execute the following command: keytool -certreq -alias <cert_alias> -file <CSR.csr> -keystore <keystore_name.jks> If you want generate a CSR with a subject alternative name (SAN), be sure to use the -ext attribute in the keytool command to specify required SAN. Following is a sample keytool command that includes a SAN: keytool -certreq -alias test -file test.csr -keystore test.jks -ext SAN=dns:test.example.com If you have the private key and public key, execute the following command: openssl x509 -x509toreq -in <cert_name.crt> -out <CSR.csr> -signkey <private_key.key> Once you generate the CSR, you need to submit the CSR to your certificate authority to get a new CA-signed certificate. For testing purposes you can go to http://www.getacert.com/signacert.html and submit your CSR to obtain a new CA-signed certificate for free. After you obtain a new certificate, you have to import the new certificate to a keystore if you are using a java keystore.","title":"Step 2:\u00a0Generate a certificate signing request"},{"location":"setup/security/renewing_ca_signed_certificate_in_keystore/#step-3-import-the-new-certificate-to-a-keystore","text":"Execute the following command to import a new certificate to a keystore: keytool -import -v -trustcacerts -alias <current_alias> -file <ca_signed_cert.cer> -keystore <keystore_name.jks> If you want to view information related to the renewed certificate,execute the following keytool command: keytool -list -keystore <keystore_name.jks> -alias <cert_alias> -v","title":"Step 3:\u00a0Import the new certificate to a keystore"},{"location":"setup/security/securing_carbon_applications/","text":"Securing Carbon Applications \u00b6 When you work with WSO2 products, you may host multiple applications (Web applications and Jaggery applications) and expose them to external users. The management console , which is shipped with every WSO2 product is also an application that is deployed in the Carbon server. Before you expose the applications to the external network, be sure to configure the security settings for all your applications. See the following topics for instructions: Enabling HTTPS access to the management console Enabling HTTP access to the management console Starting the server without the management console Enabling role-based permissions for the management console Restricting access to Carbon applications For the management console only For jaggery Apps only For all web applications For web application servlets Enabling HTTP Strict Transport Security (HSTS) Headers For the management console For web applications For Jaggery applications Preventing browser caching For the management console For web applications For Jaggery applications Enabling HTTPS access to the management console \u00b6 All WSO2 products expose the management console through the default HTTPS transport, which is configured in the catalina-server.xml file (stored in the <PRODUCT_HOME>/repository/conf/tomcat directory). This transport must be properly configured in this file for the management console to be accessible. See HTTPS Servlet Transport for instructions. Enabling HTTP access to the management console \u00b6 If you are exposing the management console to a secure network, you may sometimes allow HTTP access to the management console. Note that HTTPS access is enabled by default. You can follow the steps given below to enable HTTP access to the management console: See HTTP Servlet Transport and make sure that the HTTP transport connector is configured for your product. Note that 9763 is the default port that will be used. Open the carbon. xml file stored in the <PRODUCT_HOME>/repository/conf directory and uncomment the following element: <EnableHTTPAdminConsole>true</EnableHTTPAdminConsole> Disable secure cookies for the management console. To do this, open the web.xml file from the <PRODUCT_HOME>/repository/conf/tomcat/carbon/WEB-INF directory and set the <secure> property to false . <session-config> <cookie-config> <secure>false</secure> </cookie-config> </session-config> You can now start the product server and access the management console through HTTP. Use the following URL: http://localhost:\\<port>/carbon/admin/login.jsp , where <port> corresponds to the HTTP port configured for the server. The default HTTP port for all WSO2 servers is 9763 . However, this may change if a port offset is applied to your server as explained here . Starting the server without the management console \u00b6 If you want to provide access to the production environment without allowing any user group (including admin) to log into the management console, execute the following command: sh <PRODUCT_HOME>/bin/wso2server.sh -DworkerNode If you want to check any additional options available to be used with the startup commands, type -help after the command, such as: sh <PRODUCT_HOME>/bin/wso2server.sh -help. Enabling role-based permissions for the management console \u00b6 You can grant management console access to selected users by configuring role-based permissions . Restricting access to Carbon applications \u00b6 When hosting your products in production, it's imperative that you restrict the access to the management console from the external network. Additionally, you may also need to restrict access to other applications. Accessing Carbon applications in your server (including the management console) can be restricted to selected IP addresses. You can use the Tomcat servlet filter ( org.apache.catalina.filters.RemoteAddrFilter ) for the purpose of restricting access. Note that you can either restrict access to the management console exclusively, or you can restrict access to all web applications in your server (which includes the management console) at the same time. For the management console only \u00b6 If you want only selected IP addresses to be able to access the management console, add the required IP addresses to the <PRODUCT_HOME>/repository/conf/tomcat/carbon/META-INF/context.xml file as follows: <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"<IP-address-01>|<IP-address-02>|<IP-address-03>\"/> The RemoteAddrValve Tomcat valve defined in this file will only apply to the Carbon management console and, thereby, all outside requests to the management console will be blocked. For jaggery Apps only \u00b6 If you want only selected IP addresses to be able to access the jaggery apps like publisher, store, admin portal which resides in WSO2 API Manager, go to \\<PRODUCT_HOME>/repository/deployment/server/jaggeryapps/\\<jaggeryapp-name>/jaggery.conf file and add the configuration as follows. { \"name\":\"Remote Address Filter\", \"class\":\"org.apache.catalina.filters.RemoteAddrFilter\", \"params\":[ {\"name\" : \"allow\", \"value\" : \"localhost|127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1|your IP added here\"} ] } { \"name\":\"Remote Address Filter\", \"url\":\"*\" } The value of the \"allow\" key that we defined in this file will apply to the publisher/store/admin portal as the the particular jagger.conf file you add this configuration and, thereby, all outside requests to the publisher/store/admin portal will be blocked. For all web applications \u00b6 To control access to all web applications deployed in your server (including the management console), add the IP addresses to the <PRODUCT_HOME>/repository/conf/context. xml file as follows: <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"<IP-address-01>|<IP-address-02>|<IP-address-03>\"/> The RemoteAddrValve Tomcat valve defined in this file will apply to each web application hosted on the Carbon server. Therefore, all outside requests to any web application will be blocked. For web application servlets \u00b6 You can also restrict access to particular servlets in a web application by adding a Remote Address Filter to the web.xml file (stored in the <PRODUCT_HOME>/repository/conf/tomcat/ directory), and by mapping that filter to the servlet URL. In the Remote Address Filter that you add, you can specify the IP addresses that should be allowed to access the servlet. The following example web.xml file illustrates how access to the login.jsp of the management console ( /carbon/admin/login.jsp ) is granted only to one IP address: <filter> <filter-name>Remote Address Filter</filter-name> <filter-class>org.apache.catalina.filters.RemoteAddrFilter</filter-class> <init-param> <param-name>allow</param-name> <param-value>127.0.01</param-value> </init-param> </filter> <filter-mapping> <filter-name>Remote Address Filter</filter-name> <url-pattern>/carbon/admin/login.jsp</url-pattern> </filter-mapping> Enabling HTTP Strict Transport Security (HSTS) Headers \u00b6 Enable \"HTTP Strict Transport Security headers\" (HSTS) for the applications deployed in your server, to confirm that the relevant headers are present in the HTTP response. HSTS is not enabled for applications in WSO2 products by default. Note Note that HSTS should not be enabled in development environments because transport security validations can interrupt the development processes by validating signatures of self-signed certificates. For the management console \u00b6 If the HttpHeaderSecurityFilter element is available in the web.xml file (stored in the < PRODUCT_HOME> /repository/conf/tomcat/carbon/WEB-INF/ directory) as shown below , it implies that security headers are by default configured for the management consoles of all of your profiles. However, in a production deployment, \u2018Strict-Transport-Security\u2019 needs to be explicitly enabled by replacing the default <init-param> values of the HttpHeaderSecurityFilter filter. Shown below is the default filter configuration. <!-- Tomcat http header security filter --> <filter> <filter-name>HttpHeaderSecurityFilter</filter-name> <filter-class>org.apache.catalina.filters.HttpHeaderSecurityFilter</filter-class> <init-param> <param-name>hstsEnabled</param-name> <param-value>false</param-value> </init-param> </filter> Shown below is how you should explicitly enable HSTS. <!-- Tomcat http header security filter --> <filter> <filter-name>HttpHeaderSecurityFilter</filter-name> <filter-class>org.apache.catalina.filters.HttpHeaderSecurityFilter</filter-class> <init-param> <param-name>hstsMaxAgeSeconds</param-name> <param-value>15768000</param-value> </init-param> </filter> For web applications \u00b6 Similar to the management console, check whether the HttpHeaderSecurityFilter (stored in the <PRODUCT_HOME> /repository/deployment/server/webapps/ directory) is available in the web.xml file of that particular web application. If the filter is available, enable HSTS as shown below. <filter> <filter-name>HttpHeaderSecurityFilter</filter-name> <filter-class>org.apache.catalina.filters.HttpHeaderSecurityFilter</filter-class> </filter> <filter-mapping> <filter-name>HttpHeaderSecurityFilter</filter-name> <url-pattern>*</url-pattern> </filter-mapping> For Jaggery applications \u00b6 For Jaggery applications, the HttpHeaderSecurityFilter element should be configured in the jaggery.conf file (stored in the <PRODUCT_HOME> /repository/deployment/server/jaggeryapps/ directory). This filter configuration is applicable to the /dashboard jaggery applications in this location. To enable H STS for a Jaggery application, change the default filter configuration as shown below. The default filter configuration: \"params\" : [{\"name\" : \"hstsEnabled\", \"value\" : \"false\"}] The filter configuration after enabling HSTS: \"params\" : [{\"name\" : \"hstsMaxAgeSeconds\", \"value\" : \"15768000\"}] Note NOTE: Returning HTTP security headers could also be achieved by configuring those headers from the Proxy/LB configuration. Preventing browser caching \u00b6 If there are dynamic pages in your application, which also include sensitive information, you need to prevent caching. This can be done by making sure that the applications return the following HTTP security headers in HTTP responses. Expires:0 Pragma:no-cache Cache-Control:no-store, no-cache, must-revalidate The following topics explain how you can configure these security headers for different types of applications used in WSO2 products. For the management console \u00b6 You can enable these headers for the management console by adding the following configuration to the web.xml file (stored in the <PRODUCT_HOME>/repository/conf/tomcat/carbon/WEB-INF/ directory ). <filter> <filter-name>URLBasedCachePreventionFilter</filter-name> <filter-class>org.wso2.carbon.ui.filters.cache.URLBasedCachePreventionFilter</filter-class> </filter> <filter-mapping> <filter-name>URLBasedCachePreventionFilter</filter-name> <url-pattern>*.jsp</url-pattern> </filter-mapping> For web applications \u00b6 If your web application (stored in the <PRODUCT_HOME>/repository/deployment/server/webapps/ directory) serves dynamic pages/content, then make sure that either URLBasedCachePreventionFilter or ContentTypeBasedCachePreventionFilter is available in the web.xml file of the particular application. Note that the applications that are included in the / webapps directory by default in a WSO2 product do not serve sensitive content that requires cache prevention. However, if you are adding any new applications, you need to be mindful of this requirement. For Jaggery applications \u00b6 For Jaggery-based applications (stored in the <PRODUCT_HOME>/repository/deployment/server/jaggeryapps/ directory), either URLBasedCachePreventionFilter or ContentTypeBasedCachePreventionFilter should be available in the jaggery.conf file as shown below. \"filters\": [{\"name\": \"ContentTypeBasedCachePreventionFilter\",\"class\": \"org.wso2.carbon.ui.filters.cache.ContentTypeBasedCachePreventionFilter\",\"params\": [{\"name\":\"patterns\",\"value\":\"text/html\\\",application/json\\\",plain/text\"},{\"name\" : \"filterAction\",\"value\":\"enforce\"}, {\"name\":\"httpHeaders\",\"value\": \"Cache-Control: no-store, no-cache, must-revalidate, private\"}] }],","title":"Securing Carbon Applications"},{"location":"setup/security/securing_carbon_applications/#securing-carbon-applications","text":"When you work with WSO2 products, you may host multiple applications (Web applications and Jaggery applications) and expose them to external users. The management console , which is shipped with every WSO2 product is also an application that is deployed in the Carbon server. Before you expose the applications to the external network, be sure to configure the security settings for all your applications. See the following topics for instructions: Enabling HTTPS access to the management console Enabling HTTP access to the management console Starting the server without the management console Enabling role-based permissions for the management console Restricting access to Carbon applications For the management console only For jaggery Apps only For all web applications For web application servlets Enabling HTTP Strict Transport Security (HSTS) Headers For the management console For web applications For Jaggery applications Preventing browser caching For the management console For web applications For Jaggery applications","title":"Securing Carbon Applications"},{"location":"setup/security/securing_carbon_applications/#enabling-https-access-to-the-management-console","text":"All WSO2 products expose the management console through the default HTTPS transport, which is configured in the catalina-server.xml file (stored in the <PRODUCT_HOME>/repository/conf/tomcat directory). This transport must be properly configured in this file for the management console to be accessible. See HTTPS Servlet Transport for instructions.","title":"Enabling HTTPS access to the management console"},{"location":"setup/security/securing_carbon_applications/#enabling-http-access-to-the-management-console","text":"If you are exposing the management console to a secure network, you may sometimes allow HTTP access to the management console. Note that HTTPS access is enabled by default. You can follow the steps given below to enable HTTP access to the management console: See HTTP Servlet Transport and make sure that the HTTP transport connector is configured for your product. Note that 9763 is the default port that will be used. Open the carbon. xml file stored in the <PRODUCT_HOME>/repository/conf directory and uncomment the following element: <EnableHTTPAdminConsole>true</EnableHTTPAdminConsole> Disable secure cookies for the management console. To do this, open the web.xml file from the <PRODUCT_HOME>/repository/conf/tomcat/carbon/WEB-INF directory and set the <secure> property to false . <session-config> <cookie-config> <secure>false</secure> </cookie-config> </session-config> You can now start the product server and access the management console through HTTP. Use the following URL: http://localhost:\\<port>/carbon/admin/login.jsp , where <port> corresponds to the HTTP port configured for the server. The default HTTP port for all WSO2 servers is 9763 . However, this may change if a port offset is applied to your server as explained here .","title":"Enabling HTTP access to the management console"},{"location":"setup/security/securing_carbon_applications/#starting-the-server-without-the-management-console","text":"If you want to provide access to the production environment without allowing any user group (including admin) to log into the management console, execute the following command: sh <PRODUCT_HOME>/bin/wso2server.sh -DworkerNode If you want to check any additional options available to be used with the startup commands, type -help after the command, such as: sh <PRODUCT_HOME>/bin/wso2server.sh -help.","title":"Starting the server without the management console"},{"location":"setup/security/securing_carbon_applications/#enabling-role-based-permissions-for-the-management-console","text":"You can grant management console access to selected users by configuring role-based permissions .","title":"Enabling role-based permissions for the management console"},{"location":"setup/security/securing_carbon_applications/#restricting-access-to-carbon-applications","text":"When hosting your products in production, it's imperative that you restrict the access to the management console from the external network. Additionally, you may also need to restrict access to other applications. Accessing Carbon applications in your server (including the management console) can be restricted to selected IP addresses. You can use the Tomcat servlet filter ( org.apache.catalina.filters.RemoteAddrFilter ) for the purpose of restricting access. Note that you can either restrict access to the management console exclusively, or you can restrict access to all web applications in your server (which includes the management console) at the same time.","title":"Restricting access to Carbon applications"},{"location":"setup/security/securing_carbon_applications/#for-the-management-console-only","text":"If you want only selected IP addresses to be able to access the management console, add the required IP addresses to the <PRODUCT_HOME>/repository/conf/tomcat/carbon/META-INF/context.xml file as follows: <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"<IP-address-01>|<IP-address-02>|<IP-address-03>\"/> The RemoteAddrValve Tomcat valve defined in this file will only apply to the Carbon management console and, thereby, all outside requests to the management console will be blocked.","title":"For the management console only"},{"location":"setup/security/securing_carbon_applications/#for-jaggery-apps-only","text":"If you want only selected IP addresses to be able to access the jaggery apps like publisher, store, admin portal which resides in WSO2 API Manager, go to \\<PRODUCT_HOME>/repository/deployment/server/jaggeryapps/\\<jaggeryapp-name>/jaggery.conf file and add the configuration as follows. { \"name\":\"Remote Address Filter\", \"class\":\"org.apache.catalina.filters.RemoteAddrFilter\", \"params\":[ {\"name\" : \"allow\", \"value\" : \"localhost|127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1|your IP added here\"} ] } { \"name\":\"Remote Address Filter\", \"url\":\"*\" } The value of the \"allow\" key that we defined in this file will apply to the publisher/store/admin portal as the the particular jagger.conf file you add this configuration and, thereby, all outside requests to the publisher/store/admin portal will be blocked.","title":"For jaggery Apps only"},{"location":"setup/security/securing_carbon_applications/#for-all-web-applications","text":"To control access to all web applications deployed in your server (including the management console), add the IP addresses to the <PRODUCT_HOME>/repository/conf/context. xml file as follows: <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"<IP-address-01>|<IP-address-02>|<IP-address-03>\"/> The RemoteAddrValve Tomcat valve defined in this file will apply to each web application hosted on the Carbon server. Therefore, all outside requests to any web application will be blocked.","title":"For all web applications"},{"location":"setup/security/securing_carbon_applications/#for-web-application-servlets","text":"You can also restrict access to particular servlets in a web application by adding a Remote Address Filter to the web.xml file (stored in the <PRODUCT_HOME>/repository/conf/tomcat/ directory), and by mapping that filter to the servlet URL. In the Remote Address Filter that you add, you can specify the IP addresses that should be allowed to access the servlet. The following example web.xml file illustrates how access to the login.jsp of the management console ( /carbon/admin/login.jsp ) is granted only to one IP address: <filter> <filter-name>Remote Address Filter</filter-name> <filter-class>org.apache.catalina.filters.RemoteAddrFilter</filter-class> <init-param> <param-name>allow</param-name> <param-value>127.0.01</param-value> </init-param> </filter> <filter-mapping> <filter-name>Remote Address Filter</filter-name> <url-pattern>/carbon/admin/login.jsp</url-pattern> </filter-mapping>","title":"For web application servlets"},{"location":"setup/security/securing_carbon_applications/#enabling-http-strict-transport-security-hsts-headers","text":"Enable \"HTTP Strict Transport Security headers\" (HSTS) for the applications deployed in your server, to confirm that the relevant headers are present in the HTTP response. HSTS is not enabled for applications in WSO2 products by default. Note Note that HSTS should not be enabled in development environments because transport security validations can interrupt the development processes by validating signatures of self-signed certificates.","title":"Enabling HTTP Strict Transport Security (HSTS) Headers"},{"location":"setup/security/securing_carbon_applications/#for-the-management-console","text":"If the HttpHeaderSecurityFilter element is available in the web.xml file (stored in the < PRODUCT_HOME> /repository/conf/tomcat/carbon/WEB-INF/ directory) as shown below , it implies that security headers are by default configured for the management consoles of all of your profiles. However, in a production deployment, \u2018Strict-Transport-Security\u2019 needs to be explicitly enabled by replacing the default <init-param> values of the HttpHeaderSecurityFilter filter. Shown below is the default filter configuration. <!-- Tomcat http header security filter --> <filter> <filter-name>HttpHeaderSecurityFilter</filter-name> <filter-class>org.apache.catalina.filters.HttpHeaderSecurityFilter</filter-class> <init-param> <param-name>hstsEnabled</param-name> <param-value>false</param-value> </init-param> </filter> Shown below is how you should explicitly enable HSTS. <!-- Tomcat http header security filter --> <filter> <filter-name>HttpHeaderSecurityFilter</filter-name> <filter-class>org.apache.catalina.filters.HttpHeaderSecurityFilter</filter-class> <init-param> <param-name>hstsMaxAgeSeconds</param-name> <param-value>15768000</param-value> </init-param> </filter>","title":"For the management console"},{"location":"setup/security/securing_carbon_applications/#for-web-applications","text":"Similar to the management console, check whether the HttpHeaderSecurityFilter (stored in the <PRODUCT_HOME> /repository/deployment/server/webapps/ directory) is available in the web.xml file of that particular web application. If the filter is available, enable HSTS as shown below. <filter> <filter-name>HttpHeaderSecurityFilter</filter-name> <filter-class>org.apache.catalina.filters.HttpHeaderSecurityFilter</filter-class> </filter> <filter-mapping> <filter-name>HttpHeaderSecurityFilter</filter-name> <url-pattern>*</url-pattern> </filter-mapping>","title":"For web applications"},{"location":"setup/security/securing_carbon_applications/#for-jaggery-applications","text":"For Jaggery applications, the HttpHeaderSecurityFilter element should be configured in the jaggery.conf file (stored in the <PRODUCT_HOME> /repository/deployment/server/jaggeryapps/ directory). This filter configuration is applicable to the /dashboard jaggery applications in this location. To enable H STS for a Jaggery application, change the default filter configuration as shown below. The default filter configuration: \"params\" : [{\"name\" : \"hstsEnabled\", \"value\" : \"false\"}] The filter configuration after enabling HSTS: \"params\" : [{\"name\" : \"hstsMaxAgeSeconds\", \"value\" : \"15768000\"}] Note NOTE: Returning HTTP security headers could also be achieved by configuring those headers from the Proxy/LB configuration.","title":"For Jaggery applications"},{"location":"setup/security/securing_carbon_applications/#preventing-browser-caching","text":"If there are dynamic pages in your application, which also include sensitive information, you need to prevent caching. This can be done by making sure that the applications return the following HTTP security headers in HTTP responses. Expires:0 Pragma:no-cache Cache-Control:no-store, no-cache, must-revalidate The following topics explain how you can configure these security headers for different types of applications used in WSO2 products.","title":"Preventing browser caching"},{"location":"setup/security/securing_carbon_applications/#for-the-management-console_1","text":"You can enable these headers for the management console by adding the following configuration to the web.xml file (stored in the <PRODUCT_HOME>/repository/conf/tomcat/carbon/WEB-INF/ directory ). <filter> <filter-name>URLBasedCachePreventionFilter</filter-name> <filter-class>org.wso2.carbon.ui.filters.cache.URLBasedCachePreventionFilter</filter-class> </filter> <filter-mapping> <filter-name>URLBasedCachePreventionFilter</filter-name> <url-pattern>*.jsp</url-pattern> </filter-mapping>","title":"For the management console"},{"location":"setup/security/securing_carbon_applications/#for-web-applications_1","text":"If your web application (stored in the <PRODUCT_HOME>/repository/deployment/server/webapps/ directory) serves dynamic pages/content, then make sure that either URLBasedCachePreventionFilter or ContentTypeBasedCachePreventionFilter is available in the web.xml file of the particular application. Note that the applications that are included in the / webapps directory by default in a WSO2 product do not serve sensitive content that requires cache prevention. However, if you are adding any new applications, you need to be mindful of this requirement.","title":"For web applications"},{"location":"setup/security/securing_carbon_applications/#for-jaggery-applications_1","text":"For Jaggery-based applications (stored in the <PRODUCT_HOME>/repository/deployment/server/jaggeryapps/ directory), either URLBasedCachePreventionFilter or ContentTypeBasedCachePreventionFilter should be available in the jaggery.conf file as shown below. \"filters\": [{\"name\": \"ContentTypeBasedCachePreventionFilter\",\"class\": \"org.wso2.carbon.ui.filters.cache.ContentTypeBasedCachePreventionFilter\",\"params\": [{\"name\":\"patterns\",\"value\":\"text/html\\\",application/json\\\",plain/text\"},{\"name\" : \"filterAction\",\"value\":\"enforce\"}, {\"name\":\"httpHeaders\",\"value\": \"Cache-Control: no-store, no-cache, must-revalidate, private\"}] }],","title":"For Jaggery applications"},{"location":"setup/security/single_key_encryption/","text":"Using Symmetric Encryption \u00b6 WSO2 Carbon-based products use asymmetric encryption by default as explained in the previous section. From Carbon 4.4.3 onwards, you have the option of switching to symmetric encryption in your WSO2 product. Using symmetric encryption means that a single key will be shared for encryption and decryption of information. To enable symmetric encryption, open the esb.toml file and add the following configurations: //The config sectin... [single_key_encryption] // This property is used to enable single key encryption IsEnabled = \"false\" // This property specifies the symmetric key algorithm. Algorithm = \"\" // This property is used to specify the secret alias if secure vault has been used to encrypt the secret key. securevaultalias = \"\" // This property is used to enable single key encryption symmetric.key = \"\" If Secure Vault has been used for encrypting the symmetric key, this value will be replaced by the secret alias as shown below. For detailed instructions on how the secret key can be encrypted using Secure Vault, see Encrypting Plain Text . symmetric.key=secretAlias:symmetric.key.value","title":"Using Symmetric Encryption"},{"location":"setup/security/single_key_encryption/#using-symmetric-encryption","text":"WSO2 Carbon-based products use asymmetric encryption by default as explained in the previous section. From Carbon 4.4.3 onwards, you have the option of switching to symmetric encryption in your WSO2 product. Using symmetric encryption means that a single key will be shared for encryption and decryption of information. To enable symmetric encryption, open the esb.toml file and add the following configurations: //The config sectin... [single_key_encryption] // This property is used to enable single key encryption IsEnabled = \"false\" // This property specifies the symmetric key algorithm. Algorithm = \"\" // This property is used to specify the secret alias if secure vault has been used to encrypt the secret key. securevaultalias = \"\" // This property is used to enable single key encryption symmetric.key = \"\" If Secure Vault has been used for encrypting the symmetric key, this value will be replaced by the secret alias as shown below. For detailed instructions on how the secret key can be encrypted using Secure Vault, see Encrypting Plain Text . symmetric.key=secretAlias:symmetric.key.value","title":"Using Symmetric Encryption"},{"location":"use-cases/guides/","text":"","title":"Invoking a RESTful Service"}]}